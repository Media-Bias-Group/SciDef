{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Similarly, Visual Question Answering (VQA) systems are known to learn heavy language priors (Agrawal et al., 2016). Thus, a lack of transparency can affect the decision-making process and may lead to the erosion of trust between humans and Artificial Intelligence (AI) systems. This can further jeopardize users' safety, ethics, and accountability if such a system is deployed publicly (Madsen et al., 2021). Considering the utilization of NLP in healthcare, finance, and law domains, all of which can directly affect human lives, it can be dangerous to blindly follow machine predictions without fully understanding them. For instance, a physician following a medical recommendation or an operation procedure for a patient without full knowledge of the system can do more harm than good. In addition, systems employing Machine Learning (ML), such as most current NLP methods, are prone to adversarial attacks where small, carefully crafted local perturbations can maximally alter model predictions, essentially misguiding the model to predict incorrectly but with high confidence (Finlayson et al., 2019). The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model's decision-making process of arriving at a prediction and the ability to be verified and audited. Therefore, models with explainability are interpretable and complete. In this survey, the focus is on explainability and mainly on the outcome explanation problem where Guidotti et al. (2018) describe explainability as \"the perspective of an end-user whose goal is to understand how a model arrives at its result\". In NLP, there exist various explainable techniques such as LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016), Integrated Gradients (Sundararajan et al., 2017), and ",
        "reasoning": "The section discusses the importance of transparency and explainability in Natural Language Processing (NLP) systems to maintain user trust and ensure ethical use. It highlights that opacity in these systems can lead to a lack of accountability and even harm in critical domains like healthcare, finance, and law. The text also explores the differences between interpretability and explainability, emphasizing that both are necessary for achieving trust in black-box models.",
        "extracted_terms": [
          {
            "term": "completeness",
            "definition": "The ability to accurately describe the operations of a system that allows humans to anticipate its behavior better.",
            "context": "X Neural networks require careful design to understand their behavior. They often lack the interpretability needed to trust their decisions, especially as they become more complex. This can lead to a decline in user confidence and ethical concerns.",
            "type": "explicit"
          },
          {
            "term": "explainability",
            "definition": "Providing information to understand how a model arrives at its result.",
            "context": "This discussion often requires careful handling and rigorous evaluation to understand its contributions. The ambiguity of the system's internal mechanisms and behavior reduces the likelihood of successfully explaining how the system arrived at its predictions.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Vstfjll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernndez-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Khberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswrd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "32e71a5d430dafa32b3ac8df7d2b5554b55c3f29",
        "sections": [
          "Over 70 percent of Americans believe that there is a great deal or a fair amount of media bias in news coverage (Pew, 2004). Evidence of bias ranges from the topic choice of the New York Times (Puglisi, 2004) to the choice of think-tanks that the media refer to (Groseclose  and Milyo, 2004). Media bias was even more prevalent in the late 19th and early 20th century (Gentzkow, Glaeser, Goldin, 2005). This evidence, however, leaves open the question of whether media bias matters. Does media bias affect the political beliefs of the audience? Does it change voting behavior? These are key questions for economics and political science that the media bias literature1 has yet to address. The answers to these questions also have implications for policy, such as for the regulation of media concentration. If media bias alters voting behavior, deregulation of media markets may have a large impact on political outcomes. In this paper, we present empirical evidence on the impact of media bias on voting. We consider one of the most significant changes in the US media in recent years, the entry and expansion of the Fox News cable channel. We exploit the natural experiment induced by the timing of the entry of Fox News in local cable markets and consider the impact on voting. We employ a difference-in-difference methodology and compare changes in the Republican vote share for towns where Fox News was introduced before the 2000 elections to towns where it was not present by 2000. We exploit three key elements of the Fox News natural experiment, the fast expansion, the geographical differentiation, and the conservative slant in its coverage. The 24-hour Fox News channel was introduced by Rupert Murdoch in October 1996 to compete with CNN. Like CNN, it was offered only via cable and, to a smaller extent, via satellite. Thanks to an aggressive marketing campaign, a number of cable companies added Fox News to their programming over the next four years. The geographical expansion was accompanied by a corresponding increase in the audience share. By June 2000, 17.3 percent of the US population reported watching Fox News regularly (Scarborough Research data). The nature of the cable industry induces substantial geographical variation in access to Fox News. Cable markets are natural monopolies with capacity constraints on the number of channels. The availability of Fox News in a town depends on whether the local cable company decides to add it to the programming, possibly at the expense of another channel. Cable companies in neighboring towns often make different decisions, creating idiosyncratic variation in access. This allows us to compare voting patterns in neighboring towns which are similar except for the availability of Fox News. Even given the sudden expansion and popularity of Fox News, and the variation in Fox News diffusion, it is unclear whether the addition of any single media source could have a significant impact on the political beliefs of voters. Fox News coverage, however, is unique among the television media. Fox News is significantly to the right of all the other mainstream television networks (ABC, CBS, CNN, and NBC) (Groseclose and Milyo, 2005). The introduction of Fox News into a cable market, therefore, is likely to have a systematic and significant effect on the available political information in that cable market. This is true whether Fox News represents the political center and the rest of the media the liberal wing, or Fox News represents the right and the rest of the media the middle. In order to analyze the impact of Fox News on voting, we assemble a new panel of townlevel data on federal elections and match it with town-level data on cable programming. The data set covers 28 US states. We compare the change in the Republican vote share between 1996 and 2000 for the towns that had adopted Fox News by 2000 with those that had not. The availability of Fox News in 2000 appears to be largely idiosyncratic. Conditional on a set of demographic, geographic, and cable controls, the Fox News and the non-Fox News towns have indistinguishable voting patterns in 1996 and indistinguishable pre-1996 voting trends. Our main result is that Fox News had a significant impact on the 2000 elections. The entry of Fox News increased the Republican vote share in presidential elections by 0.4 to 0.7 percentage points, depending on the specification. Since Fox News in 2000 was available in about 35 percent of households, the impact of Fox News on the two-party vote share in 2000 is estimated to be 0.15 to 0.2 percentage points, 200,000 votes nation-wide. While this vote shift is small compared to the 3.5 percentage point shift in our sample between 1996 and 2000, it is still likely to have been decisive in the close presidential 2000 elections. We consider alternative specifications and different estimation samples, leading to similar estimates. We implement placebo specifications; in particular, we show that availability of Fox News in 2000 did not affect the vote share between 1992 and 1996, when Fox News did not yet exist. We consider also how the estimates interact with town characteristics. Fox News had a smaller effect in rural areas, in Republican congressional districts, and in the South. Our interpretation of these results is that in these towns more people already voted Republican, and therefore the share of the population at risk of being convinced was smaller. In addition, the Fox News effect was smaller in towns with more cable channels, consistent with competition reducing the media effect (Mullainathan and Shleifer, 2005). We also analyze whether Fox News affected voting in races where Fox News did not cover the candidates directly, as in most Senate races. This allows us to estimate whether the influence of Fox News is candidate-specific or whether it extends to general political beliefs. We find that Fox News significantly increased the Republican vote share for Senate by 0.8 percentage points. Additionally, the effect is not larger for the one Senatorial race that Fox News covered heavily, the New York State race between Hillary Clinton and Rick Lazio. Fox News appears to have induced a generalized ideological shift. Finally, we consider whether the Fox News effect on Presidential elections was mainly a result of voters switching party lines, or of additional voter turnout to the polls. We find that Fox News significantly increased voter turnout, particularly in the more Democratic districts. The impact of Fox News on voting appears to be due, at least in part, to the mobilization of voters, and particularly conservative voters in Democratic-leaning districts. Overall, we find a positive and sizeable impact of Fox News on the vote share for Republicans. This empirical result, however, does not directly quantify the effectiveness of media bias in altering voting. To estimate the persuasion rate of Fox News, we incorporate information on the extent of viewership and the share of Republicans in the Fox News audience. Using Scarborough Research data, we compute the impact on the Fox News audience of availability of Fox News in local cable programming. These audience estimates imply that Fox News convinced between 3 and 8 percent of its non-Republican listeners to vote Republican. Alternative audience measures imply persuasion effects of up to 30 percent. Exposure to the conservative coverage of Fox News, therefore, had a sizeable persuasion effect. We compare the persuasion rates estimated in our study with the persuasion rates implied by other studies of media effects on political beliefs or voting. 2 First, we consider field experiments on voter turnout (Green and Gerber, 2004) and on party choice (Gerber, Karlan, and  Bergan, 2006). The most effective turnout methods-canvassing and phone calling-convince up to 25 percent of potential voters to turn out. Second, we consider laboratory experiments involving exposure to 30-second political advertisements (Ansolabehre and Iyengar, 1995). The advertisements alter voting intentions elicited at the end of the experiment for about 8 percent of the subjects. Third, we review poll studies. Kull et al. (2003) find that Fox News watchers were 50 percent more likely that viewers of other networks to believe (erroneously) that weapons of mass destruction were found in Iraq. Gentzkow and Shapiro (2004) find significant effects on beliefs about 9/11 of exposure to CNN vs. Al Jazeera in the Islamic world. These surveys imply that media exposure persuades 10 to 15 percent of the audience. While this overview is by no means exhaustive 3 , it shows that our estimates of persuasion rates are in the lower range of most estimates in the literature. We consider three explanations of our results. The first explanation is that the findings are spurious and are induced by entry of Fox News in towns that were independently becoming more conservative. Contrary to this explanation, these towns were no more conservative nor were they becoming more conservative before the entry of Fox News. A second explanation is based on rational learning. As the model in Section 2 shows, to the extent that voters are initially uncertain about the bias of Fox News, exposure will have a (temporary) effect on beliefs and voting. Voters attribute the positive coverage of Bush in 2000 partly to Republican bias of the media source (Fox News) but partly also to high quality of the Republican candidate (Bush). This prediction relies on the exposure to Fox News between 1996 and 2000 being too short to precisely estimate the political bias of the media source. This model also makes the prediction that the media effect should disappear over time. However, the Fox News effect appears to be permanent, if anything increasing between 2000 and 2004. A third explanation is that viewers do not discount enough for media bias and are subject to non-rational persuasion. In this case, exposure to media slant systematically alters beliefs and voting behavior. The latter interpretation relates to the behavioral literature on over-reliance on the advice of experts with conflict of interest. Cain, Loewenstein, and Moore (2005) show in an experiment that evaluators of information do not take sufficiently into account the incentives of the advisors, even when the incentives are known. Malmendier and Shanthikumar (2005) show that small investors follow the recommendations of affiliated analysts, despite the conflict of interest of the analysts. Our paper contributes to the evidence on shifts in voter turnout following media market expansion. George and Waldfogel (2004) show that, in areas where New York Times circulation expanded in the '90s, voter turnout in local election decreased among likely readers. Gentzkow (2006) finds a similarly negative effect of the expansion of television on voter turnout between 1940 and 1972. Stromberg (2004) finds that entry of the radio increases county-level turnout between 1920 and 1940. We differ from these studies in that (i) we examine the introduction of a politically-slanted media, and (ii) we consider the media effects at a finer geographical level, the town. Like these authors, we find some evidence that the arrival of a new information source impacts voter turnout. The remainder of the paper is structured as follows. In Section 2 we present a simple model of the effect of media bias on voting. In Section 3 we provide an overview of the cable industry and of Fox News. In Section 4 we introduce the data and in Section 5 we present the empirical results. In Section 6 we present estimates of persuasion rates and interpretations and in Section 7 we conclude.",
          "We model the impact of media bias on voting in the presence of a new media source. Voting depends on both ideological preferences and perceived politician quality. We define a news media outlet to be biased if it systematically slants its broadcast of politician quality to favor either the Republican or the Democratic candidate. We consider both voters who rationally update from news reports and voters who are subject to non-rational persuasion. Setup. In each period t, the media reports about a different pair of politicians, one Republican and one Democrat. These can be candidates running against each other in an election, such as Bush and Gore, or non-elected officials, such as Rove and McAuliffe. While the politi-cal affiliation of the politicians is known, the ability is unknown and can be guessed only using the media reports. The true differential ability of the period-t Republican politician relative to the period-t Democratic politician is  t , which is i.i.d. and distributed normally with mean 0 and precision   :  t  N (0, 1/  ) . The media observes the politician ability  t and broadcasts  t =  t + , where  is the time-invariant degree of media bias, drawn from the distribution N ( 0 , 1/  ). A media source with positive  provides a more favorable broadcast of the Republican politician, and the converse for a media source with negative . Since the focus of the model is on voter behavior and not on media strategy, we simplify matters by assuming a reduced-form 'news production function', instead of deriving an optimal bias policy from the utility maximization of the media. The voters learn about the bias  over time from the sequence of media reports  t . After observing the media reports on all the candidates from period 1 to period T, voters cast their ballots in period T for the candidates that the media covered in period T . The voting is based upon the estimated quality of the period-T candidates, as well as on ideological preference. A voter casts a ballot for the Republican candidate in period T if b  T +  > 0 where b  T is the voter estimate of the differential ability of the Republican politician in period T and  is the ideological preference for the Republican candidate ( can be positive or negative). Ideological preference  is heterogeneous, with a continuum of voters, c.d.f. F (), and p.d.f. f () > 0 for all . (Voters are instead homogeneous in their updating about  and ) Signal extraction. The voter faces a signal extraction problem. Upon observing a media signal  t , the voter makes inferences with respect to the politician quality  t and the degree of media bias . A positive signal  t could be due to an able Republican candidate, or to a right-wing media source. After observing T news reports with average report  T = P T t=1  t /T , given the normal distribution of the signals, the voters estimate the bias to be The estimated bias is a convex combination of the mean bias,  0 , and the observed average report  T , with weights given by the precision of the prior   and the precision of the observed signal T   . The more signals (higher T ) the voter observes, the more weight the voter puts on the news reports. Using this estimate of media bias, rational voters estimate the differential quality of the period-T Republican candidate T . They subtract the estimated bias T from the report  T , and combine it in a precision-weighted average with the prior about  T , that is, zero (Republican and Democratic politicians are on average of the same quality). Hence where W, the precision of  T -T , equals ( So far, we have considered fully rational updating by voters. We now consider a simple reduced-form model of voters subject to persuasion. As in DeMarzo et. al. (2003), we interpret persuasion as incomplete filtering of bias. Voters subject to -persuasion believe that the report  t equals  t + (1) , even though in reality the report  T equals  t + . The parameter   [0, 1] measures the degree of persuasion, with  = 0 nesting the standard model, and  = 1 indicating extreme persuasion. These voters systematically underweight the importance of media bias in influencing news reports. This is consistent with the experimental findings in Cain et al. (2005) in which subjects did not take the bias of the information-provider sufficiently into account, even though the bias was common-knowledge. For simplicity, we assume that the degree of persuasion  affects neither the estimate of media bias T nor the precision W of  T -(1) T ; therefore,  and W are not indexed by . 5 When estimating T , voters subject to -persuasion subtract only (1) T from  T to form the estimate  T : In Proposition 1, we summarize the effect of media bias  on the estimated ability of the Republican politician  T , conditional on the realized politician quality  t , t = 1, ..., T . We consider the case of voters subject to -persuasion, since the standard case is embedded for  = 0. Proposition 1. For any finite T, (i) an increase in media bias  increases the estimated quality of the Republican politician, b   T : ( b   T )/ > 0, and (ii) this increase is higher the more voters are subject to -persuasion:  2 ( b   T )/ > 0. (iii) In the limit as T  , the effect of media bias is positive in the case of persuasion ( > 0) and zero otherwise: The intuition for Proposition 1 is straightforward. There are two effects of media bias  on expected politician quality  T . First, a more right-wing media (higher ) issues a more positive signal  T on the T -th Republican politician, which leads to a higher perceived  T . Second, a higher  is associated with a higher average of past signals  T , and therefore to a higher perceived media bias T , leading to a lower perceived  T . For finite T, the first, direct effect dominates the second, indirect effect, and hence media bias has an impact on beliefs: . Moreover, since the degree of persuasion, , decreases the second effect, media bias has a larger impact on beliefs for higher  (Proposition 1.(ii)). As T  , the estimated media bias T converges to the true bias . Since eventually the voters become fully aware of the degree of bias, the media bias has no effect on the expected perceived quality as long as voters are rational ( = 0). The same would be true if the bias  were known from the start. Voters that suffer from persuasion ( > 0), instead, are affected by media bias  even after an infinite number of media reports (Proposition 1.(iii)). Finally, we analyze the impact of media bias on voting, which takes place in period T . A voter subject to -persuasion casts a ballot for the period-T Republican candidate if  + b   T > 0. Then, the share of votes for the Republican candidate is , an increasing function of the estimated (differential) ability b   T of the Republican candidate. Proposition 2 summarizes the effect of media bias  on the vote share 1 -F (-b   T ). Proposition 2. (i) For any finite T, an increase in media bias  increases the Republican vote share In the limit as T  , the effect of media bias is positive in the case of persuasion ( > 0) and is zero otherwise. The impact of media bias on voting follows in a straightforward manner from its impact on beliefs. A new media source can temporarily move voters in the direction of its bias even if voters are rational (Proposition 2.(i)); in the long run, however, only voters subject to persuasion remain affected (Proposition 2.(ii)). Proposition 1.(ii) does not generalize to voting without restrictions on the shape of the distribution of voters F . While a higher  increases the impact of  on the estimated politician quality b   T , it does not necessarily increase the impact on the vote share 1 -F (-b   T ) if the elasticity of voting with respect to beliefs, captured in f, is lower for individuals with higher . The empirical predictions of the model depend on the length of the experience of the audience with Fox News by 2000. Since Fox News was started in 1996, by the year 2000 its audience had the chance to experience its coverage of several politicians, from Clinton and Dole, to Ted Kennedy and John McCain, to Gore and Bush. The viewers were also exposed to Fox News' reporting on non-elected political figures such as Ken Starr or Jesse Jackson. To the extent that each pair of politicians is an observation t, therefore, one reasonable assumption is that by the 2000 elections the audience had a precise estimate of the Fox News bias (case T  ). In this case, only voters subject to persuasion should be affected by the exposure to Fox News. Fox News should not impact rational voters. A different interpretation holds that, instead, four years are too short a period for the audience to form a clear opinion of the Fox News bias, especially since some of the audience listened to it for only a year or two before 2000. In this case, Fox News should affect voting on Bush and Gore also for rational voters. The impact should diminish over time, between 2000 and the subsequent elections, and should be stronger if voters suffer from persuasion.",
          "Cable industry. The cable industry is a local natural monopoly. Once one company has paid the fixed cost to lay the cables in a town, it is uncommon for a second company to pay the fixed cost as well and enter the local market. In our sample, only ten percent of towns have two competing cable companies, and only one percent have three or more companies. A second important feature of the cable industry is the technological constraint on the number of channels. Established channels like CNN are offered in almost all towns. New channels like Fox News have to convince local cable companies to be added, often at the expense of other channels. This generates substantial variation across towns in the programming provided. Local cable service providers (henceforth local cable companies) pay a monthly fee, typically between 10 cents and 40 cents per user, to the networks that they carry. They also pay fees to towns that grant them the right to broadcast. These fees are typically set as part of a 10-to 15-year contract with the town. Finally, local cable companies get their revenue from their monthly subscriber fees. The amount of the subscriber fee is partly regulated and varies between $10 and $60, depending on the company, the location, and on the tier of service. Fox News history. In March of 1996, Rupert Murdoch, CEO of News Corp., announced the introduction of a 24-hour-a-day cable news channel. The new channel, Fox News Channel (\"Fox News\" from here on), was created to compete with CNN. Prior to the launch of Fox News, news broadcasts took up a small share of programming of the Fox Broadcasting Corporation, which included channels like Fox Sports, Fox Entertainment, and Fox Family Channel. There was no broadcast news at a national level, and prime time programming on the various Fox channels did not include news. The main television sources for news before 1996 were the three major networks-ABC, CBS, and NBC-, in addition to CNN, distributed via cable. The distribution of Fox News started on October 7, 1996 in a limited number of cable markets. To facilitate the spread of the new channel, the Fox Network offered a one-time payment of $10 per subscriber to cable companies that included Fox News in their programming. TCI was one of the first companies to sign a contract with Fox News. After the initial contract was signed, the local affiliates decided whether to include Fox News among the channels transmitted. The timing of the agreement is one factor inducing idiosyncratic diffusion of Fox News. By November 2000, AT&T Broadband, which acquired TCI Cable in 1999, offered Fox News in 32.5 percent of the 1,955 towns served by its affiliates (in our sample of 28 US States). Adelphia Communications, which had a late agreement with Fox News, offered Fox News in only 7.5 percent of the 1,592 towns in our sample served by its affiliates. In addition to 24-hour cable programming, Fox News distributes short news segments to local TV stations that are affiliates of Fox Broadcasting. However, the complete programming of Fox News is only available via cable and to 12 million satellite subscribers (in 2000). 6 By the year 2000, Fox News was present in 20 percent of towns in our sample with cable service. Since the towns reached by Fox News in 2000 were more than twice as large as the remaining towns, Fox News was available to 34.3 percent of the population of these states. Fox News content and programming. A key feature of Fox News is the differentiation in its political coverage relative to CNN and the network news stations. Groseclose and Milyo  (2005) use data on citations of think-tanks between 1997 and 2003 to rank the political orientation of news programs from different media sources. In particular, they impute an ADA score for the news program based on the ADA score of the members of Congress that refer to the same think-tanks. 7 Their estimation results assign an imputed ADA score for Fox News Special Report of 39.7. This score is significantly lower than the score for any of the other mainstream television media (ABC, CBS, CNN, and NBC), with scores between 56.0 (CNN NewsNight) and 73.7 (CBS Evening News). The news coverage of Fox News is estimated to be significantly more conservative than all other TV news channels. Moreover, Fox News coverage is estimated to be to the right of the average US elected official, which has a score of 50.1. This study may, if anything, underestimate the political differentiation of Fox News. The two Fox News shows with the highest ratings, The O'Reilly Factor and Hannity & Colmes, are likely more conservative than the Special Report, which Groseclose and Milyo use to estimate the ADA citations. Both shows aired since the beginning of Fox News. Fox News Audience. We document the Fox News penetration and the political and demographic composition of its audience using micro-level data from Scarborough Research. Scarborough collects demographic variables and two measures of audience from a representative panel of households 8 . The 'regular audience' of a given TV channel is the share of respondents who answer yes to a question on whether or not they listened regularly to the channel in the three months prior to the survey. The 'diary audience' measure is tabulated from a week-long diary of TV watching in half-an-hour intervals. The diary audience for a channel is the share of respondents who watched it for at least one full half-an-hour block in the survey week. In Column 1 of Table 1 we report summary statistics for the 105,201 respondents to the August 2000-March 2001 survey. The regular audience for Fox News is 17.3 percent, and 34.1 percent for CNN. According to this measure, by the year 2000, Fox News already had an audience half as large as that of CNN. The diary audience is not available for this sample. In Table 1, we also present other summary statistics for the overall sample (Column 1), for the regular Fox News audience (Column 2), and for the rest of the sample (Column 3). The regular Fox News audience is significantly more likely to also watch CNN regularly, probably reflecting a taste for television news. The education level and unemployment rate are compa- 7 The ADA score is a measure of political orientation created by Americans for Democratic Action. 8 In an earlier version of the paper we used a June 2000 Pew Survey to estimate the determinants of the Fox News audience, with different results: in that sample, the Fox News audience does not appear to be selected on political variables, while it is heavily selected on low education. The Scarborough data has several advantages over the Pew Survey, including a substantially larger sample, zip-code identifiers for part of the sample, and a more precise measure of audience. We believe that at least part of the the differences between Scarborough and Pew data is due to confusion between the Fox News Channel and the Fox affiliates in the Pew survey. rable across the two samples, African Americans are somewhat more likely to listen regularly to Fox News, and Hispanics somewhat less likely. The Fox News audience is older (49.7 vs. 44.8 years) and more likely to be male (48.1 vs. 41.2 percent). Turning to the political variables, 37.5 percent of the regular Fox News audience self-identify as Republican, 29.4 percent as Democrat, and the remainder as Independent. Among the non-Fox News audience, 26.2 percent identify as Republican and 32.4 percent as Democrat. Fox News listeners therefore are more likely to be Republican. Since the audience data is from 2000, after the entry of Fox News, this difference could be due either to sorting of Republicans into the Fox News audience, or to a persuasion effect of exposure to Fox News. Self-reported turnout also differs: 76.9 percent of the Fox News audience state that they always vote in Presidential elections, compared to 67.7 in the complementary group. As for the media variables, 68.7 percent of the whole sample report having cable, while only 14.7 percent report having access to satellite television. The share with cable is higher for the regular Fox News audience, while the share with satellite does not vary. In Columns 4-6 we focus on the subsample for which ZIP code of residence and the diary audience measure are both available. This sample was recorded between February 2000 and August 2001 in five geographical areas 9 . We further restrict the sample to the 11,388 respondents living in one of the 568 towns with available cable and election data. In Section 6.1, we use this sub-sample to estimate the impact of Fox News availability via cable on the likelihood of being a Fox News viewer. In this sample (Column 4), the measure of the regular Fox News audience (16.6) is slightly lower than in the sample as a whole (Column 1); the demographics are comparable across the two samples, with the exception of a higher share of Hispanic viewers. The diary audience is 3 to 5 times smaller than the corresponding regular audience: The Fox News diary audience measure is .035. compared to a regular audience measure of .166. Similarly, the CNN measure of diary audience is .103, compared to a regular audience of .353. The diary audience is likely to understate the real audience, since it excludes anyone who watched less television than usual during the survey week, anyone who watched a channel repeatedly but either never watched a full half hour block, or anyone who watched it but failed to report it. The regular audience measure is immune to these issues, since it covers a longer time period, the quarter, but it has the disadvantage of subjectivity. In Columns 5 and 6 we compare the Fox News audience and the non-Fox News audience according to the diary measure. The differences between these two samples resemble the ones found according to the regular audience measures (Columns 2 and 3), except that political differences are more accentuated and that African Americans are less likely to watch Fox News according to the diary measure.",
          "Cable data. We obtained the cable data from a paper copy of the Television and Cable Factbook, 2001 edition (Warren, 2001). This edition contains information on all local cable companies as of November 2000, that is, right up to the 2000 elections. We did not collect information for the year 1996, since Fox News became available only in October 1996 and just for a limited number of markets. Each local cable company serves up to 100 communities in the neighboring region, all listed in the entry. All companies offer a basic service package, and over a half also offer up to three expanded basic service packages. The basic service programming typically includes channels such as C-Span and the ABC Family Channel. In addition, these companies are required by law to broadcast local television stations as part of their basic service. Expanded basic service, when available, includes channels such as Comedy Central and Fox Sports for an additional fee. Finally, cable companies sometimes offer  la carte channels such as HBO for a fee. In Appendix A we present further details on the data. For the 28 states for which we were able to obtain town-level election data (Appendix Table 1), we collected, for each local cable company, data on the communities served, as well as an estimate of the total number of channels (excluding the local TV channels) offered. We also recorded whether CNN and Fox News were included and, if so, as part of which contract (Basic or Expanded Basic). Overall, this data set covers 17,333 communities in 28 States. (Column 1 in Appendix Table 1). The states with the largest number of communities are California (1,110), Michigan (1,373), New York (1,465), Ohio (1,873), and especially Pennsylvania (2,667). Election data. The main sources of election data are the Federal Election Project (Lublin  and Voss, 2001) for the year 2000, the Record of American Democracy (ROAD) Project (King  et al., 1997) for the year 1988, and the Atlas Election data (Leip, 2004) for the 2004 Presidential election. For most other years we obtained the information from the Election Division of the Secretary of State of each state. Since the unit of observation for the cable data is the town, we have constructed a data set of voting information at the town level. The data collection differs depending on the state. A first group of states-California, New Jersey, New York, and the New England States-directly provide voting information at the town level, which we employ. A second group of states-Iowa, Minnesota, Michigan, Montana, Ohio, Pennsylvania, Wisconsin, and Wyoming-provide precinct-level voting information with the town name explicitly specified for each precinct; in this case, we aggregate the voting information to the town level. Finally, a third group of states-Alaska, Alabama, Arkansas, Hawaii, Idaho, Missouri, North Dakota, South Carolina, Tennessee, Utah, and Virginia-only have precinct-level voting information, with precinct names that usually include the name of the town, but sometimes do not. Examples of precinct names are \"02 -Concord Elem School\" and \"Hot Springs Retirement Hm\". For these states, we recover the town name from the precinct name by elimination of numbers and commonly used words, such as \"School\" and \"Retirement Hm\" 10 . We then aggregate the voting data over precincts with the same town name in a given county and state. 11 The twenty-eight US states that have voting information available for both years 1996 and 2000 form the sample used in this paper. The remaining states either do not have electronic voting information available at levels of aggregation below the county level, or have numeric precincts with no precinct-to-town conversion available from the state. The aggregation procedure generates 26,710 distinct localities (Columns 3 and 5 in Appendix Table 1). States like Alabama, Arkansas, and Tennessee have a very large number of localities because precinct names are often incorrectly identified as a locality by our code. (These fictitious towns drop out from the final sample, since they do not match to Census and election data.) Conversely, the New England states have a small number of exactly-identified towns because the election data is reported at the town level. Census data. For the 28 US states in our sample, we collect information on town-level demographics from the 1990 Census and the 2000 Census. More precisely, we use data at the level of \"Place\", including \"Remainders of Place\". In order to match the Census data with the election and cable data, we transform the place name and aggregate the Census data using the same code employed for the election and cable data. This procedure leaves 27,064 places with information from both the 2000 and the 1990 Census. The average population for these places (Column 7 in Appendix Table 1) varies from 719 in Alaska to 24,070 in California. Matched data. We match the cable, the election, and the Census data by state, county, and place name. The match between the cable data and the election data reduces the sample to 10,479 localities. We then drop 353 localities that fail to match to the Census data. We also drop 289 towns with multiple cable systems, at least one of which carries Fox News and at least one of which does not. For these towns, we do not know if cable consumers have access to Fox News. Additionally, we drop 324 towns with cable systems that do not offer CNN as part of the cable package. In these towns, cable offerings are typically limited to the re-programming of local cable channels. Their news programming, therefore, is not comparable to the programming of the other towns. 12 Finally, we drop 257 towns with likely voting data problems: 238 towns for which the number of precincts generating the town-level vote count differs by more than 20 percent between 1996 and 2000 13 ; and 19 towns for which the total number of votes cast in the Presidential election differs by more than 100 percent between 1996 and 2000. For these observations, the problems are likely due to imperfect matching of the precincts aggregated to the town level in 1996 and 2000. The final sample includes 9,256 towns. The states with the largest number of observations 10 The Stata ado file that translates precinct names into town names is available upon request. 11 We drop precincts such as \"Precinct 1\" where the transformation algorythm leads to an empty name. 12 The results do not vary if we include these towns. 13 We do not apply this criterion for Michigan, New Hampshire, New York, Ohio, Rhode Island, and Utah, since the numbering of precincts is not comparable across 1996 and 2000. (Column 2 of Appendix Table 1) are Pennsylvania (1,716), Michigan (930), and Ohio (904), three of the largest States. The Census regions with the greatest number of towns are the Northeast (4,030 towns) and the Midwest (3,812 towns), followed by the South (725 towns) and the West (689 towns). Figure 1 shows the distribution of observations by county. In the majority of the 1,166 counties in the sample, no town offers Fox News. This is not surprising given that Fox News is available in only 20 percent of towns and many counties have few towns. Very few counties have Fox News available in all towns. The remaining 290 counties, in which a fraction of the towns offers Fox News, are spread widely over the 28 states. The towns in the final sample cover 65.9 percent of the population of the 28 States in the year 2000 (Column 11 of Appendix Table 1). The corresponding figure for the voting data is 68.6 percent for the year 2000 (Column 9) and 70.4 percent for the year 1996 (Column 10). The coverage rate is lower than 100 percent for three reasons: (i) we drop from the sample some of the largest cities like New York which have several cable systems, only some of which carry Fox News, and some of which do not, (ii) in states like Missouri, Utah, and Virginia, some of the counties have numeric precinct names that we cannot match to a town, and (iii) in states like Arkansas and Alabama, complicated precinct names induce a poor match between the election data and the cable and Census data. The exclusion of large cities or certain counties and the presence of States with a large non-matching rate should not affect the results, as long as Fox News availability and the election outcomes are measured correctly for the matching towns. The final sample has comparable Fox News availability relative to the initial sample (Columns 1 and 2), and somewhat lower Republican vote share in 2000 and 1996 (Columns 3 through 6) because the unmatched towns are more likely to be small and rural. Except for average population, which is substantially higher in the final sample (Columns 7 and 8), the towns in the final sample are fairly comparable to the towns in the initial sample.",
          "k,2000 equals one if all cable systems in town k in year 2000 include Fox News in either the Basic package or one of the Expanded Basic packages, and zero if no cable system includes Fox News. According to this measure, 20 percent of towns offer Fox News (Columns 1 and 2 of Appendix Table 1). In most states, the percentage varies between 10 and 30 percent, indicating substantial within-state variation in Fox News availability. Appendix Table 1 also presents information on the voting data. For each town k in year t we denote the number of votes cast for the Republican candidate in race j (j = Pres., Sen.) as V R,j k,t . Similarly, we denote the votes cast for the Democratic candidate as V D,j k,t and the votes cast for other parties as V O,j k,t . We define the two-party republican vote share ). As a robustness check, we also use the all-party Republican vote share ) and the right-wing vote share v 00R,j ), where Ref. denotes the Reform party and Green denotes the Green Party. These variables are set to missing if the number of votes for either the Republican or the Democratic candidate is zero, usually reflecting an incumbent running unchallenged. Over the final sample, the average two-party Republican vote share is .470 in the year 1996 (Column 6) and .538 in the year 2000 (Column 4). These unweighted averages are higher than the population-weighted average. Our main measure of turnout t j k,t in town k for year t and race j is the log of the total votes cast: t j k,t = ln(V T OT,j k,t ). The change in t j k,t over time is the percent change in total votes. In the turnout specifications, we also control for the log of voting-age population ln (P op k,t ) . As an alternative measure, we use the the log of votes cast as a share of voting age population, t j0 k,t = ln(V T OT,j k,t /P op k,t ). 5 Empirical results",
          "Column 1 of Table 2 presents unweighted summary statistics on the 9,256 towns in our sample. In the year 2000, the average cable system included over 28 channels in the Basic and Expanded Basic programming. The mean total population reached by a cable system was 78,124, with a median of 23,921. The mean town population was 9,612 people, with a median of 2,766. As Census controls, we include in our regressions the share of the population with some college, the share of college graduates, the share of African Americans and of Hispanics, the unemployment rate, and the share of the town that is urban (shown in Table 1). In addition, we include the share of high school graduates, the share of males, the marriage rate, the employment and the unemployment rate, average income, and the share of the population living in an urban area (not shown in Table 1). These controls are present both in their 2000 value, and in changes between 1990 and 2000. Finally, the political variables are the two-party vote share for Republicans in Presidential elections (1996 and 2000) and the total turnout in Presidential elections measured as log share of the voting-age population, t j0 k,t (1996 and 2000). Columns 2 and 3 compare towns that offered Fox News in their programming (Column 2) and towns that did not (Column 3). Towns that offer Fox News have a substantially higher number of channels offered (44.5 versus 24.7), are 25 percent larger, are served by cable companies that reach three times as many people, and are more likely to be urban. More importantly, towns that offered Fox News by 2000 increased their Republican vote share by 5.9 percentage points (from 47.9 percent to 53.8 percent) between 1996 and 2000, while those that did not offer Fox News increased theirs by an even larger 7.1 percentage points (from 46.7 percent to 53.8 percent). These figures suggest a perverse Fox News effect. This result, however, does not weight towns by size, nor does it take into account differences between Fox and non-Fox towns in voting trends across geographical areas, demographic composition, and cable market. Below, we estimate the Fox News effect taking into account all these factors. The overall sample spans 235 congressional districts, out of 435 total 14 . Out of these 235 14 For the towns that are in multiple districts, we code the town as belonging to the district where the largest districts, 152 districts include both towns that offered Fox News and towns that did not. In our difference-in-difference specification with district fixed effects, the effect of Fox News is estimated on these districts. The median district in this subsample of 7,631 towns offers Fox News in 20.4 percent of towns (Columns 4 and 5 of Table 2). Towns in this subsample are smaller but otherwise comparable to the overall sample. We also consider the distribution of Fox News at the finer geographical level of the county. Only 284 counties out of 1,156 incorporate both towns with Fox News and towns without. These counties are more concentrated in the Northeast and less in the South (Figure 1). In our specification with county fixed effects, the effect of Fox News is estimated on the 3,890 towns in these counties. The median county in this subsample offers Fox News in 40.4 percent of towns. Towns with Fox News (Column 6 of Table 2) and without Fox News (Column 7) in this subsample are close geographical neighbors and therefore more closely matched on observables, such as demographic and voting characteristics.",
          "The identification in this paper relies on comparing towns with Fox News in their programming in the year 2000 to towns without Fox News. Since the assignment of towns into these two groups is not random, we investigate the nature of the selection. Fox News may well have expanded first in more Republican areas, since demand for its services is likely to be higher in these areas. If Republican areas were becoming more Republican between 1996 and 2000, the estimated Fox News effect may just be capturing political trends. To investigate this and other forms of selection, we estimate which town-level variables predict the availability of Fox News in 2000. In particular, we include the Republican vote share in presidential elections in 1996 (that is, pre-Fox News), v R,Pres k,1996 . We estimate a linear probability model 15 : where X k,2000 is the set of controls from the 2000 Census, and X k,2000-1990 is the set of changes in controls between the 1990 Census and the 2000 Census (see Table 2). In addition to the demographic variables, we control for features of the cable system by including C k,2000 , deciles in the number of channels provided and in the number of potential subscribers. Since the precision of the vote share variable v R,Pres k,1996 is increasing in the total number of votes cast, we weight the observations by the average of the total votes cast in 1996 and in 2000 16 . The fraction of the town's population is represented. 15 The results are similar with logit and conditional logit specifications. 16 The results are essentially identical if we weight by votes cast in 1996, or by votes cast in 2000. Weighting by the average reduces the potential effect of measurement error in the total votes cast. standard errors are clustered at the level of the local cable company. There are 2,992 local cable companies in the sample, giving an average of 3.09 observations per cluster. We first estimate (4) with demographic controls and without cable controls ( C = 0) (Column 1 of Table 3). In this specification, Fox News availability in 2000 is substantially higher in more Republican towns: a 10 percentage point increase in Republican vote share is associated with a 6.5 percentage point increase in the likelihood of Fox News availability. Urban towns and more highly educated towns are more likely to have Fox News by the year 2000. While these demographic variables are significant, they predict a relatively small share of the variance, with an R 2 of .0772. In Column 2, we add the non-parametric controls C k,2000 for potential subscribers in the cable system and for number of channels. The additional variables raise the R 2 to .4032. Larger cable systems are much more likely to offer Fox News. The introduction of the cable controls lowers the coefficient  on the Republican vote share by half. In Column 3, we add congressional district fixed effects to control for some of the geographic heterogeneity in Fox News penetration. With these additional geographic controls, specification (4) captures the determinants of within-district Fox News availability. In this specification, there is no evidence that towns with higher Republican vote share are more likely to offer Fox News: in fact, the estimated  = -.0324 is negative, albeit insignificant. Given the precision of the estimates, we can reject substantial effects of pre-existing political composition on the availability of Fox News. Moreover, none of 14 demographic controls is significant at the 5% level. This suggests that, once we control for geographic heterogeneity and size of the cable system, availability of Fox News in 2000 is idiosyncratic. We obtain similar results when we introduce county fixed effects instead of congressional district fixed effects (Column 4). In Column 5, we replicate the specification in Column 3 with the addition of t Pres0 k,1996 , voter turnout in the 1996 Presidential election measured by log of votes cast as share of population. As with the case of the Republican vote share, voter turnout in 1996 is uncorrelated with the availability of Fox News in 2000. Finally, in Columns 6 and 7 we test whether voting trends predict the availability of Fox News. We add the change in the Republican vote share in Presidential elections between 1988 and 1992 to the specifications in Columns 3 and 4. Since town-level data for 1992 is hard to find, this reduces the sample to 3,722 towns. The time trend variable is not significant and switches sign between the two specifications. Overall, the strongest determinant of the availability of Fox News in 2000 is the presence of a cable system with a large number of channels. Once we control for cable size and for geographical heterogeneity, the presence of Fox News in 2000 appears to be idiosyncratic. Fox News availability is not systematically related to demographics at the town level. Further, it is orthogonal to both the Republican vote share and voter turnout in the 1996 Presidential elections. Finally, it is also unrelated to the time trends in voting between 1988 and 1992.",
          "The baseline empirical specification is a standard difference-in-difference estimation. We compare towns where Fox News entered the cable market by the year 2000 with towns where Fox News was not available by the year 2000. We consider the impact of the entry of Fox News on the change in the Republican vote share between 1996 and 2000. This strategy exploits the timing of the entry of Fox News. By the November 1996 elections, Fox News had been launched in only a few markets, and, even in those markets, just one month before the elections. By the November 2000 elections, Fox News had an audience that was smaller, but nonetheless comparable to that of CNN. Our baseline specification is As in We first implement a simple difference-in-difference estimator and estimate (5) without controls ( 2000 =  00-90 = 0 and  C = 0) (Column 1 in Table 4). On average, in our sample the vote share for Republicans increased by 3.47 percentage points ( = .0347) between the 1996 and the 2000 elections. Compared to this overall increase, towns with Fox News became (insignificantly) less Republican by two tenths of a percentage point ( F = -.0026) relative to towns without Fox News. The standard error (.0037), however, is sufficiently large that we cannot rule out that the entry of Fox News increased the Republican vote share by half a percentage point. In Column 2, we add demographic controls, raising the R 2 of the regression from .0008 to .5199. The estimate for  F , F = .0027, becomes positive, but is still insignificant. The change in vote share for Republicans is more positive in towns with fewer college graduates, fewer blacks and Hispanics, and higher unemployment in 2000. The change in vote share is also positively correlated with decreases in the number of blacks and decreases in the number of unemployed workers between 1990 and 2000. In Column 3, we add controls for cable size C k,2000 , rendering the Fox News coefficient positive and significant ( F = .0078). Controlling for population, a higher number of potential cable subscribers is negatively correlated with the Republican vote share. Introducing control variables increases the point estimate of  F , suggesting that the unobservables bias the estimate of the Fox News effect downward. In the two benchmark specifications we include district fixed effects (Column 4) and county fixed effects (Column 5) in addition to the full set of controls. In these specifications the identification of  F depends on the comparison of neighboring towns with and without Fox News. These specifications control for unobserved trends in voting that are common to a geographic area, and that may be correlated with Fox News availability. In Column 4, the comparison of neighboring towns takes place within a congressional district, while in Column 5 it takes place within the finer geographical unit of the county. The key advantage of specifications with cable, demographic, and geographic controls is that, conditional on these variables, the availability of Fox News is idiosyncratic (Table 3). The estimate of the effect of Fox News is positive and significant in both cases, .0040 and .0069 respectively. In both specifications, the standard errors on the coefficient  F are halved compared to Column 1, indicating that the additional controls increase substantially the precision of the estimates. In the specifications that best control for heterogeneity, availability of Fox News increases the Republican vote share by 4 to 7 tenths of a percentage point, a sizeable and precisely estimated effect. In Columns 6 and 7, we replicate the results of Columns 4 and 5 after adding the change in Republican vote share between 1988 and 1992 as an additional control. Over this substantially smaller sample (3,722 observations), the effect of Fox News availability is less precisely estimated and somewhat smaller, but still significant in the specification with county fixed effects. The coefficient on 1988-1992 voting trends is close to zero with district fixed effects and significant but small with county fixed effects. Since previous voting trends are not substantial predictors of current voting trends, and since including them would lower the sample size substantially, we omit them in the remaining regressions. Robustness. In Table 5, we examine the robustness of these results to a number of alternative assumptions. Throughout, we use the full set of controls as well as fixed effects for congressional districts, as in Column 4 of Table 4 17 . First, we test for robustness to functional form assumptions. In Column 1, we regress the vote share in 2000, v R,Pres k,2000 , on the vote share in 1996, v R,Pres k,1996 , instead of taking their difference. The coefficient on the 1996 vote share is .9359, close enough to 1 to support the use of the difference v R,Pres k,2000v R,Pres k,1996 in the benchmark specifications 18 . In Column 2, we allow for a non-linear relationship between the 1996 and 2000 vote shares, and regress v R,Pres k,2000 on a quartic polynomial of v R,Pres k,1996 . In Column 3, we use as an alternative dependent variable the change in the log of the vote share: log(v R,Pres k,2000 )-log(v R,Pres k,1996 ). In all three specifications, the estimate for F is positive and significant and very similar to the benchmark estimate with district fixed effects. In particular, the estimate of a .85 percent effect (  = .0085) in Column 3 is consistent with the benchmark estimate (Table 4, Column 4) of .4 percentage points, given an average Republican vote share of around .5. In the next set of specifications, we consider alternative specifications which take into account the role of third parties, that is, Ross Perot in 1996 and, to a lesser extent, Ralph Nader in 2000. To the extent that towns with Fox News in 2000 may be more informed about third party candidates, neglecting third parties could lead to a bias in the estimates. Therefore, we consider as alternative dependent variables the all-party Republican vote share v 0R,Pres ) (Column 4) and the right-wing-party vote share ) (Column 5). Across these specifications, the estimate for F is positive and significant and hovers around 4 tenths of a percentage point, as in the benchmark specification with district fixed effects. Next, we consider alternative specifications of the Fox News variable d F OX k,2000 . Exposure to Fox News is likely to be higher for towns in which Fox News is provided as part of the Basic cable package, rather than as part of the (more expensive) Expanded package. We add a dummy for towns with Fox News in the Basic package (Column 6). The estimated coefficient on this dummy is positive but insignificant. The lack of a significant difference likely reflects the near equivalence between availability in a Basic or Extended package: two thirds of Basic subscribers purchase the Extended package whenever available. While this specification accounts for differences in exposure to different packages, it does not account for variation across towns in subscription rates to cable. To account for this, we compute r k,t , the ratio of the number of Fox News subscribers to the number of potential subscribers. This ratio is zero for towns that do not offer Fox News. 19 The coefficient on this ratio (Column 7), .0058, is positive and marginally significant. The lower precision in this estimate may be due to measurement error in the subscriber data, which is updated less frequently. We also test the sensitivity of the results to alternative samples. To check the robustness to the weighting procedure, we run an unweighted regression restricted to the 3,115 towns with average turnout in 1996 and 2000 of at least 2000 votes (Column 8). To examine the potential impact of outliers, we trim the top and bottom 1 percent of the dependent variable (Column 9). In both of these specifications, the effect of Fox News is positive and significant. Finally, we adopt the optimal trimming approach of Crump et al. (2005) (Column 10). This approach acknowledges that treatment and control group often differ significantly on observables, making the interpretation of average treatment effects problematic. As a solution, it focuses the analysis on an optimal subsample in which treatment and control observations are more comparable. We implement this method by estimating a propensity score for the availability of Fox News based on the controls v R,Pres k,1996 , X k,2000 , X k,00-90 , and C k,2000 . We then eliminate the 5,079 observations with propensity score smaller than .1 or larger than .9, the cutoffs suggested by Crump et al. (2005). The trimming eliminates most towns with fewer than 20 cable channels, since Fox News is almost never included as one of these channels. To the extent that cable companies in these towns under-report their offering, including potentially Fox News, their exclusion eliminates a source of contamination of the control group. Over the optimally trimmed sample of 4,177 towns, the point estimate for F is significant and larger than in the baseline specification, consistently with attenuation over the eliminated sample. We use this subsample in the rest of the paper as a specification check. In Appendix Table 2, we explore the role of weighting and of measurement error. The estimates of the Fox News effect are very similar if we weight the observations by voting-age population in 1996 (Columns 1 and 2) instead of by total votes cast. The estimates are .25 percentage points lower (.0014 and .0040) if we do not weight (Columns 3 and 4). The Fox News effect is still significant with county fixed effects but not with district fixed effects. The lower point estimates may be due to higher measurement error in very small towns or to heterogeneous treatment effects in smaller, more rural towns (see Table 6). We also present the results of another unweighted estimation procedure, nearest-neighbor matching (Abadie et  al., 2001). Each town with Fox News (treatment town) is matched to the non-Fox News town (control town) with the closest value of the controls. We match observations based on cable and Census controls (Column 5), and also on District indicator variables (Column 6). 20 The average treatment effect of Fox News (.0054 and .0042) is similar to our benchmark findings, significant, and larger than the OLS estimates without weighting (Columns 3 and 4). Different weighting procedures, therefore, have only a limited effect on the results. In Columns 7 and 8 we enlarge the baseline sample by including observations that are likely to be subject to greater measurement error. We include (as treated) 289 towns where Fox News is offered in parts but not all of the town, and 257 towns with likely voting data problems (see Section 4 for details). Consistently with increased measurement error in the Fox News variable, the estimates of the Fox News effect are lower. Finally, in Columns 9 and 10 we present a specification that is both unweighted and includes the observations with measurement error; in addition, we exclude the data from the states of Hi, Nd, Nj, and Wy. This specification is essentially the one that appeared in an older version of this paper. The failure to reject the null hypothesis stems from the inclusion of observations with greater measurement error, the equal weighting of towns of different sizes, and the smaller sample of States. Interaction effects. In Table 6, we examine how the Fox News effect interacts with town characteristics for the benchmark specifications with district and county fixed effects (Columns 1-2 and 5-6) and in the optimally trimmed sample (Columns 3-4 and 7-8). The impact of Fox News is larger in urban towns, although this effect is not significant in the optimally trimmed sample. This result may simply reflect the fact (documented below) that the Fox News effect is larger in more Democratic towns, which tend to be more urban. The Fox News effect is smaller in towns with more cable channels: an increase of 10 cable channels (.7 standard deviations) reduces the effect by .2 percentage points. When the Fox News message competes with a larger number of channels, its impact appears diminished (Mullainathan and Shleifer, 2005). The lower Fox News impact result could reflect exposure to more balanced reporting (though CNN and the network news are available in all towns in the sample) or merely lower audience rates for Fox News when more channels are available. We consider also the heterogeneity across geographic regions. The effect of Fox News is highest in the North East and in the West, and significantly lower in the Midwest. The effect is significantly lower also in the South in the benchmark specifications, but this pattern is not significant and even reverses in the optimally trimmed sample. These results should be taken with caution in light of the small number of observations in the South (725 towns) and West (689 towns). We also interact the results with the political orientation in the District. We split congressional districts into thirds by the 2000 Republican vote share. The Fox News effect is lower for the more Republican districts, significantly so with county fixed effects. The lower impact of Fox News in the Republican districts and states (the South) and in rural towns areas may be explained by the fact that in these towns most people already voted Republican, and therefore the share of the population at risk of being convinced was smaller. Magnitudes. Across the different specifications, the entry of Fox News into a cable market by the year 2000 had a significant effect on the Republican vote share in Presidential elections. The implied confidence intervals for the benchmark estimates (Columns 4 and 5 of Table 4) are (.0008, .0072) with district fixed effects and (.0041,.0097) with county fixed effects. The findings, therefore, are consistent with both a small (but positive) effect of Fox News and a fairly large effect, close to one percentage point. How large are these effects relative to shifts in vote share between 1996 and 2000? The average weighted change in vote share between 1996 and 2000 in our sample is 3.47 percentage points, with a standard deviation of 4.02. The estimated impact of Fox News is one tenth of a standard deviation with district fixed effects, and one sixth of a standard deviation with county fixed effects. The impact of Fox News is small, but not negligible. As a second measure, we estimate the number of votes that Fox News is likely to have shifted. We assume a treatment effect of Fox News of .54 percentage points, the midpoint of the benchmark estimates, and a diffusion of Fox News of 34 percent of the population, also for the 22 States for which we do not have data. The estimated impact of Fox News on the Republican vote share is then .34 * (.0054) = .0018, that is, .18 percentage points. Assuming that Fox News did not affect turnout substantially, Fox News shifted approximately 200,000 votes from the Democratic candidate to the Republican candidate. We also predict the number of votes shifted by Fox News in Florida, the pivotal State. In 2000, Fox News reached 32.8 percent of the Florida population. We assume that the Fox News effect on the 5,963,110 Florida votes cast is the same as in our sample. Under this assumption, the introduction of Fox News shifted .328 * (.0054) * 5, 963, 110 = 10, 561 votes, a number substantially larger than Bush's official margin of victory of 537 votes. The estimated impact of Fox News would be smaller, and potentially negative, under the alternative assumption that the effect in Florida is the same as in the Southern states (Table 6, Columns 5 through 8). 21 Overall, the impact of the entry of Fox News can be measured as about one tenth of a standard deviation of the shift toward Republicans between 1996 and 2000. While this estimate is moderately small, the entry of Fox News may still have contributed to the Bush victory in the unusually close 2000 election. Moreover, the impact may become larger over time as the Fox News audience and diffusion grows.",
          "So far, we have considered the impact of the availability of Fox News in 2000 on elections between 1996 and 2000. Now we further exploit the timing of the Fox News entry: (i) we consider the effect of the availability of Fox News in 1998 and 2004; (ii) we examine the effect of the availability of Fox News in 2000 on the change in vote share between 1992 and 1996, and between 2000 and 2004. These specifications allow us to estimate the effect of length of exposure, to further control for time trends, and to construct placebo treatments. For 16 of the 28 states 22 in the sample, we collected the availability of Fox News in November 1998. Out of 6,672 towns in this subsample, 1,221 towns offer Fox News in 2000, and 666 towns offer it in 1998. The difference between 1998 and 2000 is almost exclusively due to expansion of Fox News: only 29 towns have Fox News in 1998 but not in 2000. In Columns 1 and 2 of Table 7, we re-estimate specification (5) including also the indicator d F OX k,1998 for the availability of Fox News in 1998. The estimated impact is negative and insignificant. Length of exposure to Fox News does not appear to have a significant effect, but these results should be taken with caution given the smaller sample and the lower audience rates for Fox News in 1998. We also collected data on Fox News diffusion in March 2004 for the 28 states in our sample. In this period, Fox News is available in 4,844 out of 8,645 towns. 23 In Columns 3 and 4, we re-estimate specification (5) including the indicator variable d F OX k,2004 for the availability of Fox News in 2004. This is a placebo specification, since the introduction of Fox News after the year 2000 should not affect the change in vote share between 1996 and 2000. Indeed, controlling for Fox News availability in 2000, Fox News availability in 2004 has no effect on voting. In Columns 5 and 6, we take further advantage of the staggered nature of the Fox News introduction to address endogeneity issues. The endogeneity story holds that the impact of Fox News on the Republican vote share is due to the fact that Fox News was introduced in towns that were becoming more Republican anyway. While the findings in Table 3 address this concern, as an additional test we consider the 4,844 towns that got Fox News by 2004. We compare the vote share change between 1996 and 2000 for the towns that had Fox News already in 2000 with the towns that introduced it only after 2000. These two sets of towns should have similar political trends, but only the former set was exposed to Fox News before the 2000 elections. The estimated impact of the availability of Fox News in 2000 is similar to 22 To save coding time, we only collected this data for the states that we assessed to have the best election data (excepted New Jersey, that was added later to our sample): Alaska, California, Connecticut, Iowa (part of), Idaho, Massachusetts, Maine, Michigan, Minnesota, New Hampshire, New York, Ohio, Pennsylvania, Rhode Island, Vermont, and Wisconsin. 23 We exclude 281 towns which offer Fox News in 2004 in one, but not all of the cable systems in the town. the one in the benchmark specifications, with larger standard errors due to the smaller sample: F = .0034 with district fixed effects and F = .0061 (significant) with county fixed effects. Again, voting trends do not appear to explain the Fox News effect on voting. In Columns 7 through 10 we re-estimate specification (5) with voting in different years as dependent variables. The introduction of Fox News in 2000 is associated with an insignificant .2 percentage point vote share increase between 2000 and 2004 (Columns 7 and 8). The effect of Fox News appears to be permanent, if not increasing over time. As a placebo treatment we estimate whether the introduction of Fox News in 2000 predicts the vote share change between 1992 and 1996 (Columns 9 and 10). Obviously, Fox News introduction in 2000 should not affect voting between years in which Fox News did not exist. We find no evidence of a positive correlation; in fact, the estimates are negative though not significant. Voting trends are unlikely to be responsible for the Fox News effect.",
          "The previous findings suggest that Fox News had a significant effect on the Republican vote share in the Presidential election. In this Section, we consider whether the effect of Fox News extends to local politics not covered by Fox News. This allows us to test whether the Fox News effect is candidate-specific or a general ideological shift. Senate elections are a good test in this respect, because a large majority of Senate races fail to get national coverage. These elections are similar to local elections, for which unfortunately no town-level data set is available. At the same time, one or two Senate races per year attract substantial national coverage, almost like Presidential races. This allows us to compare the effect on Fox News on races that were not covered, where only ideological shifts should matter, to the effect on covered races, where candidate-specific coverage also could matter. In 2000, the Senate race that got the most coverage in Fox News by a wide margin was the Hillary Clinton-Rick Lazio race in New York State. These two candidates had 99 mentions in the O'Reilly Factor and the Hannity & Colmes show in the two months prior to the 2000 elections, with most mentions critical of Hillary Clinton 24 . All the other Senate candidates running in the 2000 campaign combined got a total of 73 mentions, with Joe Lieberman, who was typically mentioned because of his Vice-Presidential race, getting the lion's share of these mentions. We examine whether Fox News impacted the vote share in Senate elections, and whether it had a differential effect for the Clinton-Lazio race. We denote by d NY the indicator variable 24 Two excerpts from the \"O'Reilly Factor\" of 10/31/2000: \"Now, I am saying to THE FACTOR viewers, and everyone else who hears what we have to say on this program, that Mrs. Clinton has trouble with honesty\"; \"Mr. Gore does have some honesty issues about campaign finance, but they pale beside the deceit factory the Clintons have set up\". for the New York Senate races. We estimate the specification where the coefficient  F indicates the effect of Fox News on Senate races other than New York, and  F indicates the differential effect for the featured New York race. Notice that the dependent variable is the difference of the Senatorial vote share in 2000 and the Presidential vote share in 1996. This specification maximizes sample size: the voting data for the 1994 Senatorial election is available for only 5 States, not including New York state. Table 8 reports the results. The effect of Fox News on non-featured Senate races is large and significant, .0079 with district fixed effects (Column 1) and .0082 with county fixed effects (Column 2). Compared to this effect, the effect of Fox News on the New York race is not significantly different, although the standard errors on the coefficient F are relatively large. Over the optimally trimmed subsample (Columns 3 and 4), the estimated impact of Fox News on Senate races is positive but not significant. In the specification with District fixed effects, Fox News appears to have had a significantly larger effect on the New York Senate race ( F = .014), but the result does not replicate with county fixed effects (Column 4). In Column 5, we use the ratio of Fox News subscribers to population in lieu of the Fox News dummy. The result is very similar to the baseline estimate, suggesting a substantial impact of Fox News on all Senate races. We then test for heterogeneity by political areas (Column 6). The Fox News effect is (marginally) significantly lower for the more Republican districts, suggesting that Fox News had the largest impact in Democratic or swing districts. Finally, the results using the all-party vote share (Column 7) parallel the benchmark finding in Column 1. In the last two columns (Columns 8 and 9) we re-estimate (6) using the difference in the two-party Republican vote share between the 2000 and the 1994 Senate races as the dependent variable. This allows us to compare two races both featuring the 2000 incumbent, at the cost of reducing the sample to 2,037 towns in 5 States (Maine, Michigan, Minnesota, Montana, and Rhode Island). In this small sample, we find large and significant effects of Fox News in the order of one percentage point. While these estimates are less precise, they corroborate the finding of a substantial effect of Fox News on Senate races. Overall, we find evidence of an effect of Fox News also for non-featured races such as Senate races, specially in Democratic and swing districts. We fail to find a stronger effect for highly emphasized races. These results suggest that Fox News exposure induces a generalized ideological shift, as opposed to a candidate-specific effect.",
          "The significant impact of Fox News on the Republican vote share could occur for two reasons. First, Fox News entry convinced Democratic voters to vote Republican. Second, Fox News attracted new Republican voters. We use measures of turnout to test these hypotheses. The baseline regression for voter turnout is: ). The change in this measure over time is the percent change in total votes cast. This specification controls for the percentage change in the voting-age town population over time, ln (P op k,2000 )ln (P op k,1996 ), since increases in population would naturally increase the number of votes cast. Table 9 shows the results. The average change in log votes is .0869, implying a 8.69 percent higher turnout in the much tighter Presidential race of 2000. The estimate for  F is positive but insignificant with district fixed effects, and is large and significant with county fixed effects. This second estimate ( F = .018) suggests that fox News increased turnout by 1.8 percent, a large effect. In both specifications, the elasticity  of the change in votes cast with respect to the change in population is about .4. Over the optimally trimmed subsample (Columns 3 and 4), the Fox News effect is positive and significant with county fixed effects, and marginally significant with district fixed effects. The Fox News coefficient F is positive but insignificant using the ratio of subscribers to Fox News (Column 5). The turnout effect is concentrated in the more Democratic districts (Column 6). Together with the finding that the impact of Fox News on vote share is larger in the more Democratic areas, this suggests that the main effect of Fox News was to induce non-voters in Democratic districts to turn out and vote Republican. Finally, in Columns 7 and 8 we replicate the results of Columns 1 and 2 with an alternative turnout measure: t j0 k,t = ln(V T OT,j k,t /P op k,t ). This specification is equivalent to specification (7), with the additional restriction  = 1 (rejected by the data). The point estimates are slightly smaller, and still significant with county fixed effects. Overall, Fox News entry into a market appears to have mobilized new voters, specially in Democratic districts. However, the evidence is not as clear-cut as for the effect on vote share.",
          "This paper studies the effects on voting of one of the most dramatic media shifts in past years. We find that the introduction of a (comparatively speaking) conservative news channel increased the vote share of Republican presidential candidates by .4 to .7 percentage points. We now evaluate the magnitude of this effect, compare it to other media effects in the literature, and put forward interpretations of the results. In particular, we estimate a persuasion rate, that is, the share of the Fox News audience that was convinced by Fox News to vote Republican. Clearly, Republican voters or people that were not watching Fox News could not have been convinced. From an econometric standpoint, we do a calculation of treatment on the treated using the average treatment effect.",
          "Model. To compute persuasion rates, we compare treatment towns T, where Fox News is available via cable, and control towns C, where Fox News is not available via cable. We denote by r the share of Republican voters, and by d the share of Democratic voters, before the introduction of Fox News. For simplicity, we neglect third parties. Consequently, (1rd) denotes the share of non-voters. Since the two types of towns have similar political outcomes in the pre-Fox News period (Tables 2 and3), we assume that r and d are the same in towns T and C. After the nation-wide introduction of Fox News, a fraction e of the town population is exposed to the new channel. The exposure parameter e is higher in treatment towns, that is, e T > e C  0. We allow exposure e C in control towns to be non-zero because, for example, of the availability of satellite which broadcasts Fox News to subscribers in both towns. For simplicity, we also assume that the exposure e j to Fox News in town j is independent of political affiliation. That is, we assume that Republicans are as likely as Democrats or non-voters to watch Fox News when available. While the evidence in Table 1 suggests that Republicans are more likely to watch Fox News, we cannot rule out that this captures the causal convincing effect of Fox News, rather than differential exposure e j by party. The key parameter in the model is f, the fraction of the audience that is convinced by Fox News to vote Republican. This persuasion rate, f applies equally to Democratic voters and to non-voters, that is, to a fraction (1r) of the Fox News audience e j , where j = T, C. Therefore, the introduction of Fox News increases the fraction of people voting Republican by (1r) e j f. The two-party vote share v j in town j, with j = T, C, equals (Turnout increases since Fox News induces a fraction f of the non-voters to vote Republican.) Using expression (8), we solve for the difference in vote share between treatment and control towns, v Tv C , the equivalent of F in the data. We obtain Expression ( 9) is easily interpretable. The first term is the influence rate per treated population, and the second term is a factor correcting for turnout effects. The numerator of the first term, v Tv C , is the shift in Republican vote share due to the availability of Fox News via cable. The denominator, (e Te C ) (1r) , normalizes this vote shift by the share of population at-risk of treatment, that is, by the differential exposure to Fox News in treatment and control towns, times the share of non-Republicans. The second term, which disappears if turnout is perfect (r + d = 1), captures the difference between the convincing effect of Fox News on a Democrat and a non-voter. In both cases, Republicans gain a vote, but only in the first case the opposing party, the Democratic party, loses a vote. Converting a Democrat, therefore, has a larger impact on the vote share than converting a non-voter. The larger the ratio of non-Republicans (1r) to Democrats, d, hence, the bigger is the convincing impact for a given vote share change. In addition, the term t C t T in (9) corrects for the fact that a higher turnout t j increases the denominator of expression (8), and therefore decreases the impact of f on v j . We estimate the persuasion rate f using expression (9). First, we evaluate the political variables t T, t C, r, and d using the sample averages of the 2000 elections. We weight the averages by total votes cast in 2000 to better approximate the individual-level expressions ( 8) and ( 9). (In the robustness section, we discuss alternative estimates based on unweighted averages.) The average weighted turnout in 2000 as a share of the voting-age population is .560, and it is very similar in Fox News and non-Fox News towns. We therefore assume t T = t C = .560 25 . We estimate the percentage of Democratic voters d as the product of the turnout rate t and the average weighted Democratic two-party vote share in 2000 in our sample, that is, .560 * (1 -.453) = .306. This implies that f = 1.024 Audience data. We estimate the differential exposure (e Te C ) using the micro-level Scarborough data on television audiences described in Section 3. We use the subsample of 11,388 respondents for whom we observe the ZIP code of residence, and whom we can match by ZIP code to the cable data on availability of Fox News (Table 1, Columns 4-6). We use the 'diary audience' measure, since the 'regular audience' measure is not available for most of this sample. We aggregate the data at the town level to maximize comparability to the specifications in the rest of the paper. For each town k of the 568 towns in this sample, e F OX k is the fraction of town residents that belong to the Fox News audience. We estimate The regression is weighted by the number of respondents in a town, and the standard errors are clustered at the level of the local cable company. The coefficient  F is the differential Fox News diary audience due to Fox News availability via cable in the town. Table 10 shows the results. In the specification without controls (Column 1), the availability of Fox News induces 2.7 percent ( F = .0270) additional town residents to watch Fox News for at least a full half hour per week. The estimate is significant and sizeable. The intercept  = .0262 indicates that, in towns where Fox News is not available via cable, 2.62 percent of the residents still watch Fox News for at least a full half hour per week. About half of the Fox News audience, therefore, watches Fox News in ways other than via cable, possibly via satellite. This finding could also be due to measurement error in our measure of Fox News availability via cable. In either case, this implies that the estimates in Section 5 are likely to understate the impact of Fox News on voting, since they capture only the impact of Fox News availability via cable, disregarding the impact of availability in other ways. The estimates in Column 1 do not control for demographic, geographic, or cable-system differences between towns that adopt Fox News and towns that do not. In Columns 2 and 3, we re-estimate specification (10) adding the Census, cable, and geographic controls used in the body of the paper. The estimated differential exposure rates are F = .0371 with congressional district fixed effects and F = .0251 with county fixed effects. Interestingly, introducing control variables and district fixed effects increases the estimated F relative to the estimate in Column 1. Columns 4 through 6 present robustness checks. In the optimally trimmed subsample (Column 4) the estimated exposure rate is still positive and significant, as is the estimate using the Fox News subscription ratio (Column 5). Finally, as a placebo test, we check that availability of Fox News via cable in 2004 does not increase audience rates in 2000 (Column 6). Indeed, the coefficient on Fox News availability in 2004 is essentially zero. In Columns 7, 8, and 9 we replicate the specifications in Columns 1, 2, and 3 using the CNN audience as the dependent variable. This specification provides another placebo treatment. Availability of Fox News via cable should not, to a first approximation, increase the audience for CNN, since CNN is available in all towns in our sample. In the specification without controls (Column 7), availability of Fox News increases CNN audience by .0251, compared to an average audience for CNN of .0947. Once we introduce controls (Columns 8 and 9), the availability of Fox News via cable no longer forecasts the CNN audience. The audience data, therefore, passes this second placebo test. Persuasion rate estimates. We estimate the differential exposure rate e Te C using the coefficient F of Columns 2 and 3 in Table 10. As a benchmark measure of audience, we use the regular audience measure, because we believe that the diary measure underestimates the actual audience. Since we cannot estimate model (10) for the regular audience variable, we multiply the estimates of F by a conversion rate. We compute the conversion rate as the ratio between the aggregate regular audience and diary audience for CNN, that is, 35.3/10.3 = 3.42 (Table 1, Column 4). (The ratio would be somewhat higher if we used the audience measures for Fox News) The implied estimates for the differential exposure rate are T - C = 3.42 * .0371 = .1269 with district fixed effects and T -C = 3.42 * .0251 = .0858 with county fixed effects. We combine the estimates of the political variables (t T , t C , d) and of the audience (e Te C ) with the estimates of the voting impact (v Tv C ) to obtain a persuasion rate f. With congressional district fixed effects we get f = 1.024 * (.0040/.1269) = .0322, that is, Fox News convinced 3.22 percent of its listeners that were not already voting Republican to do so. With county fixed effects, we derive f = 1.024 * (.0069/.0858) = .0823, that is, Fox News convinced 8.23 percent of its audience. We summarize these results in the first rows of Table 11. Robustness. The estimates of the persuasion rate are robust to different estimates of the political parameters t C , t T , and d. If, instead of using weighted town averages, we use the unweighted averages of turnout (t = .583) and Democratic vote share (d = .583 * (1 -.538)), we obtain t T t C /d = .583/.462 = 1.261. With this ratio, the estimated persuasion rates are f = 1.261 * .0322 = .0407 with district fixed effects and f = 1.261 * .0833 = .1037 with county fixed effects. These effects are in the ballpark of the benchmark estimates. The persuasion rate estimates are more sensitive to assumptions about the exposure rate e Fe N . The persuasion rate f are 3.42 times larger if e Fe N is measured using the diary data as the measure of audience. The resulting estimates f = .1104 (district fixed effects) and f = .2814 (county fixed effects) imply substantial persuasion effects of the media. A second factor that leads to higher persuasion rates is the self-selection of Republicans in the Fox News audience. In Table 10, we estimate the selective exposure e Te C in (10) using the whole population, rather than just Democratic voters and non-voters. To the extent that Republicans self-select in the Fox News audience, this upward biases the estimate of e Te C , and therefore downward biases the estimate of f. 26 Conclusion. The benchmark estimates imply that Fox News convinced 3 to 8 percent of its audience to shift its voting behavior towards the Republican party, a sizeable media persuasion effect. Alternative estimates using the diary audience measure lead to estimates of the persuasion rate between .11 and .28, corresponding to large media effects.",
          "We estimate persuasion rates f for other studies in the literature, summarized in Table 11. We discuss field experiments, laboratory experiments, and surveys. Field experiments. Green and Gerber (2004) summarizes a series of field experiments on turnout. In some of these experiments, target households within a precinct are randomly selected to receive turn-out-the-vote treatments (canvassing, phone calls, or leaflets) right before an election. Turnout is measured using official individual voting records kept at the precinct. In other experiments, the randomization is done at the precinct level, and precinctlevel turnout is compared across precincts. We denote by t T the share of individuals that turn out to vote in the treatment group, and by t C the correspondent variable in the control group. As above, denote by e T and e C the respective exposure rates, and by t the underlying turnout rate. Also, like in Section 6.1, we assume that the treatment convinces a fraction f of the people that do not usually turn out and are exposed, that is, (1t) e j , for j = T, C. It follows that t j = t + f (1t) e j and t Tt C = (1t) (e Te C ) f. This implies In these experiments, e C = 0 since no one in the control group is exposed to the treatment, hence t = t C . In Table 11 we summarize five such experiments, with treatment (Column 1), election type (Column 2), year and place (Columns 4 and 5). We also present the sample size (Column 6), the turnout rates in the control (t C ) and treatment (t T ) groups (Columns 7 and 8), and the differential exposure rate e Te C (Column 9). Using this information and (11), we compute the persuasion rate f (Column 10). Canvassing and phone calls convinced to vote between 4 and 26 percent of the people that would not have voted otherwise. More recently, Gerber et al. (2006) randomly assign subscriptions to a right-wing newspaper (Washington Times) or a left-wing newspaper (Washington Post), and consider the effect on stated voting behavior in a post-election survey. They find a substantial increase in the share of (stated) Democratic voters for exposure to the left-wing newspaper, corresponding to a persuasion rate f of .109. (We use expression (11) where t is the share of Democratic votes out of all survey respondents, including non-voters) 27 However, they also find that the share of Democratic voters increases after exposure to the right-wing paper, albeit insignificantly. Laboratory experiments. Ansolabehre and Iyengar (1995) expose experimental subjects to 30-second political advertisements supporting a candidate (or criticizing the opposite candidate). They then elicit beliefs and voting intentions at the end of the experiment. In Table 11 we summarize the findings for three sets of experiments with 1,716 total subjects. On average, exposure to one advertisement increases the stated vote share for the sponsoring party from .530 to .568. 28 Expression (11) (where t indicates the intention to vote for the sponsoring party) yields a sizeable persuasion rate f of .08. Other experiments by the authors (results not reported) lead to persuasion rates of similar or larger magnitudes. Surveys. Following Lazarsfeld, Berelson and Gaudet (1944), political scientists have widely used surveys to assess the impact of the media. A survey in this tradition (Kull et  al., 2003) finds that 33 percent of Fox News watchers believe (erroneously) that weapons of mass destruction were found in Iraq by October 2003, compared to 22 percent for the overall sample. We compute a persuasion rate f of .141 (Table 11) using an expression parallel to (11) where t is the share of respondents who believe that weapons were found. Along similar 27 We thank the authors for providing the data necessary for this calculation. By computing the share of Democratic voters out of all survey respondents, including non-voters, we can use expression (11) instead of expression (9). 28 We use the data in Tables B1. lines, Gentzkow and Shapiro (2004) examine the effect of media exposure in the Islamic world. Members of the CNN audience were 30 percent more likely to believe, and members of the Al Jazeera audience were 40 percent less likely to believe, that Arabs carried out the 9/11 attacks, compared to survey respondents who watched neither. The estimates imply persuasion rates between .08 and .10. While the survey estimates could be due to sorting rather than causal effects, the implied persuasion rates are quite close to the other estimates.",
          "Our estimates imply that Fox News convinced 3 to 8 percent of the audience to vote Republican, a sizeable media effect. We consider three explanations for this finding: one statistical, one rational, and one non-rational. 1. Endogeneity Bias. The Fox News effect on voting may be a spurious correlation. Towns where Fox News entered were becoming more Republican and would have voted more conservatively, independent of Fox News entry. Thus, the Fox News variable proxies for underlying political trends. However, contrary to this interpretation, conditional on the controls, vote shares in 1996 and voting trends in 1988-1992 do not predict the introduction of Fox News (Table 3). Moreover, Fox News introduction does not predict political voting trends between 1992 and 1996 (before the introduction), or between 2000 and 2004 (after the introduction) (Table 7). Fox News only affects vote share changes between 1996 and 2000. Therefore, it is unlikely that endogeneity of Fox News introduction explains the results. 2. Rational Learning. The model in Section 2 suggests a rational learning interpretation for the results. When Fox News entered in 1996, viewers were unsure of its political bias. Therefore, upon seeing Fox News' positive coverage of Bush, they interpreted it as a positive signal about the Republican candidate and shifted their vote. A first issue with this interpretation is that, arguably, by the year 2000 the conservative slant of Fox News should have been clear. In this case, exposure to Fox News would not have affected voting. Second, even if the political orientation of Fox News was not clear by the year 2000, it should have become clear over time. As voters become aware of Fox News' political slant, they should adjust their updating techniques to filter out Fox's bias. Contrary to this prediction, the Fox News effect over the 2000-2004 period gets if anything larger (Table 7).",
          "The model provides an alternative, behavioral interpretation of the results. Viewers do not fully take into account the bias of the media source and therefore are subject to persuasion upon exposure. If voters suffer from persuasion, the effect of Fox News on voting may well be permanent, consistent with the empirical findings. Overall, we find the data to be most consistent with the third interpretation, that voters do not optimally filter out media bias. Rational learning, however, can also explain the findings. The two interpretations have very different long-run predictions. Learning predicts that Fox New's political effect is temporary. Persuasion predicts that Fox News permanently altered voting patterns in the United States. Whether the effect is rational or not, it would be interesting to know the exact mechanism by which Fox News affected voting. The Senate results suggest that the effect is not due only to candidate-specific coverage, but rather to a general ideological shift. Beyond this, we cannot tell if the effect is due to conservative slant of the news or to the choice of topics like National Security that favor Republicans, as implied by the agenda setting theory (Cohen, 1963).",
          "This paper studies the impact of media bias upon voting. We consider one of the most dramatic changes in the US media in recent years, the sudden expansion of the Fox News cable channel from 1996 to 2000. We exploit the natural experiment induced by the timing of the entry of the Fox News channel in local cable markets. We estimate the impact of the availability of Fox News in 2000 on the Republican vote share and on voter turnout. This provides a test of whether exposure to media bias affects political beliefs and voting. We find a significant effect of exposure to Fox News on voting. Towns with Fox News have a 0.4 to 0.7 percentage point higher Republican vote share in the 2000 Presidential elections, compared to the 1996 elections. A vote shift of this magnitude is likely to have been decisive in the 2000 elections. We also find an effect on vote share in Senate elections which Fox News does not cover, suggesting that the Fox News impact extends to general political beliefs. Finally, we find evidence that Fox News increased turnout to the polls. Based on this evidence and on micro-level audience data, we estimate that exposure to Fox News induced 3 to 8 percent of the non-Republican viewers to vote for the Republican party. This estimate is consistent with field and laboratory evidence of media effects on political beliefs and voting. This suggests that the media can have a sizeable political impact, especially when a politician controls a substantial share of the media, as Berlusconi does in Italy. We interpret the results in light of a simple model of voter learning about media bias and about politician quality. The Fox News effect could be a temporary learning effect for rational voters, or a permanent effect for voters subject to non-rational persuasion. This paper leaves a number of open questions on the impact of media bias. First, while we analyze the extensive margin of voting, we do not consider the effect on the intensity of political convictions of Republican voters. In ongoing research, we study the impact on the intensive margin of campaign contributions. Second, we have not directly examined the impact of the media on policy-making. While a vote shift toward Republicans is likely to induce a change in policy (Lee, Moretti, and Butler, 2004), direct evidence documenting this effect would be interesting. Finally, we hope that more evidence on the effect of other sources of media bias, such as local papers and radio talk shows, will complement the evidence in this paper. Each entry is listed by State under the name of the principal community, which is the town where a given local cable company's business office is located. The additional communities reached by the local cable company are listed in alphabetical order, typically without indication of the county, which is listed separately (the number of counties is rarely more than three). In the example above, the communities listed belong to one of two counties (Delaware and Montgomery) in Pennsylvania. Since we do not know which belongs to which, we generate all combinations of town and county, except in cases where the county is explicitly listed as in \"Upper Providence Twp. (Delaware County)\". When we match the cable data with the Census and election data, the fictitious town-county combinations drop out. (The town-county combination that do not match to either election or Census data are already dropped from Column 1 in Appendix Table 1) The only possibility of error in the match is if there are two towns with the same name in the multiple counties listed, but in this case we expect the county to be explicitly listed next to the town name. In a few cases, the communities reached are indicated only as fractions of a county, such as \"Alameda County (Western borders)\". We exclude these communities, since we cannot match them to voting data. As long as these communities do not include other separately-listed towns in the cable data, their deletion will not bias the measure of cable offerings for the other towns. As a robustness check, we recompute the results in the paper excluding the 149 counties which include one such community where Fox News is available. All the results hold in this smaller sample of 8,262 towns. The Basic Service description lists all the local television stations that the cable company rebroadcasts under the heading \"Programming (received off-air)\". We disregard these local stations. We estimate instead the number of cable channels broadcast in the \"Programming (via satellite)\" section. In order to save coding time, the total number of channels is estimated counting the number of lines listing cable channels in the Basic and Expanded Basic 1, 2, and 3 Services. The estimated number of channels follows by multiplying this number by 2.5, a conversion rate estimated on a subsample of 40 cable companies. Over this subsample, a regression of actual number of channels on forecasted number of channels yields an R 2 of .95 and a coefficient of 1. Finally, it is worth noting that sometimes the number of subscribers or (as in this case) the price of the subscription is missing, or refers to previous years. Computation of W . We compute the precision W of the preliminary estimate  T -b  T for the rational case ( = 0); this precision is used, by assumption, also by voters with persuasion  > 0. For a given race concerning candidates at time T , rational viewers ( = 0) form a preliminary expectation of quality, b",
          "where the second step follows from substituting  t =  t +  and combining terms. Note that since b  P T is an estimate of  T , its variance does not itself depend on  T but does on all other random variables ( 1 , ... T -1 , ). We can compute the variance of b  P T to be where W is the precision of b  P T . This implies . Now that we have obtained  and W, we solve the signal extraction for the case of -persuasion. Voters with -persuasion form a differential quality estimate for politician pair T , b   T , by taking a precision-weighted average of the prior quality differential between candidates, 0 with precision   , and the estimated one, Proofs of Propositions 1 and 2. The proof of Proposition 1 follows immediately from taking derivatives and limits for T  :  Note: Proportion for each county is calculated as the ratio of number of towns with Fox News available via cable to total number of towns in the county. Alaska and Hawaii are also in the data set, but are not included on the map due to space constraints.  Notes: Standard deviations in parenthesis. The subset \"Fox News in 2000\" is formed by the towns with availability of Fox News in 2000 in the cable package. The subset \"No Fox in 2000\" is the complementary groups of towns. Towns with district variation are towns in districts in which there is at least one town that does not get Fox News and one town that does. Towns with county variation are similarly defined except at the county level. Potential Subscribers is defined as the total voting-age population of the towns reached by a cable provider. Republican two-party vote share is the votes received by the Republican candidate in the presidential election divided by the votes received by both the republican and democratics candidates. Log turnout is measured by the log ot the ratio of total votes cast in a given town to the voting-age population of the town. Observations unweighted.",
          "Voting Variables: Census Variables for 2000: Change from 1990 to 2000:  "
        ],
        "ground_truth_definitions": {
          "biased news media outlet": {
            "definition": "it systematically slants its broadcast of politician quality to favor either the Republican or the Democratic candidate.",
            "context": "Voting depends on both ideological preferences and perceived politician quality. We define a news media outlet to be biased if it systematically slants its broadcast of politician quality to favor either the Republican or the Democratic candidate. We consider both voters who rationally update from news reports and voters who are subject to non-rational persuasion",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180  r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP  for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return  (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "You are a research assistant tasked with extracting and organizing definitions from a given section of a paper. Follow these instructions to ensure accuracy and adhere to the rules specified:\n\n1. **Identify Terms and Definitions**: Look for terms that are explicitly defined in the section, using cues such as \"is defined as,\" \"refers to,\" or \"means.\"\n2. **Extract Definitions and Context**: For each identified term, extract the definition, ensuring it is one sentence long and directly from the text. Also, include the context (one sentence before and one sentence after the definition sentence if possible) without altering any words or formatting.\n3. **Avoid Over-Generation and Hallucinations**: Only extract what is clearly defined; do not over-generate or make up information that is not in the text.\n4. **Handle Synonyms Properly**: If a term has synonyms defined, divide them with a slash (e.g., \"term1/term2\") in the term field.\n5. **Return the Output**: Return the definitions in a dictionary format, e.g., {\"term1\": [\"definition1\", \"context1...\"], ...}. If no definitions are found, return an empty dictionary.\n\nExample Input:\nSection: \"The section introduces the concepts of 'terminology' and 'domain'...\"\n\nExample Output:\n{\"terminology\": [\"the systematic analysis and categorization of terms used in a specific domain or field.\", \"The section describes and defines the two important concepts of 'terminology' and 'domain' and then describes the characteristic related to terminology.\"], \"domain\": [\"a specific field or area of study to which terminology and its characteristics pertain.\", \"We first describe and define the two important concepts of 'terminology' and 'domain' and then describe the characteristic related to terminology.\"]}\n\nEnsure that each term is explicitly defined in the section and that the definitions are not over-extended or altered in any way.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Named entities, e.g., persons, organizations, locations, etc. Bird et al. (2009) and Honnibal and Montani (2017).",
        "reasoning": "This section appears to list named entities, such as authors and publications, rather than providing a definition of a term. There are no clear indications of definitions or explanations provided here.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
        "reasoning": "This section discusses the task of quantifying whether textual data has a positive, negative, or neutral emotion by referencing a study (Zhang et al., 2018). It appears to be explaining a process rather than directly defining a term.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "In 2018, Hamborg et al. [98] concluded that (1) powerful computer science methods (such as word embeddings and deep learning) had not yet made their way into the automated detection of media bias and that (2) the interdisciplinarity of media bias research should be improved in the future. The authors suggested (3) that approaches in computer science did not account for bias having many different forms and usually only focused on narrow bias definitions [98]. Our literature review reveals that two of these propositions (1 and 3) have been addressed to some extent, but there is still considerable room for improvement. Transformer and graph-based methods have led to significant increases in the performance of automated methods for detecting media bias, and numerous types of bias have received research attention. However, these concepts are primarily used and analyzed individually, with knowledge overlaps between Manuscript submitted to ACM The Media Bias Taxonomy 25 them remaining unexplored [212]. Recent modeling techniques, such as multi-task learning, enable the use of related datasets to improve classification performance [9]. Regarding (2), datasets and systems still exhibit limited conceptual work, with the cognitive dimension of media bias rarely mentioned in computer science research. Our literature review aims to provide a foundation for increased awareness of bias in media bias datasets (through standardized annotator background assessments), enhanced interdisciplinarity in the research domain (which we believe is particularly relevant since reasonable classifications cannot exist without clear conceptualizations), and future computer science methods. We are confident that this review will facilitate entry into media bias research and help experienced researchers identify related works. We hope that our findings will contribute to the development of more effective and efficient media bias detection methods and systems to increase media bias awareness. Finally, we plan to repeat our workflow in three years to reassess the state of the research domain.",
        "reasoning": "The text discusses various aspects of media bias research, including the limitations of current methods and the need for multidisciplinary approaches. The authors suggest that the current state of research has not adequately addressed specific issues and that future work should focus on enhancing interdisciplinarity, standardization, and interdisciplinary collaboration.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "In [90] authors present a framework to mitigate bias in models learned from data with stereotypical associations. They propose a model in which they are trying to maximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary to predict the protected or sensitive variable (stereotyping variable z). The model consists of two parts-the predictor and the adversary-as shown in Figure 6. In their model, the predictor is trained to predict Y given X. With the help of a gradient-based approach like stochastic gradient descent, the model tries to learn the weights W by minimizing some loss function LP( , y). The output layer is passed to an adversary, which is another network. This network tries to predict Z. The adversary may have different inputs depending on the fairness definition needing to be achieved. For instance, in order to satisfy Demographic Parity, the adversary would try to predict the protected variable Z using only the predicted label  passed as an input to it, while preventing the adversary from learning this is the goal of the predictor. Similarly, to achieve Equality of Odds, the adversary would get the true label Y in addition to the predicted label  . To satisfy Equality of Opportunity for a given class y, they would only select instances for the adversary where Y=y. [156] takes an interesting and different direction toward solving fairness issues using adversarial networks by introducing FairGAN which generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. The architecture of their FairGAN model is shown in Figure 5. FairGAN consists of two components: a generator   which generates the fake data conditioned on the protected attribute   (, , ) =   (, |)  () where   () =   (), and two discriminators  1 and  2 .  1 is trained to differentiate the real data denoted by   (, , ) from the generated fake data denoted by   (, , ). Fig. 5. Structure of FairGAN as proposed in [156]. Fig. 6. The architecture of adversarial network proposed in [90]  Brian Hu Zhang. In addition to that, for achieving fairness constraints, such as statistical parity,   (, | = 1) =   (, | = 0), the training of  2 is such that it emphasizes differentiation of the two types of synthetic (generated by the model) samples   (, | = 1) and   (, | = 0) indicating if the synthetic samples are from the unprotected or protected groups. Here s denotes the protected or the sensitive variable, and we adapted the same notation as in [156].",
        "reasoning": "The section does not explicitly present a definition of a term, but several terms are defined or their properties are explained, such as \"stereotyping variable z\", \"predictor\", \"adversary\", \"gradient-based approach\", \"stochastic gradient descent\", \"loss function LP( , y)\", \"real data\", and \"generated synthetic data\". However, none of these are presented as clear, explicit definitions.",
        "is_definition": false
      }
    ],
    "signature": {
      "instructions": "You are a research assistant in a social sciences paper, and your task is to determine whether a given section contains a clear definition of a term. The paper must be accurate, and false positives could lead to wasted effort and incorrect conclusions. The section may contain complex sentences and multiple references. Your job is to use the following criteria to decide if a term is defined in the section:\\n\\n- Look for explicit or implicit cue phrases such as 'is defined as', 'refers to', or 'means' that clearly indicate a term is being defined.\\n- If the term is defined, the definition should be a clear explanation of the term's meaning, not just a description or explanation of its context or effects.\\n- Avoid false positives by preferring 'no' if the term is not clearly defined.\\n- If the definition is present, ensure it is concise and directly from the text, not over-generated or altered.\\n\\nExample: If the section contains 'Media bias is the intentional, non-objective coverage of news events,' then the term 'media bias' is defined. If the section contains 'The process of agenda setting is about what people think about,' then the term 'agenda setting' is defined. If the section does not contain any clear cue phrases or definitions, return 'False' for that section.\\n\\nSection: [insert section of the paper here]\\nReasoning: Let's think step by step in order to [reasoning]\\nIs Definition: [True or False]",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
