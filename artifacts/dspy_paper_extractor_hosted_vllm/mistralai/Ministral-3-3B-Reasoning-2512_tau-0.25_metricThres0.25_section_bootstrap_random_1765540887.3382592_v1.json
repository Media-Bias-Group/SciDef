{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "The study of biased news reporting has a long tradition in the social sciences going back at least to the 1950s [29]. In the classical definition of Williams, media bias must both be intentional, i.e., reflect a conscious act or choice, and it must be sustained, i.e., represent a systematic tendency rather than an isolated incident [30]. In this article, we thus focus on intentional media bias, which journalists and other involved parties implement purposely to achieve a specific goal [13]. This definition sets the media bias that we consider apart from other sources of unintentional bias in news coverage. Source of unintentional bias include the influence of news values [31] throughout the production of news [27], and later the news consumption by readers with different backgrounds [7]. Examples for news values include the geographic vicinity of a newsworthy event to the location of the news outlet and consumers, or the effects of the general visibility or societal relevance of a specific topic [32]. Various definitions of media bias and its specific forms exist, each depending on the particular context and research questions studied. Mullainathan and Shleifer define two highlevel types of media bias concerned with the intention of news outlets when writing articles: ideology and spin [33]. Ideological bias is present if an outlet biases articles to promote a specific opinion on a topic. Spin bias is present if the outlet attempts to create a memorable story. A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement (cf. [34]). Coverage bias is concerned with the visibility of topics or entities, such as a person or country, in media coverage. Gatekeeping bias, also called selection bias or agenda bias, relates to which stories media outlets select or reject for reporting. Statement bias, also called presentation bias, is concerned with how articles choose to report on concepts. For example, in the US elections, a well-observed bias arises from editorial slant [35], in which the editorial position on a given presidential candidate affects the quantity and tone of a newspaper's coverage. Further forms of media bias can be found in the extensive discussion by D'Alessio and Allen [34].",
        "reasoning": "The provided section discusses various definitions of biased news reporting, including intentional and unintentional sources. It mentions classical definitions and different types of biases, such as ideological, spin, coverage, gatekeeping, and statement bias. The definitions are explicitly stated and presented in the text.",
        "extracted_terms": [
          {
            "term": "media bias",
            "definition": "intentional, i.e., reflect a conscious act or choice, and must be sustained, i.e., represent a systematic tendency rather than an isolated incident",
            "context": "In the classical definition of Williams, media bias must both be intentional, i.e., reflect a conscious act or choice, and it must be sustained, i.e., represent a systematic tendency rather than an isolated incident. In this article, we thus focus on intentional media bias, which journalists and other involved parties implement purposely to achieve a specific goal. This definition sets the media bias that we consider apart from other sources of unintentional bias in news coverage. Source of unintentional bias include the influence of news values throughout the production of news and later the news consumption by readers with different backgrounds.",
            "type": "explicit"
          },
          {
            "term": "ideological bias",
            "definition": "an outlet biases articles to promote a specific opinion on a topic",
            "context": "Mullainathan and Shleifer define two high-level types of media bias concerned with the intention of news outlets when writing articles: ideology and spin. Ideological bias is present if an outlet biases articles to promote a specific opinion on a topic. Spin bias is present if the outlet attempts to create a memorable story. A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement.",
            "type": "explicit"
          },
          {
            "term": "spin bias",
            "definition": "an outlet attempts to create a memorable story",
            "context": "Mullainathan and Shleifer define two high-level types of media bias concerned with the intention of news outlets when writing articles: ideology and spin. Ideological bias is present if an outlet biases articles to promote a specific opinion on a topic. Spin bias is present if the outlet attempts to create a memorable story. A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement.",
            "type": "explicit"
          },
          {
            "term": "coverage bias",
            "definition": "concerned with the visibility of topics or entities, such as a person or country, in media coverage",
            "context": "A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement (cf. [34]). Coverage bias is concerned with the visibility of topics or entities, such as a person or country, in media coverage. Gatekeeping bias, also called selection bias or agenda bias, relates to which stories media outlets select or reject for reporting. Statement bias, also called presentation bias, is concerned with how articles choose to report on concepts.",
            "type": "explicit"
          },
          {
            "term": "gatekeeping bias",
            "definition": "selection bias or agenda bias, related to which stories media outlets select or reject for reporting",
            "context": "A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement (cf. [34]). Coverage bias is concerned with the visibility of topics or entities, such as a person or country, in media coverage. Gatekeeping bias, also called selection bias or agenda bias, relates to which stories media outlets select or reject for reporting. Statement bias, also called presentation bias, is concerned with how articles choose to report on concepts.",
            "type": "explicit"
          },
          {
            "term": "statement bias",
            "definition": "presentation bias, concerned with how articles choose to report on concepts",
            "context": "A second definition of media bias that is commonly used distinguishes between three types: coverage, gatekeeping, and statement (cf. [34]). Coverage bias is concerned with the visibility of topics or entities, such as a person or country, in media coverage. Gatekeeping bias, also called selection bias or agenda bias, relates to which stories media outlets select or reject for reporting. Statement bias, also called presentation bias, is concerned with how articles choose to report on concepts.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Numerous definitions of sustainability exist, but none apply for all circumstances. Making sustainability operational rather than vague and theoretical is a challenge, which is made all the more daunting due to the numerous definitions of sustainability. In theory, sustainability can be defined as \"enduring in perpetuity\". However, such a definition is neither practical nor useful. Little if anything can be sustained forever, while much can be sustained over the very short term. A timescale of 50 to 100 years, representing two to four generations, is often viewed as more practical. A simple definition sustainability was stated by Ehrenfeld, who defined sustainability as \"the possibility that human and other forms of life will flourish on the planet forever\". Although this definition incorporates environmental and societal aspects, the timescale is impractical. Taking a more technical perspective, sustainability can be defined in terms of carrying capacity, which indicates the maximum number of people that can be supported in a given area, accounting for availability of resources and the ability of the environment to accept waste emissions. The carrying capacity for a region is in particular dependent on the demand and supply of natural resources. But sustainability encompasses more than technical factors and cannot be defined from an environmental perspective exclusively. Rather, sustainability is often conceptually defined as having three dimensions: environmental, economic and social. An important feature of this approach is the extension of sustainability beyond carrying capacity to include economic and social fac- tors. A multidimensional view of sustainability is consistent with the understanding of how several factors affect whether societies thrive or decline. But these three dimensions are often in tension (e.g., environmental and social sustainability may be achieved to the detriment of economic sustainability). Achieving a sustainable balance is challenging. Sustainability, and its three main dimensions, can be conceptually illustrated in various ways: a) Sustainability can be viewed as founded or supported by three legs or pillars, representing environmental, economic and social sustainability (fig. 1). This is akin to a three-leg table that can only remain standing if environmental, economic and social sustainability dimensions are satisfied. This balance can prove challenging, since the sustainability dimensions are often in conflict; b) Sustainability can also can be thought of as the common point of overlap of three intersecting circles, where the three circles represent the environmental, economic and social dimensions of sustainability (fig. 2); c) A variation on this representation displays sustainability using concentric circles (fig. 3). Economic sustainability, being an activity of people and occurring within communities, is seen to be a subset of social sustainability, which involves many social and cultural dimensions as well as economic factors. Then social sustainability is seen to be a subset of environmental sustainability, since activities of people and societies are carried out within the environment and thus threaten environmental sustainability; d) The three dimensions of sustainability can be also shown in a hierarchal structure, where the economy is a subsystem of society, and society is embedded in the environment (fig. 4). These illustrations are similar, but have subtle differences. All of the illustrations demonstrate the interconnected nature of the environment, the economy and society, and their relations to and effects on sustainability for any a system or process. The above points also demonstrate the truly multidisciplinary nature of sustainability, and its linkages to diverse fields such as science, engineering, environment, ecology, economics, business, sociology and philosophy. Addressing sustainability requires consideration of topics such as resource use (energy and material), economic development, social development, health, environmental stewardship, engineering, design and architecture. It also requires an understanding of how people in the different disciplines interact and relate in addressing these factors.     Sustainability concepts can be focused around a discipline when needed or useful. For example, consider sustainability in engineering, which constitutes the application of science and mathematics principles for practical purposes such as the design, manufacturing and operation of products and processes, while accounting for economic, environmental, sociological and other constraints. Engineering significantly affects economic development, standards of living, societal well-being, the environment and cultural development. Numerous factors need to be addressed in moving towards engineering sustainability (fig. 5), including appropriate selection of resources bearing in mind sustainability criteria, use of sustainable engineering processes, enhancement of efficiency of engineering processes and resource use, and holistic adoption of environmental stewardship in engineering activities. In moving towards engineering sustainability, other key sustainability measures also need to be addressed, such as economics, equity, land use, lifestyle, sociopolitical factors and population. The multidimensional nature of sustainability suggests that achieving it is a systems challenge. That is because the overall in- teraction between subsystems needs to be integrated to achieve a sustainable system or process. For clarity, the three main dimensions of sustainability are each examined below.",
        "reasoning": "The section discusses various definitions and perspectives of sustainability, including theoretical, practical, technical, and multidimensional views. It also explores the interdisciplinary nature of sustainability and its key components.",
        "extracted_terms": [
          {
            "term": "sustainability",
            "definition": "enduring in perpetuity",
            "context": "Numerous definitions of sustainability exist, but none apply for all circumstances. Making sustainability operational rather than vague and theoretical is a challenge, which is made all the more daunting due to the numerous definitions of sustainability. In theory, sustainability can be defined as \"enduring in perpetuity\". However, such a definition is neither practical nor useful. Little if anything can be sustained forever, while much can be sustained over the very short term. A timescale of 50 to 100 years, representing two to four generations, is often viewed as more practical.",
            "type": "explicit"
          },
          {
            "term": "sustainability",
            "definition": "the possibility that human and other forms of life will flourish on the planet forever",
            "context": "A simple definition of sustainability was stated by Ehrenfeld, who defined sustainability as \"the possibility that human and other forms of life will flourish on the planet forever\". Although this definition incorporates environmental and societal aspects, the timescale is impractical.",
            "type": "explicit"
          },
          {
            "term": "sustainability",
            "definition": "the maximum number of people that can be supported in a given area, accounting for availability of resources and the ability of the environment to accept waste emissions",
            "context": "Sustainability can be defined in terms of carrying capacity, which indicates the maximum number of people that can be supported in a given area, accounting for availability of resources and the ability of the environment to accept waste emissions. The carrying capacity for a region is in particular dependent on the demand and supply of natural resources.",
            "type": "explicit"
          },
          {
            "term": "sustainability",
            "definition": "having three dimensions: environmental, economic and social",
            "context": "Sustainability is often conceptually defined as having three dimensions: environmental, economic and social. An important feature of this approach is the extension of sustainability beyond carrying capacity to include economic and social factors.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Rationalization techniques come the closest to this goal because they are built on natural language explanations (NLEs). NLEs are intuitive and human comprehensible because they are simply descriptive text. The textual information can be easily understood and translated into other languages if needed. Across all of the NLP tasks discussed in Section 4, we have seen the benefits of NLEs and the accessibility it provides to the nonexpert. We believe there is a critical need to focus on explainability techniques with NLEs. Considering these factors, we propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI. Further, rationality should not be confused or used interchangeably with the general AI term of a rational agent Russell and Norvig (2002). These are distinct topics with similar names. In this survey, RAI and rationality are purely considered in the context of model explainability. We also have not seen any usage or previous definitions of RAI within this context. We compare rationality to the other fields shown in Figure 5. Models with interpretability are interpretable, while those with explainability are interpretable and complete, as described in Section 1. Models with rationality are interpretable and complete and can rationalize their behavior through an NLE. The explainability methods described earlier in this subsection explain, but they do not justify in a way that is accessible and comprehensible to the nonexpert. In recent years, language models have become powerful and incredibly good at language generation tasks, but we have yet to see their full potential. As they continue to grow exponentially, we predict this is the beginning of explainability techniques using NLEs. The intuition behind RAI is that rationalization is one such technique, and many are yet to be developed. This calls for a more organized field to improve research focus and the need for RAI to exist.",
        "reasoning": "I will look for terms that are explicitly defined within this section, focusing on concepts related to natural language explanations and AI techniques. I must pay special attention to the definition of \"Rational AI\" and any related terms that are clearly defined in this context.",
        "extracted_terms": [
          {
            "term": "Rational AI",
            "definition": "a field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output.",
            "context": "We propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "The section describes background knowledge for the core concepts of terminology extraction. We first describe and define the two important concepts of \"terminology\" and \"domain\" and then describe the characteristic related to terminology.",
        "reasoning": "The section introduces the concepts of \"terminology\" and \"domain\", providing explicit definitions for each. The terms are then associated with their characteristic descriptions, but these do not constitute definitions and are therefore excluded from the extracted terms.",
        "extracted_terms": [
          {
            "term": "terminology",
            "definition": "the systematic analysis and categorization of terms used in a specific domain or field.",
            "context": "The section describes and defines the two important concepts of 'terminology' and 'domain' and then describes the characteristic related to terminology.",
            "type": "explicit"
          },
          {
            "term": "domain",
            "definition": "a specific field or area of study to which terminology and its characteristics pertain.",
            "context": "We first describe and define the two important concepts of 'terminology' and 'domain' and then describe the characteristic related to terminology.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "4d65bab6f9b1fd61885fe187e6014a89592df187",
        "sections": [
          "Openness and transparency have long been considered key pillars of the scientific ethos (Merton 1973). Yet there is growing awareness that current research practices often deviate from this ideal, and can sometimes produce misleading bodies of evidence (Miguel et al. 2014). As we survey in this article, there is growing evidence documenting the prevalence of publication bias in economics and other scientific fields, as well as specification searching, and widespread inability to replicate empirical findings. Though peer review and robustness checks aim to reduce these problems, they appear unable to solve the problem entirely. While some of these issues have been widely discussed within economics for some time (for instance, see Leamer 1983;Dewald, Thursby, and Anderson 1986;DeLong and Lang 1992), there has been a notable recent flurry of activity documenting these problems, and also generating new ideas for how to address them. The goal of this piece is to survey this emerging literature on research transparency and reproducibility, and synthesize the insights emerging in economics as well as from other fieldsawareness of these issues has also recently come to the fore in political science (Gerber, Green, and Nickerson 2001;Franco, Malhotra, and Simonovits 2014), psychology (Simmons, Nelson, and Simonsohn 2011;Open Science Collaboration 2015), sociology (Gerber and Malhotra 2008a), finance (Harvey, Liu, and Zhu 2015), and other research disciplines as well, including medicine (Ioannidis 2005). We also discuss productive avenues for future work. With the vastly greater computing power of recent decades and the ability to run a nearly infinite number of regressions (Sala-i-Martin 1997), there is renewed concern that nullhypothesis statistical testing is subject to both conscious and unconscious manipulation. At the same time, technological progress has also facilitated various new tools and potential solutions, including by streamlining the online sharing of data, statistical code, and other research materials, as well as the creation of easily accessible online study registries, data repositories, and tools for synthesizing research results across studies. Data-sharing and replication activities are certainly becoming more common within economics research. Yet, as we discuss below, the progress to date is partial, with some journals and fields within economics adopting new practices to promote transparency and reproducibility and many others not (yet) doing so. The rest of the paper is organized as follows: Section 2 focuses on documenting the problems, first framing them with a simple model of the research and publication process (subsection 2.1), then discussing publication bias (subsection 2.2), specification searching (subsection 2.3), and the inability to replicate results (subsection 2.4). Section 3 focuses on possible solutions to these issues: improved analytical methods (subsection 3.1), study registration (subsection 3.2) and pre-analysis plans (subsection 3.3), disclosure and reporting standards (subsection 3.4), and open data and materials (subsection 3.5). Section 4 discusses future directions for research as well as possible approaches to change norms and practices, and concludes.",
          "Multiple problems have been identified within the body of published research results in economics. We focus on three that have come under greater focus in the recent push for transparency: publication bias, specification searching, and an inability to replicate results. Before describing them, it is useful to frame some key issues with a simple model.",
          "A helpful model to frame some of the issues discussed below was developed in the provocatively titled \"Why Most Published Research Findings Are False\" by Ioannidis (2005), which is among the most highly cited medical research articles from recent years. Ioannidis develops a simple model that demonstrates how greater flexibility in data analysis may lead to an increased rate of false positives and thus incorrect inference. Specifically, the model estimates the positive predictive value (PPV) of research, or the likelihood that a claimed empirical relationship is actually true, under various assumptions. A high PPV means that most claimed findings in a literature are reliable; a low PPV means the body of evidence is riddled with false positives. The model is similar to that of Wacholder et al. (2004), which estimates the closely related false positive report probability (FPRP). 2For simplicity, consider the case in which a relationship or hypothesis can be classified in a binary fashion as either a \"true relationship\" or \"no relationship\". Define Ri as the ratio of true relationships to no relationships commonly tested in a research field i (e.g., development economics). Prior to a study being undertaken, the probability that a true relationship exists is thus Ri/(Ri+1). Using the usual notation for statistical power of the test (1-Œ≤) and statistical significance level (Œ±), the PPV in research field i is given by: (eqn. 1) PPVi = (1-Œ≤)Ri‚ÅÑ((1-Œ≤)Ri + Œ±) Clearly, the better powered the study, and the stricter the statistical significance level, the closer the PPV is to one, in which case false positives are largely eliminated. At the usual significance level of Œ± = 0.05 and in the case of a well-powered study (1 -Œ≤=0.80) in a literature in which half of all hypotheses are thought to be true ex ante (Ri = 0.5), the PPV is relatively high at 89%, a level that would not seem likely to threaten the validity of research in a particular economics subfield. However, reality is considerably messier than this best case scenario and, as Ioannidis describes, this could lead much high rates of false positives in practice due to the presence of underpowered studies, specification searching and researcher bias, and the possibility that only a subset of the analysis in a research literature is published. We discuss these extensions in turn. We start with the issue of statistical power. Doucouliagos and Stanley (2013), Doucouliagos, Ioannidis, and Stanley (2016), and others have documented that many empirical economics studies are actually quite underpowered. With a more realistic level of statistical power for many studies, say at 0.50, but maintaining the other assumptions above, the PPV falls to 83%, which is beginning to potentially look like more of a concern. For power = 0.20, fully 33% of statistically significant findings are false positives. This concern, and those discussed next, are all exacerbated by bias in the publication process. If all estimates in a literature were available to the scientific community, researchers could begin to undo the concerns over a low PPV by combining data across studies, effectively achieving greater statistical power and more reliable inference, for instance, using meta-analysis methods. However, as we discuss below, there is growing evidence of a pervasive bias in favor of significant results, in both economics and other fields. If only significant findings are ever seen by the researcher community, then the PPV is the relevant quantity for assessing how credible an individual result is likely to be. Ioannidis extends the basic model to account for the possibility of what he calls researcher bias. Denoted by u, researcher bias is defined as the probability that a researcher presents a non-finding as a true finding, for reasons other than chance variation in the data. This researcher bias could take many forms, including any combination of specification searching, data manipulation, selective reporting, and even outright fraud; below we attempt to quantify the prevalence of these behaviors among researchers. There are many checks in place that attempt to limit this bias, and through the lens of empirical economics research, we might hope that the robustness checks typically demanded of scholars in seminar presentations and during journal peer review manage to keep the most extreme forms of bias in check. Yet we believe most economists would agree that there remains considerable wiggle room in the presentation of results in practice, in most cases due to behaviors that fall far short of outright fraud. Extending the above framework to incorporate the researcher bias term (ui) in field i leads to the following expression: (eqn. 2) PPVi = ((1 -Œ≤)Ri + uiŒ≤Ri)/( (1 -Œ≤)Ri + Œ± + uiŒ≤Ri + ui(1-Œ±)) Here the actual number of true relationships (the numerator) is almost unchanged, though there is an additional term that captures the true effects that are correctly reported as significant only due to author bias. The total number of reported significant effects could be much larger due to both sampling variation and author bias. If we go back to the case of 50% power, Ri = 0.5, and the usual 5% significance level, but now assume that author bias is low at 10%, the PPV falls from 83% to 65%. If 30% of authors are biased in their presentation of results, the PPV drops dramatically to 49%, meaning that nearly half of reported significant effects are actually false positives. In a further extension, Ioannidis examines the case where there are ni different research teams in a field i generating estimates to test a research hypothesis. Once again, if only the statistically significant findings are published, so there is no ability to pool all estimates, then the likelihood that any published estimate is truly statistically significant can again fall dramatically. In Table 1 (a reproduction of Table 4 from Ioannidis (2005), we present a range of parameter values and the resulting PPV. Different research fields may have inherently different levels of the Ri term, where presumably literatures that are in an earlier stage and thus more exploratory presumably have lower likelihoods of true relationships. This simple framework brings a number of the issues we deal with in this article into sharper relief, and contains a number of lessons. Ioannidis (2005) himself concludes that the majority of published findings in medicine are likely to be false, and while we are not prepared to make a similar claim for empirical economics research -in part because it is difficult to quantify some of the key parameters in the model -we do feel that this exercise does raise important concerns about the reliability of findings in many literatures. First off, literatures characterized by statistically under-powered (i.e., small 1-Œ≤) studies are likely to have many false positives. A study may be under-powered both because of small sample sizes, and if the underlying effect sizes are relatively small. A possible approach to address this concern is to employ larger datasets or estimators that are more powerful. Second, the hotter a research field, with more teams (ni) actively running tests and higher stakes around the findings, the more likely it is that findings are false positives. This is both due to the fact that multiple testing generates more false positives (in absolute numbers) and also because author bias (ui) may be greater when the stakes are higher. Author bias is also a concern when there are widespread prejudices in a research field, for instance, against publishing findings that contradict core theoretical concepts or assumptions. Third, the greater the flexibility in research design, definitions, outcome measures, and analytical approaches in a field, the less likely the research findings are to be true, again due to a combination of multiple testing concerns and author bias. One possible approach to address this concern is to mandate greater data sharing so that other scholars can assess the robustness of results to alternative models. Another is through approaches such as pre-analysis plans that effectively force scholars to present a certain core set of analytical specifications, regardless of the results. With this framework in mind, we next present empirical evidence from economics and other social science fields regarding the extent of some of the problems and biases we have been discussing, and then in section 3 turn to potential ways to address them.",
          "Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors. If the research community is unable to track the complete body of statistical tests that have been run, including those that fail to reject the null (and thus are less likely to be published), then we cannot determine the true proportion of tests in a literature that reject the null. Thus it is critically important to understand how many tests have been run. The term \"file drawer problem\" was coined decades ago (Rosenthal 1979) to describe this problem of results that are missing from a body of research evidence. The issue was a concern even earlier, see, for example, Sterling (1959) which warned of \"embarrassing and unanticipated results\" from type I errors if not significant results went unpublished. Important recent research by Franco, Malhotra, and Simonovits (2014) affirms the importance of this issue in practice in contemporary social science research. They document that a large share of empirical analyses in the social sciences are never published or even written up, and the likelihood that a finding is shared with the broader research community falls sharply for \"null\" findings, i.e., that are not statistically significant (Franco, Malhotra, and Simonovits 2014). Cleverly, the authors are able to look inside the file drawer through their access to the universe of studies that passed peer review and were included in a nationally representative social science survey, namely, the NSF-funded Time-sharing Experiments in the Social Sciences, or TESS3 . TESS funded studies across research fields, including in economics, e.g., Walsh, Dolfin, and DiNardo (2009) and Allcott and Taubinsky (2015), as well as political science, sociology and other fields. Franco, Malhotra, and Simonovits successfully tracked nearly all of the original studies over time, keeping track the nature of the empirical results as well as the ultimate publication of the study, across the dozens of studies that participated in the original project. They find a striking empirical pattern: studies where the main hypothesis test yielded null results are 40 percentage points less likely to be published in a journal than a strongly statistically significant result, and a full 60 percentage points less likely to be written up in any form. This finding has potentially severe implications for our understanding of findings in whole bodies of social science research, if \"zeros\" are never seen by other scholars, even in working paper form. It implies that the positive predictive value (PPV) of research is likely to be lower than it would be otherwise, and also has negative implications for the validity of meta-analyses, if null results are not known to the scholars attempting to draw broader conclusions about a body of evidence. Figure 1 reproduces some of the main patterns from Franco, Malhotra, and Simonovits (2014), as described in (Mervis 2014b). Consistent with these findings, other recent analyses have documented how widespread publication bias appears to be in economics research. Brodeur et al. (2016)  (i.e., above 1.96 for t-statistics). This is a strong indication that some combination of author bias and publication bias is fairly common. Using a variety of possible underlying distributions of test statistics, and estimating how selection would affect these distributions, they estimate the residual (\"the valley and the echoing bump\") and conclude that between 10 to 20% of marginally significant empirical results in these journals are likely to be unreliable. They also document that the proportion of misreporting appears to be lower in articles without \"eye-catchers\" (such as asterisks in tables that denote statistical significance), as well as in papers written by more senior authors, including those with tenured authors. A similar pattern strongly suggestive of publication bias also appears in other social science fields including political science, sociology, psychology, as well as in clinical medical research. Gerber and Malhotra (2008a) have used the caliper test, which compares the frequency of test statistics just above and below the key statistical significance cutoff, which is similar in spirit to a regression discontinuity design. Specifically, they compare the number of z-scores lying in the interval [1.96 -ùëãùëã%, 1.96] to the number in (1.96, 1.96 + ùëãùëã%], where X is the size of the caliper, and they examine these differences at 5%, 10%, 15%, and 20% critical values.4  These caliper tests are used to examine reported empirical results in leading sociology journals (the American Sociological Review, American Journal of Sociology, and The Sociological Quarterly) and reject the hypothesis of no publication bias at the 1 in 10 million level (Gerber and Malhotra 2008a). Data from two leading political science journals (the American Political Science Review and American Journal of Political Science) reject the hypothesis of no publication bias at the 1 in 32 billion level (Gerber and Malhotra 2008b). Psychologists have recently developed a related tool called the \"p-curve,\" describing the density of reported p-values in a literature, that again takes advantage of the fact that if the null hypothesis were true (i.e., no effect), p-values should be uniformly distributed between 0 and 1 (Simonsohn, Nelson, and Simmons 2014a). Intuitively, under the null of no effect, a p-value <0.08 should occur 8% of the time, a p-value <0.07 occurs 7% of the time, etc., meaning a pvalue between 0.07 and 0.08, or between any other 0.01-wide interval, should occur 1% of the time. In the case of true non-zero effects, the distribution of p-values should be right-skewed (with a decreasing density), with more low values (0.01) than higher values (0.04) (Hung et al. 1997). 5 In contrast, in bodies of empirical literature suffering from publication bias, or \"phacking\" in their terminology, in which researchers evaluate significance as they collect data and only report results with statistically significant effects, the distribution of p-values would be leftskewed (assuming that researchers stop searching across specifications or collecting data once the desired level of significance is achieved). To test whether a p-curve is right or left skewed, one can construct what the authors call a \"pp-value,\" or p-value of the p-value-the probability of observing a significant p-value at least as extreme if the null were true-and then aggregate the pp-values in a literature with Fisher's method and test for skew with a œá 2 test. The authors also suggest a test of comparing whether a p-curve is flatter than the curve that would result if studies were (somewhat arbitrarily) powered at 33%, and interpret a p-curve that is significantly flatter or left skewed than this as lacking in evidentiary value. The p-curve can also potentially be used to correct effect size estimates in literatures suffering from publication bias; corrected estimates of the \"choice overload\" literature exhibit a change in direction from standard published estimates (Simonsohn,   Nelson, and Simmons 2014b). 6   Thanks to the existence of study registries and ethical review boards in clinical medical research, it is increasingly possible to survey nearly the universe of studies that have been undertaken, along the lines of Franco, Malhotra, and Simonovits (2014). Easterbrook et al. (1991) reviewed the universe of protocols submitted to the Central Oxford Research Ethics Committee, and both Turner et al. (2008) and Kirsch et al. (2008) employ the universe of tests of certain anti-depressant drugs submitted to the FDA, and all found significantly higher publication rates when tests yield statistically significant results. Turner et al. found that 37 of 38 (97%) of trials with positive, i.e., statistically significant, results were published, while only 8 of 24 (33%) with null (or negative) results were published; for a meta-meta-analysis of the latter two studies, see Ioannidis (2008). A simple model of publication bias described in McCrary, Christensen, and Fanelli (2015) suggests that, under some relatively strong assumptions regarding the rate of nonpublication of statistically non-significant results, readers of research studies could potentially adjust their significance threshold to \"undo\" the distortion by using a more stringent t-test statistic of 3.02 (rather than 1.96) to infer statistical significance at 95% confidence. They note that approximately 30% of published test statistics in the social sciences fall between these two cutoffs. It is also possible that this method would break down and result in a \"t-ratio arms race\" if all researchers were to use it, so it is mostly intended for illustrative purposes. As an aside, it is also possible that publication bias could work against rejection of the null hypothesis in some cases. For instance, within economics in cases where there is a strong theoretical presumption among some scholars that the null hypothesis of no effect is likely to hold (e.g., in certain tests of market efficiency) the publication process could be biased by a preference among editors and referees for non-rejection of the null hypothesis of no effect. This complicates efforts to neatly characterize the nature of publication bias, and may limit the application of the method in McCrary, Christensen, and Fanelli (2015). Taken together, a growing body of evidence indicates that publication bias is widespread in economics and many other scientific fields. Stepping back, these patterns do not appear to occur by chance, but are likely to indicate some combination of selective editor (and referee) decision-making, the file-drawer problem alluded to above, and/or widespread specification searching (the focus of the next subsection), which is closely related to what the Ioannidis (2005) model calls author bias.",
          "Scholars working in several specific literatures within economics have argued for the presence of considerable publication bias, including labor economics literatures on minimum wage impacts and on the value of a statistical life, and we discuss both briefly here, as well as several other bodies of evidence in economics. Card and Krueger (1995) conducted a meta-analysis of the minimum wage and unemployment literature, and test for the \"inverse-square-root\" relationship between sample size and t-ratio that one would expect if there was a true effect and no publication bias, since larger samples should generally produce more precise estimates (for a given research design). 7 They find that t-statistics from the 15 studies using quarterly data available at the time of writing are actually negatively correlated with sample sizes. A possible explanation is that a structural change in the effect of the minimum wage (a decline over time) has taken place, but the authors consider publication bias and specification searching a more likely explanation. Neumark and Wascher (1998) construct an alternative test for publication bias, which produces an attenuation of the effect size with larger sample sizes (as sample sizes increased over time) that is qualitatively similar to that in Card and Krueger (1995), but Neumark and Wascher thus place more emphasis on the structural change explanation (i.e., actual effects declined over time) and Several studies have also documented the presence of publication bias in the literature estimating the value of a statistical life (VSL). As government regulations in health, environment and transportation are frequently based on this value, accurate estimation is of great public importance, but there is growing consensus that there is substantial publication bias in this literature, leading to a strong upward bias in reported estimates (Ashenfelter and Greenstone 2004). Using the collection of 37 studies in Bellavance, Dionne, and Lebeau (2009), Doucouliagos, Stanley, and Giles (2012) find that correcting for publication bias reduces the estimates of VSL by 70-80% from that produced by a standard meta-analysis regression. Similar analysis shows that, correcting for publication bias, the VSL also appears largely inelastic to individual income (Doucouliagos, Stanley, and Viscusi 2014). An updated analysis of publication bias in the VSL literature (Viscusi 2015) shows that although publication bias is large and leads to meaningfully inflated estimates, he argues much of it may stem from early studies in the literature that used voluntary reporting of occupational fatalities, while more recent studies estimates employing the Census of Fatal Occupational Injuries (CFOI) suffer from less measurement error and tend to produce larger estimates. Evidence for publication bias has been documented in many other economics research literatures, although not in all. See Longhi, Nijkamp, and Poot (2005) and Knell and Stix (2005) for notable examples. Table 2 describes a number of related publication bias studies that might be of interest to readers, but for reasons of space they are not discussed in detail here. In the most systematic approach to date (to our knowledge), (Doucouliagos and Stanley 2013) carry out a meta-meta-analysis of 87 meta-analysis papers (many of which are reported in Table 2), and find that over half of the literatures suffer from \"substantial\" or \"severe\" publication bias, with particularly large degrees of bias in empirical macroeconomics and in empirical research based on demand theory, and somewhat less publication bias in subfields with multiple contested economic theories. The Journal of Economic Surveys has published many meta-regression papers, including a special issue devoted to meta-regression and publication bias (Roberts 2005). The statistical techniques for assessing publication bias are summarized in Stanley (2005), and many of these are applied in the articles listed in Table 2. One common data visualization approach is the use of funnel graphs, see Stanley and Doucouliagos (2010), Light and Pillemer (1984), and our discussion in section 3 below.",
          "Another important issue related to publication bias and null hypothesis testing is the reporting of the magnitude of effect sizes. Although it appears that economics may fare somewhat better than other social science disciplines in this regard, since economics studies typically report regression coefficients and standard errors while articles in some other disciplines (e.g., psychology) have historically only reported p-values, there is some evidence that under-reporting of effect magnitudes is still a concern. In a review in this Journal, McCloskey and Ziliak (1996) find that 70% of full-length American Economic Review articles did not distinguish between statistical and practical significance. Follow-up reviews in 2004 and 2008 conclude that the situation had not meaningfully improved (Ziliak and McCloskey 2004;Ziliak and McCloskey 2008). DeLong and Lang (1992) is an early contribution that addresses the issue of publication of null findings and effect sizes. They show that only 78 of 276 null hypotheses tested in empirical papers published in leading economics journals at the time were not rejected. However, using the uniform distribution of p-values under a true null hypothesis, and the startling lack of published p-values close to 1, they conclude it is likely that practically all economic hypotheses are indeed false. They also conclude that the null results that actually do get published in journals may also result from publication bias: a null result is arguably more interesting if it contradicts previous statistically significant results. DeLong and Lang go on to suggest that since almost all economic hypotheses are false, empirical evidence should pay more attention to practical significance and effect size rather than statistical significance alone, as is too often the case.",
          "While publication bias implies a distortion of a body of multiple research studies, bias is also possible within any given study (for instance, as captured in the author bias term u in Ioannidis (2005)). In the 1980's and 1990's, expanded access to computing power led to rising concerns that some researchers were carrying out growing numbers of analyses and selectively reporting econometric analysis that supported pre-conceived notions -or were seen as particularly interesting within the research community -and ignoring, whether consciously or not, other specifications that did not. One the most widely cited articles from this period is Leamer's (1983), \"Let's Take the Depending on which set of control variables, or covariates, were included (among state median income, unemployment, percent population nonwhite, percent population 15-24 years old, percent male, percent urban, percent of two-parent households, and several others), the main coefficient of interest-the number of murders estimated to be prevented by each executionranges widely on both sides of zero, from 29 lives saved to 12 lives lost. Of the five ways of classifying variables as important or doubtful that Leamer evaluated, three produced a range of estimates that included zero, suggesting that inference was quite fragile in this case. Leamer's recommendation that observational studies employ greater sensitivity checks, or extreme bounds analysis (EBA), was not limited to testing the effect of including different combinations of covariates, as in Leamer (1983). More detailed descriptions of EBA in Leamer (1978) and Leamer and Leonard (1983)  This flexibility was highlighted in Lovell (1983), who shows that with a few assumptions regarding the variance of the error terms, searching for the best k of c explanatory variables means that a coefficient that appears to be significant at the level ùõºùõº ÔøΩ is actually only significant at the level 1 -(1 -ùõºùõº ÔøΩ) ùëêùëê/ùëòùëò . In the case of ùëòùëò = 2 and 5 candidate variables, this risks greatly overstating significance levels, and the risk is massive if there are, say, 100 candidate variables. Lovell (1983) goes on to argue for the same sort of transparency in analysis as Leamer (1983). Denton (1985) expands on Lovell's work and shows that data mining can occur as a collective phenomenon even if each individual researcher tests only one pre-stated hypothesis, if there is selective reporting of statistically significant results, an argument closely related to the file drawer publication bias discussion above (Rosenthal 1979). Related points have been made in other social science fields in recent years. In psychology, Simmons, Nelson, and Simonsohn \"prove\" that listening to the Beatles' song \"When I'm Sixty-Four\" made listeners a year and a half younger (Simmons, Nelson, and Simonsohn 2011). The extent and ease of this \"fishing\" in analysis is also described in political science by Humphreys, Sierra, and Windt (2013)  The greater use of extra robustness checks in applied economics is designed to limit the extent of specification search, and is a shift in the direction proposed by Leamer (1983), but it is unclear how effective these changes are in reducing bias in practice. As noted above, the analysis of 641 articles from three top economics journals in recent years presented in Brodeur et al. (2016) still shows a disturbing two-humped distribution of p-values, with relatively few p-values between 0.10 and 0.25 and far more just below 0.05. Their analysis also explores the correlates behind this pattern, and finds that this apparent misallocation of p-values just below the accepted statistical significant level was less pronounced for articles written by tenured authors, and tentatively find it less pronounced among studies based on randomized controlled trials (suggesting that improved research design itself may partially constrain data mining), but they did not detect any discernible differences in the pattern based on whether the authors had publicly posted the study's replication data in the journal's public archive.",
          "One area of analytical flexibility that appears particularly important in practice is sub-group analysis. In many cases, there are multiple distinct interaction effects that could plausibly be justified by economic theory, and current datasets have a growing richness of potential covariates. Yet it is rare for applied economics studies to mention how many different interaction effects were tested, increasing the risk that only statistically significant false positives are reported. While there are few systematic treatments of this issue in economics, there has been extensive discussion of this issue within medical research, where the use of non-prespecified subgroup analysis is strongly frowned upon. The FDA does not use subgroup analysis in its drug approval decisions (Maggioni et al. 2007). An oft repeated, and humorous, case comes from a trial of aspirin and streptokinase use after heart attacks conducted in a large number of patients (N=17,187). Aspirin and streptokinase were found to be beneficial, except for patients born under Libra and Gemini, for whom there was a harmful (but not statistically significant) effect (ISIS-2 COLLABORATIVE GROUP 1988). The authors included the zodiac subgroup analysis because journal editors had suggested that 40 subgroups be analyzed, and the authors relented under the condition that they could include a few subgroups of their own choosing to demonstrate the unreliability of such analysis (Schulz and Grimes 2005).",
          "",
          "There have been longstanding concerns within economics over the inability to replicate the results of specific published papers. The pioneering example is a project undertaken by the Journal of Money, Credit, and Banking (JMCB) (Dewald, Thursby, and Anderson 1986). The journal launched the JMCB Data Storage and Evaluation Project with NSF funding in 1982, which requested data and code from authors who published papers in the journal. 9 Despite the adoption of an explicit policy of data sharing by the JMCB during the project, only 78% of authors provided data within six months after multiple requests, although this was certainly an improvement over the 34% data sharing rate in the control group, namely, those who published before the new journal policy went into effect. Of the papers that were still under review by the JMCB at the time of the requests for data, one quarter did not even respond to the request, despite the request coming from the same journal considering their paper. The data that was submitted was often an unlabeled and undocumented mess, a problem that has persisted with recent data sharing policies, as discussed below. Dewald, Thursby, and Anderson (1986) attempted to replicate nine empirical papers, and despite extensive assistance from the original authors, they were often unable to reproduce the papers' published results. Little changed for a long time after the publication of this landmark article. A decade later, in a follow-up piece to the JMCB Project published in the Federal Reserve Bank of St. Louis Review, Anderson and Dewald (1994)  failed to comply with the policy of providing data and code, editor Ben Bernanke made the data and code sharing policy mandatory in 2004 (Bernanke 2004;McCullough 2007). The current AER data policy states: \"It is the policy of the American Economic Review to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide to the Review, prior to publication, the data, programs, and other details of the computations sufficient to permit replication. These will be posted on the AER Web site. The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\"10  In addition to all the journals published by the American Economic Association in April 201611 . Table 3 summarizes journal policies regarding data sharing, publication of replications or comments, and funding or conflict of interest disclosures at twelve of the top economics and finance journals (according to Scientific Journal Rankings). There has clearly been considerable progress along all of these dimensions over the past decade, but journal policies remain a mixed bag. Among these leading journals, most but not all now have some data sharing requirements, and are officially open to publishing papers that could be considered \"replications.\"12 There is also greater use of disclosure statements. The AER conducted a self-review and found relatively good, though still incomplete, compliance with its data sharing policy (Glandon 2010). Despite this positive self-assessment, others observers believe that much work remains to ensure greater access to replication data in economics. Recent studies document that fewer than 15 of over 150 articles in the JMCB archive could be replicated; there is typically little to no verification that the data and code submitted to journals actually generate the published results; and the majority of economics journals still have no explicit data sharing requirements ( McCullough, McGeary, and Harrison 2006;Anderson et al. 2008;McCullough 2009). The uneven nature of progress along these dimensions across economics journals is ",
          "The American Economic Association's journal data sharing policy -which has been adopted by several other journals and organizations nearly verbatim, as shown in Table 3 -allows for some exceptions, importantly, for proprietary data. In particular, the policy reads: \"The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\" In practice, this exemption is requested fairly often by empirical researchers, and the rate is increasing over time. During the past decade, the May American Economic Review Papers & Proceedings issue has featured a \"Report of the Editor\" which details the number of submissions to the journal, as well the number of papers published, those with data, and those that were granted exemptions. Figure 2 presents the percentage of papers in each issue of the AER since 2005 (when information becomes available) through 2015. A few patterns are noteworthy. First, proportion of papers that include data has risen over time, starting at roughly 60% and since increasing into the 70-80% range, capturing the shift towards empirical research in the discipline as a whole. During this period, the proportion of papers using data that received exemptions from 22 the data-sharing policy has risen rapidly, from roughly 10% to 40% over time. Thus replication data is in practice not available in practice for nearly half of all empirical papers published in the AER in recent years. There are many common sources of proprietary or otherwise non-sharable data driving this trend. One of the most common is U.S. government data. There are currently 23 Federal Statistical Research Data Centers (RDC), which provide researchers access to sensitive federal government data that cannot simply be shared publicly on a journal website, typically due to individual or corporate privacy concerns (e.g., IRS tax records). 16 We do not believe that research conducted with this data should be penalized in any way, and in fact, studies employing administrative data may be particularly valuable both intellectually and in terms of public policy decisions. However, despite the exemption from data sharing, it would still be useful for researchers (and journals) to make their work as reproducible as possible given the circumstances, for instance, by at least posting the associated statistical code and providing details about how other scholars could gain similar access to the data. Beyond government data, there are, of course, also an increasing number of proprietary datasets created by corporations or other entities that are willing to share sensitive commercial data with researchers, but not with the public at large where similar issues arise. Beyond commercially proprietary or legally restricted government data, there is also the important issue of norms regarding the sharing of original data collected by scholars themselves. Given the years of effort and funding that goes into creating an original dataset, what special intellectual property rights (if any) do scholars involved in generating data have? Economists should be aware of the incentives created by temporary monopoly rights to intellectual property, and in many ways the issues regarding original data collection are closely linked to traditional arguments around granting private patents. Such monopoly rights, even if temporary, could be socially beneficial if they help to drive the creation of innovative new data sources, such as the explosion of original new survey datasets in development economics over the past two decades. Yet we know of no empirical research that discusses the optimal length of such \"research dataset\" patents; this is an area that demands further attention, especially around the optimal length of exclusive access afforded to originators of new data.17  The increasingly common requirement to share data at the time of journal publication is a cause for concern in some fields. For example, in response to a proposal from the International Committee of Medical Journal Editors to require data sharing within six months after the publication of an article (Taichman et al. 2016) an editorial in the leading New England Journal of Medicine caused an outcry when they responded by describing those who do secondary analysis without the co-authorship and cooperation of the original data collecting author as \"research parasites\" (Longo and Drazen 2016). The journal re-affirmed their commitment to data-sharing (Drazen 2016) and published a supporting piece by Senator Elizabeth Warren (Warren 2016), but also a separate piece calling for a longer embargo period after publication: \"2 years after publication of the primary trial results and an additional 6 months for every year it took to complete the trial, with a maximum of 5 years before trial data are made available to those who were not involved in the trial\" (The International Consortium of Investigators for Fairness in Trial Data Sharing 2016). Presumably the increasing \"patent length\" here for each additional year it took to complete data collection is an attempt to reward research effort in collecting unusually rich longitudinal data. Yet these sorts of rules regarding timeframes seem quite ad hoc (to us, at least), further highlighting the need for a more serious examination of how best to balance the research community's right to replicate and extend existing research with scholars' incentives to invest in valuable original data. In political science, many journals have recently adopted policies similar to the AEA policy described above. For example, the current policy of the American Journal of Political Science states: \"In some limited circumstances, an author may request an exemption from the replication and verification policy. This exemption would allow the author to withhold or limit public access to some or all of the data used in an analysis. All other replication materials (e.g., software commands, etc.) still must be provided. The primary reasons for such exemptions are restricted access datasets and human subjects protection.\" 18 We lack data on how often this exemption is granted, however. Additionally, this journal goes much further than economics journals in one important way: instead of simply collecting and publishing data and code from authors, the editors use a third party research center (namely, the Odum Institute for Research in Social Science, at the University of North Carolina, Chapel Hill for quantitative analysis and the Qualitative Data Repository (QDR), at Syracuse University for qualitative analyses) to verify that the data and statistical code produce the published results.",
          "There given examples such as those discussed below in which there are discrepancies between the original published findings and later replication results, it remains possible that the high-profile cases of failed replication may simply be the tip of the iceberg. Thankfully, a few recent papers have begun to provide some evidence on this question, which we highlight below. We ourselves are no strangers to replication and re-analysis debates: papers by one of the authors of this article, described below, have been part of lively debates on replication and reanalysis using data that we shared publicly. These debates have led us to appreciate the great promise of replication research, as well as its potential pitfalls: exactly like original research studies, replication studies have their own particular strengths and weaknesses, and may serve to either advance the intellectual debate or could obscure particular issues. Yet there is no doubt in our minds that an overall increase in replication research will serve a critical role in establishing the credibility of empirical findings in economics, and in equilibrium, will create stronger incentives for scholars to generate more reliable results. Further complicating matters, an imprecise definition of the term \"replication\" itself often leads to confusion. A taxonomic proposal in Hamermesh (2007) distinguished between \"pure\", \"statistical\", and \"scientific\" replications, while a more recent effort (Clemens 2015) uses the terms \"verification,\" \"reproduction,\" \"reanalysis,\" and \"extension\" to distinguish between replications (the first two) and robustness exercises (the latter two). We first present some existing evidence on the replicability of economics and social science research in the next subsection, and then provide examples of each of Clemens' categories.",
          "The articles in the 1986 Journal of Money Credit and Banking project and the 1994 St. Louis Federal Reserve follow-up mentioned above provided some of the first attempts at systematic replication in economics, with fairly discouraging results. Have things improved in the last few decades? New evidence is emerging about the reliability of empirical economics research. One of the most important recent studies is Camerer et al. (2016), which repeated 18 behavioral economics lab experiments originally published between 2011 and 2014 in the American Economic Review and the Quarterly Journal of Economics to assess their replicability. Figure 3 below reproduces a summary of their findings. Their approach is similar in design to a large scale replication of 100 studies in psychology known as the \"Replication Project: Psychology\", which we discuss in detail below. The replication studies were designed with sample sizes that aimed to have 90% power to detect the original effect size at the 5% significance level. In all, the estimated effects were statistically significant with the same sign in 11 of the 18 replication studies (61.1% Other examples of replication failures abound. Clemens (2015) provides a useful taxonomy, and we provide an example of from each of the categories there to help distinguish between them, namely the two types of replication he discusses (verification and reproduction), and the two types of robustness exercises (reanalysis and extension). Of course not all papers fit easily into one of these categories as most tend to include elements from multiple categories.",
          "Perhaps the most straightforward type of replication in economics involves using the same specification, the same sample, and the same population. Essentially, this is running the same code on the same data and testing if you get the same results. Hamermesh (2007) referred to this as a \"pure replication.\" We believe this basic standard should be expected of all published economics research, and hope this expectation is universal among researchers. One tiny tweak to the definition of verification is that it also includes errors in coding. If an author describes a statistical test in the paper, but the code indisputably does not correctly carry out the test as described, this is also considered a verification failure. One of the earliest cases of quantitative economics research failing a verification test comes from an investigation of the effect of social security on private savings. Feldstein (1974) estimates a life cycle model showing that social security reduces private savings by as much as 50%. There were significant theoretical challenges to carrying out this exercise related to assumptions about the intergenerational transfer of wealth, but Leimer and Lesnoy (1982) discovered that a flaw in Feldstein's computer program that overestimated the growth rate of Social Security wealth for widows led to larger effects of Social Security wealth than when the mistake was corrected. Feldstein replied to the critique saying he was grateful for having the error corrected, but that the central conclusion of the study remains largely unchanged (namely, that Social Security decreased private savings by 44%) (Feldstein 1982). Much of the change in coefficients in the replication exercise resulted from Leimer and Lesnoy including an expanded time series of data-this is not a failure of verification, but rather an extension, which we discuss below.  3). 20 In many (but not all) cases discussed in Clemens, the original authors clearly admit to the failure of verification, but there is vigorous, and we think, healthy scholarly debate about how important those mistakes are and whether the results are still significant -statistically and/or practicallywhen the code or data are corrected. Of course, authors whose papers are subject to replication debates should be commended for providing other scholars with access to their data and code public in the first place, especially for these earlier articles published before journal data-sharing requirements were established.",
          "The other type of replication in Clemens' taxonomy is a reproduction. This approach uses the same analytical specification and the same population, but a different sample. Hamermesh (2007) refers to this as a statistical replication. In economics, this approach would be exhibited in a study that generated a certain set of results using a 5% sample of the census, while a different 5% census sample produced different results, or an experimental economics lab study that produced one set of results with a certain sample while the reproduction study analyzed a different sample from broadly the same population (e.g., U.S. university students). There is, of course, some  2007), the possibility is discussed that one factor contributing to lack of verification is that intermediary datasets constructed from raw data were over-written when the raw data was updated, as sometimes happens with U.S. government data. The work of one of the authors of this paper could be included on this list; see Miguel and Kremer (2004), Aiken et al. (2015) and Hicks, Kremer and Miguel (2015) on the impact of school-based deworming in Kenya. Camerer et al. (2016) could be better classified as failures of reproduction, as long as the samples were in fact collected in broadly the same manner (i.e., in person vs. online). Reproduction failures are perhaps more precisely defined in the hard sciences where experimenters routinely attempt to do the exact same physical process as another lab, albeit with a different sample of molecules, or in the biological sciences where experiments may employ a different sample of animal subjects. For instance, in defining reproduction, Clemens mentions the infamous case of the \"discovery\" of cold fusion by Fleischmann and Pons (1989), which failed to reproduce in Lewis et al. (1989).",
          "Robustness exercises come in two varieties, reanalysis and extensions. Reanalysis uses a different analytical specification on the same population (with either the same or a different sample). Many economics replication studies include both a verification aspect as well as some re-analysis. For instance, Davis (2013) conducts a successful verification of Sachs and Warner (1997), but concludes that reanalysis shows the estimates are somewhat sensitive to different statistical estimation techniques. Other well-known recent reanalysis debates in empirical economics include Miguel, Satyanath, and Sergenti (2004), Ciccone (2011) and Miguel and Satyanath (2011) on civil conflict and GDP growth using rainfall as an instrumental variable; and Acemoglu, Johnson, and Robinson (2001), Albouy (2012), and Acemoglu, Johnson, and Robinson (2012) on institutions and GDP growth with settler mortality as an instrumental variable. The debates over these and other studies makes it clear that reanalysis does not typically settle all key research questions, and the exercise often reveals that empirical economists have considerable flexibility in their analytical choices. This insight makes the development of methods to account for -and possibly constrain -this flexibility, which we discuss below in section 3, all the more important.",
          "Under Clemens' classification system, an extension uses the same analytical specification as an original study but a different population and a different sample. Most often this would be conducting the same analysis carried out in a different time or place. A well-known example of an extension involves Burnside and Dollar (2000), which showed that foreign aid seemed to be effective in increasing GDP if the recipient country was well-governed. However, using the exact same regression specification but including additional countries and years to the dataset, Easterly, Levine, and Roodman (2004) do not obtain the same result. Burnside and Dollar (2004) discuss the differences between the findings and conclude that they occur largely because of the additional countries, rather than lengthening the time series. One widely debated topic in economics that has features of both replication and robustness exercises is the topic of minimum wage impacts on unemployment. In early work, Welch (1974) concluded that early minimum wage legislation decreased teenage employment, increased the cyclicality of teenage employment with respect to the business cycle, and shifted teenage employment towards sectors not covered by the law. However, in the course of using Welch's data, Siskind (1977) discovered that Welch had used data for teenagers 16-19 years old instead of 14-19 years old for certain years, and once this was corrected, the minimum wage did not appear to reduce teenage employment. This was a fairly easy mistake to understand since the Current Population Survey was undergoing changes at the time, and table headings for unpublished data had not even been updated. Welch graciously acknowledged the error, and used the corrected data to extend the analysis to probe impacts by industry sector (Welch 1977). Scholars working on this important topic have, for several decades now, continued to find significant room for disagreement on key issues of sampling, data sources, and statistical analysis methods 21 , matters on which well-intended researchers may well disagree. In this and other similarly contentious debates, we believe that the use of pre-specified research designs and analysis plans could be useful for advancing scientific progress, a point we return to below.",
          "Though we believe (or at least, would prefer to believe) that most instances in which economics studies cannot be replicated are due to inadvertent human error or analytical judgment calls, fraud cannot be completely discounted in all cases. Popular books such as Broad and Wade's Betrayers of the Truth (1983) make it clear that scientists are not always saints. A survey of 234 economists at the 1998 ASSA/AEA meeting investigated falsification of research, inappropriate inclusion or omission of co-authors, and exchange of grades for gifts, money, or sexual favors (List et al. 2001). Both a randomization coin-toss technique to elicit true responses to sensitive questions, as well as a more standard question design, indicate that 4% of respondents admit to having at some time falsified research data, 7-10% of respondents admit to having committed one of four relatively minor research infractions, while up to 0.4% admitted to exchange of grades for gifts, money, or sexual favors. Given the seriousness of some of these offenses, an obvious concern is that these figures understate the actual incidence of fraudulent research practices. A more recent survey of members of the European Economics Association described in Necker ( 2014) asks individuals about the justifiability of certain practices as well as their behavior regarding those practices. Necker shows that 2.6% of researchers admit to having falsified data, while 94% admit to at least one instance of a practice considered inappropriate by the majority of the survey, and there is a clear positive correlation between justifiability and behavior, as well as between perceived professional publication pressures and questionable research practices. Similar surveys in other fields such as Anderson, Martinson, and Vries (2007), which surveyed researchers across disciplines funded by the U.S. National Institutes of Health, and John, Loewenstein, and Prelec (2012) in psychology, as well as a meta-analysis of 18 surveys of academic misbehavior, do not paint a very rosy picture, with 2% of respondents admitting to data fabrication, and 34% admitting to lesser forms of academic misconduct (Fanelli 2009). We are not aware of a recent case in economics that received media attention similar to the Michael Lacour fraud scandal uncovered by Broockman, Kalla, and Aranow (2015) in political science, or the case of Diedrick Stapel (see Carey 2011; Bhattacharjee 2013) in psychology. However, there is considerable evidence of plagiarism and other forms of research malpractice in economics. This Journal itself published the results of a survey sent to 470 economics journal editors, which revealed significant problems (Enders and Hoover 2004). Among the 127 editors who responded, only 19% claimed that their journal had a formal policy on plagiarism, and 42 cases of plagiarism were discovered in an average year, with nearly 24% of editors encountering at least one case. A follow-up survey of rank-and-file economists revealed a general lack of consensus on how to respond to cases of alleged plagiarism (Enders and Hoover 2006). 22Article retraction is another useful indicator of research misconduct. A search of four popular article databases for terms related to article retractions identified by Karabag and Berggren (2012)   and Stapel (2009), which was retracted in 2015 27 , clearly reads \"THIS PAPER HAS BEEN RETRACTED,\" the title has been altered to begin with \"Retracted: ,\" and the pdf features an obvious RETRACTED watermark on every page. This is also the case with all six of the retractions in Karabag and Berggren (2012), as well as other notable recent retractions such as LaCour and Green (2014), which was retracted by Marcia McNutt (2015). Innovation and New Technology for re-publication in Kyklos (Frey, Frey, and Eichenberger 1999). The most recent incident that seemed to attract significant attention was the submission of a substantively identical article to multiple journals within economics, which is also a serious lapse (\"Correspondence: David H Autor and Bruno S. Frey\" 2011). Even if plagiarism of this manner would seem significantly easier to catch in the Internet age, the proliferation of journals partially counteracts this ease. 23 https://plagiarism.repec.org/index.html 24 Although note that journals may present these policies online as opposed to formally publishing them in the journal; for instance, see the The bottom line is that there is little reason to believe that economists are inherently more ethical than other social scientists or researchers in other disciplines, so policies regarding fraud and retraction from other disciplines might potentially be beneficially applied to economics.",
          "This section discusses several new methods and tools that have emerged in economics research over the past two decades -and more forcefully over the past ten years -to address the concerns discussed in Section 2. These approaches have in common a focus on greater transparency and openness in the research process. They include improved research design (including experimental designs and meta-analysis approaches), study registration and pre-analysis plans, strengthened disclosure and reporting practices, and new norms regarding open data and materials. We discuss each in turn.",
          "There have been a number of different responses within economics to the view that pervasive specification searching and publication bias was affecting the credibility of empirical literatures. As mentioned above, there has been a shift towards a greater focus on prospective research design in several fields of applied economics work. Experimental (Duflo, Glennerster, and Kremer 2007) and quasi-experimental (Angrist and Pischke 2010) research designs arguably place more constraints on researchers relative to earlier empirical approaches, since there are natural ways to present data using these designs that researchers are typically compelled to present by colleagues in seminars and by journal referees and editors. Prospective experimental studies also tend to place greater emphasis on adequately powering an analysis statistically, which may help to reduce the likelihood of publishing only false positives (Duflo, Glennerster, and Kremer 2007). There is also suggestive evidence that the adoption of experimental and quasiexperimental empirical approaches is beginning to address some concerns about specification search and publication bias: Brodeur et al. (2016) present tentative evidence that the familiar spike in p-values just below the 0.05 level is less pronounced in randomized control trial studies than in studies utilizing non-experimental methods. Yet improved research design alone may not solve several other key threats to the credibility of empirical economics research, including the possibility that null or \"uninteresting\" findings never become known within the research community.",
          "In addition to improvements in research design, Leamer (1983) argued for greater disclosure of the decisions made in analysis, in what became known as \"extreme bounds analysis\" (described in Section 2). Research along these lines has dealt with model uncertainty by employing combinations of multiple models and specifications, as well as comparisons between them. Leamer himself has continued to advance this agenda (see Leamer 2016). We describe several related approaches here.",
          "A natural way to deal with statistical model uncertainty is through Bayesian model averaging. In this approach, each model in the space of plausible models is assigned a probability of being true based on researcher priors and goodness of fit criteria. Averaging the resulting estimates generates a statistic incorporating model uncertainty: The weighted average they generate implies a large but imprecisely estimated deterrent effect of executions on homicides in the U.S. Of course, even without employing explicit probability weights, simply visualizing the distribution of estimates across the entire space of statistical models can also be quite informative on its own. Two well-cited examples of model averaging engage in a thorough investigation of the determinants of cross-country economic growth. Sala-i-Martin's (1997) famous \"I Just Ran Two Million Regressions\" article uses model weights proportional to the integrated likelihoods of each model, picks all possible three-variable combinations out of 60 covariates that have been reported as being significantly related to economic growth, and finds that only about one-third of the 60 variables can be considered robustly positively correlated with economic growth across models. Sala-i-Martin, Doppelhofer, and Miller ( 2004) conduct what they call Bayesian Averaging of Classical Estimates (BACE), weighting estimates using an approach analogous to Schwarz's BIC, and find that just 18 of 67 variables are significantly and robustly partially correlated with economic growth, once suggesting that many findings reported in the existing empirical literature may be spuriously generated by specification searching and selective reporting.",
          "While specification searching or data mining often has a negative connotation in applied economic research, some scholars have taken a more favorable view of it, as long as the data mining is carried out appropriately (Pagan 1987;Phillips 1988). Advocates of this approach have sometimes been known as the \"LSE school\" of econometrics, or the general-to-specific modelling approach (Gilbert 1989;Hendry 1987;Hendry 1995). This approach is related in spirit to the idea of \"encompassing,\" which is the principle that one statistical model can account for, or explain, another (Mizon and Richard 1986;Bontemps and Mizon 2008). To our knowledge, this approach is more often applied in time series econometrics than in applied microeconomics, but perhaps a closer consideration is warranted. There have also been recent calls within applied economics for greater application of machine learning methods and other data science techniques that share some features with these approaches (Kleinberg, Ludwig, et al. 2015;Kleinberg, Lakkaraju, et al. 2015). For a wide set of views on data mining more broadly and the LSE approach specifically, see the Journal of Economic Methodology, which devoted a special issue to the topic (Backhouse and Morgan 2000).",
          "Simonsohn, Simmons, and Nelson (2015b) propose a method, which they call the \"specification curve,\" that is similar in spirit to Leamer's extreme-bounds analysis, but recommends researchers test the exhaustive combination of analytical decisions, not just decisions about which covariates to include in the model. If the full exhaustive set is too large to be practical, a random subset can be used. After plotting the effect size from each of the specifications, researchers can assess how much the estimated effect size varies, and which combinations of decisions lead to which outcomes. Using permutation tests (for treatment with random assignment) or bootstrapping (for treatment without random assignment), researchers can generate shuffled samples with no true effect by construction, and compare the specification curves from these placebo samples to the specification curve from the actual data. Many comparisons are possible, but the authors suggest comparing the median effect size, the share of results with predicted sign, and share of statistically significant results with predicted sign. A key comparison, which is analogous to the traditional p-value, is the percent of the shuffled samples with as many or more extreme results. Bakkensen and Larson 2014). The specification curve shows that 46% of curves from permuted data show at least as large a median effect size as the original, 16% show at least as many results with the predicted sign, and 85% show at least as many significant results with the predicted sign. This indicates that the results are likely to have been generated by chance. The Bertrand and Mullainathan (2004) specification curve, on the other hand, shows that fewer than 0.2% of the permuted curves generate as large a median effect, 12.5% of permuted curves show at least as many results with the predicted sign, and less than 0.2% of permuted curves show at least as many significant results with the predicted sign, providing evidence that the results are very unlikely to have been generated by chance.",
          "There have been significant advances in the methodological literature on quantifying the extent of publication bias in a given body of literature. Early methods mentioned above include Rosenthal's (1979) method (the \"fail-safe N\"), while Galbraith (1988) advocated for radial plots of log odds ratios, and Card and Krueger (1995) tested for relationships between study sample sizes and t-statistics. Statisticians have developed methods to estimate effect sizes in meta-analyses that control for publication bias (Hedges 1992;Hedges and Vevea 1996). The tools most widely used by economists tend to be simpler, including the widely used funnel plot, which is a scatter plot of some measure of statistical precision (typically the inverse of the standard error), versus the estimated effect size. Estimates generated from smaller samples should usually form the wider base of an inverted funnel, which should be symmetric around more precise estimates in the absence of publication bias. The method is illustrated with several economics examples in Stanley and Doucouliagos (2010), and two of these are reproduced in Figure 4. In addition to scrutinizing the visual plot, a formal test of the symmetry of this plot can be conducted using data from multiple studies and regressing the relevant t-statistics on inverse standard errors: (eqn. 4) The resulting t-test on ùõΩùõΩ 0 , referred to as the Funnel Asymmetry Test (FAT) (Stanley 2008), captures the correlation between estimated effect size and precision, and thus tests for publication bias. Using the FAT, Doucouliagos and Stanley (2009) find evidence of publication bias in Card and Krueger's (1995) sample of minimum wage studies (ùõΩùõΩ 0 ‚â† 0), consistent with their own interpretation of the published literature at that time. ùõΩùõΩ 1 here can also be interpreted as the true effect (called the precision effect test, PET) free of publication bias, and Doucouliagos and Stanley (2009) find no evidence of a true effect of the minimum wage on unemployment. The authors also conduct the FAT-PET tests with 49 additional more recent studies in this literature and find the same results: evidence of significant publication bias and no evidence of an effect of the minimum wage on unemployment. Additional meta-analysis methods, including this \"FAT-PET\" approach, are summarized in Stanley and Doucouliagos (2012).",
          "Other applied econometricians have recently called for increasing the use of multiple testing corrections in order to generate more meaningful inference in study settings with many research hypotheses (Anderson 2008;Fink, McConnell, and Vollmer 2014). The practice of correcting for multiple tests is already widespread in certain scientific fields (e.g., genetics) but has yet to become the norm in economics and other social sciences. Simply put, since we know that pvalues fall below traditional significance thresholds (e.g., 0.05) purely by chance a certain proportion of the time, it makes sense to report adjusted p-values that account for the fact that we are running multiple tests, since this makes it more likely that at least one of our test statistics has a significant p-value simply by chance. There are several multiple testing approaches, some of which are used and explained by Anderson (2008), namely, reporting index tests, controlling the family-wise error rate (FWER), and controlling the false discovery rate (FDR). These are each discussed in turn below.",
          "One option for scholars in cases where there are multiple related outcome measures is to forego reporting the outcomes of numerous tests, and instead standardize the related outcomes and combine them into a smaller number of indices, sometimes referred to as a mean effect. This can be implemented for a family of related outcomes by making all signs agree (i.e., allowing positive values to denote beneficial outcomes), demeaning and dividing by the control group standard deviation, and constructing a weighted average (possibly using the inverse of the covariance matrix to weight each standardized outcome). This new index can be used as a single outcome in a regression model and evaluated with a standard t test. Kling, Liebman, and Katz (2007) implement an early index test in the Moving to Opportunity field experiment using methods developed in biomedicine by O'Brien (1984). This method addresses some concerns regarding the multiplicity of statistical tests by simply reducing the number of tests. A potential drawback is that the index may combine outcomes that are only weakly related, and may obscure impacts on specific outcomes that are of interest to particular scholars, although note that these specific outcomes could also be separately reported for completeness.",
          "The family-wise error rate (FWER) is the probability that at least one true hypothesis in a group is rejected (a type I error, or false positive). This approach is considered most useful when the \"damage\" from incorrectly claiming any hypothesis is false is high. There are several ways to implement this approach, with the simplest method being the Bonferroni correction of simply multiplying every original p-value by the number of tests carried out (Bland and Altman 1995), although this is extremely conservative, and improved methods have also been developed. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher discount factors (Holm 1979). A related and more efficient recent method is the free step-down resampling method, developed by Westfall and Young (1993), which when implemented by Anderson (2008) implies that several highly cited experimental pre-school interventions (namely, the Abecedarian, Perry, and Early Training Project studies) exhibit few positive long-run impacts for males. Another recent method improves on Holm by incorporating the dependent structure of multiple tests. Lee and Shaikh (2014) apply it to reevaluate the Mexican PROGRESA conditional cash transfer program and find that overall program impacts remain positive and significant, but are statistically significant for fewer subgroups (e.g., by gender, education) when controlling for multiple testing. List, Shaikh, and Xu (2016) propose a method of controlling the FWER for three common situations in experimental economics, namely, testing multiple outcomes, testing for heterogeneous treatment effects in multiple subgroups, and testing with multiple treatment conditions.28 ",
          "In situations where a single type I error is not considered very costly, researchers may be willing to use a somewhat less conservative method than the FWER approached discussed above, and trade off some incorrect hypothesis rejections in exchange for greater statistical power. This is made possible by controlling the false discovery rate (FDR), or the percentage of rejections that are type I errors. Benjamini and Hochberg (1995) details a simple algorithm to control this rate at a chosen level under the assumption that the p-values from the multiple tests are independent, though the same method was later shown to also be valid under weaker assumptions (Benjamini and Yekutieli 2001). Benjamini, Krieger, and Yekutieli (2006) describes a two-step procedure with greater statistical power, while Romano, Shaikh, and Wolf (2008) propose the first methods to incorporate information about the dependence structure of the test statistics. Multiple hypothesis testing adjustments have recently been used in finance (Harvey, Liu, and Zhu 2015) to re-evaluate 316 factors from 313 different papers that explain the cross-section of expected stock returns. The authors employ the Bonferroni; Holm (1979);and Benjamini, Hochberg, and Yekutieli (2006) methods to account for multiple testing, and conclude that tstatistics greater than 3.0, and possibly as high as 3.9, should be used instead of the standard 1.96, to actually conclude that a factor explains stock returns with 95% confidence. Index tests and both the FWER and FDR multiple testing corrections are also employed in Casey, Glennerster, and Miguel (2012) to estimate the impacts of a community driven development program in Sierra Leone using a dataset with hundreds of potentially relevant outcome variables.",
          "Another method that controls for, or acts as a reality check, for data snooping or data mining was developed in White (2000). The testing of multiple hypotheses, or repeated use of the same data, is a particularly central problem with time series data used over and over again by multiple scholars, such as data on stock returns, which makes this research quite important in empirical finance. Like the model averaging approach described above, the reality check requires a researcher to estimate the entire space of plausible models, but now compares the performance of the preferred model to a benchmark model (e.g., a model for stock market predictions based on the efficient market hypothesis), and does so repeatedly with bootstrapped samples. To assess whether a certain preferred model actually outperforms the benchmark after accounting for snooping with multiple models, the econometrician first calculates the performance of the preferred model (using mean squared error improvement over the benchmark, or relative profit of the strategy). She then selects a bootstrap sample (with replacement), and calculates the mean squared error improvement (or profit) with the new sample for all of the different plausible statistical models, recording the best mean squared error improvement (or profit) across all the models. This approach can then be repeated 1,000 (or more) times, gathering the 1,000 best mean squared errors (or profits). In the final step, one must compare the original preferred model's mean squared error to the best performance from each of the 1,000 bootstraps. The p-value is the fraction of bootstrapped best fits that outperform the preferred model. (A truly predictive model would have returns higher than 95% of the best performing models from each of the bootstrapped samples.) This method was implemented on a large number of trading rules in Sullivan, Timmermann, and White (1999) and a similar method that addresses the presence of poorly performing or irrelevant alternatives was developed in Hansen (2005).",
          "A leading proposed solution to the problem of publication bias is the registration of empirical studies in a public registry. This would ideally be a centralized database of all attempts to conduct research on a certain question, irrespective of the nature of the results, and such that even null (not statistically significant) findings are not lost to the research community. Top medical journals have adopted a clear standard of publishing only medical trials that are registered (De Angelis et al. 2004). The largest clinical trial registry is clinicaltrials.gov, which helped to inspire the most high-profile study registry within economics, the AEA Randomized Controlled Trial Registry (Katz et al. 2013), which was launched in May 2013. 29   While recent research in medicine finds that the clinical trial registry has not eliminated all under-reporting of null results or other forms of publication bias and specification searching (Laine et al. 2007;Mathieu et al. 2009), they do allow the research community to quantify the extent of these problems and over time may help to constrain inappropriate practices. It also helps scholars locate studies that are delayed in publication, or are never published, helping to fill in gaps in the literature and thus resolving some of the problems identified in Franco, Malhotra, and Simonovits (2014). Though it is too soon after the adoption of the AEA's trial registry to measure its impact on research practices and the robustness of empirical results, it is worth noting that the registry is already being used by many empirical researchers: since inception in 2013, over 850 studies conducted in over 100 countries have been registered, and the pace of registrations continues to rise rapidly. allowing users to create a frozen time-stamped web URL with associated digital object identifier (DOI) for any materials uploaded to OSF. Several popular data storage options (including Dropbox, Dataverse, and GitHub) can also be synced with the OSF and its storage, creating a flexible way for researchers to register their research and materials. As of October 2016, over 7,300 public registrations have been created on OSF since the service launched in 2013.",
          "In addition to serving as a useful way to search for research findings on a particular topic, most supporters of study registration also promote the pre-registration of studies, including preanalysis plans (PAPs) that can be posted and time stamped even before analysis data are collected or otherwise available (Miguel et al. 2014). Registration is now the norm in medical research for randomized trials, and registrations often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the U.S. template may be particularly useful to researchers themselves when developing their own preanalysis plans, and instructors may find it useful in their courses. Building on, and modifying, the FDA's 1998 checklist with insights from these other recent treatments of pre-analysis plans, there appears to be a growing consensus that pre-analysis plans in economics should consider discussing at least the following list of ten issues: Pre-analysis plans are relatively new to economics, and this list is likely to evolve in the coming years as researchers explore the potential, and possible limitations, of this new tool. For those concerned about the possibility of \"scooping\" of new research designs and questions based upon a publicly posted pre-analysis plan or project description, several of the social science registries allow temporary embargoing of project details. For instance, the AEA registry allows an embargo until a specific date or project completion. At the time of writing, the OSF allows a four-year embargo until the information is made public. 31",
          "Recent examples of economics papers based on experiments with PAPs include Casey, Glennerster, and Miguel (2012) and Finkelstein et al. (2012), among others. Casey, Glennerster, and Miguel (2012) discuss evidence from a large-scale field experiment on community driven development (CDD) projects in Sierra Leone. The project, called GoBifo, was intended to make local institutions in post-war Sierra Leone more democratic and egalitarian. GoBifo funds were spent on a variety of local public goods infrastructure (e.g., community centers, schools, latrines, roads), agriculture, and business training projects, and were closely monitored to limit leakage. The analysis finds significant short-run benefits in terms of the \"hardware\" aspects of infrastructure and economic wellbeing: the latrines were indeed built. However, a larger goal of the project, reshaping local institutions, making them more egalitarian, increasing trust, improving local collective action, and strengthening community groups, which the researchers call the \"software effects,\" largely failed. There are a large number of plausible outcome measures along these dimensions, hundreds in total, which the authors analyze using a mean effects index approach for nine different families of outcomes (with multiple testing 31 See http://help.osf.io/m/registrations/l/524207-embargoes. Accessed October 10, 2016. adjustments). The null hypothesis of no impact cannot be rejected at 95% confidence for any of the nine families of outcomes. Yet Casey et al (2012) go on to show that, given the large numbers of outcomes in their dataset, and the multiplicity of ways to define outcome measures, finding some statistically significant results would have been relatively easy\". In fact, the paper includes an example of how, if they had had the latitude to define outcomes without a pre-analysis plan, as has been standard practice in most empirical economics studies (and in other social science fields), the authors could have reported either statistically significant and positive effects, or significantly negative effects, depending on the nature of the \"cherry-picking\" of results. We reproduce their results here as Table 4, where Panel A presents the statistically significant positive impacts identified in the GoBifo data and Panel B highlights negative effects. This finding begs the question: how many empirical economics papers with statistically significant results are, unbeknownst to us, really just some version of either Panel A or Panel B? This is important because, as in Casey et al (2012), the researchers tested a large number of outcomes: hospital admissions through the emergency room (ER) and not through the ER; hospital days; procedures; financial strain (bankruptcy, judgments, liens, delinquency, medical debt, and non-medical debt, measured by credit report data); self-reported health from survey data, and so on. When running such a large number of tests, the researchers again could have discovered some \"significant\" effects simply by chance. The pre-analysis plan, in conjunction with multiple hypothesis testing adjustments, give us more confidence in the main results of the study: that recipients did not improve significantly in terms of physical health measurements, but they were more likely to have health insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Additional studies that have resulted from the experiment have also employed preanalysis plans, and they show that health insurance increased emergency department use (Taubman et al. 2014), had no effect on measured physical health outcomes after two years, but did increase health care use and diabetes management, as well as leading to lower rates of depression and financial strain (Baicker et al. 2013). The health care expansion had no significant effect on employment or earnings (Baicker et al. 2014). Other prominent early examples of economics studies that have employed pre-analysis plans include poverty targeting programs in Indonesia, an evaluation of the Toms shoe company donation program, and a job training program in Turkey, among many others (Olken, Onishi, and Wong 2012;Alatas et al. 2012;Wydick, Katz, and Janet 2014;Hirshleifer et al. 2015). The PAP tool is also spreading to other social sciences beyond economics. For instance, in psychology, a pre-specified replication of an earlier paper that had found a link between female conception risk and racial prejudice failed to find a similar effect (Hawkins, Fitzgerald, and Nosek 2015). One issue that arises for studies that did register a pre-analysis plan is the question of characterizing the extent to which the analysis conforms to the original plan, or if it deviates in important ways from the plan. To appreciate these differences, scholars will need to compare the analysis to the plan, a step that could be seen as adding to the burden of journal editors and referees. Even if the analysis does conform exactly to the PAP, there is still the possibility that authors are consciously or unconsciously emphasizing a subset of the pre-specified analyses in the final study. Berge et al. (2015) develop an approach to comparing the distribution of p-values in the paper's main tables versus those in the PAP in order to quantify the extent of possibly selective reporting between the plan and the paper. The Finkelstein et al (2012) study is a model of transparency regarding the presentation of results. To the authors' credit, all analyses presented in the published paper that were not prespecified are clearly labeled as such; in fact, the exact phrase \"This analysis was not prespecified.\" appears in the paper six times. Tables in the main text and appendix that report analyses that were not pre-specified are labelled with a \"^\" character to set them apart, and are clearly labeled as such.",
          "There remain many open questions about whether, when, and how pre-analysis plans could and should be used in economics research, with open debates about how useful they are in different subfields of the discipline. Olken (2015), for example, highlights both their \"promises and perils.\" On the positive side, pre-analysis plans bind the hands of researchers and greatly limit specification searching, allowing them to take full advantage of the power of their statistical tests (even making one-sided tests reasonable). A further advantage of the use of pre-analysis plans is that they are likely to help shield researchers from pressures to affirm the policy agenda of donors and policymakers, in cases where they have a vested interest in the outcome, or when research focuses on politically controversial topics (such as health care reform). This is especially the case if researchers and their institutional partners can agree on the pre-analysis plan, as a sort of evaluation contract. On the negative side, PAPs are often complex and take valuable time to write. Scientific breakthroughs often come at unexpected times and places, often as a result of exploratory analysis, and the time spent writing PAPs may thus lead less time to spend on less structured data exploration. Coffman and Niederle (2015) argue that there is limited upside from PAPs when replication (in conjunction with hypothesis registries) is possible. In experimental and behavioral economics, where lab experiments utilize samples of locally recruited students and the costs of replicating an experiment are relatively low, they argue that replication could be a viable substitute for pre-analysis plans. Yet there does appear to be a growing consensus, endorsed by Coffman and Niederle, that pre-analysis plans can significantly increase the credibility of reporting and analysis in large-scale randomized trials that are expensive or difficult to repeat, or when a study that relies on a particular contextual factor makes it impossible to replicate. For instance, Berge et al. (2015) carry out a series of lab experiments timed to take place just before the 2013 Kenya elections. Replication of this lab research is clearly impossible due to the unique context, and thus use of a pre-analysis plan is valuable. Olken (2015) as well as Coffman and Niederle (2015) discuss another potential way to address publication bias and specification search: results-blind review. Scholars in psychology have championed this method; studies that are submitted to such review are often referred to as \"registered reports\" in that discipline. Authors write a detailed study protocol and pre-analysis plan, and before the experiment is actually run and data is collected, submit the plan to a journal. Journals review the plan for the quality of the design and the scientific value of the research question, and may choose to give \"in-principle acceptance\". This can be thought of as a kind of revise and resubmit that is contingent on the data being collected and analyzed as planned. If the author follows through on the proposed design, and the data are of sufficiently high quality (e.g., with sufficiently low sample attrition rates in a longitudinal study, etc.), the results are to be published regardless of whether or not they are statistically significant, and whether they conform to the expectations of the editor or referees, or to the conventional wisdom in the discipline. Several psychology journals currently have begun using results-blind review, either regularly or in special issues (Chambers 2013;Chambers et al. 2014;Nosek and Lakens 2014). 32A forthcoming issue of Comparative Political Studies will be the first to feature results-blind review in political science (Findley et al. 2016), and it will include both experimental and observational research studies. In our view, it would also be useful to experiment with results-blind review and registered reports in economics journals. The rise in experimental studies and pre-analysis plans in economics, as evidenced by the rapid growth of the AEA registry, is likely to facilitate the eventual acceptance of this approach.",
          "An important open question is how widely the approach of study registration and hypothesis prespecification could be usefully applied in non-prospective and non-experimental studies. This issue has been extensively discussed in recent years within medical research but consensus has not yet been reached in that community. It actually appears that some of the most prestigious medical research journals, which typically publish randomized trials, are even more in favor of the registration of observational studies than the editors of journals that publish primarily in non-experimental research (see the dueling editorial statements in Epidemiology 2010; The Lancet 2010;Loder, Groves, and MacAuley 2010;Dal-R√© et al. 2014). A major logical concern with the pre-registration of non-prospective observational studies using pre-existing data is that there is often no credible way to verify that pre-registration took place before analysis was completed, which is different than the case of prospective studies in which the data has not yet been collected or accessed. In our view, proponents of the preregistration of observational work have not formulated a convincing response to this obvious concern. A more recent study bases its analysis on Neumark's exact pre-specified tests estimate the effect of minimum wages in Canada and found larger unemployment effects, but they had access to the data before estimating their models and did not have an agreement with the journal, so the value of this \"pre-specification\" is perhaps less clear (Campolieti, Gunderson, and Riddell 2006). In political science, a pre-specified observational analysis measured the effect of the immigration stances of Republican representatives on their 2010 election outcomes (Monogan 2013). It is difficult to see how a researcher could reach Neumark's level of pre-specified detail with a research question with which they were not already intimately familiar. It seems more likely that in a case where the researcher was less knowledgeable they might either pre-specify with an inadequate level of detail, or choose an inappropriate specification; this risk makes it important that researchers should not be punished for deviating from their pre-analysis plan in cases where the plan omits important details or contains errors, as argued in Casey et al (2012). It seems likely to us that the majority of observational empirical work in economics will continue largely as is for the foreseeable future. However, for important, intensely debated, and well-defined questions, it would be desirable in our view for more prospective observational research to be conducted in a pre-specified fashion, following the example in Neumark (2001). Although pre-specification will not always be possible, the fact that large amounts of government data are released to the public on regular schedules, and that many policy changes are known to occur well in advance (such as in the case of the anticipated federal minimum wage changes discussed above, with similar arguments for future elections), will make it possible for the verifiable pre-specification of research analysis to be carried out in many settings.",
          "Another frontier topic in this realm is the use of pre-specified algorithms, including machine learning approaches, rather than exact pre-analysis plans for prospective studies. For instance, the exact procedure to be used to determine which covariates should be included in order to generate the most statistically precise estimates can be laid out in advance, even if those covariates are unknown (and unknowable) before the data has been collected. This approach has not yet been widely adopted in economics (to our knowledge), but has begun to be used in medical trials and biostatistics (van der Laan et al. 2007;Sinisi et al. 2007). A proposal related to, but slightly different than, pre-analysis plans is Nobel Prize winning physicist Saul Perlmutter's suggestion for the social sciences to use \"blind analysis\" (MacCoun and Perlmutter 2015). In blind analysis, researchers add noise to the data while working with it and running the analysis, thus preventing them from knowing which way the results are turning out, and thus either consciously or unconsciously biasing their analysis, until the very end, when the noise is removed and the final results are produced. This technique is apparently quite common in experimental physics (Klein and Roodman 2005), but we are not aware of its use in economics or other social sciences. Major differences are also beginning to emerge in the use of pre-analysis plans, and in the design and interpretation of experimental evidence more broadly, among economists versus scholars in other fields, especially health researchers, with a much greater role of theory in the design of economics experiments. Economists often design experiments to shed light on underlying theoretical mechanisms, to inform ongoing theoretical debates, and measure and estimate endogeneous behavioral responses. These behavioral responses may shed light on broader issues beyond the experimental intervention at hand, and thus could contribute to greater external validity of the results. As a result, pre-analysis plans in economics are often very detailed, and make explicit reference to theoretical models. For example, Bai et al. (2015) preregistered the theoretical microeconomic model and detailed structural econometric approach that they planned to apply to a study of commitment contracts in the Indian health sector. This distinction between the types of studies carried out by medical researchers versus economists (including those working on health topics) has a number of important implications for assessing the reliability of evidence. One has to do with the quality standards and perceptions of the risk of bias in a particular design. For medical trialists accustomed to the CONSORT standards or other medical efficacy trial reporting guidelines (described below), studies that do not feature double-blinding, and thus run the risk of endogeneous behavioral responses to the medical intervention, are considered less reliable than those studies that employ double-blinding (for a detailed discussion, see Eble, Boone, and Elbourne 2014). While a few studies conducted by economists do feature double-blinding (e.g., Thomas et al. 2003;Thomas et al. 2006), in nearly all settings blinding participants to their status is either logistically difficult (for instance, if government partners are unwilling to distribute placebo treatments to some of their population) or even impossible. To illustrate, how would you provide a placebo treatment in a study investigating the impact of the distribution of cash transfers on household consumption patterns? Even in settings that might seem promising for placebo treatments, such as the community-level deworming treatments discussed in Miguel and Kremer (2004), blinding participants to their status is basically impossible: deworming generates side effects (mainly gastrointestinal distress) in roughly 10% of those who take the pills, so community members in a placebo community would quickly deduce that they were in fact not receiving real deworming drugs if there are few or no local cases of side effects. As noted above, endogeneous behavioral responses are often exactly what we economists (and other social scientists) set out to measure and estimate in our field experiments, as described in our pre-analysis plans, and thus are to be embraced rather than rejected as symptomatic of a \"low quality\" research design that is at \"high risk of bias\". Taken together, it is clear to us that the experimental literature in economics (and increasingly in other social sciences such as political science) often has very different objectives than medical, public health and epidemiological research, and thus different research methodologies are often called for. Despite the value of learning from recent experience in biomedical research, and the inspiration that the experience of medical research has played to the rise of new experimental research methods in the social sciences, economists have not simply been able to import existing medical trial methods wholesale, but are developing new and tailored approaches to pre-registration, preanalysis plans, reporting standards, and transparency more broadly.",
          "Another approach to promoting transparency is to establish detailed standards for the disclosure of information regarding study design, data, and analysis. These could serve to limit at least some forms of data mining and specification searching, or at least might make them more apparent to the reader. Detailed reporting standards have become widespread in medical research for both experimental and observational research. Most notably for clinical trials, the Consolidated Standards of Reporting Trials (CONSORT) was developed (Begg C et al. 1996). A before-andafter comparison showed improvement in some measures of study reliability (Moher D et al. 2001), and the standards have been twice revised (Moher, Schulz, and Altman 2001;Schulz et al. 2010) and since extended to at least ten specific types of research designs, interventions, or data. Among others, and possibly particularly relevant for some types of economics research, these include cluster randomized trials (Campbell, Elbourne, and Altman 2004;Campbell et al. 2012), non-pharmacological treatment interventions (Boutron et al. 2008), and patient-reported outcomes (Calvert et al. 2013) In psychology, researchers have created an extension of CONSORT for social and psychological interventions (CONSORT-SPI) (Montgomery et al. 2013;Grant et al. 2013). Others psychologists have proposed that an effective way to reform reporting and disclosure norms within their discipline is for referees to enforce desirable practices when reviewing articles (Simmons, Nelson, and Simonsohn 2011). These authors recommended six conditions for referees to consider include the following: 1. Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. 2. Authors must collect at least 20 observations per cell or else provide a compelling costof-data-collection justification. Recently, we, the authors of this article, were part of an interdisciplinary group of researchers that developed a detailed set of journal guidelines called the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015). This modular set of guidelines for journals features eight categories, namely: citation standards, data transparency, analytic methods (code) transparency, research materials transparency, design and analysis transparency, preregistration of studies, preregistration of analysis plans, and replication -with four levels (0-3) of transparency that journals could choose to endorse or require. For example, with regards to data transparency, the level zero standard is that the journal either encourages data sharing or says nothing, while the level three standard is that \"data must be posted to a trusted repository, and reported analyses will be reproduced independently prior to publication\"; levels 1 and 2 fall somewhere in between. Journals could choose to adopt higher standards in some categories than others, as they feel most appropriate for their research community. In the six months after the guidelines were published in Science, 538 journals and 57 organizations across a wide variety of scientific disciplines, including many in the social sciences, expressed their support for the standards and agreed to evaluate them for potential adoption. Science has now announced that it will be implementing the standards, effective  2016), which argues that differences in implementation between original and replication experiments were inappropriate and introduces noise in addition to the expected sampling error. When taking this into account, one should actually expect the relatively low reported replication rate, and they thus argue there is no replication crisis. Some of the original RPP authors respond that differences between original and replication studies were in fact often endorsed by original study authors and take issue with the statistical analysis in Gilbert et al. (Anderson et al. 2016). Simonsohn (2015) engages in further discussion of how one should evaluate replication results, suggesting that powering a replication based on the effect size of the original study is problematic, and to distinguish the effect size from zero, replications (at least in psychology, with their typically small sample and effect sizes) should have a sample at least 2.5 times as large as the original. An optimistic take by Patil, Peng, and Leek (2016) suggests that researchers should compare the effect in the replication study to a \"prediction interval\" defined as ùëüùëü√¥ùëú ùëúùëúùëñùëñùëúùëú ¬± ùëßùëß 0.975ÔøΩ 1 where ùëüùëüÃÇùëúùëú ùëúùëúùëñùëñùëúùëú is the correlation estimate in the original study, ùëõùëõ ùëúùëúùëúùëúùëñùëñùëúùëú and ùëõùëõ ùëúùëúùê∏ùê∏ùëüùëü are the sample sizes in the original and replication studies, respectively; and ùëßùëß 0.975 is the 97.5% quantile of the normal distribution, which incorporates uncertainty in the estimates from both the original and replication study. Applying this approach leads to much higher estimates of study replication (75%) for the RPP. Economists may be interested to know that the researchers behind the RPP also included a prediction market in their project, and the market did a fairly good job of predicting which of the effects studies would ultimately be reproduced (Dreber et al. 2015). Unlike the prediction market in Camerer et al. (2016), the RPP prediction market outperformed a survey of researcher beliefs. 43 43 For related research on expert predictions, see DellaVigna and Pope (2016) . Other psychology researchers have tried another way to crowdsource replication: instead of bringing different research groups together to all independently run the same classic experiment, other researchers have independently analyzed the same observational dataset and attempted to answer the same question, in this case, the question of whether darker skintoned soccer players receive more red cards as a result of their race, conditional on other factors (Silberzahn and Uhlmann 2015). For instance, it has been shown in multiple instances that it is often trivially easy to identify individuals in purportedly \"de-identified\" and anonymous datasets using publicly available information. In one dramatic illustration, MIT computer science Ph.D. student Latanya Sweeney sent then-Massachusetts Governor William Weld his own complete personal health records only days after anonymized state health records were released to researchers (Sweeney 2002). A new focus of computer science theorists has been developing algorithms for \"differential privacy\" that simultaneously protect individual privacy while allowing for robust analysis of datasets. They have established that there is inherently a trade-off between these two objectives (Dwork and Smith 2010;Heffetz and Ligett 2014), though few actionable approaches to squaring this circle are currently available to applied researchers, to our knowledge.",
          "The rising interest in transparency and reproducibility in economics reflects broader global trends regarding these issues, both among academics and beyond. As such, we argue that \"this statements, and open data and materials lead to improved research quality in a way that can be credibly measured and assessed? To this point, the presumption among advocates (including ourselves, admittedly) is that these changes will indeed lead to improvements, but rigorous evidence on these effects, using meta-analytic approaches or other methods, will be important in determining which practices are in fact most effective, and possibly in building further support for their adoption in the profession. There are many potential avenues for promoting the adoption of new and arguably preferable practices, such as the data sharing, disclosure and pre-registration approaches described at length in this article. One issue that this article does not directly address is how to  Notes: Reproduced from Table 4 of Ioannidis (2005) 65 Notes: Table shows a sample of recent papers conducting meta-analyses and testing for publication bias in certain literatures in economics. Positive evidence for publication bias indicated by '+', evidence for no publication bias with '-', and mixed evidence with '~'. The number of papers and total estimates used in the meta-analysis are also shown.  Longhi, Nijkamp, and Poot (2005) Immigration and wages -18 (348) Publication bias is not found to be a major factor. The negative effect of immigration is quite small (0.1%) and varies by country. Knell and Stix (2005) Income elasticity of money demand -50 (381) Publication bias does not significantly affect the literature. Income elasticities for narrow money range from 0.4 to 0.5 for the US and 1.0 to 1.3 for other countries. Doucouliagos and Laroche (2003) Union productivity effects + 73 (73) Publication bias is not considered a major issue. Negative productivity associations are found in the UK, with positive associations in the US. Gorg and Strobl (2001) Multi-national corporations and productivity spillovers + 21 (25) Study design affects results, with cross-sectional studies reporting higher coefficients than panel data studies. There is also some evidence of publication bias. Ashenfelter, Harmon, and Oosterbeek (1999) Returns to Education + 27 (96) Publication bias is found, and controlling for it significantly reduces the differences between types of estimates of returns to education.  Duvendack, Palmer-Jones, and Reed (2015) or The Replication Network list (http://replicationnetwork.com/replication-studies/) as well as journal websites. Since \"replication\" is an imprecise term, this categorization is perhaps subject to some debate.      S1 shows a non-normalized version). Fourteen studies have a significant effect in the same direction as the original study in the meta-analysis [77.8%;95% CI = (56.5%,99.1%)]."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "certain types of statistical results are more likely to be published than other results, conditional on the research design and data used.",
            "context": "2.2 Publication Bias Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "09c660d438e8b372f3e77504c069255b85c00256",
        "sections": [
          "According to CLT, then, people traverse different psychological distances by using similar mental construal processes. Because the various distances have the same egocentric reference point, they should all be cognitively related to each other and similarly affect and be affected by level of construal. As psychological distance increases, construals would become more abstract, and as level of abstraction increases, so too would the psychological distances people envisage. Construal levels thus expand and contract one's mental horizon. The different distances should also similarly influence prediction, evaluation, and action, inasmuch as these outcomes are mediated by construal. The present article builds upon our earlier work on temporal construal theory, which focused in particular on the way that temporal distance from future events influences representation and judgment (Liberman & Trope, 1998;Trope & Liberman, 2003). Going beyond this earlier theory, we now treat temporal construal theory as a special case of a general theory of psychological distance. At the core of the proposed theory is a functional approach to construal levels, according to which mental construal processes serve to traverse psychological distances and switch between proximal and distal perspectives on objects. We describe the properties of the mental construal processes that enable them to fulfill this function and further explicate our approach by relating it to extant theories of how people respond to distant objects. Some of these ideas were presented in earlier literature reviews and book chapters (Liberman & Trope, 2008;Liberman, Trope, & Stephan, 2007;Trope, Liberman, & Wakslak, 2007). The present article presents a more advanced and comprehensive formulation of the theory and examination of related theories and research. The article consists of three main parts. In the first part, we present the basic assumptions of CLT. We explain what we mean by construal levels and why they are related to psychological distance (Section I) and examine the cognitive relationships among the four dimensions of psychological distance (Section II) and their bidirectional relationship to level of construal (Section III). In the second part, we turn to the construal-mediated consequences of psychological distance for prediction, preference, and self-regulation (Sections IV-VI). In the third part, we address open questions about psychological distance and discuss new directions for future research (Sections VII-IX).",
          "In the following three sections, we examine the basic assumptions of CLT regarding psychological distance, construal level, and the relationship between the two.",
          "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental. For example, by moving from representing an object as a \"cellular phone\" to representing it as \"a communication device,\" we omit information about size; moving from representing an activity as \"playing ball\" to representing it as \"having fun,\" we omit the ball. Concrete representations typically lend themselves to multiple abstractions. For example, a cellular phone could be construed also as a \"small object,\" and \"playing ball\" could be construed as \"exercising.\" An abstract representation is selected according to its relevance to one's goals. Thus, if one's goal is to contact a friend, then \"a communication device\" is relevant, but size is not. From the perspective of that goal, the cell phone could be replaced by (i.e., would be conceptually close to) a desktop computer with an Internet connection. If, however, one's goal is to pickpocket a valuable object, then size is a relevant attribute, and function is of less relevance. Given this goal, the cell phone could be replaced by (i.e., would be conceptually close to) a wallet. Like irrelevant details, details that are inconsistent with the chosen abstract representation are omitted from the representation or assimilated into it. For example, the detail that the ball game is slow would be omitted or modified once the activity is represented as \"exercising.\" Because abstract representations necessarily impose one of many alternative interpretations, and because irrelevant or inconsistent details are omitted or assimilated to it, these representations tend to be simpler, less ambiguous, more coherent, more schematic, and more prototypical than concrete representations (Fiske & Taylor, 1991, p. 98; E. R. Smith, 1998). It is important to note, however, that higher level construals are not simply more impoverished or vague than lower level construals. They often convey additional information about the value of the stimulus and its relations to other stimuli. For example, \"having fun\" entails many characteristics that are not an obvious part of \"playing ball outside\" (e.g., valence) and places the activity within a broader context by specifying its relations to other concepts (e.g., a party). Thus, the process of abstraction involves not only a loss of specific, idiosyncratic, and incidental information, but also ascription of new meaning deduced from stored knowledge and organized in structured representations. There are multiple levels of abstractness, as one could construct increasingly more inclusive categories of objects (e.g., poodle, dog, a mammal). Actions form hierarchies, too, as goals could be translated into more abstract, superordinate goals (Carver & Scheier, 2000;Miller, Galanter, & Pribram, 1960;Vallacher & Wegner, 1987). In such hierarchies, each action (e.g., study for an exam) has a superordinate, abstract level, which answers the question of why the action is performed (e.g., do well) and a subordinate, concrete level, which provides the details of how the action is to be performed (e.g., read a textbook). Higher levels of abstractness contain less concrete details about the specific type of action performed, the objects it involves, and its immediate context and more information about the general meaning and the valence of the action (Semin & Fiedler, 1988;Trope, 1986Trope, , 1989)). On the basis of this conceptualization, we posit two related criteria that can be used to distinguish which features of an item or an event are more high level and which are more low level (i.e., which features will be increasingly captured at a higher vs. lower level of construal). The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is. The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. The topic of the lecture, on the other hand, would be important regardless of the location's convenience. In this sense, details about location are subordinated to details about topic, and thus constitute a lower level of construal. Construal and distance-CLT contends that people use increasingly higher levels of construal to represent an object as the psychological distance from the object increases. This is because high-level construals are more likely than low-level construals to remain unchanged as one gets closer to an object or farther away from it. For example, the higher level goal to contact a friend is more stable over time than the more concrete goal to send her an e-mail, because an Internet connection might be unavailable when one is actually trying to contact the friend. From a temporally distant perspective, it is therefore more useful to construe this action in terms of the high-level goal rather than the low-level goal. The same holds true for the other distances. Thus, abstract categories tend to change less across social distance. For example, more people use communication devices than cell phones, and therefore, the former construal is more useful for relating to socially distant individuals. It is critical that, although we believe that these functional relationships underlie the association between construal and distance, we propose that the effects of distance on construal and of construal on distance are overgeneralized, making them persist even when the initial reasons that gave rise to the association are no longer present. That is, a large distance from an object would activate a high-level construal of the object even when lowlevel details are not particularly likely to change over distance, and high level of construal of an object would produce a sense of distance from the object even when such construal does not afford retrieval of particularly distal exemplars of the object. For example, students may know the room in which a guest lecture will take place well in advance and be quite certain that it will not change. Nevertheless, CLT predicts that these students will ignore the quality of the lecture room (a low-level detail) when it is temporally distant, when it takes place in a remote location, when it is planned for another person, and when it is unlikely to take place. Finally, it is worth emphasizing that although psychological distance and construal levels are related, they are not the same. Psychological distance refers to the perception of when an event occurs, where it occurs, to whom it occurs, and whether it occurs. Construal levels refer to the perception of what will occur: the processes that give rise to the representation of the event itself. Thus, psychological distance from an event should be more closely related to the spatiotemporal distance of the event from the self than to its inherent properties, whereas the construal of the event should be more closely related to its inherent properties than to its spatiotemporal distance from the self. Below, we develop more fully the assumption that the different psychological distances are interrelated (Section II) and that each distance affects and is affected by level of construal (Section III).",
          "We contend that the propensity to complete the sentence \"a long time ago, in a ____ place\" with \"far away\" rather than with \"nearby\" reflects not only a literary convention but also an automatic tendency of the mind. Indeed, people use spatial metaphors to represent time in everyday language and reasoning (Boroditsky, 2007). In social psychology, spatial distance is often used to measure social distance. For example, choosing a more distant seat from another person is taken to reflect social distancing from that person (e.g., Macrae, Bodenhausen, Milne, & Jetten, 1994;Mooney, Cohn, & Swift, 1992). More generally, if a common dimension of psychological distance underlies the various distance dimensions we have enumerated, then these distance dimensions should be mentally associated. For example, remote locations should bring to mind the distant rather than the near future, other people rather than oneself, and unlikely rather than likely events. In an investigation of this hypothesis, Bar-Anan, Liberman, Trope, and Algom (2007) used a picture-word Stroop task (Stroop, 1935) to examine the cognitive interrelations among psychological distances. Participants viewed landscape photographs containing an arrow that was pointing to either a proximal or a distal point in the landscape (see Figure 1). Each arrow contained a word denoting either psychological proximity (e.g., \"tomorrow,\" \"we,\" \"sure\") or psychological distance (e.g., \"year,\" \"others,\" \"maybe\"). Participants' task was to respond by pressing one of two keys as quickly and as accurately as possible. In one version of the task, they had to indicate whether the arrow pointed to a proximal or distal location. In another version, they had to identify the word printed in the arrow. In both versions, participants responded faster to distance-congruent stimuli (in which a spatially distant arrow contained a word that denoted temporal distance, social distance, or low likelihood, or a spatially proximal arrow contained a word that denoted temporal proximity, social proximity, or high likelihood) than to distance-incongruent stimuli (in which a spatially distal arrow contained a word denoting proximity, or a spatially proximal arrow contained a word denoting distance). These findings suggest that spatial distance, temporal distance, social distance, and hypotheticality have a common meaning and that people access this common meaning automatically, even when it is not directly related to their current goals. We think that the common meaning is psychological distance and that it is automatically assessed because of its important implications for the perceiver. For example, on a rainy day, it matters whether an umbrella one notices belongs to a friend or to a stranger (social distance); in the jungle, it is important whether a tiger is real or imaginary (hypotheticality); in making a financial investment, it is important whether a recession is anticipated in the near or distant future (temporal distance), here or somewhere else (spatial distance). Yet, unlike valence, distance is not an inherent aspect of the semantic meaning of objects. Umbrellas, tigers, and recessions are inherently good or bad, but they are not inherently proximal or distal. It is perhaps for this reason that distance has not been included among the basic dimensions of meaning: evaluation, potency, and activity (Osgood & Suci, 1955). The idea that distance dimensions are automatically associated further suggests that the distance of a stimulus on one dimension may affect its perceived distance on other dimensions. In this spirit, research by Stephan, Liberman, and Trope (2010) has investigated how social distance affects and is affected by spatial distance and temporal distance. This line of research was based on past work showing that polite language signifies and creates interpersonal distance: People address strangers more politely than they address friends, and the use of polite, formal language creates a sense of distance (Brown & Levinson, 1987). Consistent with this analysis, Stephan et al. found that the use of normative, polite language rather than colloquial, less polite language led participants to believe that the target of the communication was spatially and temporally more distant. For example, using normative rather than colloquial language to address a person (e.g., \"My brother is taking our family car, so the rest of us will stay at home\" vs. \"‚Ä¶ will be stuck at home\") led participants to infer that the addressee was in a more remote location and that the conversation referred to an event in the more distant future. Another set of studies by Stephan et al. (2010) found evidence for the reverse direction of influence, namely, an effect of spatial and temporal distance from the target of communication on the use of polite language. In one study, participants wrote instructions for a person who was expected to read them either in the near future or the distant future (e.g., participants wrote sightseeing suggestions for a tourist that was supposed to arrive on the following day or a year later). The results showed that participants preferred to phrase instructions more politely when they were addressed to the distant future than to near future tourist. Similar findings were found with addressing a spatially proximal versus distal stranger. Williams and Bargh (2008, Study 4) showed, in a similar vein, that participants who were primed with spatial distance (relative to proximity) by marking close (vs. distant) points on a Cartesian plan subsequently reported greater social distances between themselves and members of their family and their hometown. Finally, Wakslak and Trope (2008) showed that hypotheticality also affects the perception of other distances, such that people expect unlikely events (which, in terms of CLT are distant on the dimension of hypotheticality), compared with likely events, to occur in situations that are relatively more distant in time, space, and social distance. For example, a rare cat blood type was expected to be found in cats in spatially remote rather than nearby places, whereas a common cat blood type was expected in a near rather than a remote location. Presumably, people map probability onto other distance dimensions and therefore expect improbable events to happen to distant others in distant places and times. Discussion-Recent research shows that the different psychological distance dimensions are associated and suggests that psychological distance is an aspect of meaning, common to spatial distance, temporal distance, social distance, and hypotheticality. This research further shows that cues of distance on one dimension affect the perceived distance of objects on other dimensions. For example, the spatial distance from an event may depend not only on its location relative to that of the perceiver but also on whether it is expected in the near or distant future, whether it occurred recently or a long time ago, whether it is expected to be experienced by oneself or another person, and whether it is probable or improbable. In this respect, the different psychological distances may be to some extent interchangeable (see Pronin, Olivola, & Kennedy, 2008). Our research has only started to explore the relationships among the various distance dimensions. Below we consider further questions about more complex relationships among the psychological distances than those we have identified thus far. The psychophysics of distance: How do objective distances in time, space, social target, and hypotheticality map onto generalized psychological distance? It seems reasonable to expect a relatively high sensitivity to changes in objective distance from the self here and now at the proximal end of the continuum. As distance increases, changes in objective distance might produce smaller corresponding changes in psychological distance. As vividly illustrated by Steinberg's painting View of the World from the 9th Avenue, the distance between 9th Avenue and 10th Avenue in Manhattan seems to Steinberg greater than the distance between New York City and Chicago. In other words, consistent with the Weber-Fechner law, psychological distance from an object might follow a concave, logarithmic function rather than a linear function. Recent research by Zauberman, Kim, Malkoc, and Bettman (2009) provides evidence showing that such a function fits subjective estimates of temporal distance from future events. Likewise, research on estimates of spatial distance uncovered increased sensitivity around an egocentric reference point (e.g., Holyoak & Mah, 1982). Investigating the functions that relate temporal distance, prospective and retrospective, as well as other distances, to psychological distance remains an important direction for future research. Another important but unexplored question is how different distances combine to affect psychological distance. One interesting possibility is that the Weber-Fechner law may apply across different psychological distance dimensions. That is, changes in distance of an object from the self on one dimension would have a greater impact on psychological distance when that object is proximal on another dimension than when it is distant on that other dimension. For example, the same changes in spatial distance of an object from the self might have a stronger impact on the psychological distance of that object when the changes are expected in the near future rather than in the distant future. To use Steinberg's drawing, this artist might think of a meeting as coming up sooner if he plans to have the meeting in Manhattan than in Chicago.",
          "According to CLT, different objects are mentally associated to the extent that they are psychologically equidistant from the self. In principle, the associations among proximal objects could be as strong as those among distal objects. However, because psychological distance is egocentric, anchored at the self here and now, objects at the proximal end may have more in common than those that are removed from that end. There is only one me and many others, and therefore, the things that are me and mine are more similar to each other than the things that are not me and mine. Only one place in space is \"here,\" but there are many places that are far away, and therefore, things that are here are more related to each other than things that are far away. Similarly, there is likely less variation among real events than possible events. The greater commonality among proximal objects than distal objects might give rise to stronger interrelationships among the former than the latter.",
          "Although we suggest that different types of distance are related, we do not wish to suggest that they are the same. It is possible that some distance dimensions are more basic or influential than others. For example, Boroditsky has recently proposed that spatial distance is primary and that temporal distance is understood as an analogy to it (Boroditsky, 2000(Boroditsky, , 2001;;Boroditsky & Ramscar, 2002). In support of this proposal, she showed that thinking about space before answering questions about time influenced the participants' responses, but thinking about time before answering questions about space did not affect participants' responses (Boroditsky, 2000, Experiments 2 and 3). In another series of studies, she presented participants with lines on a computer screen and showed that judgments of exposure duration were affected by the lines' length whereas judged length was not affected by duration of exposure (Casasanto & Boroditsky, 2008). Spatial distance may be more basic than temporal distance or other distances in that it is learned earlier, more clearly detected, less ambiguous, or easier to communicate about. It is possible, however, that the distinction between self and not-self, which underlies social distance and perspective taking (e.g., Pronin, 2008), might be a core psychological distance that exerts a particularly strong influence on the activation and perception of other distances. Perhaps hypotheticality, the distinction between real and imagined objects and between probable and improbable events, is least prominent and acquired at an older age, compared to other distance dimensions. The various distance dimensions may differ in other respects. Time is unidimensional and uncontrollable. We incessantly travel from the past to the future and have no control over time. Spatial distance has three dimensions, is relatively stable, and can be controlled by moving closer or farther away from things at wish. Social distance is only partly controllable (e.g., we can try to get closer to a person but success is not entirely dependent on our actions). Hypotheticality is also only partly controllable, because we cannot always change the probabilities of events. Another important difference among the distances is their relation to valence. Whereas social distance decreases positivity (e.g., in groups are perceived as more positive than out groups), temporal distance typically increases positivity (people are more positive about the more distant future). Exploring the psychological consequences of these differences in controllability, dimensionality, and valence is a fruitful direction for future research.",
          "The basic premise of CLT is that distance is linked to level of mental construal, such that more distant objects will be construed at a higher level, and high-level construal will bring to mind more distant objects. It may seem intuitive that from a distance we see the forest, and as we get closer, we see the trees. It may also seem intuitive that, to see the forest rather than individual trees, we need to step back. These effects should apply, however, not only to spatial distance but also to other distances, and not only to visual input, where it might seem a natural constraint of our perceptual system, but also to conceptual abstraction. We do not literally see either tomorrow or next year. Yet we may think about tomorrow in terms of trees and about next year in terms of the forest. We do not literally take a step back to forget the daily hassles and consider our life in general. Yet, when thinking of the general aspects of our life rather than about daily details, we may find ourselves looking farther into space. Evidence for these associations between distance and construal has been found at the level of both implicit associations and explicit judgments and decisions. Bar-Anan, Liberman, and Trope (2006), for example, examined the associations between level of construal and psychological distance using an Implicit Association Test. Similar to other assessments of implicit associations using this test (see Greenwald, McGhee, & Schwartz, 1998), participants in these studies were presented with stimuli from four categories: stimuli pertaining to high-level construal (e.g., category names such as \"drinks\"), stimuli pertaining to low-level construal (e.g., exemplar names such as \"coke\"), stimuli pertaining to low psychological distance (e.g., the word \"ours\" or the word \"friend\" for the social distance), and stimuli pertaining to high psychological distance (e.g., the word \"theirs\" or the word \"stranger\"). In CLT-congruent trials, high-level stimuli were paired with distant stimuli and low-level stimuli were paired with proximal stimuli, whereas on CLT-incongruent trials, high-level stimuli were paired with proximal stimuli and low-level stimuli were paired with distal stimuli. With all four dimensions of psychological distance-temporal distance, spatial distance, social distance, and hypotheticality-the participants were faster with congruent than with incongruent pairings, indicating that participants implicitly associate psychological distance with high-level construal and psychological proximity with low-level construal. This set of studies suggests that the association between psychological distance and construal level can be activated automatically without conscious deliberation. Beyond mere association, the reciprocal effects of distance on construal and of construal on distance have important implications for perception, categorization, action identification, and person perception. In the following, we discuss those implications. Visual Perception-The distinction between the (high-level) gestalt and (low-level) constituent details, such as the proverbial distinction between the trees and the forest, readily applies to visual perception. In a widely used task (Navon, 1977), participants are presented with global letters that are made of local letters (e.g., a large L made of 20 small Hs), and are asked to indicate whether a target letter appears on the screen (e.g., \"Is there an H on the screen?\"). The target letter can match a global letter or a local letter. Would psychological distance facilitate attending to global letters (e.g., the \"forest\") and impair attending to local letters (e.g., the \"trees\")? Would attending to global letters, as opposed to local letters, enhance perceived distance? Liberman and F√∂rster's (2009 Liberman and F√∂rster's (in press) studies suggest affirmative answers to both questions. In one of the studies (Liberman & F√∂rster, in press), they primed participants with temporal distance or temporal proximity by making them write essays about their lives tomorrow or their lives on a day a year later and imagine taking an attention test on that day. Navon's (1977) task was then presented. Relative to a control group, in which participants did not write an essay, temporal distance facilitated processing of global letters and impaired processing of local letters, whereas temporal proximity produced the opposite effect. Similar effects were found for priming of spatial distance and social distance. To examine the reverse direction of influence, namely, that of global perception on estimated psychological distance, Liberman and F√∂rster (2009) procedurally primed participants with either global or local perceptual processing, using a variation of Navon's (1977) task. In the global priming condition, the target letters were always global, and in the local priming condition, the target letters were always local, whereas in the control condition, the targets were global in half of the trials and local in the other half. Relative to the control condition, global processing led to greater estimates of temporal distance, spatial distance, social distance, and hypotheticality. Local processing had the opposite effect. For example, participants who were primed with global processing estimated temporal distance to a dental visit as longer and spatial distance between themselves and a designated point in the room as larger than participants primed with local processing. In a related vein, Wakslak and Trope (2009) found that priming global processing (vs. local processing) through the Navon task led participants to assign lower probability to a variety of everyday life occurrences. It is important to note that, given that psychological distance is egocentric, as CLT contends, if the distance between two objects is not mentally represented as social distance from oneself, temporal distance from now, or spatial distance from here, it should not necessarily depend on level of construal. Consistent with this prediction, in Liberman and F√∂rster's (2009) studies, primed level of construal affected estimated spatial distance between the participant and a sticker in the room, but it did not affect the estimated spatial distance between the experimenter and a marked desk in the room. Likewise, level of construal did not affect estimates of temporal distance that were not anchored on now (e.g., \"How much time after receiving an invitation would you go to the dentist?\"). High-level construal of visual information often entails abstraction of coherent images from fragmented visual input. An example is the Gestalt Completion Task (see Figure 2; Street, 1931; see also Ekstrom, French, Harman, & Dermen, 1976), where performance depends on detecting the global pattern and where attending to details interferes with performance. In a series of studies, participants completed what they believed to be sample items of the Gestalt Completion Task, supposedly as a practice version before they perform the actual task. Participants' performance improved when they anticipated working on the actual task in the more distant future (F√∂rster, Friedman, & Liberman, 2004), when they thought the actual task was less likely to take place (Wakslak, Trope, Liberman, & Alony, 2006), and when social distance was enhanced by priming of high social status (P. K. Smith & Trope, 2006). A psychologically distant perspective thus seems to enable people to better see the gestalt. Whereas distance improves the ability to perceive the gestalt in a visual array, it should have the opposite effect when the task requires attention to details. Distance should therefore have a detrimental effect on the ability to identify a missing low-level, local element within a coherent whole (e.g., a missing hand on a watch, a missing handle on a drawer chest). Wakslak et al. (2006) used the picture completion subtest of the Wechsler Intelligence Scale for Children (Wechsler, 1991) to test this prediction. As expected, participants did worse on sample items of this task when they believed they were less likely to later complete it. Pictures and Words-Pictures are concrete representations that bear physical resemblance to the referent objects, whereas words are abstract representations that carry the essence of that object (Amit, Algom, & Trope, 2009;Amit, Algom, Trope, & Liberman, 2008). Words therefore comprise a higher level of construal than do pictures. Supporting this assumption, Amit, Algom, and Trope (2009) found that participants classified items into more groups when the items were presented in pictures than when they were presented in words. In another study, they presented spatially, temporally, or socially near or distant items in either a pictorial or verbal format (see Figure 3). Participants' task was speeded identification of the object. For example, in an experiment on spatial distance, words or pictures were presented against a background that created an illusion of depth. As expected, participants responded faster to pictures of objects when they were made to seem spatially near than spatially distant, but they responded faster to words denoting those objects when they were made to seem spatially distant than spatially near. To manipulate temporal distance, they used words and pictures of modern objects and ancient objects (e.g., a cart and a car), and to manipulate social distance they used words and pictures of domestic and foreign objects (e.g., a coin of domestic and foreign currency). Responses were faster when pictures represented psychologically proximal objects and words represented psychologically distal objects than vice versa. It seems, then, that processing is most efficient when there is a congruency between the portrayed distance and the presentation medium. Moreover, using similar stimuli in free recall experiments, Amit, Trope, and Algom (2009) demonstrated better memory for proximal than for distal stimuli when the targets were represented in pictures but that it was better for distal than for proximal stimuli when the targets were represented in words. These findings were obtained with temporal, spatial, and social distances. Categorization-If the distant future is represented more abstractly, then individuals should use broader categories to classify objects for distant situations than for proximal situations. In support of this prediction, research has shown that people group objects into fewer, broader categories when they imagine the objects in a distant future scenario than a near future scenario (Liberman, Sagristano, & Trope, 2002, Study 1) or in an unlikely scenario than a likely scenario (Wakslak et al., 2006). Consistent with research on categorization, research on segmentation of ongoing events has found that people chunk behavior sequences into broader segments when the behavior is distal rather than proximal. For example, Henderson, Fujita, Trope, and Liberman (2006) asked New York University participants to partition an ongoing behavioral sequence into as many sections as they deemed appropriate. Participants viewed an animated film developed by Heider and Simmel (1944) that shows two triangles and a circle moving against and around each other and were told that the film depicted the action of three teenagers around a cabin at a well-known summer camp. In the spatially near condition, the camp was said to be located on the East Coast; in the spatially distant condition, the camp was said to be located on the West Coast. As expected, participants created fewer, broader sections out of the video when they believed the campers it depicted were in a spatially distant, as opposed to a spatially near, location. Similar effects were found for events that were described as less likely versus more likely (Wakslak et al., 2006). Do narrow, specific categories promote a sense of psychological proximity? A recent series of studies by Wakslak and Trope (2009) manipulated level of categorization in several distinct ways and found the predicted effect on event likelihoods. In one study, construal level was primed by asking participants to generate either superordinate categories or subordinate exemplars of 40 objects (e.g., table, sport, book). Next, participants completed a supposedly unrelated questionnaire where they made a series of probability judgments. As expected, participants who had been primed to have a high-level construal mindset indicated that the events were less likely to occur compared to those who had been primed to have a low-level construal mindset. In another study, participants compared either alignable or unalignable objects. In the alignable condition, participants compared two digital cameras, with information provided about the same seven features for each camera (e.g., battery life, digital zoom). In the nonalignable condition, participants were provided with information about seven different features for each camera (e.g., battery life for the digital camera; focus for the traditional camera). It was assumed that individuals who are asked to compare alternatives with nonalignable features would seek to make the attributes comparable by representing the alternatives at increasingly higher levels of abstraction (see M. D. Johnson, 1984;Malkoc, Zauberman, & Bettman, 2008). As expected, participants cued to think more abstractly (by having compared products with nonalignable attributes) judged an event involving the products as being less likely to occur than those cued to think more concretely (by having compared products with alignable attributes). Action Identification-Actions, like objects, may be construed in high-level terms, which link them to a superordinate purpose (why one performs them), or in low-level terms, which link them to subordinate means (how one performs them). Here, too, greater psychological distance promotes higher levels of construal (Liberman & Trope, 1998). In one of the studies, participants tended to describe more distant future activities (e.g., studying) in highlevel terms (e.g., \"doing well in school\") rather than in low-level terms (e.g., \"reading a textbook\"). Similar effects emerged when actions were to take place in a spatially distant location (Fujita, Henderson, Eng, Trope, & Liberman, 2006), when the actions were framed as unlikely to actually take place (Wakslak et al., 2006), and when the actor was dissimilar to the perceiver (Liviatan, Trope, & Liberman, 2008). If high-level construals serve to represent psychologically distant events, then activating high-level construals should lead people to think of events in psychologically more distant situations. Indeed, research shows that thinking about an activity in high-level, \"why\" terms rather than low-level, \"how\" terms leads people to think of the activity as taking place in more distant points in time (Liberman, Trope, Macrae, & Sherman, 2007;McCrae, Liberman, Trope, & Sherman, 2008) and of the actor as more socially distant (Stephan et al., 2010). Action-identification theory (Vallacher & Wegner, 1989) specifies how to determine the superordinate and subordinate levels of categorizing a given action. Semin and Fiedler's (1988) Linguistic Categorization Model assesses the abstractness of the action itself. At the most concrete level are descriptive action verbs (\"lift,\" \"take\"), which are typically directly observable. Interpretative action verbs (\"pretend,\" \"help\") are more abstract, as they involve interpretation and require some knowledge of a context larger than the immediately perceived. State verbs (\"prefer,\" \"understand\") are still more abstract, and adjectives (\"helpful,\" \"aggressive\") are the most abstract category. The Linguistic Categorization Model is therefore a useful tool for examining the relationships between psychological distance and abstractness of action construal. Indeed, various kinds of distance have been found to affect abstractness of language. For example, people were found to use more abstract language when describing another person's actions than their own actions (Semin & Fiedler, 1989; see also Fiedler, Semin, Finkenauer, & Berkel, 1995), when describing spatially distant interactions than spatially near interactions (Fujita, Henderson, et al., 2006), and when instructed to address another person politely than in colloquial language (Stephan et al., 2010). Would the reverse direction of influence hold? That is, would linguistic abstractness affect perceived psychological distance? Semin and Smith (1999, Studies 2 and 3) studied the effect of linguistic abstractness on the temporal distance of recalled events. They provided participants with retrieval cues of varying abstractness and examined the temporal distance of the events they recalled. For example, participants were asked to recall either an occasion on which they helped somebody (i.e., concrete retrieval cue) or an occasion on which they displayed a trait of helpfulness (i.e., abstract retrieval cue). As predicted, an abstract retrieval cue prompted older memories than memories that were prompted by a concrete retrieval cue. Person Perception-A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995;E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995;Semin & Fiedler, 1988;Semin & Smith, 1999). Distance should therefore enhance the correspondence bias. Furthermore, representing a person abstractly in terms of his or her dispositions would create a sense of distance. Naturally, social psychologists researched extensively the relations between social distance and dispositional inference. We first briefly review this literature and then describe comparable effects of other psychological distances.",
          "Considerable evidence for differences in construal between self and other comes from research on the actor-observer effect in attribution (E. E. Jones & Nisbett, 1972; for a review, see Gilbert, 1998). This research shows that a person's view of his or her behavior emphasizes the role of concrete situational factors that operate at the moment of action (e.g., \"I stepped on your toe because the bus was crowded\"), whereas his or her view of other people emphasizes the causal role of stable, general dispositional properties of the actor (\"he stepped on my toe because he is clumsy\"). Self-other differences might be explained as reflecting differences in knowledge (people know more about themselves and the variability of their behavior over situations than about others) and differences in the salience of behaviors versus situations (the latter is more salient from one's own perspective, the former from the observer's perspective). Would similar differences in construal emerge when knowledge is identical for near and distal social targets? Research relating abstractness of memories to the perspective in which they are recalled seems to offer an affirmative answer. It has been shown, for example, that personal memories of behaviors that were recalled from a third-person perspective (e.g., \"try to remember your first day at school, as if you are now watching the kid you were\") rather than from a first-person perspective (\"try to remember your first day at school, as if you are a kid again\") tended to use dispositional (as opposed to situational) terms (Frank & Gilovich, 1989;Nigro & Neisser, 1983). In a similar vein, Libby and Eibach (2002, Study 4) found that imagining performing an activity (e.g., rock climbing, playing drums) from a third-person perspective produced less vivid and rich reports of the activity than imagining the same activity from a first-person perspective. In terms of CLT, this means that a third-person perspective, which imposes more distance than a first-person perspective, induces a higher level of construal. Indeed, Pronin and Ross (2006) showed that taking a third person perspective rather a first-person perspective on one's own behavior promoted attribution of the behavior to personality traits rather than to specific situational factors. Temporal distance and spatial distance: Correspondent inference, the tendency to use high-level dispositional attributions, seems to increase not only with social distance but also with temporal distance. Over time, observers' attributions of an actor's behaviors become more dispositional and less situational (Frank & Gilovich, 1989; see also Funder & Van Ness, 1983;Moore, Sherrod, Liu, & Underwood, 1979;Nigro & Neisser, 1983;Peterson, 1980; but see Burger, 1986). These findings are consistent with memory consolidation, namely, the tendency for concrete details to fade more rapidly than general abstractions, thus making memories of the distant past more abstract than memories of the recent past (Bartlett, 1932;Wyer & Srull, 1986). However, assuming that memory is an active, reconstructive process, CLT proposes that increasing past temporal distance is associated not only with a loss of specific details but also with an increased use of high-level, schematic construals (Ross, 1989). Consistent with this proposal, McDonald and Hirt (1997) showed that, over time, student participants not only forgot a fellow student's grades but also were more likely to infer the grades from that student's general ability, resulting in a more coherent (and dispositional) impression of the target person over time. More recently, research has examined dispositional inferences from near future and distant future behavior (Nussbaum, Trope, & Liberman, 2003, Study 1) and from spatially close or spatially distant behavior (Henderson, Fujita, et al., 2006, Study 2). These studies found that student participants drew stronger corresponding attitude inferences from situationally constrained behavior when the behavior was expected in the distant future (vs. near future) or when it was believed to have occurred in a spatially remote location (vs. a near location). These findings demonstrate that the correspondence bias, the tendency to underweight lowlevel, situational constraints on observed behavior, increases with psychological distance from the behavior. Past research has shown that perceivers often form spontaneous trait inferences (STIs) when they observe trait-implying behaviors of other people. For example, upon reading the sentence, \"The secretary solved the mystery halfway through the book,\" people spontaneously inferred the trait \"clever\" (Winter & Uleman, 1984). Can temporal or spatial distance affect the extent to which abstract traits are spontaneously inferred at an early stage of information processing? Rim, Uleman, and Trope (2008) addressed this question in a series of studies that used rate of false recognition of implied (but not present) traits as a measure of STI. They found that New York University participants led to believe that actors were in a distant location (Florence, Italy), compared with a proximal location (Manhattan, New York), formed more STIs, even though behavioral information about targets was held constant across the two conditions. The same effect was found using temporal distance: Participants who were led to believe that actors were from the relatively distant past (year 1997), compared with the more recent past (year 2007), formed more STIs. Self-inferences: Distance may affect not only the inferences we draw about others but also the inferences we draw about ourselves. Pronin and Ross's (2006) research shows that people more likely view their future and past selves than their present selves in terms of general personality traits. In a related study, Wakslak, Nussbaum, Liberman, and Trope (2008) asked participants to imagine themselves in different situations either in the near future or in the distant future and to indicate the extent to which their behavior in those situations would reflect each of the Big Five personality traits. It was found that in the distant future, compared with the near future, participants expected to exhibit their traits more consistently across situations. Another study by Wakslak et al. showed that participants were faster in judging whether general personality traits describe the way they would be in the distant future than the near future. These findings suggest that the distant future self is represented more in terms of general, decontextualized traits than the near future self. People seem to have a clearer view of their personality characteristics when taking a temporally distant perspective rather than a temporally proximal perspective on the self. Indeed, using Linville's (1985Linville's ( , 1987) ) and Donahue, Robins, Roberts, and John's (1993) measures of self-complexity, Wakslak et al. found that self-descriptions were more structured and less complex when they referred to a distant future self than to a near future self. It would be interesting to examine in future research the effects of other distances on the self-concept. For example, would thinking of oneself in an unlikely situation or in a remote spatial location make one see oneself in a more coherent, schematic way? Is it possible that this effect also occurs while traveling (or even while planning traveling)? Does imagining the perspective of another person, especially an unfamiliar person, increase perceived selfintegrity? In general, the construal of the self, which is often more concrete than the construal of others, may be highly abstract and high level when the self is viewed from a distanced perspective, in remote times, places, imaginary situations, and from a third-person perspective. Discussion-Extensive research has verified that as psychological distance increases, construals become more abstract, and as level of construal increases, so too do perceptions of psychological distance. Much of past research has studied individual distance dimensions and types of construal within disparate theories. Without denying the uniqueness of the different distances and types of construal, CLT provides a unifying framework that integrates these disparate lines of research. It suggests that high-level construals and lowlevel construals serve different cognitive functions. High-level construals have evolved to represent distal objects because, with distance, one needs to conserve the essential, invariant properties of the referent object. In contrast, low-level construals preserve the object in minute detail for immediate use. High-level construals serve to transcend the here and now, whereas low-level construals instantiate the present. In the following, we expand our examination of construal levels by relating them to prospection and mental simulation, inferences of distance, and to heuristic low-effort processing.",
          "The present view is consistent with recent functional approaches to memory and cognition (Barsalou, 1999;Buckner & Carroll, 2007;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). These approaches suggest that a major function of episodic memory is prospection, namely, anticipating the future rather than merely reproducing the past. The theories of constructive simulation (Schacter & Addis, 2007) and embodied cognition (Barsalou, 1999;Niedenthal, Barsalou, Winkielman, Krauth-Gruber, & Ric, 2005;Winkielman, Niedenthal, & Oberman, 2008) specifically suggest that episodic memory enables prospection through a constructive process of mentally simulating future episodes. Such simulations are analogical and multimodal and serve to guide choice and action with respect to the future (Barsalou, 2008). CLT adds to this view in several respects. First, it is possible that simulations vary in level of construal, ranging from multimodal simulations that are rich in contextual detail and resemble the kind of analogical representation identified by embodied cognition researchers to general simulations that retain common elements and omit incidental detail. CLT predicts that the latter, higher level simulations are more likely to be used with increasing distance of the past episodes from which simulations are constructed and of the future target situation to which the simulation is applied. For example, a simulation of a future meeting with a friend is likely to be more abstract (i.e., contain less detail on the tone of her voice and the look of the room in which the meeting would take place) to the extent that it is constructed from meetings held with that friend in the relatively distant past or distant locations and to the extent that the meeting with the friend is expected in the relatively distant future or location. Second, it is possible that as distance increases, prospection is increasingly more likely to be based on amodal symbolic representations. For example, a representation of a more distant future meeting with a friend that works in a high-tech company may refer more to semantic knowledge about layoffs in the high-tech industry and include less detail related to perceptual properties, such as the tone of her voice. The research showing that people switch from pictorial to linguistic representations of objects as distance from the object increases is consistent with this possibility (Amit, Algom, & Trope, 2009). Third, symbolic representations might also differ in abstraction, ranging from broad abstractions (she is a woman) to more narrowly applied knowledge (she is a single mother to four and a high-tech manager who is afraid to lose her job). Fourth, as the research reviewed here suggests, these changes in the nature of mental representation of objects might be induced not only by prospective temporal distance from the object but also by spatial, social, and hypotheticality distances. In sum, according to CLT both analogical simulations and symbolic representations might vary in level of construal. Distance may determine whether an analogical or symbolic representation is constructed and the level of abstractness at which it will be constructed. Inferring distance from construal level: We argued that because high-level construals are broad, they bring to mind more distant instantiations of objects, and because low-level construals are narrow, they bring to mind more proximal instantiations of objects. It is also possible for construal level to affect the psychological distance of objects through metacognitive inferences (N. Schwartz & Clore, 1996). People may interpret their low-level construal of an object as indicating that the object is close and their high-level construal of an object as indicating that the object is distant. This metacognitive inference of distance from construal level might involve a more complex attributional calculus when one or more other distances are known. Specifically, the construal-based inference that an object is distant on any given dimension will be discounted when the object is known to be distant on another dimension. Correspondingly, the construal-based inference that an object is distant on any given dimension will be augmented when the object is known to be proximal on another dimension. For example, one would attribute a detailed construal of a meeting with a friend to a relatively close relationship with that friend when the meeting is known to take place in the distant future rather than the near future. Thus, direct implicit associations among different distance dimensions generally result in positive relationships among those dimensions. However, when inferring distance from construal, adjusting the inference of distance on one dimension for distance on other dimensions may result in a negative relationship among those distances.",
          "Could the relationship between distance and construal be accounted for by assuming that people are less motivated to make an accurate judgment regarding relatively distant objects? Does the shift toward higher level construals with increasing distance reflect a shift toward low-effort heuristic processing (see Petty & Cacioppo, 1984)? We think that extracting the general meaning and invariant characteristics of objects is not necessarily more or less effortful than fleshing out the minute details of the objects. For example, generating a superordinate category for the concept soda (e.g., drinks) does not necessarily differ in these respects from generating exemplars of the concept (e.g., Diet Coke). Indeed, our research has not found any systematic relationship between the effects of distance on construal and measures of involvement or accuracy motivation (e.g., P. K. Smith & Trope, 2006;Wakslak et al., 2006). Three additional sets of findings argue against the idea that distance-driven high-level construals necessarily reflect uneffortful, heuristic processing. First, this idea cannot account for performance findings showing that judgments about distal objects (compared with judgments about proximal objects) are more accurate when a correct response requires high-level construals of the objects (e.g., identifying a pattern) and are less accurate only when the correct response requires low-level construals (e.g., detecting a missing detail; e.g., Liberman & F√∂rster, in press;Wakslak et al., 2006). Second, low involvement and uneffortful processing of distant (vs. proximal) objects may account for the underutilization of low-level information about distant objects, but it cannot account for the underutilization of high-level information about proximal objects (e.g., Liberman & Trope, 1998;Nussbaum, Liberman, & Trope, 2006). Third, Fujita, Eyal, Chaiken, Trope, and Liberman (2008) directly addressed the issue of heuristic processing by examining sensitivity to argument strength, a commonly used test of heuristic processing (Chaiken, Giner-Sorolla, & Chen, 1996;Petty & Cacioppo, 1984). Fujita et al. presented participants with either strong or weak arguments from a wildlife conservation organization that either referred to a superordinate category (e.g., orcas in Puget Sound) of an attitude object or subordinate, specific exemplar (e.g., Simon, an orca in Puget Sound) of the attitude object. The results showed that when the arguments referred to a high-level category, attitudes were more sensitive to argument strength when the attitude object was temporally distant than near. However, when the arguments referred to a subordinate exemplar, the reverse was true; That is, attitudes were more sensitive to argument strength in the near than distal condition. Involvement might account for the findings obtained with arguments referring to low-level construal of the attitude object but not those obtained with arguments referring to high-level construal of the attitude object. Finally, using the number of thoughts generated about an attitude object as a measure of systematic processing, Ledgerwood, Trope, and Chaiken (2008) found that the number of thoughts was unaffected by temporal distance from the attitude object. We return to the issue of depth of processing in the sections on prediction and evaluation. Construal-Mediated Effects of Psychological Distance-What are the implications of the link between distance and construal for prediction, evaluation, and self regulation? In the following three sections, we address this question.",
          "As argued above, the very function of high-level construals is to enable people to mentally transcend the here and now by forming a representation consisting of the invariant features of the available information and projecting those representations onto distal situations. Predictions of future experiences would therefore be more schematic than the actual experiences, giving rise to a variety of prediction biases that stem from underweighting contextual and incidental features (Gilbert & Wilson, 2007;Kahneman, Krueger, Schkade, Schwarz, & Stone, 2006;Wilson & Gilbert, 2003). For example, people tend to predict that they will react more extremely to positive events (e.g., getting tenure) and negative events (e.g., a rejected paper) than they actually do. This misprediction stems from underweighting the effect of diluting low-level contextual circumstances (Wilson, Wheatley, Meyers, Gilbert, & Axsom, 2000). According to CLT, these biases should be accentuated by distance, because predicting more distal events should direct attention to even more invariant, schematic features of those events and away from low-level situational details. In the following, we examine the implications of this idea for lay prediction of research findings, predictions about one's own performance, and the effect of global trends and local deviations on extrapolation.1 Theories Versus Noise in Scientific Prediction-In scientific prediction, high-level construals reflect the theory, which fosters confidence, whereas lower level construals include noise, which might undermine confidence. A study by Nussbaum et al. (2006, Study 1) examined the confidence of advanced psychology students in replicating classic findings in psychology in either the near future or the distant future. For example, participants imagined entering a class at the university, either the next day or a year later (depending on the experimental condition), handing the students a list of words to memorize, and then testing how well they remember it after moving some of the students to a different room. Participants estimated how likely it is that those tested in the same room would outperform, on average, those that were moved to a different room, thus replicating the encoding specificity effect. Participants were more confident that they would replicate this effect when they imagined conducting the experiment in the distant future than in the near future, especially when reminded of the theory underlying prediction. The same pattern of results was obtained with other classic findings in social, cognitive, and developmental psychology. Knowledge Versus Task Format in Predicting Performance- Nussbaum et al. (2006) have also examined confidence in predictions about one's own performance. One of the studies assessed participants' confidence in predicting their performance on a general knowledge quiz expected to take place either on the same day or 2 months later (Nussbaum et al., 2006, Studies 3 and 4). The questions were the same, but in either a relatively easy or hard question format, which we assumed is a low-level aspect of the quiz, compared with the content the quiz focuses on. Specifically, the quiz consisted of either multiple-choice questions (relatively easy format) or open-ended questions (relatively hard format). In another study, the quiz consisted of questions with either two response alternatives (relatively easy) or four response alternatives (relatively hard). The results showed that the more difficult question format appropriately reduced confidence in near future performance but failed to reduce confidence in distant future performance. We think that this was the case because question format was a low-level aspect of the task. The results also showed that participants' beliefs about their knowledge in each domain predicted how confident they were in predicting their performance in that domain in the distant future better than in the near future. We think that this was the case because such beliefs concern the essence of the quiz as a test of knowledge and thus constitute a high-level construal of the quiz.",
          "Local Deviations-In a study that investigated the effect of spatial distance on the tendency to base predictions on global rather than local information (Henderson et al., 2006), New York University participants viewed a series of graphs depicting information from the years 1999-2004 (e.g., average number of photocopies per student). The information was said to pertain to the New York University campus in Manhattan (spatially near condition) or to the New York University campus in Florence, Italy (spatially distant condition). Each graph showed either an upward or downward trend, with the final year (2004) always deviating from that global trend. Participants estimated the likelihood that the year 2005 would be consistent with the general trend or with the more recent local deviation. In terms of CLT, global trends convey a highlevel construal, whereas deviations, being local exceptions, should receive more weight in low-level construals. As expected, spatial distance enhanced the tendency to predict on the basis of the global trend rather than the local deviation. Discussion-Spatial and temporal distances increase the impact of high-level information (e.g., theories, self-beliefs, general trends) and decrease the impact of low-level information (e.g., irregular outcomes, specific situational and task characteristics) on prediction. Thus, two complementary processes may contribute to the unwarrantedly higher confidence levels associated with predictions about psychologically distant events: underweighting of the uncertainty associated with low-level information and overweighting of the certainty associated with high-level information. Although we may know less about distant than near situations, our greater reliance on high-level construals in predicting the more distant situations may lead us to make more confident predictions about distant than near situations. Past research has shown that individuals tend to be more optimistic about distant future than near future outcomes (Gilovich, Kerr, & Medvec, 1993;T. R. Mitchell, Thompson, Peterson, & Cronc, 1997;Nisan, 1972). According to CLT, greater optimism is a likely but not necessary result of temporal distance. That is, greater temporal distance promotes optimism only when high-level construals imply a relatively high likelihood of desired outcomes. When low-level construals imply a high likelihood of desired outcomes, optimism may not increase and may actually decrease with greater temporal distance. Indeed, Nussbaum et al.'s (2006) studies on predicting one's performance showed that temporal distance led to more optimistic predictions when the question format was difficult but not when it was easy. Thus, underweighting a low-level aspect of outcomes (question format) increased optimism or decreased optimism depending on whether that aspect implied a high likelihood or low likelihood of success. Distance from an outcome might also reduce personal involvement, thus giving rise to shallow processing and less cautious predictions (see e.g., Petty & Cacioppo, 1984). Again, the critical difference between this view and CLT is that the latter assumes that distance reduces the use of low-level information, not high-level information. Accordingly, confidence increases with distance from an outcome only when the confidence stems from high-level construal of the outcome. The Henderson et al. (2006) and Nussbaum et al. (2006) studies described previously support this prediction.",
          "We make choices and set preferences with respect to our construals of objects rather than the objects themselves. When we choose a car, we do not decide on the car itself but rather on our construal of the car. When we decide on a diet, we do so because the construal of its outcomes seems attractive to us. We argue that construals depend not only on the actual attributes of the objects but also on the object's psychological distance. We therefore predict that all distances should similarly affect evaluation and preferences inasmuch as they all promote higher level construals. A common assumption in psychology and behavioral economics is that the value of an outcome is discounted as temporal distance from the outcome increases (see, e.g., Ainslie, 1975;Loewenstein & Prelec, 1992;Rachlin, Brown, & Cross, 2000). CLT proposes, however, that temporal distance shifts the overall attractiveness of an outcome closer to its high-level construal value than to its low-level construal value. When the low-level value of an outcome is more positive than its high-level value, the outcome should be more attractive in the near future, thus showing time discounting. However, when the high-level value of an outcome is more positive, the outcome should be more positive in the distant future, thus showing time augmentation (see Liberman &Trope, 2003, andTrope &Liberman, 2003, for more extensive discussions). In the following, we apply this principle to central versus peripheral features, feasibility versus desirability concerns, arguments in favor versus against an action, alignable versus unalignable attributes as sources of value, and attributes versus alternatives in choice matrices. Central Versus Peripheral Features-According to CLT, central, goal-related features of outcomes constitute high-level construal of outcomes, whereas peripheral, relatively goalirrelevant features of outcomes constitute low-level construal of outcomes. Distancing an outcome should therefore increase the weight of central features relative to peripheral features. Trope and Liberman (2000) found support for this prediction in studies on evaluations of objects and events containing both a primary and secondary aspect. In one study, for instance, participants imagined buying a radio set either the next day or 1 year later, to listen to morning programs. In one version, participants read that the sound quality of the radio set was good, but that the clock that was incidentally included was relatively useless. In another version, participants read that the sound quality of the radio set was poor, but that the clock aspect was quite useful. As expected, thinking about the radio set in the more distant future increased satisfaction when the sound quality was good and the clock poor, but decreased satisfaction when the sound quality was poor and the clock good, indicating that time delay increased the weight of central features and decreased the weight of peripheral features (for related findings in persuasion contexts, see Fujita et al., 2008). Conceptually similar findings were obtained with social distance, operationalized as interpersonal similarity (Liviatan, Trope, & Liberman, 2008) and social power (P. K. Smith & Trope, 2006). For example, in one study (Liviatan et al., 2008, Study 4), participants evaluated an essay of either high or low quality (a central, high-level aspect of value) allegedly written by a student that performed well or poorly on an unrelated physics exam (a peripheral, low-level aspect of value). Some participants were led to believe that the writer was similar to themselves, whereas others were led to believe that the writer was dissimilar to them. The weight of the peripheral information about the physics ability (relative to that of the essay quality) was greater in participants' evaluations of the essay written by the similar student than by the dissimilar student. It is interesting that research which compared the decisions people make for themselves to the advice they give to others obtained similar findings. Kray and Gonzalez (1999) and Kray (2000) compared participants' own choices with the advice they gave to socially close and distant others. They found that in advising others, especially socially remote others, participants tended to give more weight to a single attribute which they designated as the most important and less weight to other, more peripheral attributes. For example, when advising another person about choosing between two jobs, participants gave more weight to personal satisfaction (which they viewed as the most important dimension) and less weight to salary and location (the less important dimensions) than when choosing for themselves (Kray, 2000, Study 2). In two other studies, Kray found that this preferential weighting of important attributes was stronger in advising a distant social target (a student in another department) than a closer target (a student in one's own class). Moreover, as advisers, participants rated central attributes as highly important and peripheral ones as unimportant, whereas as deciders they rated the various attributes as relatively similar in importance. It is worth noting that participants reported greater responsibility and potential regret, and generated a larger number of decision-relevant attributes when making decisions for others than for themselves. As Kray pointed out, it therefore seems unlikely that participants simply invested less effort in advising others than in deciding for themselves. In our terms, these findings demonstrate choosing according to more central, high-level aspects for socially distant than social close others. Feasibility Versus Desirability-Desirability concerns involve the value of the action's end state (a high-level construal feature), whereas feasibility concerns involve the means used to reach the end state (a low-level construal feature). Therefore, desirability concerns should receive greater weight over feasibility concerns as psychological distance increases. Consistent with this prediction, it was found that as temporal distance from an activity (e.g., attending a guest lecture) increased, the attractiveness of the activity depended more on its desirability (e.g., how interesting the lecture was) and less on its feasibility (e.g., how convenient the timing of the lecture was; Liberman & Trope, 1998). Similar results have been found for other distance dimensions, including hypotheticality and social distance (e.g., Liviatan et al., 2008;Todorov, Goren, & Trope, 2007; see review by Liberman et al., 2007). Arguments in Favor of and Against an Action-In deciding whether to undertake an action, cons are subordinate to pros. This is because the subjective importance of cons depends on whether or not pros are present more than the subjective importance of pros depends on whether or not cons are present. For example, if we know that a medical treatment has some health benefit, we would inquire about its potential side effects before making a decision. But if the treatment has no benefits, we would decide against taking it without further inquiry about its side effects. In contrast, we would inquire whether a medical treatment has health benefits whether or not it has side effects. Thus, the importance of side effects depends on whether the treatment is known to have benefits, but the importance of benefits is independent of whether the treatment is known to have side effects. If cons are subordinate to pros, then pros should become more salient as temporal distance from the action increases, whereas cons should become less salient as temporal distance from the action increases. To test this prediction, Eyal, Liberman, Trope, and Walther (2004) asked participants to generate arguments in favor and against new (i.e., nonroutine) near future or distant future actions, such as introducing a new exam procedure (e.g., switching to open-ended questions instead of multiple-choice questions; Study 2), social policies (e.g., restricting private cars in the city center; Study 3), and a variety of personal and interpersonal behaviors (e.g., approaching a fellow student and offering to write an assignment together; Studies 4-6). As predicted, in all the studies, participants generated more pros and fewer cons as temporal distance from the actions increased. In an extension of these findings, Herzog, Hansen, and W√§nke (2007) suggested that if pros are more salient as temporal distance increases and cons are more salient as temporal distance decreases, then an increase in temporal distance should make it easier to generate pros and more difficult to generate cons. Further, because attitudes tend to be more in line with content when the retrieval is experienced as easy (W√§nke & Bless, 2000), the ease of retrieval associated with generating pros and cons of near and distant future activities should influence attitudes toward those activities, even when the number of arguments is held constant. In a test of these ideas, participants read about a proposed action that was to happen in the near or distant future and were instructed to write down either four pros or four cons regarding the activity. As expected, participants (a) found it easier to generate pros and more difficult to generate cons when the issue concerned the distant rather than near future and (b) had more favorable attitudes toward the action when it was to occur in the distant future. Alternatives and Attributes in Choice Matrices-Choice and enactment of a course of action is often based on information people search for about the available alternatives. Decision theoretic work has distinguished between searching within attributes, across alternatives and searching within alternatives, across attributes (Tversky, 1972). In a typical study, participants are presented with a matrix of information in which rows represent alternatives (e.g., different apartments), columns represent attributes (e.g., price, location, noise), and cells include the standing of each alternative on the corresponding attribute. Participants search this matrix by exposing the information in each cell, one at a time (see Payne, Bettman, & Johnson, 1988, for a review of this paradigm). We think that searching attributes within alternatives reflects a lower level construal of the choice situation than searching alternatives within attributes. This is because attributes are instantiated in specific objects and when detached from specific objects they become abstractions. Attributes are general, whereas objects are unique and variable manifestations of specific combinations of attributes. We thus expect that within-alternative search would characterize processing of proximal decision situations, whereas within-attribute search would characterize processing of distal decision situations. Borovoi, Liberman, and Trope (2010) recently tested this prediction. In a study on temporal distance, participants considered a choice for either the near future (e.g., choosing an apartment to rent in the next 2 weeks) or the distant future (e.g., choosing an apartment to rent a year later). In a study on social distance, participants considered a choice either for themselves or for another student. As expected, there were more within-alternative steps and less within-attribute steps when making decisions for psychologically proximal situations than for psychologically distal situations. It is important to note that in both studies, participants opened an equal number of cells and invested a similar amount of time in both distance conditions, indicating that they were not less motivated in the distal condition than in the near condition. It seems then that when making choices for proximal rather than distal situations, people are more likely to organize information within alternatives rather than within attributes. Alignable Versus Nonalignable Attributes-As noted earlier, comparing nonalignable objects (with information provided about different attributes for each object) requires higher level construal than comparing alignable objects (with information provided about the same attributes for each object), because making nonalignable objects comparable requires representing them in terms of more abstract attributes. On the basis of this argument, Malkoc, Zauberman, and Ulu (2005) reasoned that decisions that have distant future consequences (relative to near future consequences) would involve an increased consideration of nonalignable attributes. In a test of this prediction, participants evaluated two brands of potato chips and selected one brand to receive either at the end of the session (near future) or at the end of the semester (distant future). The two brands were designed based on pretesting to be equally attractive overall; one of the options, however, was designed to be better on its alignable attributes, whereas the other brand was better on its nonalignable attributes. As expected, temporal distance shifted both evaluations and choice toward the nonalignable better option over the alignable better option, indicating an increased reliance on nonalignable attributes when making decisions with distant future consequences. Discussion-Considerable evidence suggests that the value of outcomes is discounted or augmented over psychological distance depending on how it is distributed between highlevel and low-level aspects of the outcomes. People seem to appreciate proximal outcomes for their low-level aspects and distal outcomes for their high-level aspects. Although they often know less about distant than near outcomes, their evaluation of the distant outcomes might be more extreme when high-level construal of the outcomes have strong evaluative implications. These findings are consistent with CLT and the more general view that valuation is a constructive process (see e.g., Payne, Bettman, & Johnson, 1993;Shafir, Simonson, & Tversky, 1993;Weber et al., 2007). It is important to point out that the effect of distance on the value of outcomes is distinct from its effect on the controllability of the outcomes (Liberman & Trope, 2003). Psychological distance from outcomes often reduces the extent to which the outcomes are contingent upon one's action. For example, people typically have less control over others than themselves, over strangers than friends or relatives, and over events that occur in spatially remote than near locations. Moreover, because the number of ways of attaining an outcome often diminishes with proximity to the outcome, one's immediate action is less critical for a psychologically distant outcome to occur. For example, a long time before an exam, failing to study may be compensated by studying more intensely later, but a short time before the exam, when only a few hours remain, such possibility no longer exists. Similarly, failing to help a stranger may be compensated by help from another stranger, but failing to help a close friend is less likely to be compensated by someone else, because people typically have fewer close friends than strangers. By making outcome less contingent on one's action, greater psychological distance may decrease the motivation to engage in that action. Students may be less motivated to study for a distant exam than for a close exam because failure to study is easier to compensate for at a distance, or people may be less motivated to help a stranger than a close friend, because in the former case, their failure to help is more likely to be compensated by others. These changes in motivation do not reflect changes in value (e.g., the value of success on an exam may not increase closer to it). However, in many situations, measures of value might be contaminated with motivation and therefore show discounting over psychological distance. This could be one of the reasons for the widespread belief in economics and the behavioral sciences that value is discounted over time and psychological distance.",
          "Like predictions and evaluations, behavioral intentions and self-regulation should be increasingly based on high-level construal aspects as psychological distance increases. In the following, we examine the implications of this idea for values and ideologies, self-control, and negotiation. Values as High-Level Behavioral Guides-Values are commonly viewed as abstract structures that provide continuity and meaning under changing environmental circumstances (Feather, 1995), as stable meaning-producing superordinate cognitive structures (Rohan, 2000), and as trans-situational guides for action (S. H. Schwartz & Bilsky, 1987). On the basis of CLT, we propose that values, because of their relatively abstract and decontextualized nature, will be more readily applied to and guide intentions for psychologically distant situations. Evidence for this proposal has been recently obtained by Eyal, Sagristano, Trope, Liberman, and Chaiken (2009). One study used S. H. Schwartz's (1992) value questionnaire to assess the importance participants assigned to a wide range of values (e.g., power, benevolence, hedonism) and then asked participants to imagine 30 behaviors (e.g., \"rest as much as I can\") and to indicate the likelihood of performing each behavior either in the near future or in the distant future. Eyal et al. correlated the rated importance of each value and the mean likelihood of performing the behaviors corresponding to that value. As expected, these correlations were higher when the behaviors were planned for the distant rather than the near future, suggesting that people's values are better reflected in their intentions for the distant future than in their intentions for the immediate future or their actual behavior. For example, being high (vs. low) in hedonism might mean planning hedonic activities for the distant future, but not necessarily for the upcoming week. It is interesting that Eyal et al. (2009) also found that although values predicted participants' intentions for the distant future, feasibility concerns were more predictive of their intentions for the near future. For example, the number of hours participants volunteered in the distant future condition was predicted by their benevolence values but not by the convenience of the timing. In contrast, the number of hours participants volunteered in the near future condition was not predicted by their benevolence values and instead depended on the convenience of the timing. Extending this line of thought, Eyal, Liberman, and Trope (2008) argued that people judge immoral acts as more offensive and moral acts as more virtuous when the acts are psychologically distant than near. They showed that transgressions against core values that are deemed harmless because of extenuating circumstances (e.g., eating one's dead dog) were judged more severely when imagined from a more distant temporal or social perspective. Conversely, moral acts which might have had ulterior motives (e.g., adopting a disabled child when a government pays high adoption pensions) are judged more positively from temporal distance. The findings suggest that moral criteria are more likely to guide people's judgments of distant rather than proximal behaviors. Ideology and Incidental Social Influence-Our attitudes shift, often outside of our awareness, in response to other people in our local social context, including communication partners, significant others, and even total strangers (Baldwin & Holmes, 1987;Davis & Rusbult, 2001;Higgins & Rholes, 1978;Kawakami, Dovidio, & Dijksterhuis, 2003;Lowery, Hardin, & Sinclair, 2001). CLT predicts that when an attitude object is psychologically near, evaluations will be attuned to a particular social context and therefore more likely to be affected by incidental attitudes of others in the social situation rather than by one's ideology. Conversely, when the attitude object is psychologically distant, it will be abstracted away from its local context, and evaluation will therefore be less affected by the incidental attitudes of salient others and, instead, reflect one's ideology. A series of studies by Ledgerwood et al. (2008) tested the hypothesis that attitudes will align with those of another person in the local social context more when psychological distance is low (vs. high). Using an anticipated interaction paradigm, participants read about a policy that would increase the deportation of illegal immigrants starting either next week (near future) or next year (distant future) and learned that their discussion partner was either in favor of or against deporting illegal immigrants. They then privately reported how likely they would be to vote in favor of the policy. Participants' voting intentions shifted toward the interaction partner's attitude when the policy was set to be implemented in the near future but not when it was to be implemented in the distant future. However, voting intentions more strongly reflected participants' previously assessed ideological values when the policy was to be implemented in the distant (vs. near) future. Specifically, the more participants valued preserving the societal status quo, the more they supported a distant future policy that would enforce the deportation of illegal immigrants. Self-Control-Situations that require self-control involve a conflict between two opposing motivations (e.g., a desire to go out with friends and a need to study for an exam). Fujita, Trope, Liberman, and Levin-Sagi (2006) proposed an analysis of self-control conflicts as conflicts between behavioral implications of high-level construal (i.e., valence attached to primary, central, goal-relevant, superordinate considerations) and behavioral implications of low-level construal (i.e., valence attached to secondary, incidental, goal-irrelevant, subordinated, features). Failure of self-control, according to this proposal, is succumbing to the motivation implied by the low-level value. For example, if studying for an exam is related to more superordinate goals than going out with friends, then the latter behavior would represent a failure of self-control. Consistent with this analysis, a series of studies by Fujita et al. demonstrated that higher level construal increased self-control. In one study, participants first completed a task in which they indicated either why or how they would maintain good physical health. This task was designed to induce a high-level or low-level construal mindset, respectively (see Freitas, Gollwitzer, & Trope, 2004). Participants were then asked to hold a handgrip while connected to bogus electrodes, ostensibly as part of a psychophysiological assessment of personality. Participants were told that the longer they held the handgrip, the more diagnostic was the information obtained from the apparatus. Thus, the situation presented a conflict between a desire to get diagnostic, self-relevant information (high-level value) and the inconvenience of holding the handgrip (low-level value). The results indicated, as predicted, that participants in the high-level construal condition held the handgrip longer than those in the low-level construal condition. Just like higher level of construal, greater temporal, spatial, or social distance from a situation that poses a self-control dilemma should also enhance self-control. Indeed, research on intertemporal preference reversals has shown that people are better able to commit to self-control a long time in advance than a short time in advance (e.g., Ainslie & Haslam, 1992). A study conducted by Freitas, Salovey, and Liberman (2001) specifically showed that people were better able to forgo flattering but useless feedback for the sake of unpleasant but useful feedback when the feedback was delayed than when it was imminent. Similarly, research on children's delay of gratification has shown that greater temporal and spatial distance from a tempting object enhances self-control (Metcalfe & Mischel, 1999;Mischel, Shoda, & Rodriguez, 1989). CLT would further predict that we are more likely to advise another person to exercise self-control than to make a similar decision for ourselves, and that deciding on self-control is easier when it is premised on less likely events. For example, we could predict that deciding to start a diet upon getting an A in an exam would be facilitated to the extent that getting an A is less likely. In general, psychologically distancing oneself from a situation might promote people's ability to exercise self-control in that situation (see Kross & Mischel, in press). Negotiation-As with values, issues within an interpersonal negotiation can differ in their centrality and worth. If a pair of negotiators can trade off their lowest and highest priority issues (e.g., give in on secondary issues in exchange for getting what they want on highpriority issues, a process called logrolling), they are more likely to succeed in \"expanding the pie,\" maximizing both individual and joint outcomes. Because negotiators should be expected to focus more on central concerns and less on peripheral concerns as distance increases, we would expect to see more logrolling agreements in a distant future than near future context. Examining this idea within the context of a live negotiation, Henderson, Trope, and Carnevale (2006) found that whereas 91% of dyads with a temporally distant perspective reached a full logrolling agreement, only 50% of dyads with a temporally near perspective did so. The enhanced reciprocal concessions made by dyads with the temporally distant perspective culminated in better negotiated individual and joint outcomes. Moreover, research on the role of construal levels in the negotiation process has shown that negotiators who construed issues abstractly rather than concretely were more likely to discover integrative agreements (Henderson & Trope, 2009). For example, having negotiators think abstractly about issues (by having them generate general descriptions) rather than concretely about issues (by having them generate specific descriptions) increased negotiators' logrolling and individual and joint outcomes. These findings have implications for how negotiators handle conflicts over minor versus major issues and specific interests versus broad values and ideological differences in situations that do not allow for tradeoffs (Druckman & Broome, 1991;Harinck & De Dreu, 2004). Specifically, CLT suggests that the resolution of minor issues and specific interests should be hindered when individuals adopt a psychological proximal perspective or a lower level construal and facilitated by having a more distal perspective and abstract construal. Conversely, the resolution of conflict over major issues, values, and ideological differences should be hindered when individuals adopt a psychologically distant perspective or a higher level construal and facilitated by a less abstract construal and a psychologically proximal perspective. Discussion-The research reviewed in this section suggests that adopting a distal versus proximal perspective changes the way people make behavioral plans, resolve value conflicts, negotiate with others, and cope with self-control problems. The choices people make for psychologically distant situations are guided by their general attitudes, core values, and ideologies. As people get psychologically closer to the situation, their choices are increasingly influenced by more specific attitudes, secondary values, and incidental social influences. It seems, then, that from a distant perspective, global concerns are prioritized and unequivocally pursued, whereas from a proximal perspective, those priorities are weakened and even reversed as local concerns become more prominent. People may have less information about distant situations. Nevertheless, they often act as if they know better what to do or what they should have done when they remove themselves from the situation and take a distant rather than proximal perspective on it. Implications and Extensions-To recapitulate, the research we have reviewed thus far suggests that psychological distance triggers high-level construals, which focus on the central and enduring features of an object while screening out incidental and peripheral details. These high-level construals promote attunement to what is consistent about an object across multiple contexts, allowing individuals to transcend the particularities of the present situation and act according to their global concerns. Conversely, psychological proximity triggers low-level construals, which include the concrete and contextualized aspects of an object. These in turn are attuned to the current context, immersing individuals in the immediate situation and enabling them to flexibly respond to local concerns. In this final part of the article, we explore new directions for research on more speculative implications and extensions of CLT. These include psychological phenomena that might map onto different levels of construal (Section VII) and phenomena that might map onto different psychological distances (Section VIII). We conclude with a discussion of psychological distance in the brain (Section IX).",
          "We have shown that the distinction between different levels of construal is applicable to many psychological phenomena (e.g., gestalts vs. details, trait vs. situation attributions, pros vs. cons). In this section, we suggest that two more distinctions might map onto high-versus low-level of construal: assimilation versus contrast and high-versus low-level emotions. Assimilation and Contrast-When perceiving two stimuli in relation to each other (compared with perceiving the same stimuli separately), a perceiver may assimilate the two stimuli to each other, thereby perceiving each of the stimuli as more similar to the other stimulus, or contrast them away from each other, thereby perceiving each of the stimuli as more distinct from the other stimulus (Parducci, Perrett, & Marsh, 1969;Schwarz & Bless, 1992, 2007). For example, when considering a paper of a student in relation to the best paper in class (vs. considering it in isolation), the focal paper may be assimilated to the excellent paper and thus seem better, or it may be contrasted away and thus seem worse. In view of the opposite effects of assimilation versus contrast, the question of what makes each of them more likely becomes crucial (Mussweiler, 2001;Schwarz & Bless, 2007;Stapel, 2007). Most germane to the present framework are Schwarz andBless's (1992, 2007) inclusion/ exclusion model and F√∂rster, Liberman, and Kuschel's (2008) global/local model of assimilation versus contrast. According to these models, because global, high-level construals are more inclusive, using those construals is likely to result in including two stimuli in the same category and an assimilation effect. Using low-level construals, however, is likely to result in categorizing the two stimuli in separate categories and a contrast effect. Consistent with this view, it has been found that priming of high-level construal and greater temporal distance enhances assimilation and reduces contrast. For example, in one of the studies, (F√∂rster et al., 2008, Study 4), participants compared their athletic skills with either a moderately high standard or a moderately low standard and then rated their expected athletic performance in an athletic competition that would take place the next day (proximal temporal perspective) or a year from then (a distal temporal perspective). In the control condition, time was not specified. The results showed that a distant time perspective enhanced assimilation (i.e., produced a high self-rating after comparison to a high standard and a low self-rating after comparison to a low standard), whereas a proximal time perspective enhanced contrast (i.e., produced a low self-rating after comparison to a high standard and a high self-rating after comparison to a low standard). Borovoi et al. (2008) reasoned that if distance enhances assimilation, then it would also enhance the positive effect that attractive but unattainable alternatives might have on evaluations of the choice set. Consistent with this prediction, they showed that adding a very attractive job that was no longer available to a set of job offers increased the attractiveness of the entire set of job offers when the jobs were expected in the distant future rather than the near future. It seems that the attractiveness of available options is assimilated to that of attractive but unattainable possibilities when thinking about the relatively distant future. Affect-It is commonly assumed that psychological distance from an object diminishes the intensity of affective responses to that object. People typically react more strongly to events that are closer to them in time and space, to events that happen to themselves than to others, and to events that are real rather than hypothetical. However, building on appraisal theories of emotion (see, e.g., Beer & Keltner, 2004;Clore, Ortony, & Foss, 1987;Ortony & Turner, 1990;Tangney & Fischer, 1995;Tracy & Robins, 2007), CLT suggests that it might be useful to distinguish between emotions that may ensue from a low-level construal of an emotion-eliciting event and emotions that ensue from a high-level construal of the event. For example, love might be based on a more general and abstract representation of the target person than lust (F√∂rster, √ñzelsel, & Epstude, in press). It is interesting that some emotions involve, by their very definition, taking a distant perspective. For example, guilt and shame require taking a socially distant perspective, as they include considering the situation from the perspective of another social agent (Beer & Keltner, 2004), whereas sadness does not require taking such a socially distant perspective. Anxiety involves consideration of a potential, distal danger (for mice, anxiety is often induced by a smell of a cat), whereas fear is a reaction to an immediately present danger (e.g., for mice, the presence of a cat; Gray, 2000). Likewise, counterfactual emotions (e.g., regret) involve consideration of hypothetical alternatives to reality. According to CLT, emotions that involve taking a distant perspective require relatively abstract, high-level construal of the emotion-eliciting situation. It would be interesting to examine whether, indeed, some emotional experiences are more abstract than others. For example, would thinking of guilt and anxiety prime people to think in higher level terms in a subsequent, unrelated task? Once the classification of emotions to high level versus low level is established, CLT would predict that higher level emotions would decay less steeply and may even intensify over distance. Eyal and Fishbach (2008) recently accrued evidence in support of this hypothesis. They showed, for example, that pride decays over time less than happiness. Because of their distance transcending qualities, abstract emotions seem to assist in self-control more than concrete emotions. For example, Eyal and Fishbach showed that among dieters, priming participants with pride rather than happiness resulted in higher choice of a carrot (a self-control choice) rather than a chocolate bar (a self-control failure). CLT would further suggest that even the same emotion may be high or low-level, depending on whether it reflects high or low-level construals of the object. For example, in a funeral, sadness about the transience of life and compassion are central and thus constitute high-level emotions, but happiness upon seeing old friends is peripheral and thus constitutes a lowlevel, peripheral emotion. In a birthday party, however, the centrality of these emotions reverses. We would predict that in this respect too, secondary emotions would diminish over distance more than primary emotions. For example, the positive value that may be added to a funeral by the prospect of meeting friends would be more apparent when the funeral is closer in time and space, when we anticipate going ourselves rather than think about somebody else and when attending the funeral is likely rather than unlikely.",
          "",
          "Social power: Social power may engender a sense of distance from others. Indeed, individuals who have power see themselves as less similar to and thus more distant from other people than individuals who have less power (e.g., Hogg, 2001;Hogg & Reid, 2001;Lee & Tiedens, 2001;Snyder & Fromkin, 1980). This perception might be due to the fact that groups, organizations, and societies ordinarily have a pyramidal structure with fewer individuals occupying high-power positions than low-power positions. There is therefore greater similarity in the positions held by individuals with low power than by individuals with high power. If social power makes people feel distant from others, then it should also predispose them to construe information abstractly, focus on the central aspects of situations, disregard secondary aspects, and establish clear priorities. Several lines of research are consistent with these predictions. For example, Overbeck and Park (2001) observed that high-power participants were better at distinguishing between central and peripheral information, a hallmark of abstract processing. Guinote, Judd, and Brauer (2002) found that assigning participants to high-power positions promoted the use of abstract language. A series of studies by P. K. Smith and Trope (2006) examined the effect of power priming on a range of construal-level phenomena. Their findings show that power primed participants (a) were better able to abstract visual stimuli in an embedded figures task and Gestalt completion task, (b) were better at detecting covariation in a series of data, and (c) made more superordinate categorizations. Finally, Magee, Milliken, and Lurie (in press) have recently found that the level of abstraction in verbatim reactions to the September 11, 2001 attack was higher for individuals with high social power, compared to those with low social power. These findings suggest that the distal perspective activated by the possession of social power promotes going beyond the information given, detecting the underlying structure, and abstracting from it superordinate, central features. Power-related construal may expand people's mental horizons, enabling them to transcend the immediate circumstances and take into account the past, future, a broad range of people, and unlikely possibilities. Recent research by P. K. Smith, Wigboldus, and Dijksterhuis (2008) suggests that the relationship between power and construal level might be bidirectional. Their findings show that thinking abstractly enhances individuals' sense of their own power. Are there analogous effects of level of construal on the way we perceive others? Do we think of individuals as suitable for power positions, or as actually holding such positions, when they articulate and enact highlevel construals of the situation? Do our inferences and preferences as to the power individuals possess depend on whether they can articulate plans that transcend the present and extend to the distant future, remote places, diverse groups, and unusual circumstances? These questions await future research.",
          "The endowment effect suggests that when individuals are asked to sell an object they own, they ask for a significantly higher price than buyers tend to offer (Thaler, 1980). For example, in a typical endowment study, half of the participants receive a mug and are asked to set a price at which they would be willing to sell the mug to the other half of the participants, who in the meantime set the price at which they would be willing to buy the mug (e.g., Kahneman, Knetsch, & Thaler, 1990). Results of numerous such experiments and field studies show that average selling prices are two to three times higher than average buying prices. Given that the benefit of having the mug and, hence, its objective value, should not change with random ownership, these results present a perplexing phenomenon. The most widely accepted explanation of the endowment effect is loss aversion (Bar-Hillel & Neter, 1996;Kahneman et al., 1990; but see also Liberman, Idson, Camacho, & Higgins, 1999) that goes back to prospect theory (Kahneman & Tversky, 1979). Prospect theory suggests that individuals' perception of losses and gains differ, such that the subjective pain of a loss is more intense than the subjective pleasure of a gain of similar objective value. According to a prospect theory approach to the endowment effect, selling an object is perceived as a loss, compared with the seller's reference point of having the object; buying, in contrast, is perceived as a gain, compared with the buyer's reference point of not having an object. Because individuals are loss averse, they therefore tend to evaluate objects they consider selling more positively than they do objects they consider buying. Adopting a CLT approach to the endowment effect, Irmak, Wakslak, and Trope (2009) argued that sellers adopt a distant perspective on an object they are considering selling, thinking about the object from the perspective of the distant other to which the object will be moving; whereas buyers consider the object from a more proximal perspective, considering how they personally view this nearing object. As a result, sellers will construe the object on a high level, and selling prices will largely be influenced by the object's high-level features (and less by its low-level features). In contrast, buyers will construe the object on a lower level, and buying prices will therefore be influenced by the object's low-level features. In other words, we argue that sellers and buyers think about the object differently from one another, focusing, and thus basing their pricing decisions, on different types of object features, which, in turn, may lead to systematic differences in selling and buying prices. A series of studies by Irmak et al. (2009) provides initial support for this analysis. First, they showed that sellers indeed tended to think about an object in terms of its value for the buyer (indicating a distal perspective), whereas buyers tended to think about the object in terms of its value for themselves (indicating a proximal perspective). Second, sellers, compared with buyers, were more likely to construe actions related to those objects in high-level, endsrelated terms rather than low-level, means-related terms. Third, selling prices were higher than buying prices for objects whose attractiveness derived from high-level construal features, but this was not the case for objects whose attractiveness derived from low-level construal features. For example, selling prices were higher than buying prices when the object was superior on the desirability dimension and inferior in the feasibility dimension, but not vice versa, and when the primary (goal-relevant) aspects of the object were superior and the secondary (goal-irrelevant) aspects of the object were inferior, but not vice versa. These findings demonstrate that sellers and buyers derive value from different aspects of the object. Sellers take a psychologically distal perspective and thus extract value from the highlevel, primary aspects of the object, whereas buyers assume a psychologically proximal perspective and their perception of value is influenced less by primary aspects of the object and more by its secondary aspects.",
          "Proximal versus distal senses: We view the four psychological distance dimensions as being anchored on a single starting point (zero-distance point): me, here, and now. This view may be oversimplified, however, as some direct experiences may be more proximal than others. More specifically, the five senses-sight, hearing, smell, touch, and taste-may be mapped along spatial distance according to the maximum physical distance of the sensed object. An object has to be in one's mouth to be tasted, it has to be within one's reach to be touched, it may be farther away to be smelled, and it can be still farther away to be heard or seen. The distant senses, sight and hearing, enable people to extend the scope of perception far beyond the range of the near senses, which are limited to the person's immediate environment (Boring, Langfeld, & Weld, 1939; see also Rodaway, 1994). Do proximal senses produce a psychologically closer experience than distal senses? If they do, then the antecedents and consequences of psychological distance should apply to stimuli that are perceived with proximal versus distal senses, such that proximal senses would be associated with low-level construal and psychological proximity, whereas distal senses would be associated with high-level construal and psychological distance. For example, sharing an experience of tasting something with another person might produce more social closeness to that person than sharing an experience of looking at something. Similarly, touching a product might make a consumer give more weight to its feasibility properties and less weight to its desirability properties than only looking at that product. Novelty: Novel events are unfamiliar and often (but not always) subjectively improbable. Novel objects may therefore be perceived as more psychologically distant. The question, then, is whether novelty affects construal and construal-mediated outcomes in the same way as psychological distance does. Addressing this question, F√∂rster, Liberman, and Shapira (2009) found that when stimuli were presented as novel they were construed at a higher level than when they were presented as old. Novelty and familiarity were manipulated by either framing a task as new versus familiar or by asking participants to reflect upon novel versus familiar events prior to the task (i.e., procedural priming). The studies showed that relative to the control, no priming or no framing group, novelty priming or framing enhanced global perception, broad categorization, and high-level action identifications. Familiarity priming or framing had the opposite effect. Another study showed that presenting a product as novel (e.g., \"a novel model of a cellular phone\") versus old (e.g., \"Version 3 of an existing model of a cellular phone\") made participants assign greater weight to the products' high-level, desirability features and less weight to their low-level, feasibility features. It would be interesting to examine in future studies the effect of novelty on other construalmediated outcomes, such as creativity and self-control. Because creativity has been shown to be related to high-level construal (F√∂rster et al., 2004), we might predict that presenting a task as novel (vs. old) would enhance creative performance. It is also possible that because high-level construal is conducive to self-control (see Fujita, Trope, et al., 2006), presenting self-control programs (e.g., dieting or smoking cessation programs) as novel (vs. old) would promote success in those programs. Generalizing from experience: Our responses to a new object are often based on generalizations from a familiar, previously experienced object. The generalization may result from the co-occurrence or some resemblance between the familiar object and the new object. Of special interest are generalizations, often called projections, from ourselves to new people who become associated with us. By definition, the self is more proximal than another person. Therefore, we are likely to form low-level construals of ourselves, while generalizing our high-level construals of ourselves to others. High-level construals of the self are more likely to drive responses to others, whereas low-level construals of the self are more likely to drive responses to self. As a result, the more people value their high-level self characteristics relative to their low-level self characteristics, the more likely are they to form favorable impressions of unfamiliar others. Again, the bias toward projecting one's highlevel characteristics rather than low-level characteristics onto another person is likely to be augmented with greater psychological distance from that person.",
          "Do psychological distances and their links to construal level correspond to the structure and functions of the brain? A growing body of research suggests that they do. For example, recent research suggests that a common brain network involving the prefrontal cortex and the medial temporal lobe is implicated in prospection, retrospection, and taking another person's perspective (e.g., Buckner & Carroll, 2007;J. P. Mitchell, Macrae, & Banaji, 2006;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). In particular, different subregions of the medial prefrontal cortex (mPFC) have been associated with psychological processes that vary in their levels of abstraction. In their review on this subject, Amodio and Frith (2006) noted that representations of low-level goals and actions tend to be associated with activity in more posterior and ventral regions of the mPFC, whereas more complex and hierarchically organized representations are typically associated with more anterior and dorsal regions. On the basis of anatomical studies of connectivity in this region, Amodio and Frith (2006) proposed that activity in the mPFC corresponds to two different axes associated with the degree of abstraction (running posterior to anterior) and to action (running ventral to dorsal). In a related vein, J. P. Mitchell et al. (2006) found that processing information about a similar other engages the ventral region of the mPFC, whereas processing information about a dissimilar other engages more dorsal subregions of the mPFC. Moreover, recent research by Mitchel, Ames, and Gilbert (2008) observed an association in the brain between social distance and temporal distance. Specifically, the ventral mPFC was implicated in processing information about similar others and the present self, whereas the dorsal mPFC was implicated in processing information about dissimilar others and the future self. More generally, there is research showing that the brain is hierarchically organized, with higher points in the cortical hierarchy representing increasingly more abstract aspects of stimuli (Grill-Spector & Malach, 2004;Lieberman, Gaunt, Gilbert, & Trope, 2002). For example, progressively anterior and dorsal subregions of the prefrontal cortex have been found to be associated with more abstract representations (Badre, 2008;Koechlin & Summerfield, 2007;Ramnani, & Owen, 2004). This organization of information in the brain might be related to distance from stimuli, such that activation systematically progresses to higher points in the hierarchy as psychological distance from the stimuli increases. In sum, there may be a \"construal axis\" in brain systems that maps onto a \"distance axis.\" Brain systems that process high-construal information may be biased toward distal information, and brain systems that process low construal information may be biased toward proximal information.",
          "In reality, the different distance dimensions are separable. The fact that something happened long ago does not necessarily mean that it took place far away, that it occurred to a stranger, or that it is improbable. Nevertheless, as the research reviewed here demonstrates, there is marked commonality in the way people respond to the different distance dimensions. CLT proposes that the commonality stems from the fact that responding to an event that is increasingly distant on any of those dimensions requires relying more on mental construal and less on direct experience of the event. The findings we reviewed in the present article substantiate this claim in showing that (a) the various distances are cognitively related to each other, such that thinking of an event as distant on one dimension leads one to thinking about it as distant on other dimensions, (b) the various distances influence and are influenced by level of mental construal, and (c) the various distances are, to some extent, interchangeable in their effects on prediction, preference, and self-control. Past psychological research has typically investigated the different distance dimensions within disparate theoretical frameworks and methodologies. The research reviewed here suggests that psychological distance, as conceptualized here, captures a fundamental aspect of meaning common to all distances and may provide a unifying framework for understanding a wide range of seemingly unrelated psychological phenomena. Finally, taking an even broader perspective on psychological distance, it is worth noting that both collective and personal human development are associated with traversing increasingly greater distances. The turning points of human evolution include developing tools, which required planning for the future; making function-specific tools, which required considering hypothetical alternatives; developing consciousness, which enabled the recognition of distance and perspective taking; developing language, which enabled forming larger and more complex social groups and relations; and domestication of animals and plants, which required an extended temporal perspective (Flinn, Geary, & Ward, 2005). Human history is associated with expanding horizons: traversing greater spatial distances (e.g., discovering new continents, space travel), forming larger social groups (families vs. cities vs. states vs. global institutions), planning and investing in the more distant future, and reaching farther back into the past. Human development in the first years of life involves acquiring the ability to plan for the more distant future, consider possibilities that are nonpresent, relate to and take the perspective of more distant people (from self-centeredness to acknowledging others, from immediate social environment to larger social groups; Suddendorf & Corballis, 2007). Although the areas of evolution, history, and child development have different time scales, research in these domains seems to converge on the notion that transcending the present requires and is enabled by the human capacity for abstract mental representation. We hope that the present research on psychological distance, its relationship to level of construal, and its consequences for thought, feeling, and action advances our understanding of how individuals and groups transcend the here and now. Example of the four Spatial Location √ó Word Meaning combinations in Bar-Anan, Liberman, Trope, and Algom (2007). The illustration demonstrates words related to hypotheticality. Sample items from the Street Gestalt Completion Test (Street, 1931), provided courtesy of Teachers College, Columbia University. Identifying the pictures (from top right to bottom left: a boat, a rider on a horse, a rabbit, a baby) requires visual abstraction. Example of four Social Distance √ó Medium (picture vs. word) combinations in Amit, Algom, and Trope (2009). For the Israeli participants in this study, a shekel was socially proximal, and a dollar was distal."
        ],
        "ground_truth_definitions": {
          "psychological distance": {
            "definition": "a subjective experience that something is close or far away from the self, here, and now.",
            "context": "They serve to transcend the immediate situation and represent psychologically distant objects. Psychological distance is a subjective experience that something is close or far away from the self, here, and now. Psychological distance is thus egocentric: Its reference point is the self, here and now, and the different ways in which an object might be removed from that point-in time, space, social distance, and hypotheticality-constitute different distance dimensions.",
            "type": "explicit"
          },
          "high-level construal": {
            "definition": "relatively abstract, coherent, and superordinate mental representations, compared with low-level construals.",
            "context": "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental.",
            "type": "explicit"
          },
          "correspondence bias": {
            "definition": "a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions",
            "context": "A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995; E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995; Semin & Fiedler, 1988; Semin & Smith, 1999). Distance should therefore enhance the correspondence bias.",
            "type": "explicit"
          },
          "centrality": {
            "definition": "Changing a high-level feature has a greaterimpact on the meaning of an object than does changing a low-level feature",
            "type": "explicit",
            "context": "The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is."
          },
          "subordination": {
            "definition": "The meaning of low-level features depends on highlevel features more than vice versa",
            "type": "explicit",
            "context": ". The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. T"
          }
        }
      },
      {
        "paper_id": "2106.16117v1",
        "sections": [
          "The concept of the 'energy security' was first introduced by industrialized market economies in 1947, when the United States adopted a document regulating the actions of the state in the field of national security. Although it has been investigated and developed by a wide range of scholars and specialists, still there is no universal understanding of what is meant by this notion.",
          "In international relations, the term 'energy security' acquired significance and became widely used after the global oil crisis of 1973-1974, when the world faced a sudden and noticeable increase in prices for fuel and energy resources caused by the following political events [1]: ‚Ä¢ collective embargo imposed by the Arab OPEC countries on the export of oil to the USA; ‚Ä¢ support of Israel in the October war by a number of European countries. Being faced with the first largest energy crisis, industrial countries acutely felt their vulnerability and consequently energy security became one of the top priorities, considered a part of the country's national security. As mentioned above, currently, despite the abundance of modern research and publications on energy topics, a single generally accepted definition is still not formed. In point of fact, the meaning of energy security differentiates from country's dependence to their energy imports. Accordingly, countries which are highly dependent on imported oil and gas adheres energy security to supply whereas, countries which export oil and gas adheres energy security to demand. This is due to the fact that different groups of countries, namely exporting countries, importing countries and transit countries interpret this concept in their own way, based on their national and economic interests and priorities. Therefore, in order to give a clear definition of the concept of \"energy security\", it is imperative to consider it from the perspective of all three parties. But, nevertheless, in modern scientific literature there are many concepts and interpretations of international energy security, and representatives of different schools or countries offer their own definitions of this notion. The International Energy Agency (IEA), which is an international body that unites 29 industrialized countries under the Organization for Economic Development and Cooperation, defines energy security as 'the uninterrupted availability of energy sources at an affordable price' [2]. The limitations of this approach are directly implied by the fact that in this context, it considers energy security from the point of view of importing countries that reflects their national priorities in the face of growing dependence on imports of oil and natural gas. Nevertheless, the IEA admits that the concept of 'energy security' can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces 'timely investments to supply energy in line with economic developments and environmental needs', while the latter deals with 'the ability of the energy system to react promptly to sudden changes in the supplydemand balance' [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply. Beijing defines 'energy security' as 'reliable uninterrupted supply of energy, which is necessary for economic development of the country' [1]. In other words, energy security is seen as a need for uninterrupted supply of energy resources in order to maintain stable economic growth. Meanwhile, the exporting countries, including Russia, do not share this position. They are primarily interested in diversifying the markets of energy exports, preserving their national sovereignty and of course guaranteeing sustainable demand for the energy they export. Because of diverging interests in the energy market, exporting countries interpret 'energy security' in another way. The OPEC group (a permanent intergovernmental organization of 15 oilexporting developing nations that coordinates and unifies the petroleum policies of its Member Countries [4]) defines energy security as 'reliable and environmentally friendly energy supplies at prices reflecting fundamental principles of market economy'. This approach is also limited, as the IEA one, because it reflects only the interests of exporting countries that are focused on determining a decent price for energy resources. The interests of transit countries consist mainly in maximizing transit rent of energy resources. This is the approach Turkey follows, -the country is a natural energy corridor between the Middle East, the Caspian basin and Europe and contributes significantly to the attempts of Europe to diversify its energy suppliers for natural gas. In the meantime, the concept of energy security framed as an access to fossil fuels created the so-called 'energy nationalism' [3] that created a reality in which the behavior and decisions made on energy markets and the delivery of resources ultimately depended not on the economic market factors, but on producers, which resulted in the transformation of the energy market into the subject of inter-state relations. Oil and natural gas were used as geopolitical weapons, and geopolitics and geo-economics became an important part of world politics and foreign policy of the main players on the energy market. Energy security undoubtedly depends on the national and international background. In the context of state-controlled markets, the main guardians of energy security are the governments. On the contrary, given the energy markets are liberalized and the main actors are private companies, the security of supply consists of an efficient risk management strategy by governments, companies and consumers. Therefore, it is important not to fall into purely ideological approaches. Since Adam Smith, the state is authorized to intervene in the economy in order to prove security, including its energy component, to its citizens. Baring that in mind, it should be noted that such intervention should be done as far as possible in a concerted manner along with the companies and consumers, following the principle of subsidiarity. At the same time, energy security is associated with the evolution of the energy markets, the geopolitical situation and long-term international scenarios. There are several scenarios drawn for the long term that extrapolate to some extent conceptual differences, for example, the 'Markets and Institutions' scenario and its alternative called 'Empires and Regions' [3], and the three scenarios drawn by Shell Global Scenarios [5]: 'Low Trust Globalization', 'Open Doors' and 'Flags'. The 'Low Trust Globalization' is based on the trade-off between efficiency and security, and is characterized by limited international integration, intrusive state intervention and institutional discontinuities. The 'Open doors' arises from the dilemma between efficiency and environmental sustainability, premium market incentives and the participation of the civil society. It highlights the urgent necessity of transnational integration and harmonization, and mutual recognition of standards. The 'Flags' responds to the mercantilist logic and involves regulatory fragmentation, nationalism and the conflict between the values of the different regions. To some extent, the differentiation of scenarios roots in the neoliberal and neorealist paradigms to international energy issues. It is an old debate between two alternative visions of the world: a world in which the market disturbances are resolved by cooperation, or a fragmented world conceived as a billiard table where conflicts are resolved through the exercise of the political, economic and military power. Under the neoliberal paradigm, that is so praised by the EU, energy security is achieved through the development of markets and the management of conflicts at the multilateral level through supranational institutions. The neorealist paradigm of energy security, instead, implies the development of bilateral relations and the subordination of markets to foreign policy. The potential risks and threats related to energy security arise mainly from two circumstances: ‚Ä¢ the projected upcoming peak in the production of hydrocarbon resources, which is vital for the modern economy, and ‚Ä¢ the security of their supplies. However, recently the energy sector has started to develop a few key trends that were caused by new, very strong factors: the global financial and economic crisis and the shale revolution in the production of oil and gas. Today, energy security policies require a paradigm shift and a new model of factors and conditions for its implementation. The first factor that radically changed the context of energy policy was the global economic crisis. Since 2008, experts have determined it as the financial crisis, the economic crisis, the crisis of democracy and governance, the crisis of the culture of public consumption and material culture in general, and the environmental crisis that will eventually lead to global natural disasters. It would be reasonable to say that the world experienced a multidimensional global crisis, or the first systemic crisis of the global era. In the energy sector, this crisis coincided with the start of the gradual transition from 'industrial' and 'hydrocarbon' to 'neoindustrial' and 'smart' energy, which includes the following aspects: smart grids, energy efficiency (in a broad sense), renewable energy, new principles of energy systems and focus shift from producers to consumers [3]. The second factor that dramatically changed the energy markets was the quiet shale revolution in oil and gas production. The shale revolution, which has become a reality in the United States and Canada, as estimated by the experts, will have serious consequences for the global energy market. The unconventionally produced natural gas fundamentally changed the world market. The most serious consequence of the shale gas revolution is a shift in the focus from producers to consumers. In the context of the old paradigm, energy security is directly related to energy independence. The idea was that if a country is self-sufficient with respect to energy resources, and has an efficient (energy-saving) economy, then this will be accompanied with lower prices for energy carriers. The reality of oil prices in the United States after the shale boom has shown that this is utopia.",
          "Meanwhile, achieving self-sufficiency in the energy sector is almost impossible. Even such countries as Russia, Saudi Arabia, Venezuela, Brazil and Canada, which are rich in hydrocarbons, import a part of the energy in the form of refined petroleum products due to insufficient refining opportunities. This dependence could theoretically be eliminate with a little effort and investment in the construction of new refineries, but in practice it is not happening. Revolutionary changes of the energy security require a paradigm shift, which should be reflected in the energy security policies. Currently we are on the verge of transition to a post-industrial, 'smart' energy system, which means 'smart' networks, alternative energy sources for transport, decentralization of energy, integration of energy into the technical sphere, accompanied by an increase in energy efficiency. To conclude, it can be said that the definition of 'energy security' can be complete only if the interests of all the energy market participants are taken into account, and namely importing countries, exporting countries and transit countries. Interests and priorities of all three parties are different, which presents a difficulty in agreeing on a common concept. At the same time, with the paradigm shift all of the actors of the energy market are interested in ensuring the reduction of geopolitical and environmental risks and thus creating new opportunities."
        ],
        "ground_truth_definitions": {
          "short-term energy security": {
            "definition": "the ability of the energy system to react promptly to sudden changes in the supplydemand balance",
            "context": "Nevertheless, the IEA admits that the concept of ‚Äòenergy security‚Äô can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‚Äòtimely investments to supply energy in line with economic developments and environmental needs‚Äô, while the latter deals with ‚Äòthe ability of the energy system to react promptly to sudden changes in the supply- demand balance‚Äô [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply.",
            "type": "explicit"
          },
          "long-term energy security": {
            "definition": "timely investments to supply energy in line with economic developments and environmental needs",
            "context": "Nevertheless, the IEA admits that the concept of ‚Äòenergy security‚Äô can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‚Äòtimely investments to supply energy in line with economic developments and environmental needs‚Äô, while the latter deals with ‚Äòthe ability of the energy system to react promptly to sudden changes in the supply- demand balance‚Äô [2].",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 œÄ r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP Œ± for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return Œ± (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Pe√±a, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Pe√±a's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fern√°ndez, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fern√°ndez, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fern√°ndez, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "a7b52fa3ff2606013a6ab9ba294743d7de637e6a",
        "sections": [
          "According the United States Department of Agriculture (USDA), food insecurity \"is defined as a household-level economic and social condition of limited or uncertain access to adequate food\". 1 Recent data indicate that approximately 12.3% or 15.6 million households in the United States (U.S.) were food insecure at least some time during the last year. 1 The adverse social, physical, and psychological outcomes associated with food insecurity are well documented, particularly in households with children including higher rates of diabetes and hypertension, self-reported fair or poor health, maternal depression, behavioral problems/developmental delays in early life, and poor academic achievement. [2][3][4][5][6][7][8] These outcomes are not only detrimental to the health and well-being of individual children and families but also negatively impact broader society. Based on a combination of lower worker productivity, higher costs of public education, greater health care costs, and the cost associated with emergency food distribution, the economic burden associated with food insecurity has been estimated to be over $167.5 billion annually. 9 For over 20 years, food insecurity has been assessed and monitored by the USDA at the federal level. Although levels of food insecurity have declined and risen over this period, one trend that has continued to persist is the gap in the prevalence of food insecurity between people of color and whites. 1 An analysis examining trends in food insecurity from 2001 to 2016 found that food insecurity rates for both non-Hispanic black and Hispanic households were at least twice that of non-Hispanic white households (See Figure 1). 1 Moreover, while race/ethnic specific rates from the USDA are not available for all subgroups, other studies assessing food insecurity among American Indian/Alaska Native (AI/AN) report similar results. 10,11 Using the Current Population Survey-Food Security Supplement, Jernigan et al., 2017 found that from 2000 to 2010, 25% of AI/ANs remained consistently food insecure and AI/ANs were twice as likely to be food insecure compared to whites. 12 The relationship between race/ethnicity and food insecurity is complex and is clearly intertwined with other established determinants of food insecurity including poverty, unemployment, incarceration, and disability. Concentration of social and economic disadvantage among people of color over the life course is clearly a significant driver of higher rates of food insecurity. Substantial gaps in the wealth of people of color and whites have long existed in the U.S. For example, recent data reported by the Pew Research Center indicate that the wealth of white households was 13 times the median wealth of black and 10 times greater than Hispanic households. 13 Nevertheless, despite the intersectionality of race/ ethnicity and other social/economic determinants, some evidence suggests that the higher risk of food insecurity among people of color continue even when these other social and economic factors are removed. [14][15][16][17][18] ",
          "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
          "Several promising solutions to address food insecurity have been proposed including to strengthen and make it easier for families to connect to the existing federal food and nutrition safety net (e.g. WIC, SNAP); expand and increase access to federal income support benefits such as Earned Income Tax Credit (EITC); increase opportunities for employment and ability for individuals to earn a living wage; train health care providers to screen for food insecurity and refer individuals and families to appropriate resources and more recently, the need to take a human rights approach. 8,[31][32][33][34] Still, to address persistent racial disparities in food insecurity, food security advocates and researchers need to also examine the potential impact of utilizing approaches that address structural racism and discrimination more broadly (see Gee and Ford, 2011 and Bailey et al., 2017). 22,30 These efforts will likely pave the way for the emergence of policy and programmatic strategies that promote equity in food access and health by addressing the legacy of racial, ethnic, and class inequality. Source: Calculated by USDA, Economic Research Service, using Current Population Survey Food Security Supplement data. https://www.ers.usda.gov/topics/food-nutrition-assistance/ food-security-in-the-us/"
        ],
        "ground_truth_definitions": {
          "structural racism": {
            "definition": "the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems",
            "context": "A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\"30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "26af8e81a7ca5b137912acb81d8c19f38fce0200",
        "sections": [
          "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
          "Howard's research on the \"chlorine sunset\" controversy is based on interviews and document analysis. He conducted twenty-seven semistructured interviews, lasting an hour on average, with staff members of federal regulatory agencies in the United States and Canada, staff members of the International Joint Commission (IJC), members of the Great Lakes Science Advisory Board, staff members or individuals otherwise associated with nongovernmental organizations (NGOs), academic or governmental members of the industrial ecology or green chemistry communities, and industrial chemists in industry and academia. A number of transcripts were supplemented with additional information from follow-up correspondence. Documents analyzed included (1) reports, press releases, Web documents, and other materials published by NGOs, the chemical industry, and federal agencies; (2) articles and commentaries in newspapers and popular and trade magazines; (3) research articles and commentaries in scholarly anthologies and peer-reviewed scholarly journals; (4) books written by key actors; and (5) transcripts of Congressional testimony. A little-studied controversy involving one of the major branches of industrial chemistry documents a striking example of undone science and illustrates the role it can play in structuring conflict between competing regulatory paradigms. Much of the controversy has centered on the Great Lakes region, where extensive chemical manufacturing and contamination has occurred; where scientists have documented threats to wildlife and humans from persistent, toxic, industrial chlorinated pollutants; where extensive citizen activism has emerged around this threat; and where a quasigovernmental advisory body has assumed a leadership role in addressing this concern (Botts et al. 2001). A number of environmental and health advocates have argued, based both on fundamental toxicology and on long historical experience with chlorinated synthetic chemicals (e.g., DDT and PCBs), that the entire class of thousands of such substances should be tentatively presumed dangerous and that the chemical industry accordingly should wean itself from most major uses of chlorine (Thornton 1991(Thornton , 2000;;International Joint Commission [IJC] 1992; see Howard 2004). The analysis offered here briefly considers the character and function of undone science in the debate provoked by proposals for a \"chlorine sunset.\" The chlorine sunset controversy revolves around conflict between two sharply contrasting regulatory paradigms: risk and precaution (Thornton 2000;Howard 2004). The powerful chemical industry has coevolved with, supports, and is supported by the dominant U.S. and Canadian environmental regulatory regime, which restricts chemical industry decision making only to the extent that detailed calculation of risk indicts individual chemical substances. Meanwhile, Greenpeace, a marginalized, reputedly radical environmental NGO, and the IJC, a prominent but marginalized binational advisory organization, argued for a regulatory regime based on the precautionary principle (see Tickner 2003), which in their view justified governmental action against an entire class of industrial chemicals. The dominant paradigm assumes the unit of analysis to be the individual substance and places the burden of proof on the public to prove harm; in contrast, the challenger paradigm allows, even requires, the primary unit of analysis to be the entire class of substances and places the burden of proof on corporate officials. Within this matrix of political and epistemological conflict, the political economy and political sociology of undone science can be seen to revolve around a series of three dyads, each paradigm implying parallel formulations of \"done science\" and undone science. The three dyads are summarized in Table 1. One dyad appears in the context of health impacts research. Industry and federal officials operating in the risk paradigm hold that the legitimate goal of health impacts research performed or mandated by government is ad hoc identification of individual chlorinated chemicals that cannot be safely manufactured and used. In this paradigm, chlorine chemistry itself is seen as immune to fundamental interrogation; the role of public science is limited to documenting the odd substance that can be definitively proven harmful and, on that basis, restricted. \"We've made the point over and over again that you have to look at each product's physical and chemical characteristics to draw conclusions about what it is going to do in the environment,\" argued Brad Lienhart, of the Chlorine Chemistry Council. To do otherwise would be to \"[make] non-science-or nonsense-into science\" (quoted in Sheridan 1994, 50). Beginning in the early 1990s, \"sunset\" proponents vigorously argued that such research is incapable of interrupting a long series of chlorinated \"Pandora's poisons\" from entering the environment and human tissues long before their deleterious effects are documented. Inevitably remaining undone, they argued, is science capable of systematically identifying unsafe chemicals from among tens, perhaps hundreds, of thousands of chlorinated industrial substances, by-products, and breakdown products, a scope of research that the risk paradigm is sometimes assumed to provide but, owing to the sheer enormity of the undertaking, cannot. The government's effort to identify unsafe chlorinated chemicals is ad hoc precisely because it cannot, in any meaningful sense, be systematic; not only are available resources insufficient, but the enterprise is technically infeasible. Viewed in this light, the science is undoable. The IJC argued: There is a growing body of evidence that [suggests that] these compounds are at best foreign to maintaining ecosystem integrity and quite probably persistent and toxic and harmful to health. They are produced in conjunction with proven persistent toxic substances. In practice, the mix and exact nature of these various compounds cannot be precisely predicted or controlled in production processes. Thus, it is prudent, sensible and indeed necessary to treat these substances as a class rather than as a series of isolated, individual chemicals. (IJC 1992, 29) A second dyad appears in the risk paradigm's stance on innovation. Industry has systematically pursued the development of chlorine chemistry, developing chlorinated chemicals and expanding markets for them; meanwhile, advocates of chlorine precaution have pointed to the need to systematically develop nonchlorine alternatives. This is in part science that the risk paradigm has long left undone-historical research and development trajectories that could have led to a wider range of nonchlorine chemicals and processes being available today. The implication of the historical analysis offered by a leading sunset proponent (Thornton 2000; see also Stringer and Johnston 2001) is that over the past century the technological, economic, and political momentum of chlorine chemistry has to some extent bent the overall industry research and development agenda toward chlorine and away from nonchlorine alternatives. Here undone science consists of a body of nonchlorine chemicals and processes that might now exist but for the long dominance of research and development predicated on chlorine. It is a point seemingly acknowledged by a confidential IJC informant who did not support the commission's sunset recommendation: \"There's no reason why we couldn't, as a global society, live a non-chlorine lifestyle. It's just, you know <laughs>, that ain't gonna happen, because that is not our history! We're kind of, in a way, captives of our past.\" In the risk paradigm, with its laissez-faire orientation, such research and development need not be undertaken by the industry but instead is tacitly left to whichever agency or organization might care to undertake it. Viewed from the vantage point of the industry, with its adamantine conception of chlorine chemistry as technologically and economically inevitable, the only conceivable motivation for conducting such research and development would be some kind of ideological fetish (see, e.g., Chlorine Chemistry Council n.d.). It would represent \"a veiled attempt to return to a pre-industrial Eden,\" one industry supporter suggested (Amato 1993). Crucially, although this agenda would have been and would now be technically feasible, such research would be hobbled by the absence of a sizable cadre of technoscientists devoted to the project and by a lack of financial resources to sustain the effort. A third dyad occurs within the challenger, precautionary paradigm and directly counters the values and priorities of the dominant paradigm's dyads. Paired with precaution advocates' assertion of the need for research to systematically develop nonchlorine alternatives-here seen as industry's responsibility rather than the public's-is an explicit assertion that industry should assume the burden of making the case for any specific chlorinated chemicals (or chemical processes) that can be demonstrated to be both essential (i.e., nonsubstitutable) and capable of being manufactured and used in ways that (to some as yet unstated standard) pose no significant environmental hazard. Industry's motivation for undertaking this latter effort would, of course, be profit. And owing to the presumably quite limited number of substances to be evaluated, it would be both technically feasible and, given the industry's substantial financial and technical resources, affordable. The chlorine sunset controversy is now effectively dormant. In the face of bitter industry resistance and U.S. and Canadian governmental intransigence, the IJC and Greenpeace ceased promoting their sunset recommendations in the mid-1990s (Howard 2004). Thornton's book, which appeared in 2000, reawakened (and in significant ways deepened) the debate, but it did so only briefly. The sunset proposals have not visibly shifted policy at any level in North America. A major international treaty on persistent organic pollutants signed in 2001 represented an important victory for activists, but it also underscored the lingering, unresolved character of the chlorine debate: all twelve of the \"dirty dozen\" substances it required to be phased out are chlorinated compounds, and each was targeted on the basis of its discreet, well-documented characteristics. Meanwhile, thousands of far less extensively studied chlorinated chemicals-and chlorine chemistry as a whole-remain unregulated. This analysis of the chlorine sunset controversy illustrates how regulatory regimes influence the construction and articulation of research priorities. In this case, advocates of the risk and precaution paradigms, on the basis of competing understandings of the appropriate unit of regulatory analysis and appropriate regulatory burden of proof, promote competing conceptualizations of science both done and undone. More specifically, the case suggests that done and undone science in such a controversy can be understood as occurring in dyadic pairs and that a major role for challenger discourses is making the implicit undone portion of dyads within the dominant paradigm visible and explicit. This analysis also highlights an important category of undone science in technoscience controversies-undoable sciencethat improves understanding of how regulatory regimes constrain the identification of undone science. Here, close examination of precautionary advocates' critique of the risk paradigm clarifies the process through which conventional regulatory structures veil undoable science in the form of systematic research for which insufficient resources and insufficient technical means are available.",
          "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (L√∂wy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] ¬£1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center ‚Ä¶ there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life ‚Ä¶ and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't ‚Ä¶ appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
          "Kempner's research is based on an interview study that examines \"forbidden knowledge,\" a term used to capture scientists' decisions not to produce research because they believe it to be taboo, too contentious, or politically sensitive (a type of negative knowledge in the terminology introduced above). In 2002-2003, she and colleagues conducted ten pilot and forty-one in-depth, semistructured telephone interviews with a sample of researchers drawn from prestigious U.S. universities and representing a diverse range of disciplines, including neuroscience, microbiology, industrial/organizational psychology, sociology, and drug and alcohol research (Kempner, Perlis, and Merz 2005). Those fields were chosen to gauge the range, rather than the prevalence, of experiences with forbidden knowledge. Interviews lasted between thirty-five and forty-five minutes and were audiotaped, transcribed, coded, and analyzed according to the principles of grounded theory (Strauss and Corbin 1990). While many social movements organize around the identification and completion of undone science, others devote themselves to making sure that some kinds of knowledge are never produced. They are not alone. The idea that some knowledge ought to be forbidden is deeply embedded in Western cultures and appears in literature through the ages, from Adam and Eve's expulsion in Genesis to Dr. Frankenstein's struggle with a monster of his own creation (Shattuck 1996). Mertonian rhetoric aside, most people agree that some science poses unacceptable dangers to research subjects or to society at large. The widely accepted Nuremberg Code, for example, places strict limits on human experimentation, in an effort to ensure that some science-such as Nazi human experimentation in World War II-is never done again. Determining which knowledge ought to remain undone can often be contentious, as illustrated by current high-profile public debates surrounding the ethics and implications of stem cell research and cloning technologies. Nevertheless, as in research agenda-setting arenas (Hess 2007), debates and decisions about what knowledge should remain off limits to the scientific community typically occur among elites: legislators and federal agencies perennially issue guidelines and mandates regarding which research should not be conducted, setting limits on everything from reproductive and therapeutic cloning to studies of the psychological effects of Schedule I drugs, like heroin and marijuana. Scientists and the lay public both have limited opportunities to voice their opinion in these discussions. In dramatic cases, scientists have attempted to preempt mandates via self-regulation, as was the case in 1975 when scientists meeting at Asilomar called for a moratorium on certain kinds of recombinant DNA research (Holton and Morrison 1979). According to the forty-one elite researchers interviewed for this case study, these formal mechanisms account for only a portion of the limitations that can produce undone science (Kempner, Perlis, and Merz 2005). More often, researchers described how their research had been hamstrung by informal constraints-the noncodified, tacit rules of what could not be researched or written. Yet researchers were very clear about what constituted \"forbidden knowledge\" in their respective fields. The boundaries of what could not be done had been made known to them when either they or a colleague's work had been targeted for rebukein essence, their work had breached an unwritten rule. The management of forbidden knowledge, thus, worked much as Durkheim said it would: once someone's research had been identified as especially problematic by, for example, a group of activists, their work became a \"cautionary tale,\" warning others \"not to go there\" (Kempner, Bosk, and Merz 2008). In this way, social movement organizations and activists are able to play an important role in debates about what ought to remain undone, whether or not they are invited to the table. Besides their influence on shaping research agenda-setting arenas, social movements can and do influence individual researchers' decisions not to pursue particular types of studies. In recent decades, for example, animal rights organizations have had an enormous influence on the kinds of research that scientists choose not to produce. We found that the researchers in our sample who work with animal models took seriously the threat posed by those organizations. They spoke of \"terrorist-type attacks\" and told stories of colleagues who received \"razor blades in envelopes\" and \"threatening letters.\" Others faced activists who staked out at their houses. Researchers learned from these cautionary tales and, in many cases, said that they had self-censored as a result. One researcher, for example, explained that he would not work with primates-only \"lower order\" animals like mice and drosophilia because: I would like to lunatic-proof my life as much as possible ‚Ä¶ I, for one, do not want to do work that would attract the particular attention of terrorists ‚Ä¶ The paranoia was acute. One researcher refused to talk to the interviewer until she proved her institutional affiliation: \"For all I know, you are somebody from an animal rights organization, and you're trying to find out whatever you can before you come and storm the place.\" Over time, the overt interventions of animal rights organizations in the production of research have redefined the ethics of animal research, ushering in legislation like the Animal Welfare Act of 1985, which requires research institutions that receive federal funding to maintain \"Institutional Animal Care and Use Committees\" (Jasper and Nelkin 1992). However, lay groups do not need to use such directly confrontational tactics to influence researchers' decisions, especially if the groups are successful in their attempts to reframe a particular social problem. For example, substance abuse researchers argued that their research agendas were limited by the success of the Alcoholics Anonymous' campaign to define treatment for alcoholism as lifelong abstinence from drink. Although these researchers would like to conduct \"controlled drinking\" trials, in which alcoholics are taught to drink in moderation, they argued that \"There's a strong political segment of the population in the United States that without understanding the issues just considers the goal of controlled alcohol abuse to be totally taboo.\" The mere threat of interference from the grassroots was enough to keep many researchers from conducting certain studies. Several drug and alcohol researchers described great unwillingness to conduct studies on the health benefits of \"harm reduction\" programs, such as those that distribute free condoms in schools or clean needles in neighborhoods, because they might attract unwanted controversy from lay groups who oppose such public health interventions. Thus, in some contrast to the role that social movement organizations and lay experts/citizen scientists play in exposing undone science and encouraging knowledge creation in chemical, air monitoring, and breast cancer research, this study shows that the same actors can also play a powerful role in determining which knowledge is not produced. Moreover, conflict over the direction of funding streams, while critically important to the political of research agenda setting, do not solely determine what science is left undone. Rather, social movements are also effective beyond research agenda-setting processes that occur at the institutional level; this study provides evidence that they also shape the microlevel interactional cues and decision-making process of individual scientists. Although more research is needed to understand the circumstances under which researchers decide to selfcensor in response to pressure from outside groups, this case suggests that social movements may have much greater potential to thwart research than originally thought. The implications are intriguing and deserve greater attention. On one hand, disempowered groups may leverage these techniques to gain a voice in a system of knowledge from which they are typically excluded. On the other hand, it is troubling to learn that the subsequent \"chilling effect\" happens privately, often without public discussion and in response to intimidation and fear.",
          "The diverse cases provide an empirical basis for moving forward the theoretical conceptualization of undone science in relation to a new political sociology of science and that program's concern with how research agendas are established. Perhaps the most significant general observation is that the identification of undone science is part of a broader politics of knowledge, wherein multiple and competing groups-including academic scientists, government funders, industry, and civil society organizations-struggle over the construction and implementation of alternative research agendas. To a large extent, our case studies focus on attempts by civil society or quasigovernmental organizations to identify areas of research they feel should be targeted for more research. However, the identification of undone science can also involve claims about which lines of inquiry should warrant less attention than they currently receive, either because there are decreasing social returns on continued investments in heavily researched areas or because the knowledge is deemed not worth exploring and possibly dangerous or socially harmful-what Gross (2007) calls \"negative knowledge.\" Examples of the latter include the research programs and methods targeted by animal rights groups and research on chlorinated chemicals targeted by Greenpeace. There are many other cases that would fit this role for civil society organizations, including calls for research moratoria on weapons development, genetically modified food, nuclear energy, and nanotechnology. Five more specific insights follow from and add complexity to this general observation. First, while we see undone science as unfolding through conflict among actors positioned within a multiorganizational field, as Gibbons' case shows, definitions of undone science may also vary significantly within different organizational actors, coalitions, or social movements. Some portions of the movement may be captured by mainstream research, and consequently advocacy is channeled into support for the experts' prioritizations of research agendas. Thus, a research topic such as environmental etiologies of breast cancer may represent undone science to a marginalized segment of breast cancer advocates and their allies in the scientific community, but it may represent negative knowledge to the majority of breast cancer advocates and the dominant cancer research networks. To further complicate the picture, rapid developments and changes within the scientific field, such as the development of genomic research to better pinpoint environmental or epigenetic factors, may result in shifts in research priorities that can open up opportunities for research in areas of undone science. Here, one sees that internal changes and differences among both researchers and civil society advocates interact to define shifting coalitions of research priorities. Second, the dynamic nature of coalitions and alliances that emerge around undone science suggests that the articulation of research priorities is often a relatively fluid process; even when civil society groups target some areas of scientific research as deserving low or no priority, their views may in turn lead to the identification of other areas of research deserving higher priority. For example, the position of an animal rights group may begin with opposition to some types of animal research but lead to support for more \"humane\" forms of animal research that have been reviewed by animal research committees. Likewise, the position of an organization such as Greenpeace in opposition to chlorinated chemicals is linked to an articulation of the need for research on green chemistry alternatives. As these examples suggest, the identification of undone science can be viewed as multifaceted outcomes of coalitions and conflict among diverse groups representing various social categories, each promoting a mix of topics seen as deserving more, less, or no attention from the scientific community. Third, making sense of the complex processes that produce undone science involves attending to the distributions of power, resources, and opportunities that structure agenda setting within the scientific field. An important element of field structure is the role of regulatory regimes in shaping definitional conflicts over research priorities. Howard's work suggests that done and undone environmental science dyads can be a key expression of the regulatory paradigm in which they occur and intimately linked to the way expertise is conceptualized and deployed in the paradigm. Furthermore, he proposes that until mainstream science faces a challenger, important forms of undone science within the dominant paradigm can remain implicit and unarticulated. In other words, undone science may take the form of a latent scientific potential that is suppressed through \"mobilization of bias\" (Lukes 2005; see also Frickel and Vincent 2007). Ottinger (2005) also notes the important role of regulatory standards in defining opportunities for activists who attempt to get undone science done largely using their own resources. In the case of air monitoring devices, an alternative research protocol and data gathering device operated by laypeople provides a basis for challenging official assurances of air quality safety. Rather than advocate for shifts in a research agenda, they simply enact the shift. In Howard's terms, the lay research projects also dramatize the implicit and unarticulated bias in the dominant method of air quality monitoring. Ottinger's (2005) focus on the double role of standards as enabling and constraining factors in establishing both the conditions and limitations of undone science is intriguing, and it remains for future research to examine the efficacy of tactical dynamics in relation to structural constraints encountered across a range of regulatory and research contexts. Fourth, while access to financial resources is an implicit focus of efforts to identify undone science, Kempner's research demonstrates that the interaction of civil society and research priorities is not restricted to the broad issue of funding. Although civil society organizations can exert an effect on research funding allocations, as we have seen especially in environmental and health research priorities, Kempner notes that there are other mechanisms that can cause such shifts. Her work suggests that efforts to study the problem of undone science should also consider the role that a moral economy has in shaping scientists' decisions about what research programs they will and will not pursue (Thompson 1971; on moral economy in science, see Kohler 1994). Furthermore, even if scientists do not accept in principle the notion that certain knowledge should remain undone, they may simply decide not to invest in some areas of research because of intense direct pressures from civil society organizations such as animal rights groups. As a result of individual decisions not to engage in an area of research, changes in the research agendas of a field can occur even when funding is not shifting dramatically. Finally, sometimes structural constraints such as limited access to resources coincide with practical constraints to produce \"undoable science.\" In the case of the chlorine sunset provisions, precaution advocates see governmental programs for screening individual chemicals as obscuring a plain fact: the sheer number of chemicals and their complex interaction with ecological and biological systems make it impossible to predict whether a given concentration of a given chemical will in any meaningful sense be \"safe\" or whether it will be a risk. As a result of this \"wicked problem\" (Rittel and Weber 1973), the articulation of undone science as a goal for research prioritization and funding-in this case, the standard assumption of a need for ever more research on the environmental, health, and safety implications of new chemicals-turns against itself, because the call for research into specific chemicals tacitly supports a regulatory framework that systematically generates a policy failure (see Beck 1995).",
          "This study demonstrates some of the ways in which the analysis of undone science can enrich empirical understandings of research agenda-setting processes. The considerable variation we find in just four cases suggests that one promising avenue for future research lies in developing more systematic comparisons across academic, government, industry, and community settings. Doing so will further elaborate the ways in which the institutional contexts of research-including different sets of political and economic pressures, normative expectations, resource concentrations, and sizes and configurations of research networksshape the articulation of undone science and the successful or failed implementation of alternative research agendas. Our broader aim in seeking to give undone science higher visibility within STS is to broaden the foundations for a new political sociology of science. Much like feminist and antiracist science studies, the political sociology of science situates questions relating to the uneven distribution of power and resources in science at the center of the STS project while remaining attentive to how knowledge and its inverse-ignorance-is socially shaped, constructed, and contested. As we have argued here, one of the crucial sites where questions of power, knowledge, and ignorance come together is in the domain of research agenda setting, where intense coalitions and conflicts are forged to gain access to the limited resources that ultimately shape what science is done and what remains undone. ",
          ""
        ],
        "ground_truth_definitions": {
          "undone science": {
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.",
            "context": "What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "885c3154386e3dd1ff6a12e566152846186f9569",
        "sections": [
          "hypotheses based on a careful mapping of psychological profiles of over 2,300 American Twitter users linked to behavioral sharing data and sentiment analyses of more than 500,000 news story headlines. The findings contradict the ignorance perspective but provide some support for the disruption perspective and strong support for the partisan polarization perspective. Thus, individuals who report hating their political opponents are the most likely to share political fake news and selectively share content that is useful for derogating these opponents. Overall, our findings show that fake news sharing is fueled by the same psychological motivations that drive other forms of partisan behavior, including sharing partisan news from traditional and credible news sources. With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018). Political observers have linked the spread of fake news to recent key political events, like the \"Brexit\" vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016). Accordingly, media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017). In light of these sizable investments, it is surprising how little we know about why people share fake news on social media. Studies suggest the spread of false information cannot be attributed only to bots or \"sock puppet\" accounts (Guess, Nagler andTucker 2019, Vosoughi, Roy andAral 2018). But researchers have largely failed to conduct adequate tests of why ordinary citizens take part in the distribution of fake news (Wagner and Boczkowski 2019). This is unfortunate because potential solutions to the spread of fake news rest on psychological assumptions about the root causes of the problem. For example, fact-checking should reliably work only if citizens actually want to share true information but need help to weed out falsehoods. Presently, we do not know whether this portrayal of citizens' motivations is correct. As summarized in a recent study, \"most existing studies rely solely on publicly available social media data and there is little research that seeks to understand people's motivations for (...) sharing news\" (Chadwick, Vaccari and O'Loughlin 2018, 4258). In this study, we ask: What are the individual-level psychological motivations behind fake news sharing? The first major challenge in addressing this research question is data unavailability (King andPersily 2019, Stier et al. 2019). Data on citizens' personality and political profiles are rarely available in combination with behavioral data on their sharing activities on social media (for notable exceptions, see Guess, Nagler and Tucker 2019, Guess, Nyhan and Reifler 2020, Grinberg et al. 2019). Here, we circumvent these data limitations by linking unique behavioral data on news sharing activity and sentiment analyses of more than 500,000 news story headlines to individual-level survey data with detailed psychological measures for a large sample of American Twitter users. This approach allows us to conduct a comprehensive study of competing psychological theories of fake news sharing. The second challenge is that multiple psychological motivations could potentially influence the sharing of false and factually correct news; these motivations may differ across individuals and across news content. Building on theories about information sharing in general, we first outline these potential psychological motivations before applying them to our research topic: the sharing of political fake news on social media, a domain characterized by intense conflict. 1Then, we deduce testable implications about relevant individual differences and differences in political news content.",
          "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience. Whereas once audiences were more or less passive consumers, they are now actively participating in the distribution, and sometimes even the production of news. People today can start their own blogs, live-stream from political events they participate in (e.g. protests), and may even contribute to small media organizations often catering to niche market segments. To be sure, just as with legacy media, online audiences still face a choice of exposure: which news do they choose to consume. But on top of that, they also face a new choice of which news to share with their friends and followers. These trends, we contend, have implications for the psychological dynamics underlying the way audiences engage with this media, the novel step of news sharing potentially recruiting new psychological mechanisms. These trends have pushed to the forefront questions about information veracity. In advanced democracies, strong editorial norms and standards for good reporting meant that citizens could -at least most of the time -trust legacy media outlets to publish stories that accurately reflected the world. For most citizens and for most legacy media, then, it made sense to pay more attention to the political usefulness of news stories and, thus, it is not surprising that this news dimension has been critical in driving selective exposure. Yet, on the internet, editorial norms have lost power, and information circulating on social media will to a much larger extent vary on both dimensions of usefulness and veracity. Truth cannot be taken for granted and veracity judgments are thus, potentially, more important than ever. Yet, these two information dimensions -veracity and usefulness -are not unique for social media users. In everyday conversations when, for example, discussing or relaying a rumor, people must consider both whether they believe it and how useful they think it is. Psychologists have studied the psychology of rumor transmission for decades, highlighting the importance of both accuracy-oriented and goal-oriented motivations (for an overview, see DiFonzo and Bordia 2007). For example, people may share rumors they believe are true to be cooperative, to signal competence, or to get others to affirm the accuracy of the rumors. Sharing a rumor that turns out to be wrong, in contrast, may damage people's reputations (Altay, Hacquin and Mercier 2019). Nonetheless, goal-oriented processes may still take precedent, notably under intense social conflict. Some of the most well-studied, and most extreme, examples come from ethnic conflicts, where rumors are heavily used to mobilize the in-group against the out-group. In his comprehensive review of this role of rumors, Horowitz (2000, p. 74) concludes: \"Rumors are (. . . ) embedded in the riot situation, because they are satisfying and useful to rioters and their leaders. (. . . ) Rumor is likely to prevail over accurate information. (. . . ) Rumor prevails because it orders and organizes action-in-process.\" Below we apply these general considerations about the psychology of information sharing to the object of our empirical inquiry: The sharing of news -fake or real -about politics on social media.",
          "The prevalence of false information on social media may strengthen the role of accuracyoriented motivations and the dimension of veracity for understanding news sharing behavior. People may worry about damaging their reputation by accidentally sharing fake news (Altay, Hacquin and Mercier 2019). To the extent such reputational worries induce accuracy-oriented social media behavior, we should expect that sharing fake news stories is a rare phenomenon relative to sharing real news, but also that the predictors of sharing fake versus real news will differ. Successfully discerning between fake and real news is difficult, first, because fake news is specifically designed to appear real and, second, because the fast-paced social media platforms deplete attention and cognitive resources (Pennycook et al. 2020). Accordingly, individual differences in the ability to detect cues about information veracity should largely determine whether people share fake news (Pennycook and Rand 2019b). In essence, from this perspective, fake news sharing happens because people are ignorant that the stories they share are false and, accordingly, below we refer to this argument as the ignorance theory. Goal-oriented motivations, however, may also critically shape sharing of political news on social media. As noted above, goal-oriented motivations affect information sharing during periods of social conflict. Politics is conflictual, but political conflict in the United States in recent years has been deepened by an increasing political polarization (Mason 2018). Goaloriented motivations put a premium on the usefulness of information. Sharing a political news story (fake or real) can be used to signal one's political affiliation, by denigrating political opponents or by mobilizing politically like-minded peers. In this regard, fake news stories can be particularly useful: as fake news is not constrained by reality, they can be excessive in their negative portrayal of political opponents (Acerbi 2019). Accordingly, to the extent goal-oriented motivations dominate social media users' interactions with news stories, the same psychological motivations should drive the sharing of real and fake news. People will share useful news that fits their political goals, and, in the extreme case, they will not pay attention to the veracity dimension of the information at all. In current American politics, two types of social or political goals could be served by sharing extreme information such as fake news. First, sharing news stories could reflect motivations to help one's preferred party against opposing parties in the increasingly polarized electoral competition (Iyengar, Sood andLelkes 2012, Lelkes andWestwood 2017). We will refer to this as the polarization theory as it focuses on partisan feelings rather than ideological disagreements between partisans of different stripes. The theory is prominent in current discussions about political events, like elections and referenda, and it has a long research history in political science (Mason 2018, Taber andLodge 2006). The theory has also informed studies on partisans' beliefs in political conspiracies (Miller, Saunders and Farhart 2015). Second, sharing news stories could be fueled by deep-seated discontent with the status quo and serve a desire to disrupt the existing social and political order. For example, Tucker et al. (2017, 53) convincingly argue that \"social media can lend a voice to ... anti system forces that actively seek to undermine liberal democracy.\" Below, we will refer to this as the disruption theory. Finally, there are also multiple scenarios for the intermediate (and likely) case where both veracity and usefulness considerations are relevant for social media users' sharing behavior. People may first evaluate the veracity of news and then, conditional on it being accurate, consider its usefulness. Or people may prioritize usefulness but still pay some attention to the veracity. Hence, the sharing of blatantly false information may, for example, complicate mobilization processes by derailing discussions about whether events really happened (Petersen, Osmundsen and Tooby 2020). Thus, when assessing usefulness, people may factor in veracity and selectively share information that is as extreme as they believe they can get away with. To elucidate whether and how the ignorance, polarization, and disruption theories account for the sharing of fake news, we use two approaches: First, we examine the role of individual differences, deducing for each theory who should be most likely to share fake news and, second, we examine the role of news story content, deducing what is most likely to be shared.",
          "The accuracy-oriented ignorance theory assumes people want to share accurate information, but end up sharing falsehoods because they lack the cognitive reflection or motivation to discern between true and false information. One line of evidence for the ignorance theory comes from Pennycook and Rand (2019b), who show that people who perform better on a cognitive reflection test can better tell if news story headlines are true or false. Another line of evidence comes from Guess, Nagler and Tucker (2019), who demonstrate that older people are more likely to share fake news on Facebook, presumably because they are less digitally literate than their younger peers. Building on this, we test the ignorance theory by focusing on four individual-level predictors of fake news sharing on Twitter: age, cognitive reflection, factual political knowledge, and digital literacy. The goal-oriented polarization theory focuses on partisanship as a key driver of fake news sharing. Currently, there is mixed evidence on how partisan motivations influence fake news sharing. In general, partisans seem to share information from ideologically similar sources (Barber√° et al. 2015) and to treat articles shared by political opponents with greater suspicion (Lee, Kim and Coe 2018), suggesting that considerations about the political usefulness of information matters. Guess, Nagler and Tucker (2019, 3) find that partisanship also affects fake news sharing in that conservatives and Republicans were more likely to share fake news on Facebook in 2016 than liberals and Democrats, a partisan asymmetry attributed to \"the overwhelming pro-Trump orientation in both the supply and consumption of fake news during that period.\" Pennycook and Rand (2019b), however, tone down the effect of partisanship. They conclude that \"people fall for fake news because they fail to think; not because they think in a motivated or identity-protective way\" (ibid., 48). To test the polarization theory, we examine whether Democratic partisans are willing to share politically congruent fake news from *pro*-Democratic sources or if the partisanship asymmetry identified by Guess, Nagler and Tucker (2019) and others prevails. Second, an important question in the literature on affective polarization is whether in-party love (Lelkes and Westwood 2017) or out-party hatred (Abramowitz and Webster 2018) sparks polarization. We provide the first test of which of these dynamics explain fake news sharing by relying on two types of individual differences as predictors of news sharing: 1) standard party identification measures and 2) batteries of positive versus negative feelings towards Democrats and Republicans. Finally, we turn to the goal-oriented disruption theory. A central focus in media discussions and psychology has been on so-called \"online trolls\" who take pleasure in misleading and harassing others online (Buckels, Trapnell and Paulhus 2014). Trolling experts speculated that sharing sensational and often implausible fake news reflects a specific form of apolitical online trolling. However, political cynicism may also drive fake news sharing. For example, a recent study showed that Americans and Danes who are discontent with political elites and the political system report substantially greater willingness to circulate conspiracy theories online (Petersen, Osmundsen and Arceneaux 2018). Here, we provide the first behavioral test of this theory and zoom in on two individual differences as predictors of fake news sharing: a psychological measure of trolling behavior and a measure of political cynicism.",
          "To elucidate the psychological motivations underlying fake news sharing, we also need to understand what news content people share. Specifically, to adjudicate between the three theories, the analyses differentiate news stories along two dimensions: (1) Whether a news story originates from a fake news source (i.e., a source containing registered debunked stories) or a real news source (i.e., a generally credible source), and (2) whether a news story comes from a *pro*-Republican or a *pro*-Democratic source. The three theories make very different predictions about which dimension matters for people's sharing preferences. The ignorance theory predicts that the veracity dimension matters most and that people with high cognitive abilities will be less willing to share fake news stories and more likely to share real news stories, irrespective of the political slant of the stories. The latter two theories make the opposite prediction: they posit that people place less value on whether stories are true or false as long as the stories are politically useful. Specifically, the disruption theory implies that disruptive individuals should selectively share as many fake news as possible to sow confusion and uncertainty. For the polarization theory, however, the content-related predictions are more complex, and so it is these predictions we unfold now. The polarization theory holds that fake news sharing is not an accident caused by ignorance, it is partisan business as usual. The media landscape in the US is politically fragmented, with some news sources catering to Republicans while others offer content that fit Democrats' taste (Groeling 2013). From the perspective of the polarization theory, fake news is an extreme version of traditional partisan news that polarized partisans may find attractive due to its rarity and extremity. Accordingly, partisans motivated to selectively share hyper-partisan content will tend to view the online media landscape of news sources as a one-dimensional continuum reflecting the political usefulness of the stories they provide. For a polarized Republican social media user, the order of the usefulness of news sources goes from *pro*-Republican fake news sources, to *pro*-Republican real news sources over centrist sources to *pro*-Democratic real news sources, and finally, to *pro*-Democratic fake news. For Democrats, the order reverses. This insight, we contend, leads to three observable implications about the content of shared news. First, partisan social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. Accordingly, we expect that sharing fake news favoring one party over the other should be best predicted by sharing real news strongly favoring the same party. Conversely, sharing fake news favoring one party should correlate negatively with sharing fake news favoring the other party. Second, if news sources differ systematically in how strongly they cater to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Specifically, fake news should show the strongest partisan sorting in terms of who wants to share from these sources. Third, and finally, the negativity of news source content should change as we move across the partisan news continuum. If partisans selectively share stories with content that fits their political tastes, the observable implication is that stories shared from news sources located towards the Democratic extreme on the partisan news continuum should portray Republican elites negatively while stories from the Republican extreme should cast Democratic elites in a negative light.",
          "",
          "We commissioned the YouGov survey agency to recruit U.S. Twitter users to complete a 20-minute online survey fielded from mid-December 2018 to mid-January 2019. The survey contained questions measuring our set of potential predictors of fake news sharing (see below) and a range of demographic variables. The survey also asked participants to provide their Twitter ID and asked for permission to scrape their publicly available Twitter data and link this data to their confidential survey responses. See SI Section 1 for the invitation letter and link to the consent form. We scraped respondents' Twitter accounts by early March, 2019. Our data includes tweets and retweets posted by the panelists before that period. YouGov invited over 27,000 U.S. Twitter users from their standing web panel to participate in our study. 8,741 participants accepted the invitation and answered all questions before the final prompt to give consent to link their public Twitter data to the survey. A majority of these respondents (5,725 people) did not share their Twitter IDs at the end of the survey. Of the 2,976 respondents who did attempt to share their Twitter handles, another 639 participants presented IDs that we could not access via Twitter's Application Programming Interface. Consequently, our final sample size is N = 2,337. Because of high and non-random attrition, our sample is unlikely to perfectly reflect any welldefined population. Still, our study participants' demographic characteristics resemble the characteristics obtained from participants in other high-quality Twitter studies. Relying on a recent Pew study (Wojcik and Hughes 2019), SM Section 2a demonstrates that our sample is comparable to the Twitter-population on race, gender, income, education, and age. It also shows that participants in our final sample were demographically very similar to the 8,741 participants who accepted to participate in the survey but failed to provide their Twitter handle. In our final sample, 50% of participants are female, 70% are white/Caucasian, the median age is 40 years old (SD = 14), the median highest level of completed education is \"4-year college degree,\" and the median family income level is \"$60,000-$69,999.\" We scraped Twitter data from all our 2,337 participants. In total, we extracted 2,709,052 tweets and retweets posted by our panelists until mid-January 2019. The median number of (re)tweets for our participants was 488 (Min #of tweets = 1, Max #of tweets = 6433). Importantly, a large majority of the tweets -around 70% -were posted after the 2016 US presidential election; the period that has been the focus of previous work. This allows us to extend earlier findings and test how they hold up in periods characterized by less intense electoral competition.",
          "We follow best practices (Guess, Nyhan and Reifler 2020, Grinberg et al. 2019, Lazer et al. 2018) and operationalize \"sharing of fake news sources\" as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news. To this end, we cross-referenced all tweeted URLs against a list of 608 fake news sources, constructed by journalists and scholars (for details, see SM Section 10 and Guess et al. 2019, Allcott, Gentzkow andYu 2019). The list was compiled to document the prevalence of fake news during the 2016 U.S. presidential election but has been updated to account for fake news publishers that have emerged since. To the extent possible, the list creators have coded the most popular fake news sources for partisan slant: 12 of the most popular were *pro*-Democratic fake news sources while 30 were *pro*-Republican sources. While this number may seem low, note that almost 86% of all shared fake news sources originated from one of these 42 domains for which we have partisan ratings. (See SM Section 6 for the complete list of fake news domains and Figure SM 2b for the 20 fake news domains that were shared the most.) As a robustness test, supplementary analyses discussed below group the fake news sources according to a more fine-grained coding scheme proposed by Grinberg and colleagues (2019) that distinguishes among three classes of fake news sources differing in their likelihood of publishing misinformation. In SM Section 8, we also replicate the main results using a different, but partially overlapping list of fake news sources. Our measure of fake news sharing records actual online behavior and thus does not suffer from biases that can plague self-reported measures of online activity. Further, while the present study is certainly not the first to couple survey and social media data (e.g., Guess, Nagler and Tucker 2019), it has the most detailed set of psychological measures; see below. Still, it is important to keep in mind when interpreting our results that we cannot tell if the specific stories people share are true or false: Our fake news measure is at the publisher level, not the story level. To measure the \"sharing of real news sources,\" we cross-referenced participants' tweets against a list of real news publishers obtained from the AllSides organization. AllSides seeks to help citizens navigate the online media landscape by providing \"balanced news and civil discourse.\" Their web page (www.allsides.com/media-bias/) contains a list of 260 real news publishers with ideological ratings: \"Left\" (labeled \"Strong Democratic\" below, n = 45), \"Lean Left\" (\"Leaning Democratic\", n = 63), \"Center/Mixed\" (n = 88), \"Lean Right\" (\"Leaning Republican\", n = 29) or \"Right\" (\"Strong Republican\", n = 39). While AllSides combines different methods for estimating ideological bias (e.g., editorial reviews, third-party judgments), SM Section 7 shows that their ratings correlate highly -Pearson's r = .90with network-based measures of ideological alignment of media sites (Bakshy, Messing and Adamic 2015). See SM Section 7 and Figure SM 2b for the 20 most shared real news domains. Our measures of real and fake news sharing account for ideological and partisan slant at the source level and are thus useful for understanding the content people care about when sharing news. To gain further leverage on testing the relevance of content for sharing behavior, we also built two unique data sets of news headlines (see SM Section 11 and 12 for details). We focus on headlines because these are more accessible than full texts and because prior research demonstrates that simply seeing a headline can exert large effects on decisions to share news on social media (Effron and Raj 2019). The first data set was based on the headlines of stories shared by our participants, which gives us a clear representation of the content they deemed sufficiently important to share. To construct the data set, we first retrieved the full URLs from all 85,084 tweeted news links shared by our participants. Based on these URLs, we parsed the news domains' HTML code and successfully extracted the headlines of 75,560 news articles (89% of all article headlines). To construct our second data set, we used the Archive.org web page -an indexing system for the Internet's web pages for historical archiving -to scrape headlines that had appeared on the daily front pages of the most popular (among our participants; see Figure 5 below) news sites in the period 2016-2019 (i.e., covering roughly the same period for which we have tweets from our participants). Compared to our Twitter headlines, this data set contains a much larger number of news story headlines (we successfully scraped a little over 500,000 headlines). More importantly, the front page headlines offer a glimpse of the universe of news stories our participants could potentially see and read before deciding which stories to share. In this way, it sidesteps issues of \"selection\" biases in sharing decisions due to psychologically motivated reasoning; a point we return to during the analyses. We subjected these headlines to two types of automated analyses. To measure the sentiment of the headlines, we relied on the sentimentR R package, which includes a sophisticated algorithm for calculating text polarity sentiment. This algorithm improves the performance of more traditional dictionary-based methods by calculating sentiment on the sentence level and considering valence shifters (e.g. not, very, hardly). To identify the political affiliations of those mentioned in the headlines, we created two custom dictionaries of prominent Republican and Democratic elites. Our primary dictionary is based on YouGov's list of the 100 most famous Democrats and Republicans. 2 We replicate our results with an alternative dictionary listing all members of the US Congress, Senate and the Trump Administration in the supplemental materials. Finally, we matched these dictionaries to the headline data sets and recorded whether a given headline mentions anyone from the Republican or Democratic party. We use these more detailed content measures in the final part of the analysis, whereas the initial parts focus on the crude distinction between fake and real news sources.",
          "All participants responded to the same survey questions, designed to measure the potential predictors of fake news sharing derived from the three theories discussed earlier: the ignorance theory, the disruption theory, and the polarization theory. SM Section 1c lists the questions asked and specific wordings.  (Pennycook and Rand 2019b). Second, we included participant age to test if we could replicate earlier findings that older social media users share more fake news stories than younger users. Finally, we included measures of factual political knowledge and digital media literacy to test the claims about why older social media users share more \"fake news.\" We included a 5-item battery from the American National Election Studies (ANES) to measure factual political knowledge (e.g., \"How many members of the U.S. Supreme Court are there?\"). To measure digital media literacy in the domain of politics, we created a 10-item battery based on previous research (Feezell 2016, Gil   2 Accessible at https://today.yougov.com/ratings/politics/fame/Democrats/all and https:// today.yougov.com/ratings/politics/fame/Republicans/all de Z√∫√±iga, Molyneux and Zheng 2014) asking participants how often they engage in specific online political behaviors (\"Use the Internet to participate in volunteer activities related to a political or social campaign\") (Œ± = .93). As this measure also taps political engagement and online civic skills, we adjust for political interest in the analyses. In addition, SM Section 4 replicates the main analysis using a more narrow, validated measure of digital literacy from Hargittai and Hsieh (2011). The disruption theory. We included two measures of disruptive motivations. To measure apolitical trolling behavior, we solicited the \"cyber-bullying\" literature and implemented a 4-item trolling scale (Buckels, Trapnell and Paulhus 2014), asking participants whether they agreed or disagreed with statements like \"I like to troll people in forums or the comments section of websites\" (Œ± = .86). We measured political cynicism via an 8-item survey instrument (Dekker and Meijerink 2012), asking participants whether they agreed or disagreed with statements like \"Politics in the United States is sick\" (Œ± = .74). The polarization theory. We included two measures of political polarization: Partisanship and feelings towards Republicans and Democrats. We used a standard ANES question to measure partisanship on a seven-point scale, ranging from \"Strong Democrat\" to \"Strong Republican\" identifier. To measure partisan feelings, we asked participants how strongly they felt various positive (hope, enthusiasm, pride) and negative (anger, frustration, fear) emotions when thinking about Republicans and Democrats. We created two feelings scales to test how in-party love and out-group hate related to news sharing by aggregating responses and taking the averages for each participant (the negative emotion items were flipped). The key models presented below analyze motivations to share news from *pro*-Republican and *pro*-Democratic news sources separately. Accordingly, we coded the \"in-party\" emotion scale such that higher values always correspond to more positive emotions towards the political side supported by the news source (i.e., higher values indicate the participant has more positive feelings towards Republicans (Democrats) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). Analogously, we constructed the \"out-party\" emotion scale such that higher values indicated more negative feelings towards the political side opposed by the news source (i.e., higher values indicate the participant has more nega-tive feelings towards Democrats (Republicans) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). SM Section 2b presents summary statistics for all main variables.",
          "",
          "Before testing the key hypotheses about the psychological motivations for fake news sharing, we provide descriptive analyses of the data. Overall, we find that news sharing is a small part of our sample's activity on Twitter. (See Figure SM2c for an overview.) Of the 2.7 million tweets in our data, only 85,344 (‚âà 3%) contain links to national news websites, fake or real. Of the tweeted news links, 3,269 (‚âà 4%) come from websites with a history of publishing fake news (see Grinberg et al. 2019, for slightly larger estimates during the 2016 US presidential election). Of these stories, a large majority -2,563 stories -originates from *pro*-Republican fake news publishers. In contrast, over twice as many real news stories come from *pro*-Democratic sources (60%) than from *pro*-Republican sources (23%). Together, these two observations imply that exposure to fake news depends heavily on the ideological leanings of one's Twitter network. In *pro*-Democratic networks, only a small fraction of the news diet is likely to come from fake news sources, whereas *pro*-Republican networks are likely to display a substantially higher fake-to-real news ratio. While these aggregate numbers suggest fake news sharing could be widespread within certain social networks, they also mask large heterogeneity in sharing behavior. The left-hand panel in Figure 1 below makes clear that sharing content from fake news sources flourishes mostly among small segments of the population: 1% of the panelists are responsible for sharing about 75% of all links to fake news publishers and 11% of them are responsible for sharing all fake news sources. Meanwhile, the top 1% of real news \"supersharers\" are only responsible for sharing about 30% of all real news sources. Figure 1's two other panels show similar results when splitting the data based on the partisan slant of the news sources. Importantly, not all fake news is equally troublesome. SM Section 8 uses Grinberg and colleagues' (2019) fine-grained categorization of fake news sources and find that about 30% of the fake news links come from sources \"publishing almost exclusively fabricated stories\" (i.e. \"black\" sites). Another 5% come from sites that \"spread falsehoods that clearly reflected a flawed editorial process\" (i.e. \"red\" sites). Finally, two-thirds of the links referred to fake news sources \"where annotators were less certain that the falsehoods stemmed from a systematically flawed process\" (i.e. \"orange\" sites). While this means that the number of individual stories containing blatant falsehoods is likely smaller than 3,325 in our data, we note that sharing news stories from websites with a record of disseminating falsehoods can still be problematic. Given the still limited penetration of fact-checking, sharers most likely do not know whether a certain news story is true or false when deciding to share it. Instead, they must make informed guesses based on the reputation of the news source. Thus, decisions to share news from these sources still reflect either ignorance or lack of concern about such matters.  Note. The x-axes show percentage of participants responsible for sharing a given percentage of all news links (y-axes). The left-hand panel gives results for sharing links to fake (black line) versus real (grey line) news publishers (N Fake News = 3,269; N Real News = 78,400). The middle and right-hand panels display results for sharing links to *pro*-Democratic (blue line) versus *pro*-Republican (red line) fake and real news sources, respectively. For real news sources, we collapse the \"Leaning\" and \"Strong\" categories. The dashed black lines denote a hypothetical uniform distribution of news sharing. (N Fake News, pro-Rep = 2,563; N Fake News, pro-Dem = 263; N Real News, pro-Rep = 16,081; N Real News, pro-Dem = 46,385.)",
          "We now turn to the analysis of the psychological motivations for sharing fake news sources. Figure 2 gives average marginal effects (AME) 3 from logistic regression models estimating the association between our main predictors and the probability of sharing at least one news story from a fake news (left panel) or a real news source (right panel). To increase power and simplify the analyses, we here combine the real news sources that Allsides rate as strongly partisan and leaning partisan (e.g., we combine news stories from \"Strong Democratic\" and \"Leaning Democratic\" real news sources). As our main interest is in whether people share fake news or not, we dichotomize the dependent variables, with 1 indicating that a participant As shown in the lower right-hand panel, the partisan factors predict sharing from real news sources in an almost identical fashion. The motivations underlying fake news and real news sharing are, psychologically speaking, highly overlapping. One difference compared to sharing of fake news sources is that our estimates are obtained with greater precision due to the larger number of participants sharing real news sources. Another difference relates to the partisan asymmetry: Concerning real news sharing, we find no asymmetry between Democrats and Republicans; both are equally partisan and strongly motivated by negative feelings towards political opponents. Altogether, these results fit well with predictions by theories on partisan motivations: Partisans share politically congenial news, primarily because of hostile feelings towards the out-party. Further, the lack of evidence for the ignorance theory suggests that these sharers pay more attention to the political usefulness of news rather the information quality. Below, we put the polarization theory to an even stronger test by considering the role of news content in sharing decisions. ",
          "For the sake of clarity, our main analysis employs crude binary distinctions between fake and real news as well as between those who share any news and those who do not share news at all. This section tests the robustness of our results with respect to these distinctions. Most importantly, we operationalize the sharing of fake news as the sharing of all links to news sources with a history of publishing falsehoods. However, some fake news sources produce more misinformation than others. It is possible that when it comes to blatant falsehoods, ignorant rather than polarized people are the main culprits. To test this possibility, we rely on the fake news classification of Grinberg et al. ( 2019) described above and re-run our analysis separately for each of three fake news categories. Our results reported in SM Section 8 show high consistency across the three categories. People who share links from sources that publish \"almost exclusively fabricated stories\" are also more polarized, but not more ignorant, and only slightly more disruptive than those who do not share any such links. We also replicate our results relying on expert ratings of the trustworthiness of 60 news sources (20 mainstream news sources, 20 hyperpartisan websites, and 20 fake news sources) based on Pennycook and Rand (2019a). Whereas this conceptualization offers an even more fine-grained look into the quality of news sources, it comes at a cost of dropping more than 90% of all news sources from our analysis. Following Pennycook and Rand (2019a) we calculate the average trustworthiness of news shared for each individual in our sample and use it as our dependent variable. SM Section 9 demonstrates that the findings reflect those presented in the main text: Political variables -partisanship and feelings towards Republicans and Democrats -correlate strongly with the trustworthiness of shared news sources whereas \"ignorance variables\" -CRT and political knowledge -do not. Finally, we replicate our analysis by modeling the actual count of fake and real news shared instead of binary variables of sharing versus not sharing. This involves a change from logistic regression to Quasi-Poisson models. The two models have important substantive differences. After all, sharing a single fake news link amidst dozens of real news links could be an honest mistake. Relying on the count variables, in contrast, put more weight on respondents with higher numbers of shares within a category. SM section 5 indicates this alternative modelling procedure does not affect our main conclusions. While the effect of political cynicism is significantly reduced, partisan motivation -especially negative affect towards the out-party -remains the primary predictor of fake news sharing. 4 The Role of News Content: Fake news as the Extreme Ends of a",
          "News source sharing patterns across the partisan news continuum. If partisan motives dictate both fake and real news sharing, social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. To test this, Figure 3 plots a heatmap of correlations based on news sharing across the seven news source types. The emerging patterns firmly favor a partisan story: People who share the most articles from ideologically biased real news sources are also most likely to share fake news sources with a similar political bias. In contrast, the off-diagonal correlations show that people shy away from sharing politically opposed sources. Importantly, the plot also reveals a partisan asymmetry. Whereas sharing *pro*-Republican real news correlates highly with sharing *pro*-Republican fake news, the correlation is substantially weaker between sharing *pro*-Democratic fake and real news. Instead, participants sharing *pro*-Democratic real news often share centrist news as well. Thus, while social media users generally favor partisan news sources, the effect is most pronounced in the Republican end of the continuum. 4 In SM Section 4, we present a series of additional robustness tests. First, we show that our results hold after removing 61 panelists who followed -or were being followed by -at least one other panelist. Second, we demonstrate that applying YouGov sample-matching weights do not alter our results. Third, we re-run models that control for age and partisanship; this does not change our results. Fourth, we leverage a potentially better measure of digital literacy (Hargittai and Hsieh 2011); using this measure does not change our substantive conclusions. Fifth, we show that our results are robust to an alternative coding scheme that treats InfoWars as a *pro*-Republican fake news source rather than a real news source. Further, SM Section 8 shows that our conclusions remain intact when using an alternative list of fake news sources compiled by Grinberg et al. (2019). The predictive power of partisanship across the partisan news continuum. If news sources differ systematically in how strongly their content caters to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Figure 4 tests this assertion. The left-hand panel plots estimated logistic regression coefficients from models that examine the association between a seven-point partisanship scale (re-scaled to range from 0 = Democratic Identifier to 1 = Republican Identifier) and sharing content from the seven news source types, while the right-hand panel gives raw counts of shared links stacked by participants' partisanship (where, to ease interpretation, we trichotomize the partisanship scale). Overall, Figure 4 provides more evidence in favor of the partisan logic of news sharing: Both panels show that Republicans are more likely than Democrats to share extreme *pro*-Republican news    Left-hand panels: Headlines shared on Twitter by panelists (upper panel) and headlines scraped from front pages of news sources (lower panel). Red dots: Headlines that mention Republican politicians. Blue dots: Headlines that mention Democratic politicians. In the lower panel, we scraped headlines from the following news sources: Fake Republican: babylonbee.com, dailywire.com, ilovemyfreedom.org, theconservativetreehouse.com, iotwreport.com; Strong Republican: breitbart.com, dailycaller.com, freebeacon.com, nypost.com, townhall.com; Lean Republican: foxnews.com, hotair.com, telegraph.co.uk, washingtonexaminer.com, washingtontimes.com; Center: thehill.com, usatoday.com, npr.com, bbc.com, wsj.com; Lean Democratic: politico.com, nbcnews.com, nytimes.com, theguardian.com, washingtonpost.com; Strong Democratic: alternet.org, dailykos.com, huffingtonpost.com, rawstory.com, vox.com; Fake Democratic: bipartisanreport.com, indiatimes.com, newspunch.com, palmerreport.com, themindunleashed.com. Righthand panels: Headline counts from news sources, conditional on whether headline mentions Republican elites, Democratic elites, or both. Note that headlines mentioning both Republicans and Democrats counted twice in the sentiment analysis. See SM Section 11 and 12 for details on the analysis. are negative towards Democrats. We can go further. For partisans, a critical consideration when promoting news sources on social media is not just the level of negativity expressed towards the out-party. Instead, the most politically useful news source is one that is negative towards the out-party and positive towards the in-party. This implies that we should also examine, for each type of news source, the difference in negativity between mentions of Republican and Democratic elites in Figure 5. Recall that Democrats are more likely to share centrist news sources while Republican partisans prefer fake news sources. The net differences in headline sentiment when mention- We suspect this explains why Republican partisans find fake news more appealing than do Democratic partisans. To get a steady supply of news that caters to their political tastes, Republicans must turn to more extreme news sources, including those known to publish falsehoods. Democrats, in contrast, will have to look no further than to Centrist and *pro*-Democratic real news sources to meet their partisan goals. Importantly, this explanation cannot easily be dismissed by a \"self-selection\" story in which Republican panelists simply decide to share more extreme stories. Focusing on the front page headlines of stories journalists and editors have deemed sufficiently newsworthy to write and publish leads to the same conclusion. Still, whether this speculation is warranted, the combined observations point to an astonishing overlap in the asymmetries in (1) the sources shared by Democrat and Republican partisans, respectively, and (2) the tone of the stories shared from these sources when covering Democratic and Republican elites, respectively. Together, these three analyses corroborate that Republicans approach news source types as if they are placed on a one-dimensional partisan continuum. Democratic partisans, to be sure, do too, but with the very important exception that *pro*-Democratic fake news sources are less attractive to highly polarized Democrats when we compare (a) Democrats to Republicans and (b) *pro*-Democratic fake news to traditional *pro*-Democratic news sources. As we turn to below, this overall pattern suggests that the sharing of fake news is This suggests that fake news sharing is \"business as usual,\" dictated by the same logic partisans use to process other types of information, including real news. From a partisan motivated perspective, fake news is not categorically different from other sources of political information. As the analyses of the news content showed, partisans' decisions to share both fake and real news sources depend on how politically useful they are in derogating the out-party. Under some circumstances, this goal is best achieved with stories from fake news websites. Under other circumstances, this can best be achieved by stories from more credible websites. To the partisan mind, fake news is the extreme end of a news source continuum where the news sources differ in how well they cater to partisan goals. Accordingly, when deciding to share a story from a fake news website, most people do not prioritize whether the story is true or not. This helps explain why cognitive reflection is not related to fake news sharing even though it correlates with the ability to discern fake news from real news. These abilities are not employed in the context of real-life sharing decisions. At the same time, the sum of findings does suggest a dual involvement of accuracy-oriented and goal-oriented motivations. First, fake news sharing is a relatively rare phenomenon that flourishes only among small segments of the population. Like others, and in contrast to what some commentators have feared, we find that only 11% of our participants shared stories from fake news sources during our period of study while half of the participants shared stories from real news web domains. Second, as discussed below, Republicans were more likely than Democrats to share fake news (e.g., Guess, Nagler and Tucker 2019), even though fake news could help Democrats achieve their partisan goals. This suggests that social media users prioritize the usefulness of information when making sharing decisions but prefer useful information that is true rather than false; they care about the veracity of news as long as it does not compromise its usefulness. Two observations merit further discussion. First, it is relevant to discuss the finding that negative feelings towards political opponents rather than positive feelings towards the inparty are the strongest predictor of fake news sharing. There is strong evidence that partisans generally prefer helping the in-party rather than hurting the out-party (Lelkes and Westwood 2017). A hostile political climate, however, may shift the balance. Research suggests that the danger of \"symbolic\" threats against the moral values of one's group can make it \"necessary to strike back, to undermine the credibility of the opposition, and to 'hit them where it hurts\"' (Amira, Wright and Goya-Tocchetto 2019). Twitter -with its fast diffusion of moralizing and emotion-laden political content (Brady et al. 2017) -may have fostered such a hostile environment, making considerations of hurting the out-party loom larger when deciding which news stories to share. Second, we found a significant partisan asymmetry: Republicans were more likely than Democrats to share fake news sources. This could reflect that Republicans are less motivated by accuracy-motivations than Democrats. Yet, accuracy motivations have low explanatory power overall and additional analyses presented in the replication code show that that the associations between ignorance-related variables and fake news sharing do not consistently differ between Democrats and Republicans. Accordingly, other explanations may be at play. One possibility highlights differences in the supply of news (Grinberg et al. 2019): Democrats and Republicans may be equally motivated by accuracy goals but Republicans end up sharing more falsehoods because their Twitter networks offer a greater supply of fake news sources to share from. Another -complementary -supply-side explanation shifts the focus from accuracy-to goal-oriented motivations: Democrats and Republicans are equally motivated to share information that is useful for furthering their political agenda but the supply of useful real news stories is lower for Republicans, propelling them to turn to fake news sites for material. The sentiment analyses of news headlines are consistent with this explanation. Only *pro*-Republican fake news sources were more negative towards Democrats than towards Republicans. This pattern was consistent across news stories shared by the panelists, both partisans and Independents (see SM section 11 for analyses of Independents), and the news stories from the news site front pages. This at least suggests that this asymmetry reflects a difference in the supply of content. A Republican scanning the Internet for materials to share must turn to fake news sources to find news that, on average, satisfy their partisan need to derogate Democrats. Democrats operating in the same manner, in contrast, can stop their search at posts linking to *pro*-Democratic real news websites or even centrist websites. Without individual-level data on news exposure, it is very difficult to tease apart these explanations (see Eady et al. (2019)) and, most likely, all are at play. Still, if the latter explanations are at least partly correct, the partisan asymmetry in fake news sharing may have less to do with psychological differences between Democrats and Republicans and more to do with differences in the supply of useful information (in a partisan sense). This would dovetail with studies demonstrating that Democrats and Republicans are equally motivated by partisan goals (Brandt et al. 2014). Also, we want to note that it is unclear whether a supply-oriented explanation is evidence of actual media bias. Bias is notoriously difficult to establish. Some prior studies have found similar evidence of a liberal bias in legacy media (Groseclose and Milyo 2005), but the general negative coverage of leading Republicans could also reflect a \"bias in reality\" (Groeling 2013) during an unusual period in US politics. Overall, these conclusions have important implications for how to intervene successfully against the circulation of \"fake news.\" If people care primarily about a story's ability to hurt political enemies, we should not be surprised if fact-checking fails to reduce sharing. In fact, our findings suggest that offline realities shape online sharing of \"fake news.\" Fake news sharing relates to the increasingly polarized political debates in the United States, and it may be difficult to fix the problem of fake news without fixing the larger problem of political polarization. Unfortunately, this is much harder to accomplish than adding automatic factchecks on social media platforms. This does not imply that it is impossible to demotivate sharing of \"fake news.\" Evidence is emerging that fake news sharing decreases other people's trust in the sharer (Altay, Hacquin and Mercier 2019). Reputational concerns may explain the low prevalence of fake news. This implies that successful interventions could work by reminding people to \"not make a fool of themselves\" when they are about to share information from non-credible sources. Yet, even if such interventions can drive down sharing of genuine fake news (Pennycook et al. 2019), the underlying problem remains. Polarized partisans can often find plenty of negative content on ordinary news sites. While each individual news story may be true, the overall picture painted by exclusively sharing derogatory news stories may not be. In this perspective, the real problem is the tendency to selectively engage with derogatory news"
        ],
        "ground_truth_definitions": {
          "fake news": {
            "definition": "fabricated information that mimics news media content in form but not in organizational process or intent",
            "context": "With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0090023afc66cd2741568599057f4e82b566137c",
        "sections": [
          "Machine learning algorithms have penetrated every aspect of our lives. Algorithms make movie recommendations, suggest products to buy, and who to date. They are increasingly used in high-stakes scenarios such as loans [113] and hiring decisions [19,39]. There are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored [45,119], and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions \"unfair\" [6,121]. In the context of decision-making, fairness is the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics. Thus, an unfair algorithm is one whose decisions are skewed toward a particular group of people. A canonical example comes from a tool used by courts in the United States to make pretrial detention and release decisions. The software, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), measures the risk of a person to recommit another crime. Judges use COMPAS to decide whether to release an offender, or to keep him or her in prison. An investigation into the software found a bias against African-Americans: 1  COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or recidivism. Similar findings have been made in other areas, such as an AI system that judges beauty pageant winners but was biased against darker-skinned contestants, 2 or facial recognition software in digital cameras that overpredicts Asians as blinking. 3 These biased predictions stem from the hidden or neglected biases in data or algorithms. In this survey we identify two potential sources of unfairness in machine learning outcomesthose that arise from biases in the data and those that arise from the algorithms. We review research investigating how biases in data skew what is learned by machine learning algorithms, and nuances in the way the algorithms themselves work to prevent them from making fair decisions-even when the data is unbiased. Furthermore, we observe that biased algorithmic outcomes might impact user experience, thus generating a feedback loop between data, algorithms and users that can perpetuate and even amplify existing sources of bias. We begin the review with several highly visible real-world cases of where unfair machine learning algorithms have led to suboptimal and discriminatory outcomes in Section 2. In Section 3, we describe the different types and sources of biases that occur within the data-algorithms-users loop mentioned above. Next, in Section 4, we present the different ways that the concept of fairness has been operationalized and studied in the literature. We discuss the ways in which these two concepts are coupled. Last, we will focus on different families of machine learning approaches, how fairness manifests differently in each one, and the current state-of-the-art for tackling them in Section 5, followed by potential areas of future work in each of the domains in Section 6.",
          "With the popularity of AI and machine learning over the past decades, and their prolific spread in different applications, safety and fairness constraints have become a significant issue for researchers and engineers. Machine learning is used in courts to assess the probability that a defendant recommits a crime. It is used in different medical fields, in childhood welfare systems [35], and autonomous vehicles. All of these applications have a direct effect in our lives and can harm our society if not designed and engineered correctly, that is with considerations to fairness. [123] has a list of the applications and the ways these AI systems affect our daily lives with their inherent biases, such as the existence of bias in AI chatbots, employment matching, flight routing, and automated legal aid for immigration algorithms, and search and advertising placement algorithms. [67] discusses examples of how bias in the real world can creep into AI and robotic systems, such as bias in face recognition applications, voice recognition, and search engines. Therefore, it is important for researchers and engineers to be concerned about the downstream applications and their potential harmful effects when modeling an algorithm or a system.",
          "COMPAS is an exemplar of a discriminatory system. In addition to this, discriminatory behavior was also evident in an algorithm that would deliver advertisements promoting jobs in Science, Technology, Engineering, and Math (STEM) fields [88]. This advertisement was designed to deliver advertisements in a gender-neutral way. However, less women compared to men saw the advertisement due to gender-imbalance which would result in younger women being considered as a valuable subgroup and more expensive to show advertisements to. This optimization algorithm would deliver ads in a discriminatory way although its original and pure intention was to be gender-neutral. Bias in facial recognition systems [128] and recommender systems [140] have also been largely studied and evaluated and in many cases shown to be discriminative towards certain populations and subgroups. In order to be able to address the bias issue in these applications, it is important for us to know where these biases are coming from and what we can do to prevent them. We have enumerated the bias in COMPAS, which is a widely used commercial risk assessment software. In addition to its bias, it also contains performance issues when compared to humans. When compared to non-expert human judgment in a study, it was discovered to be not any better than a normal human [46]. It is also interesting to note that although COMPAS uses 137 features, only 7 of those were presented to the people in the study. [46] further argues that COMPAS is not any better than a simple logistic regression model when making decisions. We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples' lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools. In another similar study, while investigating sources of group unfairness (unfairness across different groups is defined later), the authors in [145] compared SAVRY, a tool used in risk assessment frameworks that includes human intervention in its process, with automatic machine learning methods in order to see which one is more accurate and more fair. Conducting these types of studies should be done more frequently, but prior to releasing the tools in order to avoid doing harm.",
          "An interesting direction that researchers have taken is introducing tools that can assess the amount of fairness in a tool or system. For example, Aequitas [136] is a toolkit that lets users to test models with regards to several bias and fairness metrics for different population subgroups. Aequitas produces reports from the obtained data that helps data scientists, machine learning researchers, and policymakers to make conscious decisions and avoid harm and damage toward certain populations. AI Fairness 360 (AIF360) is another toolkit developed by IBM in order to help moving fairness research algorithms into an industrial setting and to create a benchmark for fairness algorithms to get evaluated and an environment for fairness researchers to share their ideas [11]. These types of toolkits can be helpful for learners, researchers, and people working in the industry to move towards developing fair machine learning application away from discriminatory behavior.",
          "Most AI systems and algorithms are data driven and require data upon which to be trained. Thus, data is tightly coupled to the functionality of these algorithms and systems. In the cases where the underlying training data contains biases, the algorithms trained on them will learn these biases and reflect them into their predictions. As a result, existing biases in data can affect the algorithms using the data, producing biased outcomes. Algorithms can even amplify and perpetuate existing biases in the data. In addition, algorithms themselves can display biased behavior due to certain design choices, even if the data itself is not biased. The outcomes of these biased algorithms can then be fed into real-world systems and affect users' decisions, which will result in more biased data for training future algorithms. For example, imagine a web search engine that puts specific results at the top of its list. Users tend to interact most with the top results and pay little attention to those further down the list [92]. The interactions of users with items will then be collected by the web search engine, and the data will be used to make future decisions on how information should be presented based on popularity and user interest. As a result, results at the top will become more and more popular, not because of the nature of the result but due to the biased interaction and placement of results by these algorithms [92]. The loop capturing this feedback between biases in data, algorithms, and user interaction is illustrated in Figure 1. We use this loop to categorize definitions of bias in the section below.",
          "",
          "",
          "",
          "",
          "",
          "",
          "Bias can exist in many shapes and forms, some of which can lead to unfairness in different downstream learning tasks. In [144], authors talk about sources of bias in machine learning with their categorizations and descriptions in order to motivate future solutions to each of the sources of bias introduced in the paper. In [120], the authors prepare a complete list of different types of biases with their corresponding definitions that exist in different cycles from data origins to its collection and its processing. Here we will reiterate the most important sources of bias introduced in these two papers and also add in some work from other existing research papers. Additionally, we will introduce a different categorization of these definitions in the paper according to the data, algorithm, and user interaction loop.",
          "In this section we talk about biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes. (1) Measurement Bias. Measurement, or reporting, bias arises from how we choose, utilize, and measure particular features [144]. An example of this type of bias was observed in the recidivism risk prediction tool COMPAS, where prior arrests and friend/family arrests were used as proxy variables to measure level of \"riskiness\" or \"crime\"--which on its own can be viewed as mismeasured proxies. This is partly due to the fact that minority communities are controlled and policed more frequently, so they have higher arrest rates. However, one should not conclude that because people coming from minority groups have higher arrest rates therefore they are more dangerous as there is a difference in how these groups are assessed and controlled [144]. (2) Omitted Variable Bias. Omitted variable bias 4 occurs when one or more important variables are left out of the model [38,114,131]. An example for this case would be when someone designs a model to predict, with relatively high accuracy, the annual percentage rate at which customers will stop subscribing to a service, but soon observes that the majority of users are canceling their subscription without receiving any warning from the designed model. Now imagine that the reason for canceling the subscriptions is appearance of a new strong competitor in the market which offers the same solution, but for half the price. The appearance of the competitor was something that the model was not ready for; therefore, it is considered to be an omitted variable. (3) Representation Bias. Representation bias arises from how we sample from a population during data collection process [144]. Non-representative samples lack the diversity of the population, with missing subgroups and other anomalies. Lack of geographical diversity in datasets like ImageNet (as shown in Figures 3 and4) results in demonstrable bias towards Western cultures. (4) Aggregation Bias. Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders. Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore, a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population [144]. This is true even when they are represented equally in the training data. Any general assumptions about subgroups within the population can result in aggregation bias.  (a) Simpson's Paradox. Simpson's paradox is a type of aggregation bias that arises in the analysis of heterogeneous data [18]. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups (Fig. 2(a)). One of the better-known examples of the type of paradox arose during the gender bias lawsuit in university admissions against UC Berkeley [16]. After analyzing graduate school admissions data, it seemed like there was bias toward women, a smaller fraction of whom were being admitted to graduate programs compared to their male counterparts. However, when admissions data was separated and analyzed over the departments, women applicants had equality and in some cases even a small advantage  over men. The paradox happened as women tended to apply to departments with lower admission rates for both genders. Simpson's paradox has been observed in a variety of domains, including biology [37], psychology [81], astronomy [109], and computational social science [91]. (b) Modifiable Areal Unit Problem is a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation [56]. This bias results in different trends learned when data is aggregated at different spatial scales. (5) Sampling Bias. Sampling bias is similar to representation bias, and it arises due to nonrandom sampling of subgroups. As a consequence of sampling bias, the trends estimated for one population may not generalize to data collected from a new population. For the intuition, consider the example in Figure 2. The left plot represents data collected during a study from three subgroups, which were uniformly sampled (Fig. 2(a)). Suppose the next time the study was conducted, one of the subgroups was sampled more frequently than the rest (Fig. 2(b)). The positive trend found by the regression model in the first study almost completely disappears (solid red line in plot on the right), although the subgroup trends (dashed green lines) are unaffected. (6) Longitudinal Data Fallacy. Researchers analyzing temporal data must use longitudinal analysis to track cohorts over time to learn their behavior. Instead, temporal data is often modeled using cross-sectional analysis, which combines diverse cohorts at a single time point. The heterogeneous cohorts can bias cross-sectional analysis, leading to different conclusions than longitudinal analysis. As an example, analysis of bulk Reddit data [10] revealed that comment length decreased over time on average. However, bulk data represented a cross-sectional snapshot of the population, which in reality contained different cohorts who joined Reddit in different years. When data was disaggregated by cohorts, the comment length within each cohort was found to increase over time. (7) Linking Bias. Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users [120]. In [104] authors show how social networks can be biased toward low-degree nodes when only considering the links in the network and not considering the content and behavior of users in the network. [153] also shows that user interactions are significantly different from social link patterns that are based on features, such as method of interaction or time. The differences and biases in the networks can be a result of many factors, such as network sampling, as shown in [59,111], which can change the network measures and cause different types of problems. 3.1.2 Algorithm to User. Algorithms modulate user behavior. Any biases in algorithms might introduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic outcomes and affect user behavior as a consequence. (1) Algorithmic Bias. Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm [9]. The algorithmic design choices, such as use of certain optimization functions, regularizations, choices in applying regression models on the data as a whole or considering subgroups, and the general use of statistically biased estimators in algorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of the algorithms. (2) User Interaction Bias. User Interaction bias is a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction [9]. This type of bias can be influenced by other types and subtypes, such as presentation and ranking biases. (a) Presentation Bias. Presentation bias is a result of how information is presented [9]. For example, on the Web users can only click on content that they see, so the seen content gets clicks, while everything else gets no click. And it could be the case that the user does not see all the information on the Web [9]. (b) Ranking Bias. The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others. This bias affects search engines [9] and crowdsourcing applications [93]. (3) Popularity Bias. Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation-for example, by fake reviews or social bots [117]. As an instance, this type of bias can be seen in search engines [71,117] or recommendation systems where popular objects would be presented more to the public. But this presentation may not be a result of good quality; instead, it may be due to other biased factors. (4) Emergent Bias. Emergent bias occurs as a result of use and interaction with real users. This bias arises as a result of change in population, cultural values, or societal knowledge usually some time after the completion of design [53]. This type of bias is more likely to be observed in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of prospective users by design [53]. This type of bias can itself be divided into more subtypes, as discussed in detail in [53]. (5) Evaluation Bias. Evaluation bias happens during model evaluation [144]. This includes the use of inappropriate and disproportionate benchmarks for evaluation of applications such as Adience and IJB-A benchmarks. These benchmarks are used in the evaluation of facial recognition systems that were biased toward skin color and gender [24], and can serve as examples for this type of bias [144]. 3.1.3 User to Data. Many data sources used for training ML models are user-generated. Any inherent biases in users might be reflected in the data they generate. Furthermore, when user behavior is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in the data generation process. Here we list several important types of such biases. (1) Historical Bias. Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection [144]. An example of this type of bias can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5% of Fortune 500 CEOs were woman-which would cause the search results to be biased towards male CEOs [144]. These search results were of course reflecting the reality, but whether or not the search algorithms should reflect this reality is an issue worth considering. (2) Population Bias. Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population [120]. Population bias creates non-representative data. An example of this type of bias can arise from different user demographics on different social platforms, such as women being more likely to use Pinterest, Facebook, Instagram, while men being more active in online forums like Reddit or Twitter. More such examples and statistics related to social media use among young adults according to gender, race, ethnicity, and parental educational background can be found in [64]. (3) Self-Selection Bias. Self-selection bias4 is a subtype of the selection or sampling bias in which subjects of the research select themselves. An example of this type of bias can be observed in an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic supporters are more likely to complete the poll. (4) Social Bias. Social bias happens when others' actions affect our judgment. [9]. An example of this type of bias can be a case where we want to rate or review an item with a low score, but when influenced by other high ratings, we change our scoring thinking that perhaps we are being too harsh [9,151]. (5) Behavioral Bias. Behavioral bias arises from different user behavior across platforms, contexts, or different datasets [120]. An example of this type of bias can be observed in [108], where authors show how differences in emoji representations among platforms can result in different reactions and behavior from people and sometimes even leading to communication errors. (6) Temporal Bias. Temporal bias arises from differences in populations and behaviors over time [120]. An example can be observed in Twitter where people talking about a particular topic start using a hashtag at some point to capture attention, then continue the discussion about the event without using the hashtag [120,146]. (7) Content Production Bias. Content Production bias arises from structural, lexical, semantic, and syntactic differences in the contents generated by users [120]. An example of this type of bias can be seen in [118] where the differences in use of language across different gender and age groups is discussed. The differences in use of language can also be seen across and within countries and populations. Existing work tries to categorize these bias definitions into groups, such as definitions falling solely under data or user interaction. However, due to the existence of the feedback loop phenomenon [36], these definitions are intertwined, and we need a categorization which closely models this situation. This feedback loop is not only existent between the data and the algorithm, but also between the algorithms and user interaction [29]. Inspired by these papers, we modeled categorization of bias definitions, as shown in Figure 1, and grouped these definitions on the arrows of the loop where we thought they were most effective. We emphasize the fact again that these definitions are intertwined, and one should consider how they affect each other in this cycle, and address them accordingly.",
          "There are multiple ways that discriminatory bias can seep into data. For instance, using unbalanced data can create biases against underrepresented groups. [170] analyzes some examples of the biases that can exist in the data and algorithms and offer some recommendations and suggestions toward mitigating these issues.",
          "Learning Data. In [24], the authors show that datasets like IJB-A and Adience are imbalanced and contain mainly light-skinned subjects-79.6% in IJB-A and 86.2% in Adience. This can bias the analysis towards dark-skinned groups who are underrepresented in the data. In another instance, the way we use and analyze our data can create bias when we do not consider different subgroups in the data. In [24], the authors also show that considering only malefemale groups is not enough, but there is also a need to use race to further subdivide the gender groups into light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. It's only in this case that we can clearly observe the bias towards dark-skinned females, as previously dark-skinned males would compromise for dark-skinned females and would hide the underlying bias towards this subgroup. Popular machine-learning datasets that serve as a base for most of the developed algorithms and tools can also be biased-which can be harmful to the downstream applications that are based on these datasets. For instance, ImageNet [135] and Open Images [86] are two widely used datasets in machine-learning. In [142], researchers showed that these datasets suffer from representation bias and advocate for the need to incorporate geographic diversity and inclusion while creating such datasets. In addition, authors in [105] write about the existing representational biases in different knowledge bases that are widely used in Natural Language Processing (NLP) applications for different commonsense reasoning tasks.",
          "Applications. These data biases can be more dangerous in other sensitive applications. For example, in medical domains there are many instances in which the data studied and used are skewed toward certain populations-which can have dangerous consequences for the underrepresented communities. [98] showed how exclusion of African-Americans resulted in their misclassification in clinical studies, so they became advocates for sequencing the genomes of diverse populations in the data to prevent harm to underrepresented populations. Authors in [143] studied the 23andMe genotype dataset and found that out of 2,399 individuals, who have openly shared their genotypes in public repositories, 2,098 (87%) are European, while only 58 (2%) are Asian and 50 (2%) African. Other such studies were conducted in [54] which states that UK Biobank, a large and widely used genetic dataset, may not represent the sampling population. Researchers found evidence of a \"healthy volunteer\" selection bias. [150] has other examples of studies on existing biases in the data used in the medical domain. [157] also looks at machine-learning algorithms and data utilized in medical fields, and writes about how artificial intelligence in health care has not impacted all patients equally.",
          "Similar to bias, discrimination is also a source of unfairness. Discrimination can be considered as a source for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes, which may happen intentionally or unintentionally, while bias can be considered as a source for unfairness that is due to the data collection, sampling, and measurement. Although bias can also be seen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic fairness literature it is more intuitive to categorize them as such according to the existing research in these areas. In this survey, we mainly focus on concepts that are relevant to algorithmic fairness issues. [99,133,152] contain more broad information on discrimination theory that involve more multidisciplinary concepts from legal theory, economics, and social sciences which can be referenced by the interested readers.",
          "Differences in treatment and outcomes amongst different groups can be justified and explained via some attributes in some cases. In situations where these differences are justified and explained, it is not considered to be illegal discrimination and hence called explainable [77]. For instance, authors in [77] state that in the UCI Adult dataset [7], a widely used dataset in the fairness domain, males on average have a higher annual income than females. However, this is because on average females work fewer hours than males per week. Work hours per week is an attribute that can be used to explain low income which needs to be considered. If we make decisions, without considering working hours, such that males and females end up averaging the same income, we will lead to reverse discrimination since we would cause male employees to get lower salary than females. Therefore, explainable discrimination is acceptable and legal as it can be explained through other attributes like working hours. In [77], authors present a methodology to quantify the explainable and illegal discrimination in data. They argue that methods that do not take the explainable part of the discrimination into account may result in non-desirable outcomes, so they introduce a reverse discrimination which is equally harmful and undesirable. They explain how to quantify and measure discrimination in data or a classifier's decisions which directly considers illegal and explainable discrimination.",
          "In contrast to explainable discrimination, there is unexplainable discrimination in which the discrimination toward a group is unjustified and therefore considered illegal. Authors in [77] also present local techniques for removing only the illegal or unexplainable discrimination, allowing only for explainable differences in decisions. These are preprocessing techniques that change the training data such that it contains no unexplainable discrimination. We expect classifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination. Unexplainable discrimination consists of direct and indirect discrimination. (1) Direct Discrimination. Direct discrimination happens when protected attributes of individuals explicitly result in non-favorable outcomes toward them [164]. Typically, there are some traits identified by law on which it is illegal to discriminate against, and it is usually these traits that are considered to be \"protected\" or \"sensitive\" attributes in computer science literature. A list of some of these protected attributes is provided in Table 3 as specified in the Fair Housing and Equal Credit Opportunity Acts (FHA and ECOA) [30]. (2) Indirect Discrimination. In indirect discrimination, individuals appear to be treated based on seemingly neutral and non-protected attributes; however, protected groups, or individuals still get to be treated unjustly as a result of implicit effects from their protected attributes (e.g., the residential zip code of a person can be used in decision making processes such as loan applications. However, this can still lead to racial discrimination, such as redlining, as despite the fact that zip code appears to be a non-sensitive attribute, it may correlate with race because of the population of residential areas.) [130,164].",
          "(1) Systemic Discrimination. Systemic discrimination refers to policies, customs, or behaviors that are a part of the culture or structure of an organization that may perpetuate discrimination against certain subgroups of the population [40]. [132] found that employers overwhelmingly preferred competent candidates that were culturally similar to them, and shared similar experiences and hobbies. If the decision-makers happen to belong overwhelmingly to certain subgroups, this may result in discrimination against competent candidates that do not belong to these subgroups. (2) Statistical Discrimination. Statistical discrimination is a phenomenon where decision-makers use average group statistics to judge an individual belonging to that group. It usually occurs when the decision-makers (e.g., employers, or law enforcement officers) use an individual's obvious, recognizable characteristics as a proxy for either hidden or more-difficult-to-determine characteristics, that may actually be relevant to the outcome [124].",
          "Fighting against bias and discrimination has a long history in philosophy and psychology, and recently in machine-learning. However, in order to be able to fight against discrimination and achieve fairness, one should first define fairness. Philosophy and psychology have tried to define the concept of fairness long before computer science. The fact that no universal definition of fairness exists shows the difficulty of solving this problem [138]. Different preferences and outlooks in different cultures lend a preference to different ways of looking at fairness, which makes it harder to come up with just a single definition that is acceptable to everyone in a situation. Indeed, even in computer science, where most of the work on proposing new fairness constraints for algorithms has come from the West, and a lot of these papers use the same datasets and problems to show how their constraints perform, there is still no clear agreement on which constraints are the most appropriate for those problems. Broadly, fairness is the absence of any prejudice or favoritism towards an individual or a group based on their intrinsic or acquired traits in the context of decision-making [139]. Even though fairness is an incredibly desirable quality in society, it can be surprisingly difficult to achieve in practice. With these challenges in mind, many fairness definitions are proposed to address different algorithmic bias and discrimination issues discussed in the previous section.",
          "In [17], authors studied fairness definitions in political philosophy and tried to tie them to machinelearning. Authors in [70] studied the 50-year history of fairness definitions in the areas of education and machine-learning. In [149], authors listed and explained some of the definitions used for fairness in algorithmic classification problems. In [139], authors studied the general public's perception of some of these fairness definitions in computer science literature. Here we will reiterate and provide some of the most widely used definitions, along with their explanations inspired from [149]. Definition 1. (Equalized Odds). The definition of equalized odds, provided by [63], states that \"A predictor ≈∂ satisfies equalized odds with respect to protected attribute A and outcome Y, if ≈∂ and A are independent conditional on Y. P( ≈∂=1|A=0,Y =y) = P( ≈∂=1|A=1,Y =y) , y‚àà{0,1}\". This means that the probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members [149]. In other words, the equalized odds definition states that the protected and unprotected groups should have equal rates for true positives and false positives. Definition 2. (Equal Opportunity). \"A binary predictor ≈∂ satisfies equal opportunity with respect to A and Y if P( ≈∂=1|A=0,Y=1) = P( ≈∂=1|A=1,Y=1)\" [63]. This means that the probability of a person in a positive class being assigned to a positive outcome should be equal for both protected and unprotected (female and male) group members [149]. In other words, the equal opportunity definition states that the protected and unprotected groups should have equal true positive rates. Definition 3. (Demographic Parity). Also known as statistical parity. \"A predictor ≈∂ satisfies demographic parity if P( ≈∂ |A = 0) = P( ≈∂|A = 1)\" [48,87]. The likelihood of a positive outcome [149] should be the same regardless of whether the person is in the protected (e.g., female) group. (1) Individual Fairness. Give similar predictions to similar individuals [48,87]. (2) Group Fairness. Treat different groups equally [48,87]. (3) Subgroup Fairness. Subgroup fairness intends to obtain the best properties of the group and individual notions of fairness. It is different than these notions but uses them in order to obtain better outcomes. It picks a group fairness constraint like equalizing false positive and asks whether this constraint holds over a large collection of subgroups [79,80]. It is important to note that according to [83], it is impossible to satisfy some of the fairness constraints at once except in highly constrained special cases. In [83], the authors show the inherent incompatibility of two conditions: calibration and balancing the positive and negative classes. These cannot be satisfied simultaneously with each other unless under certain constraints; therefore, it is important to take the context and application in which fairness definitions need to be used into consideration and use them accordingly [141]. Another important aspect to consider is time and temporal analysis of the impacts that these definitions may have on individuals or groups. In [95] authors show that current fairness definitions are not always helpful and do not promote improvement for sensitive groups-and can actually be harmful when analyzed over time in some cases. They also show that measurement errors can also act in favor of these fairness definitions; therefore, they show how temporal modeling and measurement are important in evaluation of fairness criteria and introduce a new range of trade-offs and challenges toward this direction. It is also important to pay attention to the sources of bias and their types when trying to solve fairness-related questions.",
          "There have been numerous attempts to address bias in artificial intelligence in order to achieve fairness; these stem from domains of AI. In this section we will enumerate different domains of AI, and the work that has been produced by each community to combat bias and unfairness in their methods. Table 2 provides an overview of the different areas that we focus upon in this survey. While this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories: (1) Pre-processing. Pre-processing techniques try to transform the data so that the underlying discrimination is removed [43]. If the algorithm is allowed to modify the training data, then pre-processing can be used [11]. (2) In-processing. In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process [43]. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model-either by incorporating changes into the objective function or imposing a constraint [11,14]. (3) Post-processing. Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model [43]. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase [11,14]. Examples of some existing work and their categorization into these types is shown in Table 4. These methods are not just limited to general machine learning techniques, but because of AI's popularity, they have expanded to different domains such as natural language processing and deep learning. From learning fair representations [42,97,112] to learning fair word embeddings [20,58,169], debiasing methods have been proposed in different AI applications and domains. Most of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. In addition, some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes during the training phase but avoiding them during prediction time [94]. A list of protected or sensitive attributes is provided in Table 3. They point out what attributes should not affect the outcome of the decision in housing loan or credit card decision-making [30] according to the law. Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision. Different bias-mitigating methods and techniques are discussed below for different domains-each targeting a different problem in different areas of machine learning in detail. This can expand the horizon of the reader on where and how bias can affect the system and try to help researchers carefully look at various new problems concerning potential places where discrimination and bias can affect the outcome of a system.",
          "Every dataset is the result of several design decisions made by the data curator. Those decisions have consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In order to mitigate the effects of bias in data, some general methods have been proposed that advocate having good practices while using data, such as having datasheets that would act like a supporting document for the data reporting the dataset creation method, its characteristics, motivations, and its skews [13,55]. [12] proposes a similar approach for the NLP applications. A similar suggestion has been proposed for models in [110]. Authors in [66] also propose having labels, just like nutrition labels on food, in order to better categorize each data for each task. In addition to these general techniques, some work has targeted more specific types of biases. For example, [81] has proposed methods to test for cases of Simpson's paradox in the data, and [3,4] proposed methods to discover Simpson's paradoxes in data automatically. Causal models and graphs were also used in some work to detect direct discrimination in the data along with its prevention technique that modifies the data such that the predictions would be absent from direct discrimination [163]. [62] also worked on preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. Other pre-processing approaches, such as messaging [74], preferential sampling [75,76], disparate impact removal [51], also aim to remove biases from the data. ",
          "To address this issue, a variety of methods have been proposed that satisfy some of the fairness definitions or other new definitions depending on the application.",
          "Since classification is a canonical task in machine learning and is widely used in different areas that can be in direct contact with humans, it is important that these types of methods be fair and be absent from biases that can harm some populations. Therefore, certain methods have been proposed [57,78,85,106] that satisfy certain definitions of fairness in classification. For instance, in [147] authors try to satisfy subgroup fairness in classification, equality of opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2,159], and equalized odds in [154]. Other methods try to not only satisfy some fairness constraints but to also be stable toward change in the test set [69]. The authors in [155], propose a general framework for learning fair classifiers. This framework can be used for formulating fairness-aware classification with fairness guarantees. In another work [25], authors propose three different modifications to the existing Naive Bayes classifier for discrimination-free classification. [122] takes a new approach into fair classification by imposing fairness constraints into a Multitask learning (MTL) framework. In addition to imposing fairness during training, this approach can benefit the minority groups by focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy as a whole without attention to accuracy across different groups. In a similar work [49], authors propose a decoupled classification system where a separate classifier is learned for each group. They use transfer learning to reduce the issue of having less data for minority groups. In [73] authors propose to achieve fair classification by mitigating the dependence of the classification outcome on the sensitive attributes by utilizing the Wasserstein distance measure. In [75] authors propose the Preferential Sampling (PS) method to create a discrimination free train data set. They then learn a classifier on this discrimination free dataset to have a classifier with no discrimination. In [102], authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for classification and that can provide interpretability.  4. Algorithms categorized into their appropriate groups based on being pre-processing, inprocessing, or post-processing.",
          "Regression. [14] proposes a fair regression method along with evaluating it with a measure introduced as the \"price of fairness\" (POF) to measure accuracy-fairness trade-offs. They introduce three fairness penalties as follows: Individual Fairness: The definition for individual fairness as stated in [14], \"for every cross pair (ùë•, ùë¶) ‚àà ùëÜ 1 , (ùë• ‚Ä≤ , ùë¶ ‚Ä≤ ) ‚àà ùëÜ 2 , a model ùë§ is penalized for how differently it treats ùë• and ùë• ‚Ä≤ (weighted by a function of |ùë¶ -ùë¶ ‚Ä≤ |) where ùëÜ 1 and ùëÜ 2 are different groups from the sampled population.\" Formally, this is operationalized as Group Fairness: \"On average, the two groups' instances should have similar labels (weighted by the nearness of the labels of the instances)\" [14]. Hybrid Fairness: \"Hybrid fairness requires both positive and both negatively labeled cross pairs to be treated similarly in an average over the two groups\" [14]. In addition to the previous work, [1] considers the fair regression problem formulation with regards to two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision trees to satisfy disparate impact and treatment in regression tasks in addition to classification.",
          "In [167], authors studied the semantic role-labeling models and a famous dataset, imSitu, and realized that only 33% of agent roles in cooking images are man, and the rest of 67% cooking images have woman as agents in the imSitu training set. They also noticed that in addition to the existing bias in the dataset, the model would amplify the bias such that after training a model5 on the dataset, bias is magnified for \"man\", filling only 16% of cooking images. Under these observations, the authors of the paper [167] show that structured prediction models have the risk of leveraging social bias. Therefore, they propose a calibration algorithm called RBA (reducing bias amplification); RBA is a technique for debiasing models by calibrating prediction in structured prediction. The idea behind RBA is to ensure that the model predictions follow the same distribution in the training data. They study two cases: multi-label object and visual semantic role labeling classification. They show how these methods amplify the existing bias in data.",
          "In [137] authors show that vanilla PCA can exaggerate the error in reconstruction in one group of people over a different group of equal size, so they propose a fair method to create representations with similar richness for different populations-not to make them indistinguishable, or to hide dependence on a sensitive or protected attribute. They show that vanilla PCA on the labeled faces in the wild (LFW) dataset [68] has a lower reconstruction error rate for men than for women faces, even if the sampling is done with an equal weight for both genders. They intend to introduce a dimensionality reduction technique which maintains similar fidelity for different groups and populations in the dataset. Therefore, they introduce Fair PCA and define a fair dimensionality reduction algorithm. Their definition of Fair PCA (as an optimization function) is as follows, in which ùê¥ and ùêµ denote two subgroups, ùëà ùê¥ and ùëà ùêµ denote matrices whose rows correspond to rows of ùëà that contain members of subgroups ùê¥ and ùêµ given ùëö data points in ùëÖ ùëõ : And their proposed algorithm is a two-step process listed below: (1) Relax the Fair PCA objective to a semidefinite program (SDP) and solve it. (2) Solve a linear program that would reduce the rank of the solution.",
          "Embedding/Clustering. Inequalities in online communities and social networks can also potentially be another place where bias and discrimination can affect the populations. For example, in online communities users with a fewer number of friends or followers face a disadvantage of being heard in online social media [104]. In addition, existing methods, such as community detection methods, can amplify this bias by ignoring these low-connected users in the network or by wrongfully assigning them to the irrelevant and small communities. In [104] authors show how this type of bias exists and is perpetuated by the existing community detection methods. They propose a new attributed community detection method, called CLAN, to mitigate the harm toward disadvantaged groups in online social communities. CLAN is a two-step process that considers the network structure alongside node attributes to address exclusion bias, as indicated below: (1) Detect communities using modularity values (Step 1-unsupervised using only network structure). ( 2) Train a classifier to classify users in the minor groups, putting them into one of the major groups using held-out node attributes (Step 2-supervised using other node attributes). Fair methods in domains similar to community detection are also proposed, such as graph embedding [22] and clustering [8,31].",
          "Causal Approach to Fairness. Causal models can ascertain causal relationships between variables. Using causal graphs one can represent these causal relationships between variables (nodes of the graph) through the edges of the graph. These models can be used to remove unwanted causal dependence of outcomes on sensitive attributes such as gender or race in designing systems or policies [96]. Many researchers have used causal models and graphs to solve fairness-related concerns in machine learning. In [33,96], authors discuss in detail the subject of causality and its importance while designing fair algorithms. There has been much research on discrimination discovery and removal that uses causal models and graphs in order to make decisions that are irrespective of sensitive attributes of groups or individuals. For instance, in [164] authors propose a causal-based framework that detects direct and indirect discrimination in the data along with their removal techniques. [165] is an extension to the previous work. [160] gives a nice overview of most of the previous work done in this area by the authors, along with discussing system-, group-, and individual-level discrimination and solving each using their previous methods, in addition to targeting direct and indirect discrimination. By expanding on the previous work and generalizing it, authors in [116] propose a similar pathway approach for fair inference using causal graphs; this would restrict certain problematic and discriminative pathways in the causal graph flexibly given any set of constraints. This holds when the path-specific effects can be identified from the observed distribution. In [32] authors introduce the path-specific counterfactual fairness definition which is an extension to counterfactual fairness definition [87] and propose a method to achieve it further extending the work in [116]. In [115] authors extended a formalization of algorithmic fairness from their previous work to the setting of learning optimal policies that are subject to constraints based on definitions of fairness. They describe several strategies for learning optimal policies by modifying some of the existing strategies, such as Q-learning, value search, and G-estimation, based on some fairness considerations. In [162] authors only target discrimination discovery and no removal by finding instances similar to another instance and observing if a change in the protected attribute will change the outcome of the decision. If so, they declare the existence of discrimination. In [82], authors define the following two notions of discrimination-unresolved discrimination and proxy discrimination-as follows: Unresolved Discrimination: \"A variable V in a causal graph exhibits unresolved discrimination if there exists a directed path from A to V that is not blocked by a resolving variable, and V itself is non-resolving\" [82]. Proxy Discrimination: \"A variable V in a causal graph exhibits potential proxy discrimination, if there exists a directed path from A to V that is blocked by a proxy variable and V itself is not a proxy\" [82]. They proposed methods to prevent and avoid them. They also show that no observational criterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal reasoning framework needs to be incorporated. In [127], Instead of using the usual risk difference ùëÖùê∑ = ùëù 1 -ùëù 2 , authors propose a causal risk difference ùëÖùê∑ ùëê = ùëù 1 -ùëù ùëê 2 for causal discrimination discovery. They define ùëù ùëê 2 to be: s‚ààùëÜ ùë§ (s) ùëÖùê∑ ùëê not close to zero means that there is a bias in decision value due to group membership (causal discrimination) or to covariates that have not been accounted for in the analysis (omitted variable bias). This ùëÖùê∑ ùëê then becomes their causal discrimination measure for discrimination discovery. [161] is another work of this type that uses causal networks for discrimination discovery.",
          "",
          "Learning fair representations and avoiding the unfair interference of sensitive attributes has been introduced in many different research papers. A well-known example is the Variational Fair Autoencoder introduced in [97]. Here,they treat the sensitive variable as the nuisance variable, so that by removing the information about this variable they will get a fair representation. They use a maximum mean discrepancy regularizer to obtain invariance in the posterior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into the lower bound of their VAE architecture satisfies their proposed model for having the Variational Fair Autoencoder. Similar work, but not targeting fairness specifically, has been introduced in [72]. In [5] authors also propose a debiased VAE architecture called DB-VAE which learns sensitive latent variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE using these latent variables to debias systems like facial detection systems. In [112] authors model their representation-learning task as an optimization objective that would minimize the loss of the mutual information between the encoding and the sensitive variable. The relaxed version of this assumption is shown in Equation 1. They use this in order to learn fair representation and show that adversarial training is unnecessary and in some cases even counter-productive. In Equation 1, c is the sensitive variable and z the encoding of x. In [42], authors introduce flexibly fair representation learning by disentanglement that disentangles information from multiple sensitive attributes. Their flexible and fair variational autoencoder is not only flexible with respect to downstream task labels but also flexible with respect to sensitive attributes. They address the demographic parity notion of fairness, which can target multiple sensitive attributes or any subset combination of them.",
          "In [90] authors present a framework to mitigate bias in models learned from data with stereotypical associations. They propose a model in which they are trying to maximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary to predict the protected or sensitive variable (stereotyping variable z). The model consists of two parts-the predictor and the adversary-as shown in Figure 6. In their model, the predictor is trained to predict Y given X. With the help of a gradient-based approach like stochastic gradient descent, the model tries to learn the weights W by minimizing some loss function LP( ≈∑, y). The output layer is passed to an adversary, which is another network. This network tries to predict Z. The adversary may have different inputs depending on the fairness definition needing to be achieved. For instance, in order to satisfy Demographic Parity, the adversary would try to predict the protected variable Z using only the predicted label ≈∂ passed as an input to it, while preventing the adversary from learning this is the goal of the predictor. Similarly, to achieve Equality of Odds, the adversary would get the true label Y in addition to the predicted label ≈∂ . To satisfy Equality of Opportunity for a given class y, they would only select instances for the adversary where Y=y. [156] takes an interesting and different direction toward solving fairness issues using adversarial networks by introducing FairGAN which generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. The architecture of their FairGAN model is shown in Figure 5. FairGAN consists of two components: a generator ùê∫ ùê∑ùëíùëê which generates the fake data conditioned on the protected attribute ùëÉ ùê∫ (ùë•, ùë¶, ùë†) = ùëÉ ùê∫ (ùë•, ùë¶|ùë†)ùëÉ ùê∫ (ùë†) where ùëÉ ùê∫ (ùë†) = ùëÉ ùëëùëéùë°ùëé (ùë†), and two discriminators ùê∑ 1 and ùê∑ 2 . ùê∑ 1 is trained to differentiate the real data denoted by ùëÉ ùëëùëéùë°ùëé (ùë•, ùë¶, ùë†) from the generated fake data denoted by ùëÉ ùê∫ (ùë•, ùë¶, ùë†). Fig. 5. Structure of FairGAN as proposed in [156]. Fig. 6. The architecture of adversarial network proposed in [90] ¬© Brian Hu Zhang. In addition to that, for achieving fairness constraints, such as statistical parity, ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 1) = ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 0), the training of ùê∑ 2 is such that it emphasizes differentiation of the two types of synthetic (generated by the model) samples ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 1) and ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 0) indicating if the synthetic samples are from the unprotected or protected groups. Here s denotes the protected or the sensitive variable, and we adapted the same notation as in [156].",
          "5.4.1 Word Embedding. In [20] authors noticed that while using state-of-the-art word embeddings in word analogy tests, \"man\" would be mapped to \"computer programmer\" and \"woman\" would be mapped to \"homemaker.\" This bias toward woman triggered the authors to propose a method to debias word embeddings by proposing a method that respects the embeddings for gender-specific words but debiases embeddings for gender-neutral words by following these steps: (Notice that Step 2 has two different options. Depending on whether you target hard debiasing or soft debiasing, you would use either step 2a or 2b) (1) Identify gender subspace. Identifying a direction of the embedding that captures the bias [20]. (2) Hard debiasing or soft debiasing: (a) Hard debiasing (neutralize and equalize). Neutralize puts away the gender subspace from gender-neutral words and makes sure that all the gender-neutral words are removed and zeroed out in the gender subspace [20]. Equalize makes gender-neutral words to be equidistant from the equality set of gendered words [20]. (b) Soft bias correction. Tries to move as little as possible to retain its similarity to the original embedding as much as possible, while reducing the gender bias. This trade-off is controlled by a parameter [20]. Following on the footsteps of these authors, other future work attempted to tackle this problem [169] by generating a gender-neutral version of (Glove called GN-Glove) that tries to retain gender information in some of the word embedding's learned dimensions, while ensuring that other dimensions are free from this gender effect. This approach primarily relies on Glove as its base model with gender as the protected attribute. However, a recent paper [58] argues against these debiasing techniques and states that many recent works on debiasing word embeddings have been superficial, that those techniques just hide the bias and don't actually remove it. A recent work [23] took a new direction and proposed a preprocessing method for the discovery of the problematic documents in the training corpus that have biases in them, and tried to debias the system by perturbing or removing these documents efficiently from the training corpus. In a very recent work [166], authors target bias in ELMo's contextualized word vectors and attempt to analyze and mitigate the observed bias in the embeddings. They show that the corpus used for training of ELMo has a significant gender skew, with male entities being nearly three times more common than female entities. This automatically leads to gender bias in these pretrained contextualized embeddings. They propose the following two methods for mitigating the existing bias while using the pretrained embeddings in a downstream task, coreference resolution: (1) train-time data augmentation approach, and (2) test-time neutralization approach.",
          "Resolution. The [168] paper shows that coreference systems have a gender bias. They introduce a benchmark, called WinoBias, focusing on gender bias in coreference resolution. In addition to that, they introduce a data-augmentation technique that removes bias in the existing state-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques. Their general approach is as follows: They first generate auxiliary datasets using a rule-based approach in which they replace all the male entities with female entities and the other way around. Then they train models with a combination of the original and the auxiliary datasets. They use the above solution in combination with word2vec debiasing techniques to generate word embeddings. They also point out sources of gender bias in coreference systems and propose solutions to them. They show that the first source of bias comes from the training data and propose a solution that generates an auxiliary data set by swapping male and female entities. Another case arises from the resource bias (word embeddings are bias), so the proposed solution is to replace Glove with a debiased embedding method. Last, another source of bias can come from unbalanced gender lists, and balancing the counts in the lists is a solution they proposed. In another work [134], authors also show the existence of gender bias in three state-of-the-art coreference resolution systems by observing that for many occupations, these systems resolve pronouns in a biased fashion by preferring one gender over the other.",
          "In [21] authors introduce a metric for measuring gender bias in a generated text from a language model based on recurrent neural networks that is trained on a text corpus along with measuring the bias in the training text itself. They use Equation 2, where ùë§ is any word in the corpus, ùëì is a set of gendered words that belong to the female category, such as she, her, woman, etc., and ùëö to the male category, and measure the bias using the mean absolute and standard deviation of the proposed metric along with fitting a univariate linear regression model over it and then analyzing the effectiveness of each of those metrics while measuring the bias. In their language model, they also introduce a regularization loss term that would minimize the projection of embeddings trained by the encoder onto the embedding of the gender subspace following the soft debiasing technique introduced in [20]. Finally, they evaluate the effectiveness of their method on reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise on perplexity. They also point out the effectiveness of word-level bias metrics over the corpus-level metrics.",
          "In [100] authors extend the research in detecting bias in word embedding techniques to that of sentence embedding. They try to generalize bias-measuring techniques, such as using the Word Embedding Association Test (WEAT [26]) in the context of sentence encoders by introducing their new sentence encoding bias-measuring techniques, the Sentence Encoder Association Test (SEAT). They used state-of-the-art sentence encoding techniques, such as CBoW, GPT, ELMo, and BERT, and find that although there was varying evidence of human-like bias in sentence encoders using SEAT, more recent methods like BERT are more immune to biases. That being said, they are not claiming that these models are bias-free, but state that more sophisticated bias discovery techniques may be used in these cases, thereby encouraging more future work in this area.",
          "In [52] authors noticed that when translating the word \"friend\" in the following two sentences from English to Spanish, they achieved different results-although in both cases this word should be translated the same way. \"She works in a hospital, my friend is a nurse.\" \"She works in a hospital, my friend is a doctor.\" In both of these sentences, \"friend\" should be translated to the female version of Spanish friend \"amiga,\" but the results were not reflecting this expectation. For the second sentence, friend was translated to \"amigo,\"-the male version of friend in Spanish. This is because doctor is more stereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it in its performance. To solve this, authors in [52] build an approach that leverages the fact that machine translation uses word embeddings. They use the existing debiasing methods in word embedding and apply them in the machine translation pipeline. This not only helped them to mitigate the existing bias in their system, but also boosted the performance of their system by one BLUE score. In [126] authors show that Google's translate system can suffer from gender bias by making sentences taken from the U.S. Bureau of Labor Statistics into a dozen languages that are gender neutral, including Yoruba, Hungarian, and Chinese, translating them into English, and showing that Google Translate shows favoritism toward males for stereotypical fields such as STEM jobs. In [148] authors annotated and analyzed the Europarl dataset [84], a large political, multilingual dataset used in machine translation, and discovered that with the exception of the youngest age group (20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30), which represents only a very small percentage of the total amount of sentences (0.71%), more male data is available in all age groups. They also looked at the entire dataset and showed that 67.39% of the sentences are produced by male speakers. Furthermore, to mitigate the gender-related issues and to improve morphological agreement in machine translation, they augmented every sentence with a tag on the English source side, identifying the gender of the speaker. This helped the system in most of the cases, but not always, so further work has been suggested for integrating speaker information in other ways. 5.4.6 Named Entity Recognition. In [101], authors investigate a type of existing bias in various named entity recognition (NER) systems. In particular, they observed that in a context where an entity should be tagged as a person entity, such as \"John is a person\" or \"John is going to school\", more female names as opposed to male names are being tagged as non-person entities or not being tagged at all. To further formalize their observations, authors propose six different evaluation metrics that would measure amount of bias among different genders in NER systems. They curated templated sentences pertaining to human actions and applied these metrics on names from U.S census data incorporated into the templates. The six introduced measures each aim to demonstrate a certain type of bias and serve a specific purpose in showing various results as follows: ‚Ä¢ Error Type-1 Unweighted: Through this type of error, authors wanted to recognize the proportion of entities that are tagged as anything other than the person entity in each of the male vs female demographic groups. This could be the entity not being tagged or tagged as other entities, such as location. ùëõ ‚ààùëÅ ùëì ùêº (ùëõ ùë° ùë¶ùëùùëí ‚â† ùëÉùê∏ùëÖùëÜùëÇùëÅ ) |ùëÅ ùëì | ‚Ä¢ Error Type-1 Weighted: This type of error is similar to its unweighted case except authors considered the frequency or popularity of names so that they could penalize if a more popular name is being tagged wrongfully. , where ùëì ùëüùëíùëû ùëì (‚Ä¢) indicates the frequency of a name for a particular year in the female census data. Likewise, ùëì ùëüùëíùëû ùëö (‚Ä¢) indicates the frequency of a name for a particular year in the male census data. ‚Ä¢ Error Type-2 Unweighted: This is a type of error in which the entity is tagged as other entities, such as location or city. Notice that this error does not count if the entity is not tagged. where ‚àÖ indicates that the name is not tagged. ‚Ä¢ Error Type-2 Weighted: This error is again similar to its unweighted case except the frequency is taken into consideration. ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ ùë° ùë¶ùëùùëí ‚àâ {‚àÖ, ùëÉùê∏ùëÖùëÜùëÇùëÅ }) ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ) ‚Ä¢ Error Type-3 Unweighted: This is a type of error in which it reports if the entity is not tagged at all. Notice that even if the entity is tagged as a non-person entity this error type would not consider it. ‚Ä¢ Error Type-3 Weighted: Again, this error is similar to its unweighted case with frequency taken into consideration. ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ ùë° ùë¶ùëùùëí = ‚àÖ) ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ) Authors also investigate the data that these NER systems are trained on and find that the data is also biased toward female gender by not including as versatile names as there should be to represent female names.",
          "The field of algorithmic fairness is a relatively new area of research and work still needs to be done for its improvement. With that being said, there are already papers that propose fair AI algorithms and bias mitigation techniques and compare different mitigation algorithms using different benchmark datasets in the fairness domain. For instance, authors in [65] propose a geometric solution to learn fair representations that removes correlation between protected and unprotected features. The proposed approach can control the trade-off between fairness and accuracy via an adjustable parameter. In this work, authors evaluate the performance of their approach on different benchmark datasets, such as COMPAS, Adult and German, and compare them against various different approaches for fair learning algorithms considering fairness and accuracy measures [65,72,158,159]. In addition, IBM's AI Fairness 360 (AIF360) toolkit [11] has implemented many of the current fair learning algorithms and has demonstrated some of the results as demos which can be utilized by interested users to compare different methods with regards to different fairness measures.",
          "While there have been many definitions of, and approaches to, fairness in the literature, the study in this area is anything but complete. Fairness and algorithmic bias still holds a number of research opportunities. In this section, we provide pointers to outstanding challenges in fairness research, and an overview of opportunities for development of understudied problems.",
          "There are several remaining challenges to be addressed in the fairness literature. Among them are: (1) Synthesizing a definition of fairness. Several definitions of what would constitute fairness from a machine learning perspective have been proposed in the literature. These definitions cover a wide range of use cases, and as a result are somewhat disparate in their view of fairness. Because of this, it is nearly impossible to understand how one fairness solution would fare under a different definition of fairness. Synthesizing these definitions into one remains an open research problem since it can make evaluation of these systems more unified and comparable. having a more unified fairness definition and framework can also help with the incompatibility issue of some current fairness definitions. (2) From Equality to Equity. The definitions presented in the literature mostly focus on equality, ensuring that each individual or group is given the same amount of resources, attention or outcome. However, little attention has been paid to equity, which is the concept that each individual or group is given the resources they need to succeed [60,103]. Operationalizing this definition and studying how it augments or contradicts existing definitions of fairness remains an exciting future direction. (3) Searching for Unfairness. Given a definition of fairness, it should be possible to identify instances of this unfairness in a particular dataset. Inroads toward this problem have been made in the areas of data bias by detecting instances of Simpson's Paradox in arbitrary datasets [3]; however, unfairness may require more consideration due to the variety of definitions and the nuances in detecting each one. ",
          "In this work we have taxonomized and summarized the current state of research into algorithmic biases and fairness-with a particular focus on machine learning. Even in this area alone, the research is broad. Subareas, from natural language processing, to representation learning, to community detection, have all seen efforts to make their methodologies more fair. Nevertheless, every area has not received the same amount of attention from the research community. Figure 7 provides an overview of what has been done in different areas to address fairness-categorized by the fairness definition type and domain. Some areas (e.g., community detection at the subgroup level) have received no attention in the literature, and could be fertile future research areas.",
          "In this survey we introduced problems that can adversely affect AI systems in terms of bias and unfairness. The issues were viewed primarily from two dimensions: data and algorithms. We illustrated problems that demonstrate why fairness is an important issue. We further showed examples of the potential real-world harm that unfairness can have on society-such as applications in judicial systems, face recognition, and promoting algorithms. We then went over the definitions of fairness and bias that have been proposed by researchers. To further stimulate the interest of readers, we provided some of the work done in different areas in terms of addressing the biases that may affect AI systems and different methods and domains in AI, such as general machine learning, deep learning and natural language processing. We then further subdivided the fields into a more fine-grained analysis of each subdomain and the work being done to address fairness constraints in each. The hope is to expand the horizons of the readers to think deeply while working on a system or a method to ensure that it has a low likelihood of causing potential harm or bias toward a particular group. With the expansion of AI use in our world, it is important that researchers take this issue seriously and expand their knowledge in this field. In this survey we categorized and created a taxonomy of what has been done so far to address different issues in different domains regarding the fairness issue. Other possible future work and directions can be taken to address the existing problems and biases in AI that we discussed in the previous sections.",
          "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR0011890019. We would like to thank the organizers, speakers and the attendees at the IVADO-Mila 2019 Summer School on Bias and Discrimination in AI. We would like to also thank Brian Hu Zhang and Shreya Shankar.",
          "",
          "Aside from the existence of bias in datasets, there are datasets that are specifically used to address bias and fairness issues in machine learning. There are also some datasets that are introduced to target the issues and biases previously observed in older existing datasets. Below we list some of the widely known datasets that have the characteristics discussed in this survey. 9.1.1 UCI Adult Dataset. UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people's annual incomes, or various other studies [7]. 9.1.2 German Credit Dataset. The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [47].  [28] 4,753 crime records Social Diversity in faces dataset [107] 1 million images Facial images Table 5. Most widely used datasets in the fairness domain with additional information about each of the datasets including their size and area of concentration."
        ],
        "ground_truth_definitions": {
          "data to algorithm bias": {
            "definition": "biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes",
            "context": "3.1.1 Data to Algorithm. In this section we talk about biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes. (1) Measurement Bias.",
            "type": "explicit"
          },
          "measurement bias/reporting bias": {
            "definition": "how we choose, utilize, and measure particular features",
            "context": "Measurement Bias. Measurement, or reporting, bias arises from how we choose, utilize, and measure particular features [144]. An example of this type of bias was observed in the recidivism risk prediction tool COMPAS, where prior arrests and friend/family arrests were used as proxy variables to measure level of \"riskiness\" or \"crime\"--which on its own can be viewed as mismeasured proxies.",
            "type": "explicit"
          },
          "omitted variable bias": {
            "definition": "one or more important variables are left out of the model",
            "context": "Omitted Variable Bias. Omitted variable bias4 occurs when one or more important variables are left out of the model [38, 114, 131]. An example for this case would be when someone designs a model to predict, with relatively high accuracy, the annual percentage rate at which customers will stop subscribing to a service, but soon observes that the majority of users are canceling their subscription without receiving any warning from the designed model.",
            "type": "explicit"
          },
          "representation bias": {
            "definition": "how we sample from a population during data collection process",
            "context": "Representation Bias. Representation bias arises from how we sample from a population during data collection process [144]. Non-representative samples lack the diversity of the population, with missing subgroups and other anomalies.",
            "type": "explicit"
          },
          "aggregation bias": {
            "definition": "false conclusions are drawn about individuals from observing the entire population.",
            "context": "Aggregation Bias. Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools.",
            "type": "explicit"
          },
          "simpson's paradox": {
            "definition": "a type of aggregation bias that arises in the analysis of heterogeneous data. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups.",
            "context": "Simpson's Paradox. Simpson's paradox is a type of aggregation bias that arises in the analysis of heterogeneous data [18]. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups (Fig. 2(a)). One of the better-known examples of the type of paradox arose during the gender bias lawsuit in university admissions against UC Berkeley [16].",
            "type": "explicit"
          },
          "modifiable areal unit problem": {
            "definition": "a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation",
            "context": "Modifiable Areal Unit Problem is a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation [56]. This bias results in different trends learned when data is aggregated at different spatial scales.",
            "type": "explicit"
          },
          "sampling bias": {
            "definition": "nonrandom sampling of subgroups.",
            "context": "Sampling Bias. Sampling bias is similar to representation bias, and it arises due to nonrandom sampling of subgroups. As a consequence of sampling bias, the trends estimated for one population may not generalize to data collected from a new population",
            "type": "explicit"
          },
          "linking bias": {
            "definition": "network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users",
            "context": "Linking Bias. Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users [120]. In [104] authors show how social networks can be biased toward low-degree nodes when only considering the links in the network and not considering the content and behavior of users in the network.",
            "type": "explicit"
          },
          "algorithm to user bias": {
            "definition": "a result of algorithmic outcomes and affect user behavior as a consequence.",
            "context": "Any biases in algorithms might introduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic outcomes and affect user behavior as a consequence. (1) Algorithmic Bias.",
            "type": "explicit"
          },
          "algorithmic bias": {
            "definition": "when the bias is not present in the input data and is added purely by the algorithm.",
            "context": "Algorithmic Bias. Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm [9]. The algorithmic design choices, such as use of certain optimization functions, regularizations, choices in applying regression models on the data as a whole or considering subgroups, and the general use of statistically biased estimators in algorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of the algorithms.",
            "type": "explicit"
          },
          "user interaction bias": {
            "definition": "a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction.",
            "context": "User Interaction Bias. User Interaction bias is a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction [9]. This type of bias can be influenced by other types and subtypes, such as presentation and ranking biases.",
            "type": "explicit"
          },
          "ranking bias": {
            "definition": "The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others.",
            "context": "Ranking Bias. The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others. This bias affects search engines [9] and crowdsourcing applications [93].",
            "type": "explicit"
          },
          "popularity bias": {
            "definition": "Items that are more popular tend to be exposed more.",
            "context": "Popularity Bias. Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation-for example, by fake reviews or social bots [117]. As an instance, this type of bias can be seen in search engines [71, 117] or recommendation systems where popular objects would be presented more to the public. But this presentation may not be a result of good quality; instead, it may be due to other biased factors.",
            "type": "explicit"
          },
          "emergent bias": {
            "definition": "a result of change in population, cultural values, or societal knowledge usually some time after the completion of design",
            "context": "Emergent Bias. Emergent bias occurs as a result of use and interaction with real users. This bias arises as a result of change in population, cultural values, or societal knowledge usually some time after the completion of design [53]. This type of bias is more likely to be observed in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of prospective users by design [53].",
            "type": "explicit"
          },
          "user to data bias": {
            "definition": "Any inherent biases in users might be reflected in the data they generate.",
            "context": "Many data sources used for training ML models are user-generated. Any inherent biases in users might be reflected in the data they generate. Furthermore, when user behavior is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in the data generation process.",
            "type": "explicit"
          },
          "historical bias": {
            "definition": "the already existing bias and socio-technical issues in the world.",
            "context": "Historical Bias. Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection [144]. An example of this type of bias can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5% of Fortune 500 CEOs were woman-which would cause the search results to be biased towards male CEOs [144]",
            "type": "explicit"
          },
          "population bias": {
            "definition": "when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population",
            "context": "Population Bias. Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population [120]. Population bias creates non-representative data.",
            "type": "explicit"
          },
          "self-selection bias": {
            "definition": "a subtype of the selection or sampling bias in which subjects of the research select themselves.",
            "context": "Self-Selection Bias. Self-selection bias4 is a subtype of the selection or sampling bias in which subjects of the research select themselves. An example of this type of bias can be observed in an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic supporters are more likely to complete the poll.",
            "type": "explicit"
          },
          "behavioral bias": {
            "definition": "different user behavior across platforms, contexts, or different datasets.",
            "context": "Behavioral Bias. Behavioral bias arises from different user behavior across platforms, contexts, or different datasets [120]. An example of this type of bias can be observed in [108], where authors show how differences in emoji representations among platforms can result in different reactions and behavior from people and sometimes even leading to communication errors.",
            "type": "explicit"
          },
          "temporal bias": {
            "definition": "differences in populations and behaviors over time.",
            "context": "Temporal Bias. Temporal bias arises from differences in populations and behaviors over time [120]. An example can be observed in Twitter where people talking about a particular topic start using a hashtag at some point to capture attention, then continue the discussion about the event without using the hashtag [120, 146].",
            "type": "explicit"
          },
          "content production bias": {
            "definition": "structural, lexical, semantic, and syntactic differences in the contents generated by users.",
            "context": "Content Production Bias. Content Production bias arises from structural, lexical, semantic, and syntactic differences in the contents generated by users [120]. An example of this type of bias can be seen in [118] where the differences in use of language across different gender and age groups is discussed.",
            "type": "explicit"
          },
          "discrimination": {
            "definition": "human prejudice and stereotyping based on the sensitive attributes",
            "context": "Similar to bias, discrimination is also a source of unfairness. Discrimination can be considered as a source for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes, which may happen intentionally or unintentionally, while bias can be considered as a source for unfairness that is due to the data collection, sampling, and measurement. Although bias can also be seen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic fairness literature it is more intuitive to categorize them as such according to the existing research in these areas.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0f7eda998bbce003745ff2fdbcaa1d9a8119368b",
        "sections": [
          "as corrupt and untrustworthy. Many of us have started to wonder: are we trapped in echo chambers of our own making? 1The recent conversation, however, has blurred two distinct, but interrelated, social epistemic phenomena, which I will call epistemic bubbles and echo chambers. Both are problematic social structures that lead their members astray. Both reinforce ideological separation. But they are different in their origins, mechanisms for operation, and avenues for treatment. Both are structures of exclusion -but epistemic bubbles exclude through omission, while echo chambers exclude by manipulating trust and credence. However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation. We seek to stay in touch with our friends, who also tend to have similar political views. But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation. According to Jamieson and Capella, an echo chamber's members share beliefs which include reasons to distrust those outside the echo chamber. Echo chambers work by systematically isolating their members from all outside epistemic sources (Jamieson and Cappella 2008, 163-236). This mechanism bears a striking resemblance to some accounts of cult indoctrination. By discrediting outsiders, echo chambers leave their members overly dependent on approved inside sources for information. In epistemic bubbles, other voices are merely not heard; in echo chambers, other voices are actively undermined. (This is a conceptual distinction; a community can practice both forms of exclusion to varying degrees.) The contemporary discussion has been mostly focused on the phenomenon of epistemic bubbles. Cass Sunstein's famous discussions of group polarization, extremism, and the Internet largely focus on matters of constricted information flow and omitted viewpoints (Sunstein 2001(Sunstein , 2009b(Sunstein , 2009a)). Eli Pariser's The Filter Bubble focuses entirely on filtration effects from personalization technology, as in Google searches, and self-selected informational networks, as in Facebook (Pariser 2011). Popular conversation has tended to follow Pariser's focus on technologically-mediated filtration. The term \"echo chamber\" has, in recent usage, been reduced to a synonym for such bubbles and their constricted information flow. When the specifically trust-oriented manipulations of echo chambers are discussed, they are usually lumped in with epistemic bubbles as part of one unified phenomenon. This is a mistake; it is vital to distinguish between these two phenomena. An epistemic bubble is an epistemic structure emerging from the informational architecture of communities, social networks, media, and other sources of information and argument. It is an impaired informational topology -a structure with poor connectivity. An echo chamber, on the other hand, is an epistemic structure created through the manipulation of trust; it can exist within a healthy informational topology by adding a superstructure of discredit and authority. I hope to show, contra the recent focus on epistemic bubbles, that echo chambers pose a significant and distinctive threat -perhaps even a more dangerous one -that requires a very different mode of repair. Furthermore, echo chambers can explain what epistemic bubbles cannot: the apparent resistance to clear evidence found in some groups, for example, climate change deniers and anti-vaccination groups. It may be tempting to treat members of echo chambers as mere sheep, and accuse them of problematic acquiescence to epistemic authority. But that accusation relies on an unreasonable expectation for radical epistemic autonomy. Contemporary epistemology, especially social epistemology, has taught us that trust in others is necessary and ineradicable. We are, as John Hardwig says, irredeemably epistemically dependent on each other (Hardwig 1985(Hardwig , 1991)). Echo chambers prey on our epistemic interdependence. Thus, in some circumstances, echo chamber members do not have full epistemic responsibility for their beliefs. Once one is trapped in an echo chamber, one might follow good epistemic practices and still be led further astray. And some people can be trapped in echo chambers because of circumstances beyond their control -for example, they can be raised in them. Which leads to the most important questions: how can one tell if one is in an echo chamber? And how can one escape? I will argue that, for those trapped within an echo chamber, prospects for detection are poor and the escape path daunting. Detecting and escaping from echo chambers will require a radical restructuring of a member's relationship to their epistemic past, which may be more than we can reasonably expect of one another.",
          "Let's start with a simple picture of how many of us conduct our epistemic lives right now. I get much of my news via Facebook. I have selected most of my Facebook friends for social reasons; they are my friends and colleagues. A significant conduit for my learning about events in the world is by people re-posting articles that they have found newsworthy or interesting. When I go outside of Facebook, I usually turn to sources which, by and large, are affiliated with my own political beliefs and intellectual culture. This process imposes a filter on my uptake of information. Selection and exclusion are not bad in and of themselves -the world is overstuffed with supposed sources of information, many of them terrible. The better my filter, the more it will focus my attention on relevant, useful, and reliable information. But the bare fact that each individual member of the system is themselves reliable will not guarantee any broadness or completeness of coverage. Suppose, for example, that my social network was composed entirely of intelligent, reliable professors of aesthetics whose interests were largely focused on new developments in opera, ballet, and avant-garde contemporary art. Through this system, I might learn about all the exciting new developments in the New York Art scene, but entirely miss, say, relevant developments in rap, or the fact that my country was slowly sliding into fascism. The system lacks what Goldberg calls coverage-reliability -the completeness of relevant testimony from across one's whole epistemic community (Goldberg 2011, 93-4). Bad coverage can not only leave out relevant facts and evidence; it can also fail to bring relevant arguments to our attention. Thus, bad coverage can starve us of adequate exposure to relevant arguments. Notice that bad coverage is an epistemic flaw of epistemic systems and networks, not of individuals. I can now specify my use of \"epistemic bubble\" with greater precision. An epistemic bubble is a social epistemic structure which has inadequate coverage through a process of exclusion by omission. Epistemic bubbles form by leaving out relevant epistemic sources, rather than actively discrediting them. There are at two primary forces encouraging this omission. First, there is an epistemic agent's own tendency to seek like-minded sources. This phenomenon is sometimes called \"selective exposure\" by social scientists (Nelson and Webster 2017). In many contemporary cases, such as Facebook, the process of omission can occur inadvertently. I typically put people in my Facebook feed for social reasons -because I like them or I find them funny. But social selection does not guarantee good coverage reliability; in fact, the typical bases of social selection are inimical to good coverage reliability. 2 We usually like people who are similar to us, and such similarity makes coverage gaps more likely. Friends make for good parties, but poor information networks. We now have a straightforward account of one way in which epistemic bubbles can form. We can build a structure for one set of purposes -maintaining social relations, finding -and then proceed to use it for an entirely different purpose, for which it functions badly: information gathering. Second, there are the processes by which an epistemic agent's informational landscape is modified by other agents. This might include, say, systematic censorship or media control by the state or other actors. The most worrisome of these external forces, at the moment, seems to be the algorithmic personal filtering of online experiences (Pariser 2011;Watson 2015). Internet search engines, for example, will track personal information for each particular user, and adapt their search results to suit each user's interest. Certainly, newspapers and other traditional media technologies do place external filters on their readers, but the modern instantiation is particularly powerful and troubling. As Boaz Miller and Isaac Record argue, Internet technologies create hyper-individualized, secret filters. The secrecy is particularly threatening. Many users do not know about the existence of algorithmic personal filtering. Even amongst those that do, most do not have access to the particularities of the algorithms 2 For an overview of empirical research on personal similarity and polarization, see (Sunstein 2009a, 83-5). Curiously, Sunstein notes the group polarization literature has thought relatively little about the impact of personal similarity. doing the filtering; thus, the very opacity of the process makes it harder for a user to successfully evaluate and epistemically compensate for such filtering (Miller and Record 2013). Thus, most users significantly underestimate the degree to which their exposure to information, via search results, has already been tailored to present search results to which the user will already be amenable. Both the agent-driven process of selective exposure, and the externalities of algorithmic filtering, contribute to the creation of epistemic bubbles. I introduce the term \"epistemic bubble\" here to indicate a broader set of phenomena. Pariser introduced the term \"filter bubble\" to refer specifically to technologically mediated filtering, especially via algorithmic matching. Epistemic bubbles are those structures which omit relevant voices by any means, technological or otherwise. Epistemic bubbles include filter bubbles, but also nontechnological selection processes, such as physically sorting oneself into neighborhoods of like-minded people (Bishop 2009). The account I've given of epistemic bubbles focuses on the way they omit relevant information, but that omission can also threaten us with bootstrapped corroboration. Users of social networks and personalized search technologies will encounter agreement more frequently and so be tempted to over-inflate their epistemic self-confidence. This danger threatens because, in general, corroboration is often a very good reason to increase one's confidence in the relevant beliefs (Nguyen 2010(Nguyen , 2018a)). But corroboration ought to only have weight if it adds something epistemically, rather than being a mere copy. To borrow an example from Wittgenstein: imagine looking through a stack of identical newspapers and treating each next newspaper headline saying p as a reason to increase your belief that p (Wittgenstein 2010, 100). This is clearly a mistake; the fact that a newspaper claims that p has some epistemic weight, but the number of copies of that newspaper one encounters ought not add any extra weight. Similarly, imagine speaking to a bunch of acolytes of Guru Jane, who repeat anything that Guru Jane says without any further thought. The fact that all these acolytes repeat Guru Jane's testimony should add no extra weight. So long as the disciplines repeat anything Guru Jane says -so long as they are mere conduits for information, rather than sources of information -they are simply another sort of copy. But copying isn't the only route to a problematic form of non-independence. Suppose I believe that the Paleo diet is the best diet. I proceed to assemble a body of peers who I trust precisely because they also believe that Paleo is the best diet. In that case, the existence of perfect agreement on Paleo's amazingness throughout that group ought to count for far less than it might for other groups that I had not assembled on that basis. Even if all the group members arrived at their beliefs independently, their agreement is already guaranteed by my selection principle. To the degree that I have pre-selected the members in my epistemic network from agreement with some set of beliefs of mine, then their agreement with that set of beliefs and any other beliefs that it entails ought to be epistemically discounted. 3 If we fail to so discount, we are ignoring a pernicious hidden circularity in our corroborative process. It is easy to forget to discount because the bootstrap here is obscured by time and interface. But we must actively adjust for the increased likelihood of agreement inside our bubbles, or we will unwarrantedly bootstrap our confidence levels. 4To summarize: an epistemic bubble is an epistemic network that has inadequate coverage through a process of exclusion by omission. That omission need not be malicious or even intentional, but members of that community will not receive all the relevant evidence, nor be exposed to a balanced set of arguments.",
          "Luckily for us, epistemic bubbles are relatively fragile. Relevant sources have simply been left out; they have not been discredited. It is possible to pop an epistemic bubble by exposing a member to relevant information or arguments that they have missed. Echo chambers, on the other hand, are significantly more robust. My analysis here combines empirical work and analysis from Jamieson and Cappella on the nature of right-wing echo-chambers with recent insights from social epistemology. Jamieson and Cappella studied echo chambers built around particular charismatic personalities -Rush Limbaugh and the news team of Fox News, and certain other members of conservative talk radio. Their data and analysis suggest that Limbaugh uses methods to actively isolate his community of followers from other epistemic sources. Limbaugh's consistent attacks on the \"mainstream media\" serve to discredit all potential sources of knowledge or testimony besides Limbaugh and a select inner-cadre of other approved sources (Jamieson and Cappella 2008, 140-76). Limbaugh also develops what they call a private language, full of alternate meanings for familiar terms and new jargon (for example, \"SJWs\"), in order to exaggerate the insularity and separateness of the in-group. Finally, Limbaugh provides counter-explanations of all contrary views, intended not only to attack each particular view, but also to undermine the general trustworthiness and integrity of anybody expressing a contrary view. The resulting world-view is one of highly opposed forces; once one has subscribed to Limbaugh's view, one has reason to think that anybody who does not also subscribe is actively opposed to the side of right, and thereby morally unsound and so generally untrustworthy (177-90). Jamieson and Cappella suggest that this makes a follower dependent on a single source or group of sources, and makes them highly resistant to any outside sources. They offer the following definition of an echo chamber: an echo chamber is a bounded and enclosed group that magnifies the internal voices and insulates them from rebuttal (76). I will use the term \"echo chamber\" here following their analysis, but I adapt the definition slightly for philosophical use. I use \"echo chamber\" to mean an epistemic community which creates a significant disparity in trust between members and non-members. This disparity is created by excluding non-members through epistemic discrediting, while simultaneously amplifying insider members' epistemic credential. Finally, echo chambers are such that in which general agreement with some core set of beliefs is a pre-requisite for membership, where those core beliefs include beliefs that support that disparity in trust. By \"epistemic discrediting\", I mean that non-members are not simply omitted or not heard, but are actively assigned some epistemic demerit, such as unreliability, epistemic maliciousness, or dishonesty. By \"amplifying epistemic credentials\", I mean that members are assigned very high levels of trust. Of course, these two processes can feedback into one another. So long as an echo chamber's trusted insiders continue to claim that outsiders are untrustworthy, then the inner trust will reinforce the outward distrust. And so long as outsiders are largely distrusted, then the insiders will be insulated from various forms of counter-evidence and rebuff, thus increasing their relative credence. Once a sufficient disparity in credence between insiders and outsiders has been established, so long as trusted insiders continue to hold and espouse epistemically dismissive beliefs towards outsiders, then the echo chambers' beliefs system will be extremely difficult to dislodge. Compare this process of credence manipulation to the process of omission found in epistemic bubbles. In one standard scenario, I add others as trusted members of my epistemic network based on agreement. I am then less likely to encounter an outside voicebut when I do actually have such an encounter with an outsider, I have no background reason to dismiss them. Bubbles restrict access to outsiders, but don't necessarily change their credibility. Echo chambers, on the other hand, work by offering a pre-emptive discredit towards any outside sources. 5The result is a rather striking parallel to the techniques of isolation typically practiced in cult indoctrination. The standard techniques of cult indoctrination, by a traditional account, are the aggressive emotional isolation of cult members from all non-cult members, which amplifies indoctrinated member's dependency on the cult (Singer 1979;Langone 1994;Lifton 1991). 6 New cult members are brought to distrust all non-cult members, which provides an epistemic buffer against any attempts to extract the indoctrinated person from the cult. This is nothing like how epistemic bubbles work. Epistemic bubbles merely leave their members ignorant, but ignorance can be fixed with simple exposure. The function of an echo chamber, on the other hand, is to credentially isolate its members by a manipulation of trust. By this, I mean that members are not just cut off, but are actively alienated from any of the usual sources of contrary argument, consideration, or evidence. Members have been prepared to discredit and distrust any outside sources; thus, mere exposure to relevant outside information will have no effect. In fact, echo chambers can avail themselves of another epistemic protective mechanism: they can contain what I'll call a disagreement-reinforcement mechanism. Members can be brought to hold a set of beliefs such that the existence and expression of contrary beliefs reinforces the original set of beliefs and the discrediting story. A toy example: suppose I am a cult leader, and I have taught my followers to believe that every human except the members of our group has been infested and mind-controlled by alien ghosts from Mars. I also teach my followers that these alien ghosts from Mars hate our group for knowing the truth, and so will constantly seek to undermine our knowledge of their existence through mechanisms like calling us a 'cult' and calling us lunatics. Endre Begby has offered a careful analysis of this particular sort of disagreement-reinforcement mechanism, which he calls \"evidential preemption.\" Suppose that I tell my followers to expect outsiders to falsely claim that there are no ghosts from Mars. When my followers do confront such contrary claims from outsiders, those contrary claims are exactly what they expected to hear. Thus, new contrary testimony is neutralized, because it was predicted by past beliefs. This, says Begby, functions as a kind of epistemic inoculation. There is also a secondary effect. When my followers hear exactly what I predicted, then my claims have been verified, and so my followers will have some reason to increase their trust in me. Thus, the echo chamber's belief system not only neutralizes the epistemic impact of exposure to outsiders with contrary beliefs; the existence of those contrary beliefs will actively corroborate the pre-emptor and so increase the credence level of the entire echo chamber (Begby 2017). This creates a feedback mechanism within the echo chamber -in making undermining predictions about contrary testimony, inside authorities not only discredit that contrary testimony, but increase their trustworthiness for future predictions. Once such a system of beliefs is set up, it can be very difficult to dislodge -it is selfreinforcing, bounded, and built to discount any contrary input. In fact, though my definition of echo chambers is conceptually separable from such a disagreement-reinforcement mechanism, every plausible real-world candidate for an echo chamber I've ever encountered includes some version of a disagreement-reinforcement mechanism. For a depressing realworld example, consider Pizzagate. Pizzagate is a conspiracy theory that boiled out of a rightwing online forum on Reddit, which included beliefs that Comet Ping Pong, a pizza restaurant, was the site of a child sex trafficking ring owned by a liberal conspiracy involving Hillary Clinton and Barack Obama. Eventually, Edgar Welch, a member of that forum, forcibly entered the pizza parlor armed with an assault rifle to investigate; when he satisfied himself that the restaurant contained no child slaves, he gave himself up to the police. The online forum, however, did not take this as contrary evidence. Instead, they leaned on their belief that the liberal conspiracy had total control of the mainstream media, and was willing to stage fake events to discredit the right-wing. The forum took Welch's claims that there was no sex trafficking ring as evidence that Welch was a paid actor, and thus as further confirmation of the existence of a powerful cabal of liberal child sex traffickers (Mengus 2016;Vogt and Goldman 2016). Conspiracy theories function here in a fascinating inversion to corroborative bootstrapping. In corroborative bootstrapping, the mistake is to treat problematically dependently selected insiders as if they were independent, and thus overweight their testimony. When an echo chamber uses a conspiracy theory in this manner, they are attributing a problematic form of non-independence to outsiders who are actually independent, and thereby underweighting outside testimony. An echo chamber here works by discrediting the apparent independence of, say, different climate change scientists by claiming that all their various testimonies are problematically derived from a single source. Incidentally, I am not claiming here that conspiracy theories are always or necessarily incorrect or the product of epistemic vice. As others have argued, believing in conspiracy theories isn't bad per se, because some conspiracy theories are true (Coady 2012, 110-137;Dentith 2017). But the fact that conspiracy theories can function to reinforce the boundaries of echo chambers -though they do not necessarily do so -might explain part of conspiracy theories' bad rap. Because of their effectiveness in setting up disagreement-reinforcement mechanisms, conspiracy theories are often conscripted as a powerful tool in the bad epistemic behavior of certain groups. It is important to note that the epistemic mechanisms by which echo chambers work, though problematic, are not sui generis. They are perversions of natural, useful, and necessary attitudes of individual and institutional trust. The problem isn't that we trust and distrust groups and institutions. In fact, we must do so. Eljiah Millgram calls it the problem of hyperspecialization. Human knowledge has splintered into a vast set of specialized fields that depend on each other. No one human can manage that information; we are forced to trust each other (Millgram 2015, 27-44). 7 None of us is in a position to reliably identify an expert in 7 Though this paper relies on insights from modern work in the epistemology of testimony, I have tried to rely only on uncontroversial claims from that field, and not on the technical details of any particular view. In particular, I have attempted to construct my analysis so as to be independent of the debate on whether or not testimony is a basic source of knowledge. I have also attempted to make the paper compatible with the major accounts of trust. most specialist fields outside of our own. I am, on my own, helpless to evaluate the virtues of antibiotics or the expertise of a particular doctor or surgeon. I am, instead, reliant on a vast network of institutional licensing practices in order to choose my health care sourcesincluding journal peer review, medical board exams, university hiring practices, and the like (Nguyen 2017a). Often, I trust via what Philip Kitcher calls indirect calibration -I trust mechanical engineers because they make things that work, but I know that mechanical engineers trust applied physicists, and I know that applied physicists trust theoretical physicists, so I acquire trust through a long chain of field-wide links (Kitcher 1993, 320-3). I even use litmus tests: the fact that any person or group is in favor of, say, sexual orientation conversion therapy is enough for me to discredit them on any social or moral topics. We must resort to such tactics in order to navigate the hyper-specialized world (Nguyen forthcoming). Echo chambers function parasitically on these practices by applying discredits without regard for the actual epistemic worth of the discredited institutions or individuals. The discredit is instead applied strategically and defensively, towards all outsiders solely on the basis of their being outsiders. Once the discrediting beliefs are in place, the ensuing beliefs and action the echo chambers' members are surprisingly close to rational. In fact, we can easily imagine alternative scenarios in which a very similar set of beliefs were appropriate and veristic. If I was an anti-Nazi in Germany during the rise of the Nazi party, I would do well to maintain the beliefs that the most people were corrupt, untrustworthy, and out to maliciously undermine my own true beliefs. But if such beliefs become implanted in an inappropriate context, they can lead their believers entirely astray. Epistemic bubbles can easily form accidentally. But the most plausible explanation for the particular features of echo chambers is something more malicious. Echo chambers are excellent tools to maintain, reinforce and expand power through epistemic control. Thus, it is likely (though not necessary) that echo chambers are set up intentionally, or at least maintained for this functionality. My account thus bears some resemblance to some work on testimonial injustice and the epistemology of ignorance, but it is importantly distinct. Miranda Fricker has argued for a kind of testimonial injustice, based on a gap between actual reliability and perceived credibility. For example, says Fricker, being white and being male are both bonuses to credibility. Since credibility is a source of power, anybody with credibility will seek to increase it, using that very credibility. Thus, says Fricker, credibility gaps can be turned into epistemic tools of social oppression (Fricker 2011). Similarly, Charles Mills argues that there is an active practice of ignorance among members of oppressive groups, such as white Americans. It is to the benefit of those in power to actively ignore many aspects of the existence of oppressed groups (Mills 2007;Alcoff 2007, 47-57). My account is compatible with, but independent from, Fricker's and Mills' accounts. Echo chambers can and surely are used to maintain social oppression through enhancing credibility gaps and supporting practices of active ignorance. The systematic mistrust of an echo chambers is a powerful tool for perpetuating epistemic injustice and active ignorance. However, the concept of an echo chamber does not require that they be deployed only in political contexts, nor does it require that they only be deployed only in the service of oppression. Echo chambers could potentially exist among the oppressed, and surely exist in apolitical contexts. I believe I have witnessed echo chambers forming around topics such as anti-vaccination, multi-level marketing programs, particular diets, exercise programs, liberal activism, therapeutic methodologies, philosophies of child-rearing, particular academic subdisciplines, and Crossfit (Weathers 2014).",
          "It has often been claimed, during and after the American political season of 2016, that we have entered a 'post-truth era'. Not only do some political figures seem to speak with a blatant disregard for the facts, their supporters seem unswayed by reason or contrary evidence. To many, it seems as if a vast swath of the electorate has become entirely unmoored from any interest in facts or evidence. Call this the \"total irrationality\" explanation of the post-truth phenomenon. But echo chambers offer an alternative explanation for the apparent post-truth mood. It seems likely that there is at least one vast partisan echo-chamber present in the political landscape. Jamieson and Cappella's study is a decade old, but sources like Breitbart and Alex Jones' Infowars seem like clear extensions of the same right-wing echo chamber. (Other echo chambers surely exist elsewhere on the political spectrum, though, to my mind, the left-wing echo chambers have been unable to exert a similar level of political force.) In that case, the account of echo chambers I've offered has significant explanatory force. The apparent \"posttruth\" attitude can be explained, at least in part, as the result of credence manipulations wrought by echo chambers. In healthy epistemic communities, there is something like an upper ceiling on the credence level accorded to any individual. A healthy epistemic network will supply a steady stream of contrary evidence and counterarguments; thus, no single individual or group will ever go unchallenged. Epistemic bubbles make the discovery of mistakes significantly less likely, and so tend to exaggerate the credence levels of epistemic sources inside the bubble. But when an echo chamber is in place and all outside sources have been effectively discredited, that ceiling disappears categorically. Echo chambers can create runaway credence levels for approved individuals. By removing disconfirmations and discorroboration from the system through the systematic discrediting of outsiders, echo chambers can create exceptionally high -one is tempted to say unnaturally high -levels of trust. That potential for runaway credence is built right into the foundations of any echo chamber, and arises from an interaction between the two main components of any echo chamber. First, an echo chamber involves a significant disparity of trust between the insiders and the outsiders. Second, an echo chamber involves beliefs, espoused by the insiders, reinforcing that disparity. The essential features of echo chambers seem designed to selfreinforce their peculiar arrangement of trust. Notice that epistemic bubbles alone cannot explain the post-truth phenomenon. Since epistemic bubbles work only via coverage gaps, they offer little in the way of explanation for why an individual would reject clear evidence when they actually do encounter it. Coverage gaps cannot explain how somebody could, say, continue to deny the existence of climate change when actually confronted with the overwhelming evidence. One would be tempted, then, to accuse climate change deniers of some kind of brute error. But echo chambers offer an explanation of the phenomenon without resorting to attributions of brute irrationality. Climate change deniers have entered an epistemic structure whereby all outside sources of evidence have been thoroughly discredited. Entering that epistemic structure might itself involve various epistemic mistakes and vices -but here the story can be one of the slow accumulation of minor mistakes, which gradually embed the believer in a self-reinforcing, internally coherent, but ultimately misleading epistemic structure. Similarly, some have suggested that, in the post-truth era, many people's interest in the truth has evaporated. Once again, this account of echo chambers suggests a less damning and more modest explanation. An echo chamber doesn't erode a member's interest in the truth; it merely manipulates their credence levels such that radically different sources and institutions will be considered proper sources of evidence. This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak -deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth. But by my account, we should expect discourse within echo chambers to be entirely differentwe should expect such discourse to be crisp and clear, and to present unambiguous claims about what is the case, what secret conspiracies are in place, and which sources are to be entirely distrusted. And this is precisely what we find (Jamieson and Cappella 2008, 3-41,140-176). Consider, for example, Breitbart's attacks on other media sources. One article begins: \"Mainstream media outlets continue to print false and defamatory descriptions of Breitbart News in a nakedly political effort to marginalize a growing competitor\" (Pollak 2017). This is not the double-speak of administrators and bureaucrats-this is a clear, strident, and unambiguously worded discredit. One might be tempted to say: but just give them the real evidence! You can't discredit neutral evidence! But this response radically underestimates the degree of trust and social processing involved in most presentations of evidence. Except for empirical evidence I myself have gathered, all other presentations of evidence rely on trust. My belief in the reality of climate change depends on enormous amounts of institutional trust. I have not gathered the climate change evidence myself; I mostly just trust science journalists who, in turn, trust institutional credentialing systems. Even if I had been on, say, a core sampling expedition to the Arctic, I would be unable to process that information for myself, or even vet whether somebody else has properly processed it. Even the climatologist who actually processes that information must also depend on trusting a vast array of other experts, including statisticians, chemists, and the programmers of their data analysis software. Most so-called \"neutral evidence\" depends on long and robust chains of trust (Millgram 2015, 27-44). Members of an echo chamber have acquired beliefs which break the usual arrangements of trust. But despite their evident explanatory force, echo chambers have been largely neglected by recent empirical research. Much of the recent research on causes of belief polarization focuses on the causal role of individual psychology, such as the tendency towards laziness in the scrutiny of one's own beliefs (Trouche et al. 2016). Similarly, recent studies on climate change denial focus on studying the relationship between an individual's stated political beliefs and their reactions to climate change information, without inquiring into the social epistemic structures in which the individuals are embedded (Corner, Whitmarsh and Xenias 2012). Famously, Dan Kahan and Donald Braman argue for the cultural cognition thesisthat is, that cultural commitments are prior to factual beliefs, and that non-evidentially formed cultural values inform which future presentations of evidence will be admitted as weighty (Kahan and Braman 2006). Though the values may originally come from an individual's culture, Kahan and Braman focus their analysis on how those acquired values function in individual reasoning to create polarization. They pay little attention to the continuing role of the contingent social structures in which the individual is embedded. The direct literature on echo chambers and epistemic bubbles is new and relatively small, compared to the sizable literature on individual belief polarization. Unfortunately, even in that literature, echo chambers and epistemic bubbles have often been confused. They are usually addressed in the popular media together, and the terms 'epistemic bubble' and 'echo chamber' are typically used interchangeably (El-Bermawy 2016). The same blurring has occurred in the treatment of the phenomena in academic epistemology in the surprisingly small literature on echo chambers. For example, Bert Baumgaertner, in his analysis of echo chambers via computer modeling, lumps together under the heading 'echo chamber' both Though he professes to cover both filter bubbles and echo chambers, his work focuses almost entirely on epistemic bubble effects: constricted information flow, lack of exposure to alternate arguments, and bootstrapped corroboration (Sunstein 2009b(Sunstein , xi,19-06, 2009a, 1-98), 1-98). The point here is about more just than his choice of words: his subjects of analysis include, among other things, Facebook friend groups, hate groups, extremist online political forums, conspiracy theorists, and terrorist groups (99-125, 2009b, 46-96). Clearly, this list includes prime candidates for both epistemic bubbles and echo chambers. But his analysis focuses almost entirely on the effects of bootstrapped corroboration and lack of exposure. For Sunstein, the primary mechanism driving polarization and extremism is the loss of truly public forums, because technology has over-empowered people's tendency to self-select sources offering familiar views. Thus, his solution is to re-create, in the new media environment, the kind of general public forums where people might be more likely to serendipitously encounter contrary views and arguments. His solutions include governmentfunded public news websites with diverse coverage and voluntary work by corporations and individuals to burst their bubbles. His recommendations for repair largely have to do with increasing exposure (Sunstein 2009a(Sunstein , 135-48, 2009b, 19-45,190-211), 19-45,190-211). But, again, if what's going on is actually an echo chamber effect, exposure is useless or worse. The blurring of the two concepts has also lead to some problematic dismissals of the whole cluster of phenomena. A number of recent articles in social science, communications, and media studies have argued that the whole set of worries about bubbles and echo chambers is wildly overstated. These articles share the same argumentative pattern. First, they use the terms \"filter bubble\" and \"echo chamber\" interchangeably, and address themselves to the same cluster of phenomena as Sunstein, treating them as singular. In fact, James Nelson and James Webster conflate Jamieson and Cappella's analysis of echo chambers and Pariser's analysis of filter bubbles, and erroneously attribute to Jamieson and Cappella the view that political partisans only seek out and encounter media from sources with matching political alignments -that is, Nelson and Webster attribute to an epistemic bubbles account to Jamieson and Cappella, where Jamieson and Cappella's actual text is clearly an echo chambers account (Nelson and Webster 2017, 2). More importantly, these recent articles proceed to argue against the existence of filter bubbles and echo chambers by demonstrating that, through the analysis of empirical data about media consumption, most people in fact expose themselves to media from across the political spectrum. Nelson and Webster, for example, argue against Jamieson and Capella, claiming that filter bubbles and echo chambers don't exist. Nelson and Webster support their claim with data showing that both liberals and conservatives visit the same media sites and spend comparable amounts of time at those sites (6-7). Again, this misses the mark -this is evidence only against the existence of epistemic bubbles, and not against the existence of echo chambers. Similarly, Seth Flaxman et al seeks to problematize the existence of filter bubbles and echo chambers with data that social media platforms seem to actually increase people's exposure to media from across the political divide (Flaxman, Goel and Rao 2016). Again, these data only concern the exposure and omission, and only weigh against the existence of epistemic bubbles. They say nothing about whether echo chambers exist. Echo chambers, recall, are structures of strategic discrediting, rather than bad informational connectivity. Echo chambers can exist even when information flows well. In fact, echo chambers should hope that their members are exposed to media from the outside; if the right disagreement reinforcement mechanisms are in place, that exposure will only reinforce the echo chambers' members' allegiance. We ought not conclude then, from data that epistemic bubbles do not exist, that echo chambers also do not exist. We can see now crucial it is to keep these two categories distinct. Epistemic bubbles are rather ramshackle -they go up easily, but they are easy to take down. Since there is no systematic discrediting of outsiders, simple exposure to excluded voices can relieve the problem. Echo chambers, on the other hand, are much harder to escape. Echo chambers can start to seem almost like living things -the belief systems provide structural integrity and resilience. Mere exposure will be ineffective. Jamieson and Cappella offer evidence of this effect: once listeners are caught in Rush Limbaugh's language, framing, and discredentialing of the mainstream media, their beliefs can survive frequent contact with contrary viewpoints. Limbaugh's technique, say Jamieson and Cappella, serves to insulate and inoculate his audience from being affected by exposure to contrary viewpoints (Jamieson and Cappella 2008, 163-190). In fact, if the appropriate disagreement-reinforcement mechanisms are in place, exposure will simply strengthen the attacked belief systems. Thus, an outsider's attempt to break an echo chamber as if it were a mere bubble is likely to backfire and reinforce the echo chamber's grip. 9",
          "So what, then, are the epistemic responsibilities of an agent to discover whether they are in one of these social epistemic traps, and what are their prospects for actually discovering their predicament and successfully escaping? To answer this, we must consider two distinct questions: The escape route question: Is there any way out of an echo chamber or epistemic bubble? The escape responsibility question: Could one behave epistemically virtuously, and yet still remain caught within an echo chamber or epistemic bubble? In other words, to what degree is an epistemic agent embedded within such a structure blameworthy, or blameless, for the faultiness of their beliefs? The first question asks about the possible existence of an escape route. The second asks whether there is an escape route that we might reasonably expect an epistemically virtuous agent to discover and enact. These are distinct questions, because an escape route might turn 9 Sunstein does briefly note the empirical data for the disagreement-reinforcement effect in passing, but then seems to ignore it in proposing his solutions (Sunstein 2009a, 54-5) out to be possible, but so difficult to discover or use that it was beyond what we might reasonably expect of an agent of moderate epistemic virtue. For epistemic bubbles, the answers are straightforward. As I've argued, epistemic bubbles are quite easy to shatter. One just needs exposure to excluded information. Insofar as that information is available, but simply not part of one's standard network, then members of epistemic bubbles are failing to live up to their epistemic duties, which include proactively gathering relevant data. To translate into contemporary terms: if you're subject to an epistemic bubble because you get all your news from Facebook and don't bother to look at other sources, you are, indeed, blameworthy for that failure. If one finds the language of epistemic virtues and vices appealing, then we can say that members of epistemic bubbles are committing the vice of epistemic laziness. Answering these two questions is much more difficult for echo chambers. Recall: where encountering excluded voices and evidence will shatter an epistemic bubble, such encounters are likely to reinforce an echo chamber. Let's grant that intentionally constructing an echo chamber, as Jamieson and Cappella claim that Rush Limbaugh did, is epistemically (and morally) blameworthy. Furthermore, actively entering an echo chamber seems epistemically blameworthy in many circumstances. For agent in full possession of a wide range of informational sources, to abandon most of them and place their trust in an echo chamber for, say, an increased sense of comfort and security, is surely some form of epistemic vice. There is some evidence that this may be the case; Jamieson and Cappella suggest that people enter echo chambers for the sake of the community bonds and the sense of belonging to an ingroup (Jamieson and Cappella 2008, 180). But there are many cases in which the agent seems plausibly blameless. Imagine a person raised in an echo chamber. Their earliest epistemic contacts -let's say their parents, relatives, and close family friends -are all firmly committed members of the echo chamber. Suppose that the child is either home-schooled by those echo chamber members or sent to a school that reinforces the beliefs of that particular echo chamber. I take it that it is reasonable for a child to trust their parents and those of seeming epistemic authority, and that a child is epistemically blameless for having done so (Goldberg 2013). Thus, when that child eventually comes into contact with the larger epistemic world -say, as a teenager -the echo chamber's beliefs are fully in place, such that the teenager discredits all sources outside of their echo chamber. It seems, at first glance, that our teenager could be acting very much like a reasonable epistemic agent. They could, in fact, be epistemically voracious: seeking out new sources, investigating them, and evaluating them using their background beliefs. They investigate the reliability of purported experts and discredit experts when they have apparently good reason to do so, using their background beliefs. Our teenager seems, in fact, to be behaving with many epistemic virtues. They are not at all lazy; they are proactive in seeking out new sources. They are not blindly trusting; they investigate claims of epistemic authority and decide for themselves, using all the evidence and beliefs that they presently accept, whether to accept or deny the purported expertise of others. They have theories, which they have acquired by reasonable methods, predicting the maliciousness of outsiders; they increase their trust in those theories when their predictions are confirmed. 10The worry here is that agents raised within an echo chamber are, through no fault of their own, epistemically trapped -their earnest attempts at good epistemic practices are transformed into something epistemically harmful by the social structure into which they have been embedded and which they have ingested. Paul Smart has argued for the possibility of a transformative social epistemic phenomenon which he dubs \"mandevillian intelligence,\" in honor of Bernard Mandeville. Mandeville argued that, in the right social context, individual vices could lead to collective economic prosperity. For a certain kind of economic theorist, capitalism is such a transformative structure -individuals act selfishly, but the structure of the market transforms that selfishness into virtuous collective action. According to Smart, there is an epistemic analog: the mandevillian intelligence, which transforms the individual epistemic vices of its members into a collective epistemic virtue by virtue of the social structure into which they are embedded (Smart 2017). Intellectual stubbornness, for example, might be an intellectual vice for individuals. But set those stubborn individuals in a properly arranged social structure (like, perhaps, academia) and you might get a collective system that properly explores every relevant nook and cranny with optimal thoroughness. But echo chambers are the very opposite; they are reverse-mandevillian intelligences. Echo chambers are social epistemic structures which convert individually epistemically virtuous activity into collective epistemic vice. In fact, the reverse-mandevillian nature contributes to the stickiness of the echo chamber trap. If our teenager self-reflects on their epistemic practices, what they will see might be rather gratifying. Their epistemic behavior might very well be earnest, vigorous, and engaged. It is their external context -the social epistemic system into which they have been unluckily raised -which makes such behavior problematic. Contrast this account with Quassim Cassam's treatment of Oliver, his fictional 9/11 conspiracy theorist. Oliver believes that the collapse of the twin towers was an inside job, and he is happy to provide reasons and point to supporting evidence from a great many conspiracy theorist websites. Says Cassam: Oliver is obviously mistaken -Oliver relies on outrageous, baseless claims from clearly discredited sources. The best explanation for Oliver's beliefs is in terms of epistemic vice -that is, in terms of Oliver's bad intellectual character traits. Oliver is \"gullible, cynical, and prejudiced,\" says Cassam. Oliver is gullible with regard to his conspiracy theorist sites, cynical with regard to the mainstream media, and his prejudice consists of, among other things, intellectual pride, wishful thinking, closedmindedness, and a lack of thoroughness (Cassam 2016, 162-4). And I certainly grant that such epistemic vices can lead to these sorts of beliefs. But the story of our hapless teenager offers an alternate epistemic path to such beliefs and such narrowcasted trust -one in which epistemically virtuous character traits have been wrong-footed by the social epistemic structure in which the agent has been embedded. The crucial difference between the reversemandevillian account and Cassam's account is where the brunt of the responsibility lies. In Cassam's account, the responsibility lies with the individual, and their own intellectual habits and practices. 11 In a reverse-mandevillian account, a significant part of the responsibility lies with the social structure in which the actors are embedded. The epistemic vice is a feature of the collective intelligence, rather than of the individual. Or, if one is averse to thinking in terms of collective intelligences, here's a conceptually minimal way to put the claim: echo chambers are local background conditions that turn generally good epistemic practices into locally unreliable ones. But the possibility of a truly faultless epistemic agent, wholly misled by an echo chamber, also depends on the lack of an accessible escape route. So: are there escape routes from an 11 Note, however, that Cassam distinguishes between epistemic responsibility and epistemic blameworthiness, and does not take blameworthiness to necessarily follow from responsibility (168-9). Cassam leaves room for the view that the individual's intellectual vices were epistemically responsible for their bad beliefs, but that the individual wasn't blameworthy for those vices, because the vices were inculcated in them at an early age. However, my complaint still stands, for I contest Cassam's claim that the responsibility is in the individual. echo chamber, and how reasonable is it to expect echo chamber members to discover and make use of them? Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass. Thus, their critical reflection is likely to reinforce previously held beliefs. Kelly notes that the belief polarization violates the Commutativity of Evidence Principle: The Commutativity of Evidence Principle: to the extent that what it is reasonable for one to believe depends on one's total evidence, historical facts about the order in which that evidence is acquired make no difference to what it is reasonable for one to believe. (Kelly 2008, 616) In short, belief polarization makes it matter what order they received the evidence, but the historical ordering of evidence ought not matter. Note that our epistemically hapless teenager has also violated the Commutativity of Evidence Principle. For them, it matters very much what order that they received the evidence. If they had been raised outside the echo chamber and fed a broader diet of epistemic sources before encountering the echo chamber, then they would likely have found the echo chamber's world-view to be problematic. But since our teenager encountered the echo chamber and assimilated its beliefs first, their use of background beliefs to vet new sources leads them to continually increase their trust in the echo chamber and their distrust of outsiders. Even if our echo chambered teenager eventually came to encounter all the same evidence as their epistemically free-range counterpart, their early education within the echo chamber would be decisive. So long as each new piece of evidence is assessed using the currently held set of beliefs, then early education in an echo chamber becomes domineeringly powerful. However, the Commutativity of Evidence Principle suggests a way out. In order to free themselves of the echo chamber's grip, our teenager needs to undo the influence of the historical ordering of their encounters with the evidence. How could they possibly do this? Our teenager would have to suspend belief in all their particular background knowledge and restart the knowledge-gathering process, treating all testimony as equally viable. They would need to, in a sense, throw away all their beliefs and start over again. This suggests a process that, in its outlines, might sound awfully familiar. Our escape route turns out to be a something like a modified version of Descartes' infamous method. What proceeds from this point is admittedly something of a fantasy, but perhaps it is a fantasy from which we can eventually draw some sort of moral. The story of the history of Western epistemology might be cartoonishly summarized thusly: Descartes had a dream of radical intellectual autonomy. By his accounting, he came to realize that many of the beliefs he had acquired in his early life were false, and that those early false beliefs might have infected any number of other beliefs. His response was that famed method: to get rid of his beliefs and start over again, trusting no-one and nothing and only permitting those beliefs of which he was entirely certain. Call this the Cartesian epistemic reboot. But if recent epistemology has taught us anything, it's that this total reboot is nothing but a pipe dream. Any sort of reasonable epistemic life is essentially impossible without trusting the testimony of others (Burge 1993;Faulkner 2000;Goldberg 2010;Zagzebski 2012;Hardwig 1985Hardwig , 1991)). But recall that the reason Descartes wanted to discard everything and start over from scratch -the motivation for his project, and not the method -was explained in the very first line of \"Meditation 1\": He was worried by the falsehoods he had learned in childhood and the shakiness of the edifice that had been built from those falsehoods (Descartes 1984, 24). Our teenager faces a problem quite similar in structure. The credentialing structure of their upbringing is flawed; that credentialing structure has influenced any number of their other beliefs, and the degree of that influence is impossible to track. Furthermore, these later beliefs, approved by the echo chambers' credentialed sources, will often serve to reinforce that credentialing structure. The pernicious effect of an echo chamber cannot be attacked one belief at a time. Any single belief that our teenager re-considered would come under the influence of the network of the flawed background beliefs that sustains an echo chamber. What they need is some way to start over. In order to undo the influence of historical ordering, an epistemic agent will have to temporarily suspend belief in all their beliefs, in particular their credentialing beliefs, and start from scratch. But when they start from scratch, they need not disregard the testimony of others, nor need they hold to Descartes' stringent demand for certainty. Let's call this procedure the social epistemic reboot. In the social epistemic reboot, the agent is permitted, during the belief re-acquisition process, to trust that things are as they seem and to trust in the testimony of others. But they must begin afresh socially, by re-considering all testimonial sources with presumptive equanimity, without deploying their previous credentialing beliefs. Furthermore, they must discard all their other background beliefs, because those potentially arose from the flawed credential structure of the echo chamber, and very likely have been designed to support and reinforce that very credential structure. Our rebooter must take on the social epistemic posture that we might expect of a cognitive newborn: one of tentative, but defeasible, trust in all apparent testimonial sources (Burge 1993) (Nguyen 2011). This method will, if successfully applied, undo the historical dependence of our epistemic agent and remove the undue influence of the echo chamber. The social epistemic reboot is, theoretically at least, the escape route we've been searching for. 12This reboot, described in such clinical terms, might seem rather fantastical. But it is not, I think, utterly unrealistic. Consider the stories of actual escapees from echo chambers. Take, for example, the story of Derek Black, who was raised by a neo-Nazi father, groomed from childhood to be a neo-Nazi leader, and who became a teenaged breakout star of white nationalist talk radio. When Black left the movement, he went through years-long process of self-transformation. He had to completely abandon his belief system, and he spent years rebuilding a world-view of his own, immersing himself broadly and open-mindedly in everything he'd missed -pop culture, Arabic literature, the pronouncements of the mainstream media and the US government, rap -all with an overall attitude of trust (Saslow 2016). Of course, all we have shown so far is that the social epistemic reboot would, if pulled off, undo the effects of an echo chambered upbringing. Whether or not an epistemic agent might reasonably be expected to reboot, or blameworthy for failing to reboot, is a separate and significantly more difficult question. First, a social epistemic reboot might be psychologically impossible, or at least beyond what we could reasonably expect of normal epistemic agents. Second, what reason would an epistemic agent have to undertake a social epistemic reboot? Such an undertaking would be justified only if the agent had a significant reason to think that their belief system was systematically flawed. But echo chamber members don't seem likely to have access to any such apparent reason. After all, they have clear and coherent explanations for all the evidence and testimony they encounter. If this is all right then we arrive at a worrying conclusion: that echo chambers may, theoretically, be escapable, but we have little reason to expect members of echo chambers realize that they are members of something that needs escaping. What could hope do we have, then, of motivating a reboot? Derek Black's own story gives us a hint. Black went to college and was shunned by almost everyone in his college community. But then Matthew Stevenson, a Jewish fellow undergraduate, began to invite Black to his Shabbat dinners. Stevenson was unfailingly kind, open, and generous, and he slowly earned Black's trust. This eventually lead to a massive upheaval for Black -a slow dawning realization of the depths to which he had been systematically misled. Black went through a profound transformation, and is now an anti-Nazi spokesperson. The turning point seems to be precisely that Stevenson, an outsider, gained Black's trust. And this is exactly where we should expect the turning point to be. Since echo chambers work by building distrust towards outside members, then the route to unmaking them should involve cultivating trust between echo chamber members and outsiders. In order to motivate the social epistemic reboot, an echo chamber member needs to become aware of how in the echo chamber's grip they are, and forming a trust relationship with an outsider might could mediate that awareness. But how that trust could be reliably cultivated is a very difficult matter, and a topic for future investigation. We have, however, arrived at a tentative moral of the story. Echo chambers work by a manipulation of trust. Thus, the route to undoing their influence is not through direct exposure to supposedly neutral facts and information; those sources have been preemptively undermined. It is to address the structures of discredit --to"
        ],
        "ground_truth_definitions": {
          "epistemic bubble": {
            "definition": "a social epistemic structure in which some relevant voices have been excluded through omission.",
            "context": "However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation.",
            "type": "explicit"
          },
          "echo chamber": {
            "definition": "a social epistemic structure in which other relevant voices have been actively discredited.",
            "context": "But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation.",
            "type": "explicit"
          },
          "belief polarization": {
            "definition": "the tendency of individuals, once they believe that p, to increase their belief that p.",
            "context": "Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass.",
            "type": "explicit"
          },
          "orwellian double speak": {
            "definition": "deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker.",
            "context": "This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak - deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "News coverage has a strong impact on public opinion, i.e., what people think about (agenda setting), the context in which news is perceived (priming), or how topics are communicated (framing). Researchers from the social sciences have extensively studied such forms of media bias, i.e., the intentional, non-objective coverage of news events. The extensive research has resulted in a broad literature on different forms and possible sources of media bias and their impact on (political) communication or opinion formation. In tandem, various well-established research methodologies, such as content analysis, frame analysis, and meta-analysis, have emerged in the social sciences. The three forms of analysis discussed in Sect. 2.3 require significant manual effort and expertise [27], since those analyses require human interpretation of the texts and cannot be fully automated. For example, a quantitative content analysis might (semi-)automatically count words that have previously been manually defined in a codebook but they would be unable to read for \"meaning between the lines,\" which is why such methods continue to be considered less comprehensive than a qualitative analysis. However, the recent methodological progress in natural language processing in computer science promises to help alleviate many of these concerns. In the remainder of this article, we discuss different forms of media bias defined by the news production and consumption process. The process we have laid out in detail previously is in our view the most suitable conceptual framework to map analysis workflows from the social sciences to computer science, and thus helps us to discuss where and how computer scientists can make unique contributions to the study of media bias.",
        "reasoning": "The section describes the impact of news coverage on public opinion and the various methods (content analysis, frame analysis, etc.) used to study media bias in the social sciences. It also mentions the challenges and limitations of these methods and the potential of natural language processing from computer science. The text does not explicitly define any terms, but it uses phrases that could be interpreted as definitions, such as \"media bias\" and \"agenda setting,\" without explicit \"is defined as\" or similar cues.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Although sometimes vague and often complex, sustainability is an important aim for societies and humanity. Sustainability is a multidimensional concept, encompassing economic, social, environmental and other factors. Sustainability assessment tools are needed to evaluate processes and systems, and how changes affect their sustainability. This usually necessitates a comprehensive set of indicators of the sustainability of the environment, the economy, technology, society, and institutional systems. Many human civilizations in the past proved to be unsustainable to some degree. It is hoped that the coverage provided in this chapter, including sustainability definitions and historical contexts, its economic, environmental and social dimensions, sustainable development, sustainability assessment measures and relevant applications, will assist efforts to address sustainability challenges today and, as an even more important priority, in the future. ",
        "reasoning": "The text explains that sustainability is an important aim for societies and humanity, describing it as a multidimensional concept encompassing various factors and the need for assessment tools to evaluate these factors. It also discusses the challenges of past human civilizations and the importance of this chapter in addressing sustainability issues in the future.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
        "reasoning": "This section discusses the task of quantifying whether textual data has a positive, negative, or neutral emotion by referencing a study (Zhang et al., 2018). It appears to be explaining a process rather than directly defining a term.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Using TextRank Algorithm, but the weight is a embedding weight w(i, j) vi , e vj are the term embeddings for v i , v j .",
        "reasoning": "The section describes the TextRank algorithm, which is a graph-based algorithm used to rank documents. It mentions that the algorithm is similar to PageRank but processes text data. However, it does not explicitly define the term \"weight\" in this context or provide a clear definition of how the embedding weights are utilized within the algorithm.",
        "is_definition": false
      },
      {
        "paper_id": "2106.16117v1",
        "sections": [
          "The concept of the 'energy security' was first introduced by industrialized market economies in 1947, when the United States adopted a document regulating the actions of the state in the field of national security. Although it has been investigated and developed by a wide range of scholars and specialists, still there is no universal understanding of what is meant by this notion.",
          "In international relations, the term 'energy security' acquired significance and became widely used after the global oil crisis of 1973-1974, when the world faced a sudden and noticeable increase in prices for fuel and energy resources caused by the following political events [1]: ‚Ä¢ collective embargo imposed by the Arab OPEC countries on the export of oil to the USA; ‚Ä¢ support of Israel in the October war by a number of European countries. Being faced with the first largest energy crisis, industrial countries acutely felt their vulnerability and consequently energy security became one of the top priorities, considered a part of the country's national security. As mentioned above, currently, despite the abundance of modern research and publications on energy topics, a single generally accepted definition is still not formed. In point of fact, the meaning of energy security differentiates from country's dependence to their energy imports. Accordingly, countries which are highly dependent on imported oil and gas adheres energy security to supply whereas, countries which export oil and gas adheres energy security to demand. This is due to the fact that different groups of countries, namely exporting countries, importing countries and transit countries interpret this concept in their own way, based on their national and economic interests and priorities. Therefore, in order to give a clear definition of the concept of \"energy security\", it is imperative to consider it from the perspective of all three parties. But, nevertheless, in modern scientific literature there are many concepts and interpretations of international energy security, and representatives of different schools or countries offer their own definitions of this notion. The International Energy Agency (IEA), which is an international body that unites 29 industrialized countries under the Organization for Economic Development and Cooperation, defines energy security as 'the uninterrupted availability of energy sources at an affordable price' [2]. The limitations of this approach are directly implied by the fact that in this context, it considers energy security from the point of view of importing countries that reflects their national priorities in the face of growing dependence on imports of oil and natural gas. Nevertheless, the IEA admits that the concept of 'energy security' can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces 'timely investments to supply energy in line with economic developments and environmental needs', while the latter deals with 'the ability of the energy system to react promptly to sudden changes in the supplydemand balance' [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply. Beijing defines 'energy security' as 'reliable uninterrupted supply of energy, which is necessary for economic development of the country' [1]. In other words, energy security is seen as a need for uninterrupted supply of energy resources in order to maintain stable economic growth. Meanwhile, the exporting countries, including Russia, do not share this position. They are primarily interested in diversifying the markets of energy exports, preserving their national sovereignty and of course guaranteeing sustainable demand for the energy they export. Because of diverging interests in the energy market, exporting countries interpret 'energy security' in another way. The OPEC group (a permanent intergovernmental organization of 15 oilexporting developing nations that coordinates and unifies the petroleum policies of its Member Countries [4]) defines energy security as 'reliable and environmentally friendly energy supplies at prices reflecting fundamental principles of market economy'. This approach is also limited, as the IEA one, because it reflects only the interests of exporting countries that are focused on determining a decent price for energy resources. The interests of transit countries consist mainly in maximizing transit rent of energy resources. This is the approach Turkey follows, -the country is a natural energy corridor between the Middle East, the Caspian basin and Europe and contributes significantly to the attempts of Europe to diversify its energy suppliers for natural gas. In the meantime, the concept of energy security framed as an access to fossil fuels created the so-called 'energy nationalism' [3] that created a reality in which the behavior and decisions made on energy markets and the delivery of resources ultimately depended not on the economic market factors, but on producers, which resulted in the transformation of the energy market into the subject of inter-state relations. Oil and natural gas were used as geopolitical weapons, and geopolitics and geo-economics became an important part of world politics and foreign policy of the main players on the energy market. Energy security undoubtedly depends on the national and international background. In the context of state-controlled markets, the main guardians of energy security are the governments. On the contrary, given the energy markets are liberalized and the main actors are private companies, the security of supply consists of an efficient risk management strategy by governments, companies and consumers. Therefore, it is important not to fall into purely ideological approaches. Since Adam Smith, the state is authorized to intervene in the economy in order to prove security, including its energy component, to its citizens. Baring that in mind, it should be noted that such intervention should be done as far as possible in a concerted manner along with the companies and consumers, following the principle of subsidiarity. At the same time, energy security is associated with the evolution of the energy markets, the geopolitical situation and long-term international scenarios. There are several scenarios drawn for the long term that extrapolate to some extent conceptual differences, for example, the 'Markets and Institutions' scenario and its alternative called 'Empires and Regions' [3], and the three scenarios drawn by Shell Global Scenarios [5]: 'Low Trust Globalization', 'Open Doors' and 'Flags'. The 'Low Trust Globalization' is based on the trade-off between efficiency and security, and is characterized by limited international integration, intrusive state intervention and institutional discontinuities. The 'Open doors' arises from the dilemma between efficiency and environmental sustainability, premium market incentives and the participation of the civil society. It highlights the urgent necessity of transnational integration and harmonization, and mutual recognition of standards. The 'Flags' responds to the mercantilist logic and involves regulatory fragmentation, nationalism and the conflict between the values of the different regions. To some extent, the differentiation of scenarios roots in the neoliberal and neorealist paradigms to international energy issues. It is an old debate between two alternative visions of the world: a world in which the market disturbances are resolved by cooperation, or a fragmented world conceived as a billiard table where conflicts are resolved through the exercise of the political, economic and military power. Under the neoliberal paradigm, that is so praised by the EU, energy security is achieved through the development of markets and the management of conflicts at the multilateral level through supranational institutions. The neorealist paradigm of energy security, instead, implies the development of bilateral relations and the subordination of markets to foreign policy. The potential risks and threats related to energy security arise mainly from two circumstances: ‚Ä¢ the projected upcoming peak in the production of hydrocarbon resources, which is vital for the modern economy, and ‚Ä¢ the security of their supplies. However, recently the energy sector has started to develop a few key trends that were caused by new, very strong factors: the global financial and economic crisis and the shale revolution in the production of oil and gas. Today, energy security policies require a paradigm shift and a new model of factors and conditions for its implementation. The first factor that radically changed the context of energy policy was the global economic crisis. Since 2008, experts have determined it as the financial crisis, the economic crisis, the crisis of democracy and governance, the crisis of the culture of public consumption and material culture in general, and the environmental crisis that will eventually lead to global natural disasters. It would be reasonable to say that the world experienced a multidimensional global crisis, or the first systemic crisis of the global era. In the energy sector, this crisis coincided with the start of the gradual transition from 'industrial' and 'hydrocarbon' to 'neoindustrial' and 'smart' energy, which includes the following aspects: smart grids, energy efficiency (in a broad sense), renewable energy, new principles of energy systems and focus shift from producers to consumers [3]. The second factor that dramatically changed the energy markets was the quiet shale revolution in oil and gas production. The shale revolution, which has become a reality in the United States and Canada, as estimated by the experts, will have serious consequences for the global energy market. The unconventionally produced natural gas fundamentally changed the world market. The most serious consequence of the shale gas revolution is a shift in the focus from producers to consumers. In the context of the old paradigm, energy security is directly related to energy independence. The idea was that if a country is self-sufficient with respect to energy resources, and has an efficient (energy-saving) economy, then this will be accompanied with lower prices for energy carriers. The reality of oil prices in the United States after the shale boom has shown that this is utopia.",
          "Meanwhile, achieving self-sufficiency in the energy sector is almost impossible. Even such countries as Russia, Saudi Arabia, Venezuela, Brazil and Canada, which are rich in hydrocarbons, import a part of the energy in the form of refined petroleum products due to insufficient refining opportunities. This dependence could theoretically be eliminate with a little effort and investment in the construction of new refineries, but in practice it is not happening. Revolutionary changes of the energy security require a paradigm shift, which should be reflected in the energy security policies. Currently we are on the verge of transition to a post-industrial, 'smart' energy system, which means 'smart' networks, alternative energy sources for transport, decentralization of energy, integration of energy into the technical sphere, accompanied by an increase in energy efficiency. To conclude, it can be said that the definition of 'energy security' can be complete only if the interests of all the energy market participants are taken into account, and namely importing countries, exporting countries and transit countries. Interests and priorities of all three parties are different, which presents a difficulty in agreeing on a common concept. At the same time, with the paradigm shift all of the actors of the energy market are interested in ensuring the reduction of geopolitical and environmental risks and thus creating new opportunities."
        ],
        "ground_truth_definitions": {
          "short-term energy security": {
            "definition": "the ability of the energy system to react promptly to sudden changes in the supplydemand balance",
            "context": "Nevertheless, the IEA admits that the concept of ‚Äòenergy security‚Äô can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‚Äòtimely investments to supply energy in line with economic developments and environmental needs‚Äô, while the latter deals with ‚Äòthe ability of the energy system to react promptly to sudden changes in the supply- demand balance‚Äô [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply.",
            "type": "explicit"
          },
          "long-term energy security": {
            "definition": "timely investments to supply energy in line with economic developments and environmental needs",
            "context": "Nevertheless, the IEA admits that the concept of ‚Äòenergy security‚Äô can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‚Äòtimely investments to supply energy in line with economic developments and environmental needs‚Äô, while the latter deals with ‚Äòthe ability of the energy system to react promptly to sudden changes in the supply- demand balance‚Äô [2].",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "26af8e81a7ca5b137912acb81d8c19f38fce0200",
        "sections": [
          "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
          "Howard's research on the \"chlorine sunset\" controversy is based on interviews and document analysis. He conducted twenty-seven semistructured interviews, lasting an hour on average, with staff members of federal regulatory agencies in the United States and Canada, staff members of the International Joint Commission (IJC), members of the Great Lakes Science Advisory Board, staff members or individuals otherwise associated with nongovernmental organizations (NGOs), academic or governmental members of the industrial ecology or green chemistry communities, and industrial chemists in industry and academia. A number of transcripts were supplemented with additional information from follow-up correspondence. Documents analyzed included (1) reports, press releases, Web documents, and other materials published by NGOs, the chemical industry, and federal agencies; (2) articles and commentaries in newspapers and popular and trade magazines; (3) research articles and commentaries in scholarly anthologies and peer-reviewed scholarly journals; (4) books written by key actors; and (5) transcripts of Congressional testimony. A little-studied controversy involving one of the major branches of industrial chemistry documents a striking example of undone science and illustrates the role it can play in structuring conflict between competing regulatory paradigms. Much of the controversy has centered on the Great Lakes region, where extensive chemical manufacturing and contamination has occurred; where scientists have documented threats to wildlife and humans from persistent, toxic, industrial chlorinated pollutants; where extensive citizen activism has emerged around this threat; and where a quasigovernmental advisory body has assumed a leadership role in addressing this concern (Botts et al. 2001). A number of environmental and health advocates have argued, based both on fundamental toxicology and on long historical experience with chlorinated synthetic chemicals (e.g., DDT and PCBs), that the entire class of thousands of such substances should be tentatively presumed dangerous and that the chemical industry accordingly should wean itself from most major uses of chlorine (Thornton 1991(Thornton , 2000;;International Joint Commission [IJC] 1992; see Howard 2004). The analysis offered here briefly considers the character and function of undone science in the debate provoked by proposals for a \"chlorine sunset.\" The chlorine sunset controversy revolves around conflict between two sharply contrasting regulatory paradigms: risk and precaution (Thornton 2000;Howard 2004). The powerful chemical industry has coevolved with, supports, and is supported by the dominant U.S. and Canadian environmental regulatory regime, which restricts chemical industry decision making only to the extent that detailed calculation of risk indicts individual chemical substances. Meanwhile, Greenpeace, a marginalized, reputedly radical environmental NGO, and the IJC, a prominent but marginalized binational advisory organization, argued for a regulatory regime based on the precautionary principle (see Tickner 2003), which in their view justified governmental action against an entire class of industrial chemicals. The dominant paradigm assumes the unit of analysis to be the individual substance and places the burden of proof on the public to prove harm; in contrast, the challenger paradigm allows, even requires, the primary unit of analysis to be the entire class of substances and places the burden of proof on corporate officials. Within this matrix of political and epistemological conflict, the political economy and political sociology of undone science can be seen to revolve around a series of three dyads, each paradigm implying parallel formulations of \"done science\" and undone science. The three dyads are summarized in Table 1. One dyad appears in the context of health impacts research. Industry and federal officials operating in the risk paradigm hold that the legitimate goal of health impacts research performed or mandated by government is ad hoc identification of individual chlorinated chemicals that cannot be safely manufactured and used. In this paradigm, chlorine chemistry itself is seen as immune to fundamental interrogation; the role of public science is limited to documenting the odd substance that can be definitively proven harmful and, on that basis, restricted. \"We've made the point over and over again that you have to look at each product's physical and chemical characteristics to draw conclusions about what it is going to do in the environment,\" argued Brad Lienhart, of the Chlorine Chemistry Council. To do otherwise would be to \"[make] non-science-or nonsense-into science\" (quoted in Sheridan 1994, 50). Beginning in the early 1990s, \"sunset\" proponents vigorously argued that such research is incapable of interrupting a long series of chlorinated \"Pandora's poisons\" from entering the environment and human tissues long before their deleterious effects are documented. Inevitably remaining undone, they argued, is science capable of systematically identifying unsafe chemicals from among tens, perhaps hundreds, of thousands of chlorinated industrial substances, by-products, and breakdown products, a scope of research that the risk paradigm is sometimes assumed to provide but, owing to the sheer enormity of the undertaking, cannot. The government's effort to identify unsafe chlorinated chemicals is ad hoc precisely because it cannot, in any meaningful sense, be systematic; not only are available resources insufficient, but the enterprise is technically infeasible. Viewed in this light, the science is undoable. The IJC argued: There is a growing body of evidence that [suggests that] these compounds are at best foreign to maintaining ecosystem integrity and quite probably persistent and toxic and harmful to health. They are produced in conjunction with proven persistent toxic substances. In practice, the mix and exact nature of these various compounds cannot be precisely predicted or controlled in production processes. Thus, it is prudent, sensible and indeed necessary to treat these substances as a class rather than as a series of isolated, individual chemicals. (IJC 1992, 29) A second dyad appears in the risk paradigm's stance on innovation. Industry has systematically pursued the development of chlorine chemistry, developing chlorinated chemicals and expanding markets for them; meanwhile, advocates of chlorine precaution have pointed to the need to systematically develop nonchlorine alternatives. This is in part science that the risk paradigm has long left undone-historical research and development trajectories that could have led to a wider range of nonchlorine chemicals and processes being available today. The implication of the historical analysis offered by a leading sunset proponent (Thornton 2000; see also Stringer and Johnston 2001) is that over the past century the technological, economic, and political momentum of chlorine chemistry has to some extent bent the overall industry research and development agenda toward chlorine and away from nonchlorine alternatives. Here undone science consists of a body of nonchlorine chemicals and processes that might now exist but for the long dominance of research and development predicated on chlorine. It is a point seemingly acknowledged by a confidential IJC informant who did not support the commission's sunset recommendation: \"There's no reason why we couldn't, as a global society, live a non-chlorine lifestyle. It's just, you know <laughs>, that ain't gonna happen, because that is not our history! We're kind of, in a way, captives of our past.\" In the risk paradigm, with its laissez-faire orientation, such research and development need not be undertaken by the industry but instead is tacitly left to whichever agency or organization might care to undertake it. Viewed from the vantage point of the industry, with its adamantine conception of chlorine chemistry as technologically and economically inevitable, the only conceivable motivation for conducting such research and development would be some kind of ideological fetish (see, e.g., Chlorine Chemistry Council n.d.). It would represent \"a veiled attempt to return to a pre-industrial Eden,\" one industry supporter suggested (Amato 1993). Crucially, although this agenda would have been and would now be technically feasible, such research would be hobbled by the absence of a sizable cadre of technoscientists devoted to the project and by a lack of financial resources to sustain the effort. A third dyad occurs within the challenger, precautionary paradigm and directly counters the values and priorities of the dominant paradigm's dyads. Paired with precaution advocates' assertion of the need for research to systematically develop nonchlorine alternatives-here seen as industry's responsibility rather than the public's-is an explicit assertion that industry should assume the burden of making the case for any specific chlorinated chemicals (or chemical processes) that can be demonstrated to be both essential (i.e., nonsubstitutable) and capable of being manufactured and used in ways that (to some as yet unstated standard) pose no significant environmental hazard. Industry's motivation for undertaking this latter effort would, of course, be profit. And owing to the presumably quite limited number of substances to be evaluated, it would be both technically feasible and, given the industry's substantial financial and technical resources, affordable. The chlorine sunset controversy is now effectively dormant. In the face of bitter industry resistance and U.S. and Canadian governmental intransigence, the IJC and Greenpeace ceased promoting their sunset recommendations in the mid-1990s (Howard 2004). Thornton's book, which appeared in 2000, reawakened (and in significant ways deepened) the debate, but it did so only briefly. The sunset proposals have not visibly shifted policy at any level in North America. A major international treaty on persistent organic pollutants signed in 2001 represented an important victory for activists, but it also underscored the lingering, unresolved character of the chlorine debate: all twelve of the \"dirty dozen\" substances it required to be phased out are chlorinated compounds, and each was targeted on the basis of its discreet, well-documented characteristics. Meanwhile, thousands of far less extensively studied chlorinated chemicals-and chlorine chemistry as a whole-remain unregulated. This analysis of the chlorine sunset controversy illustrates how regulatory regimes influence the construction and articulation of research priorities. In this case, advocates of the risk and precaution paradigms, on the basis of competing understandings of the appropriate unit of regulatory analysis and appropriate regulatory burden of proof, promote competing conceptualizations of science both done and undone. More specifically, the case suggests that done and undone science in such a controversy can be understood as occurring in dyadic pairs and that a major role for challenger discourses is making the implicit undone portion of dyads within the dominant paradigm visible and explicit. This analysis also highlights an important category of undone science in technoscience controversies-undoable sciencethat improves understanding of how regulatory regimes constrain the identification of undone science. Here, close examination of precautionary advocates' critique of the risk paradigm clarifies the process through which conventional regulatory structures veil undoable science in the form of systematic research for which insufficient resources and insufficient technical means are available.",
          "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (L√∂wy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] ¬£1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center ‚Ä¶ there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life ‚Ä¶ and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't ‚Ä¶ appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
          "Kempner's research is based on an interview study that examines \"forbidden knowledge,\" a term used to capture scientists' decisions not to produce research because they believe it to be taboo, too contentious, or politically sensitive (a type of negative knowledge in the terminology introduced above). In 2002-2003, she and colleagues conducted ten pilot and forty-one in-depth, semistructured telephone interviews with a sample of researchers drawn from prestigious U.S. universities and representing a diverse range of disciplines, including neuroscience, microbiology, industrial/organizational psychology, sociology, and drug and alcohol research (Kempner, Perlis, and Merz 2005). Those fields were chosen to gauge the range, rather than the prevalence, of experiences with forbidden knowledge. Interviews lasted between thirty-five and forty-five minutes and were audiotaped, transcribed, coded, and analyzed according to the principles of grounded theory (Strauss and Corbin 1990). While many social movements organize around the identification and completion of undone science, others devote themselves to making sure that some kinds of knowledge are never produced. They are not alone. The idea that some knowledge ought to be forbidden is deeply embedded in Western cultures and appears in literature through the ages, from Adam and Eve's expulsion in Genesis to Dr. Frankenstein's struggle with a monster of his own creation (Shattuck 1996). Mertonian rhetoric aside, most people agree that some science poses unacceptable dangers to research subjects or to society at large. The widely accepted Nuremberg Code, for example, places strict limits on human experimentation, in an effort to ensure that some science-such as Nazi human experimentation in World War II-is never done again. Determining which knowledge ought to remain undone can often be contentious, as illustrated by current high-profile public debates surrounding the ethics and implications of stem cell research and cloning technologies. Nevertheless, as in research agenda-setting arenas (Hess 2007), debates and decisions about what knowledge should remain off limits to the scientific community typically occur among elites: legislators and federal agencies perennially issue guidelines and mandates regarding which research should not be conducted, setting limits on everything from reproductive and therapeutic cloning to studies of the psychological effects of Schedule I drugs, like heroin and marijuana. Scientists and the lay public both have limited opportunities to voice their opinion in these discussions. In dramatic cases, scientists have attempted to preempt mandates via self-regulation, as was the case in 1975 when scientists meeting at Asilomar called for a moratorium on certain kinds of recombinant DNA research (Holton and Morrison 1979). According to the forty-one elite researchers interviewed for this case study, these formal mechanisms account for only a portion of the limitations that can produce undone science (Kempner, Perlis, and Merz 2005). More often, researchers described how their research had been hamstrung by informal constraints-the noncodified, tacit rules of what could not be researched or written. Yet researchers were very clear about what constituted \"forbidden knowledge\" in their respective fields. The boundaries of what could not be done had been made known to them when either they or a colleague's work had been targeted for rebukein essence, their work had breached an unwritten rule. The management of forbidden knowledge, thus, worked much as Durkheim said it would: once someone's research had been identified as especially problematic by, for example, a group of activists, their work became a \"cautionary tale,\" warning others \"not to go there\" (Kempner, Bosk, and Merz 2008). In this way, social movement organizations and activists are able to play an important role in debates about what ought to remain undone, whether or not they are invited to the table. Besides their influence on shaping research agenda-setting arenas, social movements can and do influence individual researchers' decisions not to pursue particular types of studies. In recent decades, for example, animal rights organizations have had an enormous influence on the kinds of research that scientists choose not to produce. We found that the researchers in our sample who work with animal models took seriously the threat posed by those organizations. They spoke of \"terrorist-type attacks\" and told stories of colleagues who received \"razor blades in envelopes\" and \"threatening letters.\" Others faced activists who staked out at their houses. Researchers learned from these cautionary tales and, in many cases, said that they had self-censored as a result. One researcher, for example, explained that he would not work with primates-only \"lower order\" animals like mice and drosophilia because: I would like to lunatic-proof my life as much as possible ‚Ä¶ I, for one, do not want to do work that would attract the particular attention of terrorists ‚Ä¶ The paranoia was acute. One researcher refused to talk to the interviewer until she proved her institutional affiliation: \"For all I know, you are somebody from an animal rights organization, and you're trying to find out whatever you can before you come and storm the place.\" Over time, the overt interventions of animal rights organizations in the production of research have redefined the ethics of animal research, ushering in legislation like the Animal Welfare Act of 1985, which requires research institutions that receive federal funding to maintain \"Institutional Animal Care and Use Committees\" (Jasper and Nelkin 1992). However, lay groups do not need to use such directly confrontational tactics to influence researchers' decisions, especially if the groups are successful in their attempts to reframe a particular social problem. For example, substance abuse researchers argued that their research agendas were limited by the success of the Alcoholics Anonymous' campaign to define treatment for alcoholism as lifelong abstinence from drink. Although these researchers would like to conduct \"controlled drinking\" trials, in which alcoholics are taught to drink in moderation, they argued that \"There's a strong political segment of the population in the United States that without understanding the issues just considers the goal of controlled alcohol abuse to be totally taboo.\" The mere threat of interference from the grassroots was enough to keep many researchers from conducting certain studies. Several drug and alcohol researchers described great unwillingness to conduct studies on the health benefits of \"harm reduction\" programs, such as those that distribute free condoms in schools or clean needles in neighborhoods, because they might attract unwanted controversy from lay groups who oppose such public health interventions. Thus, in some contrast to the role that social movement organizations and lay experts/citizen scientists play in exposing undone science and encouraging knowledge creation in chemical, air monitoring, and breast cancer research, this study shows that the same actors can also play a powerful role in determining which knowledge is not produced. Moreover, conflict over the direction of funding streams, while critically important to the political of research agenda setting, do not solely determine what science is left undone. Rather, social movements are also effective beyond research agenda-setting processes that occur at the institutional level; this study provides evidence that they also shape the microlevel interactional cues and decision-making process of individual scientists. Although more research is needed to understand the circumstances under which researchers decide to selfcensor in response to pressure from outside groups, this case suggests that social movements may have much greater potential to thwart research than originally thought. The implications are intriguing and deserve greater attention. On one hand, disempowered groups may leverage these techniques to gain a voice in a system of knowledge from which they are typically excluded. On the other hand, it is troubling to learn that the subsequent \"chilling effect\" happens privately, often without public discussion and in response to intimidation and fear.",
          "The diverse cases provide an empirical basis for moving forward the theoretical conceptualization of undone science in relation to a new political sociology of science and that program's concern with how research agendas are established. Perhaps the most significant general observation is that the identification of undone science is part of a broader politics of knowledge, wherein multiple and competing groups-including academic scientists, government funders, industry, and civil society organizations-struggle over the construction and implementation of alternative research agendas. To a large extent, our case studies focus on attempts by civil society or quasigovernmental organizations to identify areas of research they feel should be targeted for more research. However, the identification of undone science can also involve claims about which lines of inquiry should warrant less attention than they currently receive, either because there are decreasing social returns on continued investments in heavily researched areas or because the knowledge is deemed not worth exploring and possibly dangerous or socially harmful-what Gross (2007) calls \"negative knowledge.\" Examples of the latter include the research programs and methods targeted by animal rights groups and research on chlorinated chemicals targeted by Greenpeace. There are many other cases that would fit this role for civil society organizations, including calls for research moratoria on weapons development, genetically modified food, nuclear energy, and nanotechnology. Five more specific insights follow from and add complexity to this general observation. First, while we see undone science as unfolding through conflict among actors positioned within a multiorganizational field, as Gibbons' case shows, definitions of undone science may also vary significantly within different organizational actors, coalitions, or social movements. Some portions of the movement may be captured by mainstream research, and consequently advocacy is channeled into support for the experts' prioritizations of research agendas. Thus, a research topic such as environmental etiologies of breast cancer may represent undone science to a marginalized segment of breast cancer advocates and their allies in the scientific community, but it may represent negative knowledge to the majority of breast cancer advocates and the dominant cancer research networks. To further complicate the picture, rapid developments and changes within the scientific field, such as the development of genomic research to better pinpoint environmental or epigenetic factors, may result in shifts in research priorities that can open up opportunities for research in areas of undone science. Here, one sees that internal changes and differences among both researchers and civil society advocates interact to define shifting coalitions of research priorities. Second, the dynamic nature of coalitions and alliances that emerge around undone science suggests that the articulation of research priorities is often a relatively fluid process; even when civil society groups target some areas of scientific research as deserving low or no priority, their views may in turn lead to the identification of other areas of research deserving higher priority. For example, the position of an animal rights group may begin with opposition to some types of animal research but lead to support for more \"humane\" forms of animal research that have been reviewed by animal research committees. Likewise, the position of an organization such as Greenpeace in opposition to chlorinated chemicals is linked to an articulation of the need for research on green chemistry alternatives. As these examples suggest, the identification of undone science can be viewed as multifaceted outcomes of coalitions and conflict among diverse groups representing various social categories, each promoting a mix of topics seen as deserving more, less, or no attention from the scientific community. Third, making sense of the complex processes that produce undone science involves attending to the distributions of power, resources, and opportunities that structure agenda setting within the scientific field. An important element of field structure is the role of regulatory regimes in shaping definitional conflicts over research priorities. Howard's work suggests that done and undone environmental science dyads can be a key expression of the regulatory paradigm in which they occur and intimately linked to the way expertise is conceptualized and deployed in the paradigm. Furthermore, he proposes that until mainstream science faces a challenger, important forms of undone science within the dominant paradigm can remain implicit and unarticulated. In other words, undone science may take the form of a latent scientific potential that is suppressed through \"mobilization of bias\" (Lukes 2005; see also Frickel and Vincent 2007). Ottinger (2005) also notes the important role of regulatory standards in defining opportunities for activists who attempt to get undone science done largely using their own resources. In the case of air monitoring devices, an alternative research protocol and data gathering device operated by laypeople provides a basis for challenging official assurances of air quality safety. Rather than advocate for shifts in a research agenda, they simply enact the shift. In Howard's terms, the lay research projects also dramatize the implicit and unarticulated bias in the dominant method of air quality monitoring. Ottinger's (2005) focus on the double role of standards as enabling and constraining factors in establishing both the conditions and limitations of undone science is intriguing, and it remains for future research to examine the efficacy of tactical dynamics in relation to structural constraints encountered across a range of regulatory and research contexts. Fourth, while access to financial resources is an implicit focus of efforts to identify undone science, Kempner's research demonstrates that the interaction of civil society and research priorities is not restricted to the broad issue of funding. Although civil society organizations can exert an effect on research funding allocations, as we have seen especially in environmental and health research priorities, Kempner notes that there are other mechanisms that can cause such shifts. Her work suggests that efforts to study the problem of undone science should also consider the role that a moral economy has in shaping scientists' decisions about what research programs they will and will not pursue (Thompson 1971; on moral economy in science, see Kohler 1994). Furthermore, even if scientists do not accept in principle the notion that certain knowledge should remain undone, they may simply decide not to invest in some areas of research because of intense direct pressures from civil society organizations such as animal rights groups. As a result of individual decisions not to engage in an area of research, changes in the research agendas of a field can occur even when funding is not shifting dramatically. Finally, sometimes structural constraints such as limited access to resources coincide with practical constraints to produce \"undoable science.\" In the case of the chlorine sunset provisions, precaution advocates see governmental programs for screening individual chemicals as obscuring a plain fact: the sheer number of chemicals and their complex interaction with ecological and biological systems make it impossible to predict whether a given concentration of a given chemical will in any meaningful sense be \"safe\" or whether it will be a risk. As a result of this \"wicked problem\" (Rittel and Weber 1973), the articulation of undone science as a goal for research prioritization and funding-in this case, the standard assumption of a need for ever more research on the environmental, health, and safety implications of new chemicals-turns against itself, because the call for research into specific chemicals tacitly supports a regulatory framework that systematically generates a policy failure (see Beck 1995).",
          "This study demonstrates some of the ways in which the analysis of undone science can enrich empirical understandings of research agenda-setting processes. The considerable variation we find in just four cases suggests that one promising avenue for future research lies in developing more systematic comparisons across academic, government, industry, and community settings. Doing so will further elaborate the ways in which the institutional contexts of research-including different sets of political and economic pressures, normative expectations, resource concentrations, and sizes and configurations of research networksshape the articulation of undone science and the successful or failed implementation of alternative research agendas. Our broader aim in seeking to give undone science higher visibility within STS is to broaden the foundations for a new political sociology of science. Much like feminist and antiracist science studies, the political sociology of science situates questions relating to the uneven distribution of power and resources in science at the center of the STS project while remaining attentive to how knowledge and its inverse-ignorance-is socially shaped, constructed, and contested. As we have argued here, one of the crucial sites where questions of power, knowledge, and ignorance come together is in the domain of research agenda setting, where intense coalitions and conflicts are forged to gain access to the limited resources that ultimately shape what science is done and what remains undone. ",
          ""
        ],
        "ground_truth_definitions": {
          "undone science": {
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.",
            "context": "What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "09c660d438e8b372f3e77504c069255b85c00256",
        "sections": [
          "According to CLT, then, people traverse different psychological distances by using similar mental construal processes. Because the various distances have the same egocentric reference point, they should all be cognitively related to each other and similarly affect and be affected by level of construal. As psychological distance increases, construals would become more abstract, and as level of abstraction increases, so too would the psychological distances people envisage. Construal levels thus expand and contract one's mental horizon. The different distances should also similarly influence prediction, evaluation, and action, inasmuch as these outcomes are mediated by construal. The present article builds upon our earlier work on temporal construal theory, which focused in particular on the way that temporal distance from future events influences representation and judgment (Liberman & Trope, 1998;Trope & Liberman, 2003). Going beyond this earlier theory, we now treat temporal construal theory as a special case of a general theory of psychological distance. At the core of the proposed theory is a functional approach to construal levels, according to which mental construal processes serve to traverse psychological distances and switch between proximal and distal perspectives on objects. We describe the properties of the mental construal processes that enable them to fulfill this function and further explicate our approach by relating it to extant theories of how people respond to distant objects. Some of these ideas were presented in earlier literature reviews and book chapters (Liberman & Trope, 2008;Liberman, Trope, & Stephan, 2007;Trope, Liberman, & Wakslak, 2007). The present article presents a more advanced and comprehensive formulation of the theory and examination of related theories and research. The article consists of three main parts. In the first part, we present the basic assumptions of CLT. We explain what we mean by construal levels and why they are related to psychological distance (Section I) and examine the cognitive relationships among the four dimensions of psychological distance (Section II) and their bidirectional relationship to level of construal (Section III). In the second part, we turn to the construal-mediated consequences of psychological distance for prediction, preference, and self-regulation (Sections IV-VI). In the third part, we address open questions about psychological distance and discuss new directions for future research (Sections VII-IX).",
          "In the following three sections, we examine the basic assumptions of CLT regarding psychological distance, construal level, and the relationship between the two.",
          "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental. For example, by moving from representing an object as a \"cellular phone\" to representing it as \"a communication device,\" we omit information about size; moving from representing an activity as \"playing ball\" to representing it as \"having fun,\" we omit the ball. Concrete representations typically lend themselves to multiple abstractions. For example, a cellular phone could be construed also as a \"small object,\" and \"playing ball\" could be construed as \"exercising.\" An abstract representation is selected according to its relevance to one's goals. Thus, if one's goal is to contact a friend, then \"a communication device\" is relevant, but size is not. From the perspective of that goal, the cell phone could be replaced by (i.e., would be conceptually close to) a desktop computer with an Internet connection. If, however, one's goal is to pickpocket a valuable object, then size is a relevant attribute, and function is of less relevance. Given this goal, the cell phone could be replaced by (i.e., would be conceptually close to) a wallet. Like irrelevant details, details that are inconsistent with the chosen abstract representation are omitted from the representation or assimilated into it. For example, the detail that the ball game is slow would be omitted or modified once the activity is represented as \"exercising.\" Because abstract representations necessarily impose one of many alternative interpretations, and because irrelevant or inconsistent details are omitted or assimilated to it, these representations tend to be simpler, less ambiguous, more coherent, more schematic, and more prototypical than concrete representations (Fiske & Taylor, 1991, p. 98; E. R. Smith, 1998). It is important to note, however, that higher level construals are not simply more impoverished or vague than lower level construals. They often convey additional information about the value of the stimulus and its relations to other stimuli. For example, \"having fun\" entails many characteristics that are not an obvious part of \"playing ball outside\" (e.g., valence) and places the activity within a broader context by specifying its relations to other concepts (e.g., a party). Thus, the process of abstraction involves not only a loss of specific, idiosyncratic, and incidental information, but also ascription of new meaning deduced from stored knowledge and organized in structured representations. There are multiple levels of abstractness, as one could construct increasingly more inclusive categories of objects (e.g., poodle, dog, a mammal). Actions form hierarchies, too, as goals could be translated into more abstract, superordinate goals (Carver & Scheier, 2000;Miller, Galanter, & Pribram, 1960;Vallacher & Wegner, 1987). In such hierarchies, each action (e.g., study for an exam) has a superordinate, abstract level, which answers the question of why the action is performed (e.g., do well) and a subordinate, concrete level, which provides the details of how the action is to be performed (e.g., read a textbook). Higher levels of abstractness contain less concrete details about the specific type of action performed, the objects it involves, and its immediate context and more information about the general meaning and the valence of the action (Semin & Fiedler, 1988;Trope, 1986Trope, , 1989)). On the basis of this conceptualization, we posit two related criteria that can be used to distinguish which features of an item or an event are more high level and which are more low level (i.e., which features will be increasingly captured at a higher vs. lower level of construal). The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is. The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. The topic of the lecture, on the other hand, would be important regardless of the location's convenience. In this sense, details about location are subordinated to details about topic, and thus constitute a lower level of construal. Construal and distance-CLT contends that people use increasingly higher levels of construal to represent an object as the psychological distance from the object increases. This is because high-level construals are more likely than low-level construals to remain unchanged as one gets closer to an object or farther away from it. For example, the higher level goal to contact a friend is more stable over time than the more concrete goal to send her an e-mail, because an Internet connection might be unavailable when one is actually trying to contact the friend. From a temporally distant perspective, it is therefore more useful to construe this action in terms of the high-level goal rather than the low-level goal. The same holds true for the other distances. Thus, abstract categories tend to change less across social distance. For example, more people use communication devices than cell phones, and therefore, the former construal is more useful for relating to socially distant individuals. It is critical that, although we believe that these functional relationships underlie the association between construal and distance, we propose that the effects of distance on construal and of construal on distance are overgeneralized, making them persist even when the initial reasons that gave rise to the association are no longer present. That is, a large distance from an object would activate a high-level construal of the object even when lowlevel details are not particularly likely to change over distance, and high level of construal of an object would produce a sense of distance from the object even when such construal does not afford retrieval of particularly distal exemplars of the object. For example, students may know the room in which a guest lecture will take place well in advance and be quite certain that it will not change. Nevertheless, CLT predicts that these students will ignore the quality of the lecture room (a low-level detail) when it is temporally distant, when it takes place in a remote location, when it is planned for another person, and when it is unlikely to take place. Finally, it is worth emphasizing that although psychological distance and construal levels are related, they are not the same. Psychological distance refers to the perception of when an event occurs, where it occurs, to whom it occurs, and whether it occurs. Construal levels refer to the perception of what will occur: the processes that give rise to the representation of the event itself. Thus, psychological distance from an event should be more closely related to the spatiotemporal distance of the event from the self than to its inherent properties, whereas the construal of the event should be more closely related to its inherent properties than to its spatiotemporal distance from the self. Below, we develop more fully the assumption that the different psychological distances are interrelated (Section II) and that each distance affects and is affected by level of construal (Section III).",
          "We contend that the propensity to complete the sentence \"a long time ago, in a ____ place\" with \"far away\" rather than with \"nearby\" reflects not only a literary convention but also an automatic tendency of the mind. Indeed, people use spatial metaphors to represent time in everyday language and reasoning (Boroditsky, 2007). In social psychology, spatial distance is often used to measure social distance. For example, choosing a more distant seat from another person is taken to reflect social distancing from that person (e.g., Macrae, Bodenhausen, Milne, & Jetten, 1994;Mooney, Cohn, & Swift, 1992). More generally, if a common dimension of psychological distance underlies the various distance dimensions we have enumerated, then these distance dimensions should be mentally associated. For example, remote locations should bring to mind the distant rather than the near future, other people rather than oneself, and unlikely rather than likely events. In an investigation of this hypothesis, Bar-Anan, Liberman, Trope, and Algom (2007) used a picture-word Stroop task (Stroop, 1935) to examine the cognitive interrelations among psychological distances. Participants viewed landscape photographs containing an arrow that was pointing to either a proximal or a distal point in the landscape (see Figure 1). Each arrow contained a word denoting either psychological proximity (e.g., \"tomorrow,\" \"we,\" \"sure\") or psychological distance (e.g., \"year,\" \"others,\" \"maybe\"). Participants' task was to respond by pressing one of two keys as quickly and as accurately as possible. In one version of the task, they had to indicate whether the arrow pointed to a proximal or distal location. In another version, they had to identify the word printed in the arrow. In both versions, participants responded faster to distance-congruent stimuli (in which a spatially distant arrow contained a word that denoted temporal distance, social distance, or low likelihood, or a spatially proximal arrow contained a word that denoted temporal proximity, social proximity, or high likelihood) than to distance-incongruent stimuli (in which a spatially distal arrow contained a word denoting proximity, or a spatially proximal arrow contained a word denoting distance). These findings suggest that spatial distance, temporal distance, social distance, and hypotheticality have a common meaning and that people access this common meaning automatically, even when it is not directly related to their current goals. We think that the common meaning is psychological distance and that it is automatically assessed because of its important implications for the perceiver. For example, on a rainy day, it matters whether an umbrella one notices belongs to a friend or to a stranger (social distance); in the jungle, it is important whether a tiger is real or imaginary (hypotheticality); in making a financial investment, it is important whether a recession is anticipated in the near or distant future (temporal distance), here or somewhere else (spatial distance). Yet, unlike valence, distance is not an inherent aspect of the semantic meaning of objects. Umbrellas, tigers, and recessions are inherently good or bad, but they are not inherently proximal or distal. It is perhaps for this reason that distance has not been included among the basic dimensions of meaning: evaluation, potency, and activity (Osgood & Suci, 1955). The idea that distance dimensions are automatically associated further suggests that the distance of a stimulus on one dimension may affect its perceived distance on other dimensions. In this spirit, research by Stephan, Liberman, and Trope (2010) has investigated how social distance affects and is affected by spatial distance and temporal distance. This line of research was based on past work showing that polite language signifies and creates interpersonal distance: People address strangers more politely than they address friends, and the use of polite, formal language creates a sense of distance (Brown & Levinson, 1987). Consistent with this analysis, Stephan et al. found that the use of normative, polite language rather than colloquial, less polite language led participants to believe that the target of the communication was spatially and temporally more distant. For example, using normative rather than colloquial language to address a person (e.g., \"My brother is taking our family car, so the rest of us will stay at home\" vs. \"‚Ä¶ will be stuck at home\") led participants to infer that the addressee was in a more remote location and that the conversation referred to an event in the more distant future. Another set of studies by Stephan et al. (2010) found evidence for the reverse direction of influence, namely, an effect of spatial and temporal distance from the target of communication on the use of polite language. In one study, participants wrote instructions for a person who was expected to read them either in the near future or the distant future (e.g., participants wrote sightseeing suggestions for a tourist that was supposed to arrive on the following day or a year later). The results showed that participants preferred to phrase instructions more politely when they were addressed to the distant future than to near future tourist. Similar findings were found with addressing a spatially proximal versus distal stranger. Williams and Bargh (2008, Study 4) showed, in a similar vein, that participants who were primed with spatial distance (relative to proximity) by marking close (vs. distant) points on a Cartesian plan subsequently reported greater social distances between themselves and members of their family and their hometown. Finally, Wakslak and Trope (2008) showed that hypotheticality also affects the perception of other distances, such that people expect unlikely events (which, in terms of CLT are distant on the dimension of hypotheticality), compared with likely events, to occur in situations that are relatively more distant in time, space, and social distance. For example, a rare cat blood type was expected to be found in cats in spatially remote rather than nearby places, whereas a common cat blood type was expected in a near rather than a remote location. Presumably, people map probability onto other distance dimensions and therefore expect improbable events to happen to distant others in distant places and times. Discussion-Recent research shows that the different psychological distance dimensions are associated and suggests that psychological distance is an aspect of meaning, common to spatial distance, temporal distance, social distance, and hypotheticality. This research further shows that cues of distance on one dimension affect the perceived distance of objects on other dimensions. For example, the spatial distance from an event may depend not only on its location relative to that of the perceiver but also on whether it is expected in the near or distant future, whether it occurred recently or a long time ago, whether it is expected to be experienced by oneself or another person, and whether it is probable or improbable. In this respect, the different psychological distances may be to some extent interchangeable (see Pronin, Olivola, & Kennedy, 2008). Our research has only started to explore the relationships among the various distance dimensions. Below we consider further questions about more complex relationships among the psychological distances than those we have identified thus far. The psychophysics of distance: How do objective distances in time, space, social target, and hypotheticality map onto generalized psychological distance? It seems reasonable to expect a relatively high sensitivity to changes in objective distance from the self here and now at the proximal end of the continuum. As distance increases, changes in objective distance might produce smaller corresponding changes in psychological distance. As vividly illustrated by Steinberg's painting View of the World from the 9th Avenue, the distance between 9th Avenue and 10th Avenue in Manhattan seems to Steinberg greater than the distance between New York City and Chicago. In other words, consistent with the Weber-Fechner law, psychological distance from an object might follow a concave, logarithmic function rather than a linear function. Recent research by Zauberman, Kim, Malkoc, and Bettman (2009) provides evidence showing that such a function fits subjective estimates of temporal distance from future events. Likewise, research on estimates of spatial distance uncovered increased sensitivity around an egocentric reference point (e.g., Holyoak & Mah, 1982). Investigating the functions that relate temporal distance, prospective and retrospective, as well as other distances, to psychological distance remains an important direction for future research. Another important but unexplored question is how different distances combine to affect psychological distance. One interesting possibility is that the Weber-Fechner law may apply across different psychological distance dimensions. That is, changes in distance of an object from the self on one dimension would have a greater impact on psychological distance when that object is proximal on another dimension than when it is distant on that other dimension. For example, the same changes in spatial distance of an object from the self might have a stronger impact on the psychological distance of that object when the changes are expected in the near future rather than in the distant future. To use Steinberg's drawing, this artist might think of a meeting as coming up sooner if he plans to have the meeting in Manhattan than in Chicago.",
          "According to CLT, different objects are mentally associated to the extent that they are psychologically equidistant from the self. In principle, the associations among proximal objects could be as strong as those among distal objects. However, because psychological distance is egocentric, anchored at the self here and now, objects at the proximal end may have more in common than those that are removed from that end. There is only one me and many others, and therefore, the things that are me and mine are more similar to each other than the things that are not me and mine. Only one place in space is \"here,\" but there are many places that are far away, and therefore, things that are here are more related to each other than things that are far away. Similarly, there is likely less variation among real events than possible events. The greater commonality among proximal objects than distal objects might give rise to stronger interrelationships among the former than the latter.",
          "Although we suggest that different types of distance are related, we do not wish to suggest that they are the same. It is possible that some distance dimensions are more basic or influential than others. For example, Boroditsky has recently proposed that spatial distance is primary and that temporal distance is understood as an analogy to it (Boroditsky, 2000(Boroditsky, , 2001;;Boroditsky & Ramscar, 2002). In support of this proposal, she showed that thinking about space before answering questions about time influenced the participants' responses, but thinking about time before answering questions about space did not affect participants' responses (Boroditsky, 2000, Experiments 2 and 3). In another series of studies, she presented participants with lines on a computer screen and showed that judgments of exposure duration were affected by the lines' length whereas judged length was not affected by duration of exposure (Casasanto & Boroditsky, 2008). Spatial distance may be more basic than temporal distance or other distances in that it is learned earlier, more clearly detected, less ambiguous, or easier to communicate about. It is possible, however, that the distinction between self and not-self, which underlies social distance and perspective taking (e.g., Pronin, 2008), might be a core psychological distance that exerts a particularly strong influence on the activation and perception of other distances. Perhaps hypotheticality, the distinction between real and imagined objects and between probable and improbable events, is least prominent and acquired at an older age, compared to other distance dimensions. The various distance dimensions may differ in other respects. Time is unidimensional and uncontrollable. We incessantly travel from the past to the future and have no control over time. Spatial distance has three dimensions, is relatively stable, and can be controlled by moving closer or farther away from things at wish. Social distance is only partly controllable (e.g., we can try to get closer to a person but success is not entirely dependent on our actions). Hypotheticality is also only partly controllable, because we cannot always change the probabilities of events. Another important difference among the distances is their relation to valence. Whereas social distance decreases positivity (e.g., in groups are perceived as more positive than out groups), temporal distance typically increases positivity (people are more positive about the more distant future). Exploring the psychological consequences of these differences in controllability, dimensionality, and valence is a fruitful direction for future research.",
          "The basic premise of CLT is that distance is linked to level of mental construal, such that more distant objects will be construed at a higher level, and high-level construal will bring to mind more distant objects. It may seem intuitive that from a distance we see the forest, and as we get closer, we see the trees. It may also seem intuitive that, to see the forest rather than individual trees, we need to step back. These effects should apply, however, not only to spatial distance but also to other distances, and not only to visual input, where it might seem a natural constraint of our perceptual system, but also to conceptual abstraction. We do not literally see either tomorrow or next year. Yet we may think about tomorrow in terms of trees and about next year in terms of the forest. We do not literally take a step back to forget the daily hassles and consider our life in general. Yet, when thinking of the general aspects of our life rather than about daily details, we may find ourselves looking farther into space. Evidence for these associations between distance and construal has been found at the level of both implicit associations and explicit judgments and decisions. Bar-Anan, Liberman, and Trope (2006), for example, examined the associations between level of construal and psychological distance using an Implicit Association Test. Similar to other assessments of implicit associations using this test (see Greenwald, McGhee, & Schwartz, 1998), participants in these studies were presented with stimuli from four categories: stimuli pertaining to high-level construal (e.g., category names such as \"drinks\"), stimuli pertaining to low-level construal (e.g., exemplar names such as \"coke\"), stimuli pertaining to low psychological distance (e.g., the word \"ours\" or the word \"friend\" for the social distance), and stimuli pertaining to high psychological distance (e.g., the word \"theirs\" or the word \"stranger\"). In CLT-congruent trials, high-level stimuli were paired with distant stimuli and low-level stimuli were paired with proximal stimuli, whereas on CLT-incongruent trials, high-level stimuli were paired with proximal stimuli and low-level stimuli were paired with distal stimuli. With all four dimensions of psychological distance-temporal distance, spatial distance, social distance, and hypotheticality-the participants were faster with congruent than with incongruent pairings, indicating that participants implicitly associate psychological distance with high-level construal and psychological proximity with low-level construal. This set of studies suggests that the association between psychological distance and construal level can be activated automatically without conscious deliberation. Beyond mere association, the reciprocal effects of distance on construal and of construal on distance have important implications for perception, categorization, action identification, and person perception. In the following, we discuss those implications. Visual Perception-The distinction between the (high-level) gestalt and (low-level) constituent details, such as the proverbial distinction between the trees and the forest, readily applies to visual perception. In a widely used task (Navon, 1977), participants are presented with global letters that are made of local letters (e.g., a large L made of 20 small Hs), and are asked to indicate whether a target letter appears on the screen (e.g., \"Is there an H on the screen?\"). The target letter can match a global letter or a local letter. Would psychological distance facilitate attending to global letters (e.g., the \"forest\") and impair attending to local letters (e.g., the \"trees\")? Would attending to global letters, as opposed to local letters, enhance perceived distance? Liberman and F√∂rster's (2009 Liberman and F√∂rster's (in press) studies suggest affirmative answers to both questions. In one of the studies (Liberman & F√∂rster, in press), they primed participants with temporal distance or temporal proximity by making them write essays about their lives tomorrow or their lives on a day a year later and imagine taking an attention test on that day. Navon's (1977) task was then presented. Relative to a control group, in which participants did not write an essay, temporal distance facilitated processing of global letters and impaired processing of local letters, whereas temporal proximity produced the opposite effect. Similar effects were found for priming of spatial distance and social distance. To examine the reverse direction of influence, namely, that of global perception on estimated psychological distance, Liberman and F√∂rster (2009) procedurally primed participants with either global or local perceptual processing, using a variation of Navon's (1977) task. In the global priming condition, the target letters were always global, and in the local priming condition, the target letters were always local, whereas in the control condition, the targets were global in half of the trials and local in the other half. Relative to the control condition, global processing led to greater estimates of temporal distance, spatial distance, social distance, and hypotheticality. Local processing had the opposite effect. For example, participants who were primed with global processing estimated temporal distance to a dental visit as longer and spatial distance between themselves and a designated point in the room as larger than participants primed with local processing. In a related vein, Wakslak and Trope (2009) found that priming global processing (vs. local processing) through the Navon task led participants to assign lower probability to a variety of everyday life occurrences. It is important to note that, given that psychological distance is egocentric, as CLT contends, if the distance between two objects is not mentally represented as social distance from oneself, temporal distance from now, or spatial distance from here, it should not necessarily depend on level of construal. Consistent with this prediction, in Liberman and F√∂rster's (2009) studies, primed level of construal affected estimated spatial distance between the participant and a sticker in the room, but it did not affect the estimated spatial distance between the experimenter and a marked desk in the room. Likewise, level of construal did not affect estimates of temporal distance that were not anchored on now (e.g., \"How much time after receiving an invitation would you go to the dentist?\"). High-level construal of visual information often entails abstraction of coherent images from fragmented visual input. An example is the Gestalt Completion Task (see Figure 2; Street, 1931; see also Ekstrom, French, Harman, & Dermen, 1976), where performance depends on detecting the global pattern and where attending to details interferes with performance. In a series of studies, participants completed what they believed to be sample items of the Gestalt Completion Task, supposedly as a practice version before they perform the actual task. Participants' performance improved when they anticipated working on the actual task in the more distant future (F√∂rster, Friedman, & Liberman, 2004), when they thought the actual task was less likely to take place (Wakslak, Trope, Liberman, & Alony, 2006), and when social distance was enhanced by priming of high social status (P. K. Smith & Trope, 2006). A psychologically distant perspective thus seems to enable people to better see the gestalt. Whereas distance improves the ability to perceive the gestalt in a visual array, it should have the opposite effect when the task requires attention to details. Distance should therefore have a detrimental effect on the ability to identify a missing low-level, local element within a coherent whole (e.g., a missing hand on a watch, a missing handle on a drawer chest). Wakslak et al. (2006) used the picture completion subtest of the Wechsler Intelligence Scale for Children (Wechsler, 1991) to test this prediction. As expected, participants did worse on sample items of this task when they believed they were less likely to later complete it. Pictures and Words-Pictures are concrete representations that bear physical resemblance to the referent objects, whereas words are abstract representations that carry the essence of that object (Amit, Algom, & Trope, 2009;Amit, Algom, Trope, & Liberman, 2008). Words therefore comprise a higher level of construal than do pictures. Supporting this assumption, Amit, Algom, and Trope (2009) found that participants classified items into more groups when the items were presented in pictures than when they were presented in words. In another study, they presented spatially, temporally, or socially near or distant items in either a pictorial or verbal format (see Figure 3). Participants' task was speeded identification of the object. For example, in an experiment on spatial distance, words or pictures were presented against a background that created an illusion of depth. As expected, participants responded faster to pictures of objects when they were made to seem spatially near than spatially distant, but they responded faster to words denoting those objects when they were made to seem spatially distant than spatially near. To manipulate temporal distance, they used words and pictures of modern objects and ancient objects (e.g., a cart and a car), and to manipulate social distance they used words and pictures of domestic and foreign objects (e.g., a coin of domestic and foreign currency). Responses were faster when pictures represented psychologically proximal objects and words represented psychologically distal objects than vice versa. It seems, then, that processing is most efficient when there is a congruency between the portrayed distance and the presentation medium. Moreover, using similar stimuli in free recall experiments, Amit, Trope, and Algom (2009) demonstrated better memory for proximal than for distal stimuli when the targets were represented in pictures but that it was better for distal than for proximal stimuli when the targets were represented in words. These findings were obtained with temporal, spatial, and social distances. Categorization-If the distant future is represented more abstractly, then individuals should use broader categories to classify objects for distant situations than for proximal situations. In support of this prediction, research has shown that people group objects into fewer, broader categories when they imagine the objects in a distant future scenario than a near future scenario (Liberman, Sagristano, & Trope, 2002, Study 1) or in an unlikely scenario than a likely scenario (Wakslak et al., 2006). Consistent with research on categorization, research on segmentation of ongoing events has found that people chunk behavior sequences into broader segments when the behavior is distal rather than proximal. For example, Henderson, Fujita, Trope, and Liberman (2006) asked New York University participants to partition an ongoing behavioral sequence into as many sections as they deemed appropriate. Participants viewed an animated film developed by Heider and Simmel (1944) that shows two triangles and a circle moving against and around each other and were told that the film depicted the action of three teenagers around a cabin at a well-known summer camp. In the spatially near condition, the camp was said to be located on the East Coast; in the spatially distant condition, the camp was said to be located on the West Coast. As expected, participants created fewer, broader sections out of the video when they believed the campers it depicted were in a spatially distant, as opposed to a spatially near, location. Similar effects were found for events that were described as less likely versus more likely (Wakslak et al., 2006). Do narrow, specific categories promote a sense of psychological proximity? A recent series of studies by Wakslak and Trope (2009) manipulated level of categorization in several distinct ways and found the predicted effect on event likelihoods. In one study, construal level was primed by asking participants to generate either superordinate categories or subordinate exemplars of 40 objects (e.g., table, sport, book). Next, participants completed a supposedly unrelated questionnaire where they made a series of probability judgments. As expected, participants who had been primed to have a high-level construal mindset indicated that the events were less likely to occur compared to those who had been primed to have a low-level construal mindset. In another study, participants compared either alignable or unalignable objects. In the alignable condition, participants compared two digital cameras, with information provided about the same seven features for each camera (e.g., battery life, digital zoom). In the nonalignable condition, participants were provided with information about seven different features for each camera (e.g., battery life for the digital camera; focus for the traditional camera). It was assumed that individuals who are asked to compare alternatives with nonalignable features would seek to make the attributes comparable by representing the alternatives at increasingly higher levels of abstraction (see M. D. Johnson, 1984;Malkoc, Zauberman, & Bettman, 2008). As expected, participants cued to think more abstractly (by having compared products with nonalignable attributes) judged an event involving the products as being less likely to occur than those cued to think more concretely (by having compared products with alignable attributes). Action Identification-Actions, like objects, may be construed in high-level terms, which link them to a superordinate purpose (why one performs them), or in low-level terms, which link them to subordinate means (how one performs them). Here, too, greater psychological distance promotes higher levels of construal (Liberman & Trope, 1998). In one of the studies, participants tended to describe more distant future activities (e.g., studying) in highlevel terms (e.g., \"doing well in school\") rather than in low-level terms (e.g., \"reading a textbook\"). Similar effects emerged when actions were to take place in a spatially distant location (Fujita, Henderson, Eng, Trope, & Liberman, 2006), when the actions were framed as unlikely to actually take place (Wakslak et al., 2006), and when the actor was dissimilar to the perceiver (Liviatan, Trope, & Liberman, 2008). If high-level construals serve to represent psychologically distant events, then activating high-level construals should lead people to think of events in psychologically more distant situations. Indeed, research shows that thinking about an activity in high-level, \"why\" terms rather than low-level, \"how\" terms leads people to think of the activity as taking place in more distant points in time (Liberman, Trope, Macrae, & Sherman, 2007;McCrae, Liberman, Trope, & Sherman, 2008) and of the actor as more socially distant (Stephan et al., 2010). Action-identification theory (Vallacher & Wegner, 1989) specifies how to determine the superordinate and subordinate levels of categorizing a given action. Semin and Fiedler's (1988) Linguistic Categorization Model assesses the abstractness of the action itself. At the most concrete level are descriptive action verbs (\"lift,\" \"take\"), which are typically directly observable. Interpretative action verbs (\"pretend,\" \"help\") are more abstract, as they involve interpretation and require some knowledge of a context larger than the immediately perceived. State verbs (\"prefer,\" \"understand\") are still more abstract, and adjectives (\"helpful,\" \"aggressive\") are the most abstract category. The Linguistic Categorization Model is therefore a useful tool for examining the relationships between psychological distance and abstractness of action construal. Indeed, various kinds of distance have been found to affect abstractness of language. For example, people were found to use more abstract language when describing another person's actions than their own actions (Semin & Fiedler, 1989; see also Fiedler, Semin, Finkenauer, & Berkel, 1995), when describing spatially distant interactions than spatially near interactions (Fujita, Henderson, et al., 2006), and when instructed to address another person politely than in colloquial language (Stephan et al., 2010). Would the reverse direction of influence hold? That is, would linguistic abstractness affect perceived psychological distance? Semin and Smith (1999, Studies 2 and 3) studied the effect of linguistic abstractness on the temporal distance of recalled events. They provided participants with retrieval cues of varying abstractness and examined the temporal distance of the events they recalled. For example, participants were asked to recall either an occasion on which they helped somebody (i.e., concrete retrieval cue) or an occasion on which they displayed a trait of helpfulness (i.e., abstract retrieval cue). As predicted, an abstract retrieval cue prompted older memories than memories that were prompted by a concrete retrieval cue. Person Perception-A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995;E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995;Semin & Fiedler, 1988;Semin & Smith, 1999). Distance should therefore enhance the correspondence bias. Furthermore, representing a person abstractly in terms of his or her dispositions would create a sense of distance. Naturally, social psychologists researched extensively the relations between social distance and dispositional inference. We first briefly review this literature and then describe comparable effects of other psychological distances.",
          "Considerable evidence for differences in construal between self and other comes from research on the actor-observer effect in attribution (E. E. Jones & Nisbett, 1972; for a review, see Gilbert, 1998). This research shows that a person's view of his or her behavior emphasizes the role of concrete situational factors that operate at the moment of action (e.g., \"I stepped on your toe because the bus was crowded\"), whereas his or her view of other people emphasizes the causal role of stable, general dispositional properties of the actor (\"he stepped on my toe because he is clumsy\"). Self-other differences might be explained as reflecting differences in knowledge (people know more about themselves and the variability of their behavior over situations than about others) and differences in the salience of behaviors versus situations (the latter is more salient from one's own perspective, the former from the observer's perspective). Would similar differences in construal emerge when knowledge is identical for near and distal social targets? Research relating abstractness of memories to the perspective in which they are recalled seems to offer an affirmative answer. It has been shown, for example, that personal memories of behaviors that were recalled from a third-person perspective (e.g., \"try to remember your first day at school, as if you are now watching the kid you were\") rather than from a first-person perspective (\"try to remember your first day at school, as if you are a kid again\") tended to use dispositional (as opposed to situational) terms (Frank & Gilovich, 1989;Nigro & Neisser, 1983). In a similar vein, Libby and Eibach (2002, Study 4) found that imagining performing an activity (e.g., rock climbing, playing drums) from a third-person perspective produced less vivid and rich reports of the activity than imagining the same activity from a first-person perspective. In terms of CLT, this means that a third-person perspective, which imposes more distance than a first-person perspective, induces a higher level of construal. Indeed, Pronin and Ross (2006) showed that taking a third person perspective rather a first-person perspective on one's own behavior promoted attribution of the behavior to personality traits rather than to specific situational factors. Temporal distance and spatial distance: Correspondent inference, the tendency to use high-level dispositional attributions, seems to increase not only with social distance but also with temporal distance. Over time, observers' attributions of an actor's behaviors become more dispositional and less situational (Frank & Gilovich, 1989; see also Funder & Van Ness, 1983;Moore, Sherrod, Liu, & Underwood, 1979;Nigro & Neisser, 1983;Peterson, 1980; but see Burger, 1986). These findings are consistent with memory consolidation, namely, the tendency for concrete details to fade more rapidly than general abstractions, thus making memories of the distant past more abstract than memories of the recent past (Bartlett, 1932;Wyer & Srull, 1986). However, assuming that memory is an active, reconstructive process, CLT proposes that increasing past temporal distance is associated not only with a loss of specific details but also with an increased use of high-level, schematic construals (Ross, 1989). Consistent with this proposal, McDonald and Hirt (1997) showed that, over time, student participants not only forgot a fellow student's grades but also were more likely to infer the grades from that student's general ability, resulting in a more coherent (and dispositional) impression of the target person over time. More recently, research has examined dispositional inferences from near future and distant future behavior (Nussbaum, Trope, & Liberman, 2003, Study 1) and from spatially close or spatially distant behavior (Henderson, Fujita, et al., 2006, Study 2). These studies found that student participants drew stronger corresponding attitude inferences from situationally constrained behavior when the behavior was expected in the distant future (vs. near future) or when it was believed to have occurred in a spatially remote location (vs. a near location). These findings demonstrate that the correspondence bias, the tendency to underweight lowlevel, situational constraints on observed behavior, increases with psychological distance from the behavior. Past research has shown that perceivers often form spontaneous trait inferences (STIs) when they observe trait-implying behaviors of other people. For example, upon reading the sentence, \"The secretary solved the mystery halfway through the book,\" people spontaneously inferred the trait \"clever\" (Winter & Uleman, 1984). Can temporal or spatial distance affect the extent to which abstract traits are spontaneously inferred at an early stage of information processing? Rim, Uleman, and Trope (2008) addressed this question in a series of studies that used rate of false recognition of implied (but not present) traits as a measure of STI. They found that New York University participants led to believe that actors were in a distant location (Florence, Italy), compared with a proximal location (Manhattan, New York), formed more STIs, even though behavioral information about targets was held constant across the two conditions. The same effect was found using temporal distance: Participants who were led to believe that actors were from the relatively distant past (year 1997), compared with the more recent past (year 2007), formed more STIs. Self-inferences: Distance may affect not only the inferences we draw about others but also the inferences we draw about ourselves. Pronin and Ross's (2006) research shows that people more likely view their future and past selves than their present selves in terms of general personality traits. In a related study, Wakslak, Nussbaum, Liberman, and Trope (2008) asked participants to imagine themselves in different situations either in the near future or in the distant future and to indicate the extent to which their behavior in those situations would reflect each of the Big Five personality traits. It was found that in the distant future, compared with the near future, participants expected to exhibit their traits more consistently across situations. Another study by Wakslak et al. showed that participants were faster in judging whether general personality traits describe the way they would be in the distant future than the near future. These findings suggest that the distant future self is represented more in terms of general, decontextualized traits than the near future self. People seem to have a clearer view of their personality characteristics when taking a temporally distant perspective rather than a temporally proximal perspective on the self. Indeed, using Linville's (1985Linville's ( , 1987) ) and Donahue, Robins, Roberts, and John's (1993) measures of self-complexity, Wakslak et al. found that self-descriptions were more structured and less complex when they referred to a distant future self than to a near future self. It would be interesting to examine in future research the effects of other distances on the self-concept. For example, would thinking of oneself in an unlikely situation or in a remote spatial location make one see oneself in a more coherent, schematic way? Is it possible that this effect also occurs while traveling (or even while planning traveling)? Does imagining the perspective of another person, especially an unfamiliar person, increase perceived selfintegrity? In general, the construal of the self, which is often more concrete than the construal of others, may be highly abstract and high level when the self is viewed from a distanced perspective, in remote times, places, imaginary situations, and from a third-person perspective. Discussion-Extensive research has verified that as psychological distance increases, construals become more abstract, and as level of construal increases, so too do perceptions of psychological distance. Much of past research has studied individual distance dimensions and types of construal within disparate theories. Without denying the uniqueness of the different distances and types of construal, CLT provides a unifying framework that integrates these disparate lines of research. It suggests that high-level construals and lowlevel construals serve different cognitive functions. High-level construals have evolved to represent distal objects because, with distance, one needs to conserve the essential, invariant properties of the referent object. In contrast, low-level construals preserve the object in minute detail for immediate use. High-level construals serve to transcend the here and now, whereas low-level construals instantiate the present. In the following, we expand our examination of construal levels by relating them to prospection and mental simulation, inferences of distance, and to heuristic low-effort processing.",
          "The present view is consistent with recent functional approaches to memory and cognition (Barsalou, 1999;Buckner & Carroll, 2007;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). These approaches suggest that a major function of episodic memory is prospection, namely, anticipating the future rather than merely reproducing the past. The theories of constructive simulation (Schacter & Addis, 2007) and embodied cognition (Barsalou, 1999;Niedenthal, Barsalou, Winkielman, Krauth-Gruber, & Ric, 2005;Winkielman, Niedenthal, & Oberman, 2008) specifically suggest that episodic memory enables prospection through a constructive process of mentally simulating future episodes. Such simulations are analogical and multimodal and serve to guide choice and action with respect to the future (Barsalou, 2008). CLT adds to this view in several respects. First, it is possible that simulations vary in level of construal, ranging from multimodal simulations that are rich in contextual detail and resemble the kind of analogical representation identified by embodied cognition researchers to general simulations that retain common elements and omit incidental detail. CLT predicts that the latter, higher level simulations are more likely to be used with increasing distance of the past episodes from which simulations are constructed and of the future target situation to which the simulation is applied. For example, a simulation of a future meeting with a friend is likely to be more abstract (i.e., contain less detail on the tone of her voice and the look of the room in which the meeting would take place) to the extent that it is constructed from meetings held with that friend in the relatively distant past or distant locations and to the extent that the meeting with the friend is expected in the relatively distant future or location. Second, it is possible that as distance increases, prospection is increasingly more likely to be based on amodal symbolic representations. For example, a representation of a more distant future meeting with a friend that works in a high-tech company may refer more to semantic knowledge about layoffs in the high-tech industry and include less detail related to perceptual properties, such as the tone of her voice. The research showing that people switch from pictorial to linguistic representations of objects as distance from the object increases is consistent with this possibility (Amit, Algom, & Trope, 2009). Third, symbolic representations might also differ in abstraction, ranging from broad abstractions (she is a woman) to more narrowly applied knowledge (she is a single mother to four and a high-tech manager who is afraid to lose her job). Fourth, as the research reviewed here suggests, these changes in the nature of mental representation of objects might be induced not only by prospective temporal distance from the object but also by spatial, social, and hypotheticality distances. In sum, according to CLT both analogical simulations and symbolic representations might vary in level of construal. Distance may determine whether an analogical or symbolic representation is constructed and the level of abstractness at which it will be constructed. Inferring distance from construal level: We argued that because high-level construals are broad, they bring to mind more distant instantiations of objects, and because low-level construals are narrow, they bring to mind more proximal instantiations of objects. It is also possible for construal level to affect the psychological distance of objects through metacognitive inferences (N. Schwartz & Clore, 1996). People may interpret their low-level construal of an object as indicating that the object is close and their high-level construal of an object as indicating that the object is distant. This metacognitive inference of distance from construal level might involve a more complex attributional calculus when one or more other distances are known. Specifically, the construal-based inference that an object is distant on any given dimension will be discounted when the object is known to be distant on another dimension. Correspondingly, the construal-based inference that an object is distant on any given dimension will be augmented when the object is known to be proximal on another dimension. For example, one would attribute a detailed construal of a meeting with a friend to a relatively close relationship with that friend when the meeting is known to take place in the distant future rather than the near future. Thus, direct implicit associations among different distance dimensions generally result in positive relationships among those dimensions. However, when inferring distance from construal, adjusting the inference of distance on one dimension for distance on other dimensions may result in a negative relationship among those distances.",
          "Could the relationship between distance and construal be accounted for by assuming that people are less motivated to make an accurate judgment regarding relatively distant objects? Does the shift toward higher level construals with increasing distance reflect a shift toward low-effort heuristic processing (see Petty & Cacioppo, 1984)? We think that extracting the general meaning and invariant characteristics of objects is not necessarily more or less effortful than fleshing out the minute details of the objects. For example, generating a superordinate category for the concept soda (e.g., drinks) does not necessarily differ in these respects from generating exemplars of the concept (e.g., Diet Coke). Indeed, our research has not found any systematic relationship between the effects of distance on construal and measures of involvement or accuracy motivation (e.g., P. K. Smith & Trope, 2006;Wakslak et al., 2006). Three additional sets of findings argue against the idea that distance-driven high-level construals necessarily reflect uneffortful, heuristic processing. First, this idea cannot account for performance findings showing that judgments about distal objects (compared with judgments about proximal objects) are more accurate when a correct response requires high-level construals of the objects (e.g., identifying a pattern) and are less accurate only when the correct response requires low-level construals (e.g., detecting a missing detail; e.g., Liberman & F√∂rster, in press;Wakslak et al., 2006). Second, low involvement and uneffortful processing of distant (vs. proximal) objects may account for the underutilization of low-level information about distant objects, but it cannot account for the underutilization of high-level information about proximal objects (e.g., Liberman & Trope, 1998;Nussbaum, Liberman, & Trope, 2006). Third, Fujita, Eyal, Chaiken, Trope, and Liberman (2008) directly addressed the issue of heuristic processing by examining sensitivity to argument strength, a commonly used test of heuristic processing (Chaiken, Giner-Sorolla, & Chen, 1996;Petty & Cacioppo, 1984). Fujita et al. presented participants with either strong or weak arguments from a wildlife conservation organization that either referred to a superordinate category (e.g., orcas in Puget Sound) of an attitude object or subordinate, specific exemplar (e.g., Simon, an orca in Puget Sound) of the attitude object. The results showed that when the arguments referred to a high-level category, attitudes were more sensitive to argument strength when the attitude object was temporally distant than near. However, when the arguments referred to a subordinate exemplar, the reverse was true; That is, attitudes were more sensitive to argument strength in the near than distal condition. Involvement might account for the findings obtained with arguments referring to low-level construal of the attitude object but not those obtained with arguments referring to high-level construal of the attitude object. Finally, using the number of thoughts generated about an attitude object as a measure of systematic processing, Ledgerwood, Trope, and Chaiken (2008) found that the number of thoughts was unaffected by temporal distance from the attitude object. We return to the issue of depth of processing in the sections on prediction and evaluation. Construal-Mediated Effects of Psychological Distance-What are the implications of the link between distance and construal for prediction, evaluation, and self regulation? In the following three sections, we address this question.",
          "As argued above, the very function of high-level construals is to enable people to mentally transcend the here and now by forming a representation consisting of the invariant features of the available information and projecting those representations onto distal situations. Predictions of future experiences would therefore be more schematic than the actual experiences, giving rise to a variety of prediction biases that stem from underweighting contextual and incidental features (Gilbert & Wilson, 2007;Kahneman, Krueger, Schkade, Schwarz, & Stone, 2006;Wilson & Gilbert, 2003). For example, people tend to predict that they will react more extremely to positive events (e.g., getting tenure) and negative events (e.g., a rejected paper) than they actually do. This misprediction stems from underweighting the effect of diluting low-level contextual circumstances (Wilson, Wheatley, Meyers, Gilbert, & Axsom, 2000). According to CLT, these biases should be accentuated by distance, because predicting more distal events should direct attention to even more invariant, schematic features of those events and away from low-level situational details. In the following, we examine the implications of this idea for lay prediction of research findings, predictions about one's own performance, and the effect of global trends and local deviations on extrapolation.1 Theories Versus Noise in Scientific Prediction-In scientific prediction, high-level construals reflect the theory, which fosters confidence, whereas lower level construals include noise, which might undermine confidence. A study by Nussbaum et al. (2006, Study 1) examined the confidence of advanced psychology students in replicating classic findings in psychology in either the near future or the distant future. For example, participants imagined entering a class at the university, either the next day or a year later (depending on the experimental condition), handing the students a list of words to memorize, and then testing how well they remember it after moving some of the students to a different room. Participants estimated how likely it is that those tested in the same room would outperform, on average, those that were moved to a different room, thus replicating the encoding specificity effect. Participants were more confident that they would replicate this effect when they imagined conducting the experiment in the distant future than in the near future, especially when reminded of the theory underlying prediction. The same pattern of results was obtained with other classic findings in social, cognitive, and developmental psychology. Knowledge Versus Task Format in Predicting Performance- Nussbaum et al. (2006) have also examined confidence in predictions about one's own performance. One of the studies assessed participants' confidence in predicting their performance on a general knowledge quiz expected to take place either on the same day or 2 months later (Nussbaum et al., 2006, Studies 3 and 4). The questions were the same, but in either a relatively easy or hard question format, which we assumed is a low-level aspect of the quiz, compared with the content the quiz focuses on. Specifically, the quiz consisted of either multiple-choice questions (relatively easy format) or open-ended questions (relatively hard format). In another study, the quiz consisted of questions with either two response alternatives (relatively easy) or four response alternatives (relatively hard). The results showed that the more difficult question format appropriately reduced confidence in near future performance but failed to reduce confidence in distant future performance. We think that this was the case because question format was a low-level aspect of the task. The results also showed that participants' beliefs about their knowledge in each domain predicted how confident they were in predicting their performance in that domain in the distant future better than in the near future. We think that this was the case because such beliefs concern the essence of the quiz as a test of knowledge and thus constitute a high-level construal of the quiz.",
          "Local Deviations-In a study that investigated the effect of spatial distance on the tendency to base predictions on global rather than local information (Henderson et al., 2006), New York University participants viewed a series of graphs depicting information from the years 1999-2004 (e.g., average number of photocopies per student). The information was said to pertain to the New York University campus in Manhattan (spatially near condition) or to the New York University campus in Florence, Italy (spatially distant condition). Each graph showed either an upward or downward trend, with the final year (2004) always deviating from that global trend. Participants estimated the likelihood that the year 2005 would be consistent with the general trend or with the more recent local deviation. In terms of CLT, global trends convey a highlevel construal, whereas deviations, being local exceptions, should receive more weight in low-level construals. As expected, spatial distance enhanced the tendency to predict on the basis of the global trend rather than the local deviation. Discussion-Spatial and temporal distances increase the impact of high-level information (e.g., theories, self-beliefs, general trends) and decrease the impact of low-level information (e.g., irregular outcomes, specific situational and task characteristics) on prediction. Thus, two complementary processes may contribute to the unwarrantedly higher confidence levels associated with predictions about psychologically distant events: underweighting of the uncertainty associated with low-level information and overweighting of the certainty associated with high-level information. Although we may know less about distant than near situations, our greater reliance on high-level construals in predicting the more distant situations may lead us to make more confident predictions about distant than near situations. Past research has shown that individuals tend to be more optimistic about distant future than near future outcomes (Gilovich, Kerr, & Medvec, 1993;T. R. Mitchell, Thompson, Peterson, & Cronc, 1997;Nisan, 1972). According to CLT, greater optimism is a likely but not necessary result of temporal distance. That is, greater temporal distance promotes optimism only when high-level construals imply a relatively high likelihood of desired outcomes. When low-level construals imply a high likelihood of desired outcomes, optimism may not increase and may actually decrease with greater temporal distance. Indeed, Nussbaum et al.'s (2006) studies on predicting one's performance showed that temporal distance led to more optimistic predictions when the question format was difficult but not when it was easy. Thus, underweighting a low-level aspect of outcomes (question format) increased optimism or decreased optimism depending on whether that aspect implied a high likelihood or low likelihood of success. Distance from an outcome might also reduce personal involvement, thus giving rise to shallow processing and less cautious predictions (see e.g., Petty & Cacioppo, 1984). Again, the critical difference between this view and CLT is that the latter assumes that distance reduces the use of low-level information, not high-level information. Accordingly, confidence increases with distance from an outcome only when the confidence stems from high-level construal of the outcome. The Henderson et al. (2006) and Nussbaum et al. (2006) studies described previously support this prediction.",
          "We make choices and set preferences with respect to our construals of objects rather than the objects themselves. When we choose a car, we do not decide on the car itself but rather on our construal of the car. When we decide on a diet, we do so because the construal of its outcomes seems attractive to us. We argue that construals depend not only on the actual attributes of the objects but also on the object's psychological distance. We therefore predict that all distances should similarly affect evaluation and preferences inasmuch as they all promote higher level construals. A common assumption in psychology and behavioral economics is that the value of an outcome is discounted as temporal distance from the outcome increases (see, e.g., Ainslie, 1975;Loewenstein & Prelec, 1992;Rachlin, Brown, & Cross, 2000). CLT proposes, however, that temporal distance shifts the overall attractiveness of an outcome closer to its high-level construal value than to its low-level construal value. When the low-level value of an outcome is more positive than its high-level value, the outcome should be more attractive in the near future, thus showing time discounting. However, when the high-level value of an outcome is more positive, the outcome should be more positive in the distant future, thus showing time augmentation (see Liberman &Trope, 2003, andTrope &Liberman, 2003, for more extensive discussions). In the following, we apply this principle to central versus peripheral features, feasibility versus desirability concerns, arguments in favor versus against an action, alignable versus unalignable attributes as sources of value, and attributes versus alternatives in choice matrices. Central Versus Peripheral Features-According to CLT, central, goal-related features of outcomes constitute high-level construal of outcomes, whereas peripheral, relatively goalirrelevant features of outcomes constitute low-level construal of outcomes. Distancing an outcome should therefore increase the weight of central features relative to peripheral features. Trope and Liberman (2000) found support for this prediction in studies on evaluations of objects and events containing both a primary and secondary aspect. In one study, for instance, participants imagined buying a radio set either the next day or 1 year later, to listen to morning programs. In one version, participants read that the sound quality of the radio set was good, but that the clock that was incidentally included was relatively useless. In another version, participants read that the sound quality of the radio set was poor, but that the clock aspect was quite useful. As expected, thinking about the radio set in the more distant future increased satisfaction when the sound quality was good and the clock poor, but decreased satisfaction when the sound quality was poor and the clock good, indicating that time delay increased the weight of central features and decreased the weight of peripheral features (for related findings in persuasion contexts, see Fujita et al., 2008). Conceptually similar findings were obtained with social distance, operationalized as interpersonal similarity (Liviatan, Trope, & Liberman, 2008) and social power (P. K. Smith & Trope, 2006). For example, in one study (Liviatan et al., 2008, Study 4), participants evaluated an essay of either high or low quality (a central, high-level aspect of value) allegedly written by a student that performed well or poorly on an unrelated physics exam (a peripheral, low-level aspect of value). Some participants were led to believe that the writer was similar to themselves, whereas others were led to believe that the writer was dissimilar to them. The weight of the peripheral information about the physics ability (relative to that of the essay quality) was greater in participants' evaluations of the essay written by the similar student than by the dissimilar student. It is interesting that research which compared the decisions people make for themselves to the advice they give to others obtained similar findings. Kray and Gonzalez (1999) and Kray (2000) compared participants' own choices with the advice they gave to socially close and distant others. They found that in advising others, especially socially remote others, participants tended to give more weight to a single attribute which they designated as the most important and less weight to other, more peripheral attributes. For example, when advising another person about choosing between two jobs, participants gave more weight to personal satisfaction (which they viewed as the most important dimension) and less weight to salary and location (the less important dimensions) than when choosing for themselves (Kray, 2000, Study 2). In two other studies, Kray found that this preferential weighting of important attributes was stronger in advising a distant social target (a student in another department) than a closer target (a student in one's own class). Moreover, as advisers, participants rated central attributes as highly important and peripheral ones as unimportant, whereas as deciders they rated the various attributes as relatively similar in importance. It is worth noting that participants reported greater responsibility and potential regret, and generated a larger number of decision-relevant attributes when making decisions for others than for themselves. As Kray pointed out, it therefore seems unlikely that participants simply invested less effort in advising others than in deciding for themselves. In our terms, these findings demonstrate choosing according to more central, high-level aspects for socially distant than social close others. Feasibility Versus Desirability-Desirability concerns involve the value of the action's end state (a high-level construal feature), whereas feasibility concerns involve the means used to reach the end state (a low-level construal feature). Therefore, desirability concerns should receive greater weight over feasibility concerns as psychological distance increases. Consistent with this prediction, it was found that as temporal distance from an activity (e.g., attending a guest lecture) increased, the attractiveness of the activity depended more on its desirability (e.g., how interesting the lecture was) and less on its feasibility (e.g., how convenient the timing of the lecture was; Liberman & Trope, 1998). Similar results have been found for other distance dimensions, including hypotheticality and social distance (e.g., Liviatan et al., 2008;Todorov, Goren, & Trope, 2007; see review by Liberman et al., 2007). Arguments in Favor of and Against an Action-In deciding whether to undertake an action, cons are subordinate to pros. This is because the subjective importance of cons depends on whether or not pros are present more than the subjective importance of pros depends on whether or not cons are present. For example, if we know that a medical treatment has some health benefit, we would inquire about its potential side effects before making a decision. But if the treatment has no benefits, we would decide against taking it without further inquiry about its side effects. In contrast, we would inquire whether a medical treatment has health benefits whether or not it has side effects. Thus, the importance of side effects depends on whether the treatment is known to have benefits, but the importance of benefits is independent of whether the treatment is known to have side effects. If cons are subordinate to pros, then pros should become more salient as temporal distance from the action increases, whereas cons should become less salient as temporal distance from the action increases. To test this prediction, Eyal, Liberman, Trope, and Walther (2004) asked participants to generate arguments in favor and against new (i.e., nonroutine) near future or distant future actions, such as introducing a new exam procedure (e.g., switching to open-ended questions instead of multiple-choice questions; Study 2), social policies (e.g., restricting private cars in the city center; Study 3), and a variety of personal and interpersonal behaviors (e.g., approaching a fellow student and offering to write an assignment together; Studies 4-6). As predicted, in all the studies, participants generated more pros and fewer cons as temporal distance from the actions increased. In an extension of these findings, Herzog, Hansen, and W√§nke (2007) suggested that if pros are more salient as temporal distance increases and cons are more salient as temporal distance decreases, then an increase in temporal distance should make it easier to generate pros and more difficult to generate cons. Further, because attitudes tend to be more in line with content when the retrieval is experienced as easy (W√§nke & Bless, 2000), the ease of retrieval associated with generating pros and cons of near and distant future activities should influence attitudes toward those activities, even when the number of arguments is held constant. In a test of these ideas, participants read about a proposed action that was to happen in the near or distant future and were instructed to write down either four pros or four cons regarding the activity. As expected, participants (a) found it easier to generate pros and more difficult to generate cons when the issue concerned the distant rather than near future and (b) had more favorable attitudes toward the action when it was to occur in the distant future. Alternatives and Attributes in Choice Matrices-Choice and enactment of a course of action is often based on information people search for about the available alternatives. Decision theoretic work has distinguished between searching within attributes, across alternatives and searching within alternatives, across attributes (Tversky, 1972). In a typical study, participants are presented with a matrix of information in which rows represent alternatives (e.g., different apartments), columns represent attributes (e.g., price, location, noise), and cells include the standing of each alternative on the corresponding attribute. Participants search this matrix by exposing the information in each cell, one at a time (see Payne, Bettman, & Johnson, 1988, for a review of this paradigm). We think that searching attributes within alternatives reflects a lower level construal of the choice situation than searching alternatives within attributes. This is because attributes are instantiated in specific objects and when detached from specific objects they become abstractions. Attributes are general, whereas objects are unique and variable manifestations of specific combinations of attributes. We thus expect that within-alternative search would characterize processing of proximal decision situations, whereas within-attribute search would characterize processing of distal decision situations. Borovoi, Liberman, and Trope (2010) recently tested this prediction. In a study on temporal distance, participants considered a choice for either the near future (e.g., choosing an apartment to rent in the next 2 weeks) or the distant future (e.g., choosing an apartment to rent a year later). In a study on social distance, participants considered a choice either for themselves or for another student. As expected, there were more within-alternative steps and less within-attribute steps when making decisions for psychologically proximal situations than for psychologically distal situations. It is important to note that in both studies, participants opened an equal number of cells and invested a similar amount of time in both distance conditions, indicating that they were not less motivated in the distal condition than in the near condition. It seems then that when making choices for proximal rather than distal situations, people are more likely to organize information within alternatives rather than within attributes. Alignable Versus Nonalignable Attributes-As noted earlier, comparing nonalignable objects (with information provided about different attributes for each object) requires higher level construal than comparing alignable objects (with information provided about the same attributes for each object), because making nonalignable objects comparable requires representing them in terms of more abstract attributes. On the basis of this argument, Malkoc, Zauberman, and Ulu (2005) reasoned that decisions that have distant future consequences (relative to near future consequences) would involve an increased consideration of nonalignable attributes. In a test of this prediction, participants evaluated two brands of potato chips and selected one brand to receive either at the end of the session (near future) or at the end of the semester (distant future). The two brands were designed based on pretesting to be equally attractive overall; one of the options, however, was designed to be better on its alignable attributes, whereas the other brand was better on its nonalignable attributes. As expected, temporal distance shifted both evaluations and choice toward the nonalignable better option over the alignable better option, indicating an increased reliance on nonalignable attributes when making decisions with distant future consequences. Discussion-Considerable evidence suggests that the value of outcomes is discounted or augmented over psychological distance depending on how it is distributed between highlevel and low-level aspects of the outcomes. People seem to appreciate proximal outcomes for their low-level aspects and distal outcomes for their high-level aspects. Although they often know less about distant than near outcomes, their evaluation of the distant outcomes might be more extreme when high-level construal of the outcomes have strong evaluative implications. These findings are consistent with CLT and the more general view that valuation is a constructive process (see e.g., Payne, Bettman, & Johnson, 1993;Shafir, Simonson, & Tversky, 1993;Weber et al., 2007). It is important to point out that the effect of distance on the value of outcomes is distinct from its effect on the controllability of the outcomes (Liberman & Trope, 2003). Psychological distance from outcomes often reduces the extent to which the outcomes are contingent upon one's action. For example, people typically have less control over others than themselves, over strangers than friends or relatives, and over events that occur in spatially remote than near locations. Moreover, because the number of ways of attaining an outcome often diminishes with proximity to the outcome, one's immediate action is less critical for a psychologically distant outcome to occur. For example, a long time before an exam, failing to study may be compensated by studying more intensely later, but a short time before the exam, when only a few hours remain, such possibility no longer exists. Similarly, failing to help a stranger may be compensated by help from another stranger, but failing to help a close friend is less likely to be compensated by someone else, because people typically have fewer close friends than strangers. By making outcome less contingent on one's action, greater psychological distance may decrease the motivation to engage in that action. Students may be less motivated to study for a distant exam than for a close exam because failure to study is easier to compensate for at a distance, or people may be less motivated to help a stranger than a close friend, because in the former case, their failure to help is more likely to be compensated by others. These changes in motivation do not reflect changes in value (e.g., the value of success on an exam may not increase closer to it). However, in many situations, measures of value might be contaminated with motivation and therefore show discounting over psychological distance. This could be one of the reasons for the widespread belief in economics and the behavioral sciences that value is discounted over time and psychological distance.",
          "Like predictions and evaluations, behavioral intentions and self-regulation should be increasingly based on high-level construal aspects as psychological distance increases. In the following, we examine the implications of this idea for values and ideologies, self-control, and negotiation. Values as High-Level Behavioral Guides-Values are commonly viewed as abstract structures that provide continuity and meaning under changing environmental circumstances (Feather, 1995), as stable meaning-producing superordinate cognitive structures (Rohan, 2000), and as trans-situational guides for action (S. H. Schwartz & Bilsky, 1987). On the basis of CLT, we propose that values, because of their relatively abstract and decontextualized nature, will be more readily applied to and guide intentions for psychologically distant situations. Evidence for this proposal has been recently obtained by Eyal, Sagristano, Trope, Liberman, and Chaiken (2009). One study used S. H. Schwartz's (1992) value questionnaire to assess the importance participants assigned to a wide range of values (e.g., power, benevolence, hedonism) and then asked participants to imagine 30 behaviors (e.g., \"rest as much as I can\") and to indicate the likelihood of performing each behavior either in the near future or in the distant future. Eyal et al. correlated the rated importance of each value and the mean likelihood of performing the behaviors corresponding to that value. As expected, these correlations were higher when the behaviors were planned for the distant rather than the near future, suggesting that people's values are better reflected in their intentions for the distant future than in their intentions for the immediate future or their actual behavior. For example, being high (vs. low) in hedonism might mean planning hedonic activities for the distant future, but not necessarily for the upcoming week. It is interesting that Eyal et al. (2009) also found that although values predicted participants' intentions for the distant future, feasibility concerns were more predictive of their intentions for the near future. For example, the number of hours participants volunteered in the distant future condition was predicted by their benevolence values but not by the convenience of the timing. In contrast, the number of hours participants volunteered in the near future condition was not predicted by their benevolence values and instead depended on the convenience of the timing. Extending this line of thought, Eyal, Liberman, and Trope (2008) argued that people judge immoral acts as more offensive and moral acts as more virtuous when the acts are psychologically distant than near. They showed that transgressions against core values that are deemed harmless because of extenuating circumstances (e.g., eating one's dead dog) were judged more severely when imagined from a more distant temporal or social perspective. Conversely, moral acts which might have had ulterior motives (e.g., adopting a disabled child when a government pays high adoption pensions) are judged more positively from temporal distance. The findings suggest that moral criteria are more likely to guide people's judgments of distant rather than proximal behaviors. Ideology and Incidental Social Influence-Our attitudes shift, often outside of our awareness, in response to other people in our local social context, including communication partners, significant others, and even total strangers (Baldwin & Holmes, 1987;Davis & Rusbult, 2001;Higgins & Rholes, 1978;Kawakami, Dovidio, & Dijksterhuis, 2003;Lowery, Hardin, & Sinclair, 2001). CLT predicts that when an attitude object is psychologically near, evaluations will be attuned to a particular social context and therefore more likely to be affected by incidental attitudes of others in the social situation rather than by one's ideology. Conversely, when the attitude object is psychologically distant, it will be abstracted away from its local context, and evaluation will therefore be less affected by the incidental attitudes of salient others and, instead, reflect one's ideology. A series of studies by Ledgerwood et al. (2008) tested the hypothesis that attitudes will align with those of another person in the local social context more when psychological distance is low (vs. high). Using an anticipated interaction paradigm, participants read about a policy that would increase the deportation of illegal immigrants starting either next week (near future) or next year (distant future) and learned that their discussion partner was either in favor of or against deporting illegal immigrants. They then privately reported how likely they would be to vote in favor of the policy. Participants' voting intentions shifted toward the interaction partner's attitude when the policy was set to be implemented in the near future but not when it was to be implemented in the distant future. However, voting intentions more strongly reflected participants' previously assessed ideological values when the policy was to be implemented in the distant (vs. near) future. Specifically, the more participants valued preserving the societal status quo, the more they supported a distant future policy that would enforce the deportation of illegal immigrants. Self-Control-Situations that require self-control involve a conflict between two opposing motivations (e.g., a desire to go out with friends and a need to study for an exam). Fujita, Trope, Liberman, and Levin-Sagi (2006) proposed an analysis of self-control conflicts as conflicts between behavioral implications of high-level construal (i.e., valence attached to primary, central, goal-relevant, superordinate considerations) and behavioral implications of low-level construal (i.e., valence attached to secondary, incidental, goal-irrelevant, subordinated, features). Failure of self-control, according to this proposal, is succumbing to the motivation implied by the low-level value. For example, if studying for an exam is related to more superordinate goals than going out with friends, then the latter behavior would represent a failure of self-control. Consistent with this analysis, a series of studies by Fujita et al. demonstrated that higher level construal increased self-control. In one study, participants first completed a task in which they indicated either why or how they would maintain good physical health. This task was designed to induce a high-level or low-level construal mindset, respectively (see Freitas, Gollwitzer, & Trope, 2004). Participants were then asked to hold a handgrip while connected to bogus electrodes, ostensibly as part of a psychophysiological assessment of personality. Participants were told that the longer they held the handgrip, the more diagnostic was the information obtained from the apparatus. Thus, the situation presented a conflict between a desire to get diagnostic, self-relevant information (high-level value) and the inconvenience of holding the handgrip (low-level value). The results indicated, as predicted, that participants in the high-level construal condition held the handgrip longer than those in the low-level construal condition. Just like higher level of construal, greater temporal, spatial, or social distance from a situation that poses a self-control dilemma should also enhance self-control. Indeed, research on intertemporal preference reversals has shown that people are better able to commit to self-control a long time in advance than a short time in advance (e.g., Ainslie & Haslam, 1992). A study conducted by Freitas, Salovey, and Liberman (2001) specifically showed that people were better able to forgo flattering but useless feedback for the sake of unpleasant but useful feedback when the feedback was delayed than when it was imminent. Similarly, research on children's delay of gratification has shown that greater temporal and spatial distance from a tempting object enhances self-control (Metcalfe & Mischel, 1999;Mischel, Shoda, & Rodriguez, 1989). CLT would further predict that we are more likely to advise another person to exercise self-control than to make a similar decision for ourselves, and that deciding on self-control is easier when it is premised on less likely events. For example, we could predict that deciding to start a diet upon getting an A in an exam would be facilitated to the extent that getting an A is less likely. In general, psychologically distancing oneself from a situation might promote people's ability to exercise self-control in that situation (see Kross & Mischel, in press). Negotiation-As with values, issues within an interpersonal negotiation can differ in their centrality and worth. If a pair of negotiators can trade off their lowest and highest priority issues (e.g., give in on secondary issues in exchange for getting what they want on highpriority issues, a process called logrolling), they are more likely to succeed in \"expanding the pie,\" maximizing both individual and joint outcomes. Because negotiators should be expected to focus more on central concerns and less on peripheral concerns as distance increases, we would expect to see more logrolling agreements in a distant future than near future context. Examining this idea within the context of a live negotiation, Henderson, Trope, and Carnevale (2006) found that whereas 91% of dyads with a temporally distant perspective reached a full logrolling agreement, only 50% of dyads with a temporally near perspective did so. The enhanced reciprocal concessions made by dyads with the temporally distant perspective culminated in better negotiated individual and joint outcomes. Moreover, research on the role of construal levels in the negotiation process has shown that negotiators who construed issues abstractly rather than concretely were more likely to discover integrative agreements (Henderson & Trope, 2009). For example, having negotiators think abstractly about issues (by having them generate general descriptions) rather than concretely about issues (by having them generate specific descriptions) increased negotiators' logrolling and individual and joint outcomes. These findings have implications for how negotiators handle conflicts over minor versus major issues and specific interests versus broad values and ideological differences in situations that do not allow for tradeoffs (Druckman & Broome, 1991;Harinck & De Dreu, 2004). Specifically, CLT suggests that the resolution of minor issues and specific interests should be hindered when individuals adopt a psychological proximal perspective or a lower level construal and facilitated by having a more distal perspective and abstract construal. Conversely, the resolution of conflict over major issues, values, and ideological differences should be hindered when individuals adopt a psychologically distant perspective or a higher level construal and facilitated by a less abstract construal and a psychologically proximal perspective. Discussion-The research reviewed in this section suggests that adopting a distal versus proximal perspective changes the way people make behavioral plans, resolve value conflicts, negotiate with others, and cope with self-control problems. The choices people make for psychologically distant situations are guided by their general attitudes, core values, and ideologies. As people get psychologically closer to the situation, their choices are increasingly influenced by more specific attitudes, secondary values, and incidental social influences. It seems, then, that from a distant perspective, global concerns are prioritized and unequivocally pursued, whereas from a proximal perspective, those priorities are weakened and even reversed as local concerns become more prominent. People may have less information about distant situations. Nevertheless, they often act as if they know better what to do or what they should have done when they remove themselves from the situation and take a distant rather than proximal perspective on it. Implications and Extensions-To recapitulate, the research we have reviewed thus far suggests that psychological distance triggers high-level construals, which focus on the central and enduring features of an object while screening out incidental and peripheral details. These high-level construals promote attunement to what is consistent about an object across multiple contexts, allowing individuals to transcend the particularities of the present situation and act according to their global concerns. Conversely, psychological proximity triggers low-level construals, which include the concrete and contextualized aspects of an object. These in turn are attuned to the current context, immersing individuals in the immediate situation and enabling them to flexibly respond to local concerns. In this final part of the article, we explore new directions for research on more speculative implications and extensions of CLT. These include psychological phenomena that might map onto different levels of construal (Section VII) and phenomena that might map onto different psychological distances (Section VIII). We conclude with a discussion of psychological distance in the brain (Section IX).",
          "We have shown that the distinction between different levels of construal is applicable to many psychological phenomena (e.g., gestalts vs. details, trait vs. situation attributions, pros vs. cons). In this section, we suggest that two more distinctions might map onto high-versus low-level of construal: assimilation versus contrast and high-versus low-level emotions. Assimilation and Contrast-When perceiving two stimuli in relation to each other (compared with perceiving the same stimuli separately), a perceiver may assimilate the two stimuli to each other, thereby perceiving each of the stimuli as more similar to the other stimulus, or contrast them away from each other, thereby perceiving each of the stimuli as more distinct from the other stimulus (Parducci, Perrett, & Marsh, 1969;Schwarz & Bless, 1992, 2007). For example, when considering a paper of a student in relation to the best paper in class (vs. considering it in isolation), the focal paper may be assimilated to the excellent paper and thus seem better, or it may be contrasted away and thus seem worse. In view of the opposite effects of assimilation versus contrast, the question of what makes each of them more likely becomes crucial (Mussweiler, 2001;Schwarz & Bless, 2007;Stapel, 2007). Most germane to the present framework are Schwarz andBless's (1992, 2007) inclusion/ exclusion model and F√∂rster, Liberman, and Kuschel's (2008) global/local model of assimilation versus contrast. According to these models, because global, high-level construals are more inclusive, using those construals is likely to result in including two stimuli in the same category and an assimilation effect. Using low-level construals, however, is likely to result in categorizing the two stimuli in separate categories and a contrast effect. Consistent with this view, it has been found that priming of high-level construal and greater temporal distance enhances assimilation and reduces contrast. For example, in one of the studies, (F√∂rster et al., 2008, Study 4), participants compared their athletic skills with either a moderately high standard or a moderately low standard and then rated their expected athletic performance in an athletic competition that would take place the next day (proximal temporal perspective) or a year from then (a distal temporal perspective). In the control condition, time was not specified. The results showed that a distant time perspective enhanced assimilation (i.e., produced a high self-rating after comparison to a high standard and a low self-rating after comparison to a low standard), whereas a proximal time perspective enhanced contrast (i.e., produced a low self-rating after comparison to a high standard and a high self-rating after comparison to a low standard). Borovoi et al. (2008) reasoned that if distance enhances assimilation, then it would also enhance the positive effect that attractive but unattainable alternatives might have on evaluations of the choice set. Consistent with this prediction, they showed that adding a very attractive job that was no longer available to a set of job offers increased the attractiveness of the entire set of job offers when the jobs were expected in the distant future rather than the near future. It seems that the attractiveness of available options is assimilated to that of attractive but unattainable possibilities when thinking about the relatively distant future. Affect-It is commonly assumed that psychological distance from an object diminishes the intensity of affective responses to that object. People typically react more strongly to events that are closer to them in time and space, to events that happen to themselves than to others, and to events that are real rather than hypothetical. However, building on appraisal theories of emotion (see, e.g., Beer & Keltner, 2004;Clore, Ortony, & Foss, 1987;Ortony & Turner, 1990;Tangney & Fischer, 1995;Tracy & Robins, 2007), CLT suggests that it might be useful to distinguish between emotions that may ensue from a low-level construal of an emotion-eliciting event and emotions that ensue from a high-level construal of the event. For example, love might be based on a more general and abstract representation of the target person than lust (F√∂rster, √ñzelsel, & Epstude, in press). It is interesting that some emotions involve, by their very definition, taking a distant perspective. For example, guilt and shame require taking a socially distant perspective, as they include considering the situation from the perspective of another social agent (Beer & Keltner, 2004), whereas sadness does not require taking such a socially distant perspective. Anxiety involves consideration of a potential, distal danger (for mice, anxiety is often induced by a smell of a cat), whereas fear is a reaction to an immediately present danger (e.g., for mice, the presence of a cat; Gray, 2000). Likewise, counterfactual emotions (e.g., regret) involve consideration of hypothetical alternatives to reality. According to CLT, emotions that involve taking a distant perspective require relatively abstract, high-level construal of the emotion-eliciting situation. It would be interesting to examine whether, indeed, some emotional experiences are more abstract than others. For example, would thinking of guilt and anxiety prime people to think in higher level terms in a subsequent, unrelated task? Once the classification of emotions to high level versus low level is established, CLT would predict that higher level emotions would decay less steeply and may even intensify over distance. Eyal and Fishbach (2008) recently accrued evidence in support of this hypothesis. They showed, for example, that pride decays over time less than happiness. Because of their distance transcending qualities, abstract emotions seem to assist in self-control more than concrete emotions. For example, Eyal and Fishbach showed that among dieters, priming participants with pride rather than happiness resulted in higher choice of a carrot (a self-control choice) rather than a chocolate bar (a self-control failure). CLT would further suggest that even the same emotion may be high or low-level, depending on whether it reflects high or low-level construals of the object. For example, in a funeral, sadness about the transience of life and compassion are central and thus constitute high-level emotions, but happiness upon seeing old friends is peripheral and thus constitutes a lowlevel, peripheral emotion. In a birthday party, however, the centrality of these emotions reverses. We would predict that in this respect too, secondary emotions would diminish over distance more than primary emotions. For example, the positive value that may be added to a funeral by the prospect of meeting friends would be more apparent when the funeral is closer in time and space, when we anticipate going ourselves rather than think about somebody else and when attending the funeral is likely rather than unlikely.",
          "",
          "Social power: Social power may engender a sense of distance from others. Indeed, individuals who have power see themselves as less similar to and thus more distant from other people than individuals who have less power (e.g., Hogg, 2001;Hogg & Reid, 2001;Lee & Tiedens, 2001;Snyder & Fromkin, 1980). This perception might be due to the fact that groups, organizations, and societies ordinarily have a pyramidal structure with fewer individuals occupying high-power positions than low-power positions. There is therefore greater similarity in the positions held by individuals with low power than by individuals with high power. If social power makes people feel distant from others, then it should also predispose them to construe information abstractly, focus on the central aspects of situations, disregard secondary aspects, and establish clear priorities. Several lines of research are consistent with these predictions. For example, Overbeck and Park (2001) observed that high-power participants were better at distinguishing between central and peripheral information, a hallmark of abstract processing. Guinote, Judd, and Brauer (2002) found that assigning participants to high-power positions promoted the use of abstract language. A series of studies by P. K. Smith and Trope (2006) examined the effect of power priming on a range of construal-level phenomena. Their findings show that power primed participants (a) were better able to abstract visual stimuli in an embedded figures task and Gestalt completion task, (b) were better at detecting covariation in a series of data, and (c) made more superordinate categorizations. Finally, Magee, Milliken, and Lurie (in press) have recently found that the level of abstraction in verbatim reactions to the September 11, 2001 attack was higher for individuals with high social power, compared to those with low social power. These findings suggest that the distal perspective activated by the possession of social power promotes going beyond the information given, detecting the underlying structure, and abstracting from it superordinate, central features. Power-related construal may expand people's mental horizons, enabling them to transcend the immediate circumstances and take into account the past, future, a broad range of people, and unlikely possibilities. Recent research by P. K. Smith, Wigboldus, and Dijksterhuis (2008) suggests that the relationship between power and construal level might be bidirectional. Their findings show that thinking abstractly enhances individuals' sense of their own power. Are there analogous effects of level of construal on the way we perceive others? Do we think of individuals as suitable for power positions, or as actually holding such positions, when they articulate and enact highlevel construals of the situation? Do our inferences and preferences as to the power individuals possess depend on whether they can articulate plans that transcend the present and extend to the distant future, remote places, diverse groups, and unusual circumstances? These questions await future research.",
          "The endowment effect suggests that when individuals are asked to sell an object they own, they ask for a significantly higher price than buyers tend to offer (Thaler, 1980). For example, in a typical endowment study, half of the participants receive a mug and are asked to set a price at which they would be willing to sell the mug to the other half of the participants, who in the meantime set the price at which they would be willing to buy the mug (e.g., Kahneman, Knetsch, & Thaler, 1990). Results of numerous such experiments and field studies show that average selling prices are two to three times higher than average buying prices. Given that the benefit of having the mug and, hence, its objective value, should not change with random ownership, these results present a perplexing phenomenon. The most widely accepted explanation of the endowment effect is loss aversion (Bar-Hillel & Neter, 1996;Kahneman et al., 1990; but see also Liberman, Idson, Camacho, & Higgins, 1999) that goes back to prospect theory (Kahneman & Tversky, 1979). Prospect theory suggests that individuals' perception of losses and gains differ, such that the subjective pain of a loss is more intense than the subjective pleasure of a gain of similar objective value. According to a prospect theory approach to the endowment effect, selling an object is perceived as a loss, compared with the seller's reference point of having the object; buying, in contrast, is perceived as a gain, compared with the buyer's reference point of not having an object. Because individuals are loss averse, they therefore tend to evaluate objects they consider selling more positively than they do objects they consider buying. Adopting a CLT approach to the endowment effect, Irmak, Wakslak, and Trope (2009) argued that sellers adopt a distant perspective on an object they are considering selling, thinking about the object from the perspective of the distant other to which the object will be moving; whereas buyers consider the object from a more proximal perspective, considering how they personally view this nearing object. As a result, sellers will construe the object on a high level, and selling prices will largely be influenced by the object's high-level features (and less by its low-level features). In contrast, buyers will construe the object on a lower level, and buying prices will therefore be influenced by the object's low-level features. In other words, we argue that sellers and buyers think about the object differently from one another, focusing, and thus basing their pricing decisions, on different types of object features, which, in turn, may lead to systematic differences in selling and buying prices. A series of studies by Irmak et al. (2009) provides initial support for this analysis. First, they showed that sellers indeed tended to think about an object in terms of its value for the buyer (indicating a distal perspective), whereas buyers tended to think about the object in terms of its value for themselves (indicating a proximal perspective). Second, sellers, compared with buyers, were more likely to construe actions related to those objects in high-level, endsrelated terms rather than low-level, means-related terms. Third, selling prices were higher than buying prices for objects whose attractiveness derived from high-level construal features, but this was not the case for objects whose attractiveness derived from low-level construal features. For example, selling prices were higher than buying prices when the object was superior on the desirability dimension and inferior in the feasibility dimension, but not vice versa, and when the primary (goal-relevant) aspects of the object were superior and the secondary (goal-irrelevant) aspects of the object were inferior, but not vice versa. These findings demonstrate that sellers and buyers derive value from different aspects of the object. Sellers take a psychologically distal perspective and thus extract value from the highlevel, primary aspects of the object, whereas buyers assume a psychologically proximal perspective and their perception of value is influenced less by primary aspects of the object and more by its secondary aspects.",
          "Proximal versus distal senses: We view the four psychological distance dimensions as being anchored on a single starting point (zero-distance point): me, here, and now. This view may be oversimplified, however, as some direct experiences may be more proximal than others. More specifically, the five senses-sight, hearing, smell, touch, and taste-may be mapped along spatial distance according to the maximum physical distance of the sensed object. An object has to be in one's mouth to be tasted, it has to be within one's reach to be touched, it may be farther away to be smelled, and it can be still farther away to be heard or seen. The distant senses, sight and hearing, enable people to extend the scope of perception far beyond the range of the near senses, which are limited to the person's immediate environment (Boring, Langfeld, & Weld, 1939; see also Rodaway, 1994). Do proximal senses produce a psychologically closer experience than distal senses? If they do, then the antecedents and consequences of psychological distance should apply to stimuli that are perceived with proximal versus distal senses, such that proximal senses would be associated with low-level construal and psychological proximity, whereas distal senses would be associated with high-level construal and psychological distance. For example, sharing an experience of tasting something with another person might produce more social closeness to that person than sharing an experience of looking at something. Similarly, touching a product might make a consumer give more weight to its feasibility properties and less weight to its desirability properties than only looking at that product. Novelty: Novel events are unfamiliar and often (but not always) subjectively improbable. Novel objects may therefore be perceived as more psychologically distant. The question, then, is whether novelty affects construal and construal-mediated outcomes in the same way as psychological distance does. Addressing this question, F√∂rster, Liberman, and Shapira (2009) found that when stimuli were presented as novel they were construed at a higher level than when they were presented as old. Novelty and familiarity were manipulated by either framing a task as new versus familiar or by asking participants to reflect upon novel versus familiar events prior to the task (i.e., procedural priming). The studies showed that relative to the control, no priming or no framing group, novelty priming or framing enhanced global perception, broad categorization, and high-level action identifications. Familiarity priming or framing had the opposite effect. Another study showed that presenting a product as novel (e.g., \"a novel model of a cellular phone\") versus old (e.g., \"Version 3 of an existing model of a cellular phone\") made participants assign greater weight to the products' high-level, desirability features and less weight to their low-level, feasibility features. It would be interesting to examine in future studies the effect of novelty on other construalmediated outcomes, such as creativity and self-control. Because creativity has been shown to be related to high-level construal (F√∂rster et al., 2004), we might predict that presenting a task as novel (vs. old) would enhance creative performance. It is also possible that because high-level construal is conducive to self-control (see Fujita, Trope, et al., 2006), presenting self-control programs (e.g., dieting or smoking cessation programs) as novel (vs. old) would promote success in those programs. Generalizing from experience: Our responses to a new object are often based on generalizations from a familiar, previously experienced object. The generalization may result from the co-occurrence or some resemblance between the familiar object and the new object. Of special interest are generalizations, often called projections, from ourselves to new people who become associated with us. By definition, the self is more proximal than another person. Therefore, we are likely to form low-level construals of ourselves, while generalizing our high-level construals of ourselves to others. High-level construals of the self are more likely to drive responses to others, whereas low-level construals of the self are more likely to drive responses to self. As a result, the more people value their high-level self characteristics relative to their low-level self characteristics, the more likely are they to form favorable impressions of unfamiliar others. Again, the bias toward projecting one's highlevel characteristics rather than low-level characteristics onto another person is likely to be augmented with greater psychological distance from that person.",
          "Do psychological distances and their links to construal level correspond to the structure and functions of the brain? A growing body of research suggests that they do. For example, recent research suggests that a common brain network involving the prefrontal cortex and the medial temporal lobe is implicated in prospection, retrospection, and taking another person's perspective (e.g., Buckner & Carroll, 2007;J. P. Mitchell, Macrae, & Banaji, 2006;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). In particular, different subregions of the medial prefrontal cortex (mPFC) have been associated with psychological processes that vary in their levels of abstraction. In their review on this subject, Amodio and Frith (2006) noted that representations of low-level goals and actions tend to be associated with activity in more posterior and ventral regions of the mPFC, whereas more complex and hierarchically organized representations are typically associated with more anterior and dorsal regions. On the basis of anatomical studies of connectivity in this region, Amodio and Frith (2006) proposed that activity in the mPFC corresponds to two different axes associated with the degree of abstraction (running posterior to anterior) and to action (running ventral to dorsal). In a related vein, J. P. Mitchell et al. (2006) found that processing information about a similar other engages the ventral region of the mPFC, whereas processing information about a dissimilar other engages more dorsal subregions of the mPFC. Moreover, recent research by Mitchel, Ames, and Gilbert (2008) observed an association in the brain between social distance and temporal distance. Specifically, the ventral mPFC was implicated in processing information about similar others and the present self, whereas the dorsal mPFC was implicated in processing information about dissimilar others and the future self. More generally, there is research showing that the brain is hierarchically organized, with higher points in the cortical hierarchy representing increasingly more abstract aspects of stimuli (Grill-Spector & Malach, 2004;Lieberman, Gaunt, Gilbert, & Trope, 2002). For example, progressively anterior and dorsal subregions of the prefrontal cortex have been found to be associated with more abstract representations (Badre, 2008;Koechlin & Summerfield, 2007;Ramnani, & Owen, 2004). This organization of information in the brain might be related to distance from stimuli, such that activation systematically progresses to higher points in the hierarchy as psychological distance from the stimuli increases. In sum, there may be a \"construal axis\" in brain systems that maps onto a \"distance axis.\" Brain systems that process high-construal information may be biased toward distal information, and brain systems that process low construal information may be biased toward proximal information.",
          "In reality, the different distance dimensions are separable. The fact that something happened long ago does not necessarily mean that it took place far away, that it occurred to a stranger, or that it is improbable. Nevertheless, as the research reviewed here demonstrates, there is marked commonality in the way people respond to the different distance dimensions. CLT proposes that the commonality stems from the fact that responding to an event that is increasingly distant on any of those dimensions requires relying more on mental construal and less on direct experience of the event. The findings we reviewed in the present article substantiate this claim in showing that (a) the various distances are cognitively related to each other, such that thinking of an event as distant on one dimension leads one to thinking about it as distant on other dimensions, (b) the various distances influence and are influenced by level of mental construal, and (c) the various distances are, to some extent, interchangeable in their effects on prediction, preference, and self-control. Past psychological research has typically investigated the different distance dimensions within disparate theoretical frameworks and methodologies. The research reviewed here suggests that psychological distance, as conceptualized here, captures a fundamental aspect of meaning common to all distances and may provide a unifying framework for understanding a wide range of seemingly unrelated psychological phenomena. Finally, taking an even broader perspective on psychological distance, it is worth noting that both collective and personal human development are associated with traversing increasingly greater distances. The turning points of human evolution include developing tools, which required planning for the future; making function-specific tools, which required considering hypothetical alternatives; developing consciousness, which enabled the recognition of distance and perspective taking; developing language, which enabled forming larger and more complex social groups and relations; and domestication of animals and plants, which required an extended temporal perspective (Flinn, Geary, & Ward, 2005). Human history is associated with expanding horizons: traversing greater spatial distances (e.g., discovering new continents, space travel), forming larger social groups (families vs. cities vs. states vs. global institutions), planning and investing in the more distant future, and reaching farther back into the past. Human development in the first years of life involves acquiring the ability to plan for the more distant future, consider possibilities that are nonpresent, relate to and take the perspective of more distant people (from self-centeredness to acknowledging others, from immediate social environment to larger social groups; Suddendorf & Corballis, 2007). Although the areas of evolution, history, and child development have different time scales, research in these domains seems to converge on the notion that transcending the present requires and is enabled by the human capacity for abstract mental representation. We hope that the present research on psychological distance, its relationship to level of construal, and its consequences for thought, feeling, and action advances our understanding of how individuals and groups transcend the here and now. Example of the four Spatial Location √ó Word Meaning combinations in Bar-Anan, Liberman, Trope, and Algom (2007). The illustration demonstrates words related to hypotheticality. Sample items from the Street Gestalt Completion Test (Street, 1931), provided courtesy of Teachers College, Columbia University. Identifying the pictures (from top right to bottom left: a boat, a rider on a horse, a rabbit, a baby) requires visual abstraction. Example of four Social Distance √ó Medium (picture vs. word) combinations in Amit, Algom, and Trope (2009). For the Israeli participants in this study, a shekel was socially proximal, and a dollar was distal."
        ],
        "ground_truth_definitions": {
          "psychological distance": {
            "definition": "a subjective experience that something is close or far away from the self, here, and now.",
            "context": "They serve to transcend the immediate situation and represent psychologically distant objects. Psychological distance is a subjective experience that something is close or far away from the self, here, and now. Psychological distance is thus egocentric: Its reference point is the self, here and now, and the different ways in which an object might be removed from that point-in time, space, social distance, and hypotheticality-constitute different distance dimensions.",
            "type": "explicit"
          },
          "high-level construal": {
            "definition": "relatively abstract, coherent, and superordinate mental representations, compared with low-level construals.",
            "context": "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental.",
            "type": "explicit"
          },
          "correspondence bias": {
            "definition": "a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions",
            "context": "A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995; E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995; Semin & Fiedler, 1988; Semin & Smith, 1999). Distance should therefore enhance the correspondence bias.",
            "type": "explicit"
          },
          "centrality": {
            "definition": "Changing a high-level feature has a greaterimpact on the meaning of an object than does changing a low-level feature",
            "type": "explicit",
            "context": "The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is."
          },
          "subordination": {
            "definition": "The meaning of low-level features depends on highlevel features more than vice versa",
            "type": "explicit",
            "context": ". The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. T"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "885c3154386e3dd1ff6a12e566152846186f9569",
        "sections": [
          "hypotheses based on a careful mapping of psychological profiles of over 2,300 American Twitter users linked to behavioral sharing data and sentiment analyses of more than 500,000 news story headlines. The findings contradict the ignorance perspective but provide some support for the disruption perspective and strong support for the partisan polarization perspective. Thus, individuals who report hating their political opponents are the most likely to share political fake news and selectively share content that is useful for derogating these opponents. Overall, our findings show that fake news sharing is fueled by the same psychological motivations that drive other forms of partisan behavior, including sharing partisan news from traditional and credible news sources. With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018). Political observers have linked the spread of fake news to recent key political events, like the \"Brexit\" vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016). Accordingly, media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017). In light of these sizable investments, it is surprising how little we know about why people share fake news on social media. Studies suggest the spread of false information cannot be attributed only to bots or \"sock puppet\" accounts (Guess, Nagler andTucker 2019, Vosoughi, Roy andAral 2018). But researchers have largely failed to conduct adequate tests of why ordinary citizens take part in the distribution of fake news (Wagner and Boczkowski 2019). This is unfortunate because potential solutions to the spread of fake news rest on psychological assumptions about the root causes of the problem. For example, fact-checking should reliably work only if citizens actually want to share true information but need help to weed out falsehoods. Presently, we do not know whether this portrayal of citizens' motivations is correct. As summarized in a recent study, \"most existing studies rely solely on publicly available social media data and there is little research that seeks to understand people's motivations for (...) sharing news\" (Chadwick, Vaccari and O'Loughlin 2018, 4258). In this study, we ask: What are the individual-level psychological motivations behind fake news sharing? The first major challenge in addressing this research question is data unavailability (King andPersily 2019, Stier et al. 2019). Data on citizens' personality and political profiles are rarely available in combination with behavioral data on their sharing activities on social media (for notable exceptions, see Guess, Nagler and Tucker 2019, Guess, Nyhan and Reifler 2020, Grinberg et al. 2019). Here, we circumvent these data limitations by linking unique behavioral data on news sharing activity and sentiment analyses of more than 500,000 news story headlines to individual-level survey data with detailed psychological measures for a large sample of American Twitter users. This approach allows us to conduct a comprehensive study of competing psychological theories of fake news sharing. The second challenge is that multiple psychological motivations could potentially influence the sharing of false and factually correct news; these motivations may differ across individuals and across news content. Building on theories about information sharing in general, we first outline these potential psychological motivations before applying them to our research topic: the sharing of political fake news on social media, a domain characterized by intense conflict. 1Then, we deduce testable implications about relevant individual differences and differences in political news content.",
          "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience. Whereas once audiences were more or less passive consumers, they are now actively participating in the distribution, and sometimes even the production of news. People today can start their own blogs, live-stream from political events they participate in (e.g. protests), and may even contribute to small media organizations often catering to niche market segments. To be sure, just as with legacy media, online audiences still face a choice of exposure: which news do they choose to consume. But on top of that, they also face a new choice of which news to share with their friends and followers. These trends, we contend, have implications for the psychological dynamics underlying the way audiences engage with this media, the novel step of news sharing potentially recruiting new psychological mechanisms. These trends have pushed to the forefront questions about information veracity. In advanced democracies, strong editorial norms and standards for good reporting meant that citizens could -at least most of the time -trust legacy media outlets to publish stories that accurately reflected the world. For most citizens and for most legacy media, then, it made sense to pay more attention to the political usefulness of news stories and, thus, it is not surprising that this news dimension has been critical in driving selective exposure. Yet, on the internet, editorial norms have lost power, and information circulating on social media will to a much larger extent vary on both dimensions of usefulness and veracity. Truth cannot be taken for granted and veracity judgments are thus, potentially, more important than ever. Yet, these two information dimensions -veracity and usefulness -are not unique for social media users. In everyday conversations when, for example, discussing or relaying a rumor, people must consider both whether they believe it and how useful they think it is. Psychologists have studied the psychology of rumor transmission for decades, highlighting the importance of both accuracy-oriented and goal-oriented motivations (for an overview, see DiFonzo and Bordia 2007). For example, people may share rumors they believe are true to be cooperative, to signal competence, or to get others to affirm the accuracy of the rumors. Sharing a rumor that turns out to be wrong, in contrast, may damage people's reputations (Altay, Hacquin and Mercier 2019). Nonetheless, goal-oriented processes may still take precedent, notably under intense social conflict. Some of the most well-studied, and most extreme, examples come from ethnic conflicts, where rumors are heavily used to mobilize the in-group against the out-group. In his comprehensive review of this role of rumors, Horowitz (2000, p. 74) concludes: \"Rumors are (. . . ) embedded in the riot situation, because they are satisfying and useful to rioters and their leaders. (. . . ) Rumor is likely to prevail over accurate information. (. . . ) Rumor prevails because it orders and organizes action-in-process.\" Below we apply these general considerations about the psychology of information sharing to the object of our empirical inquiry: The sharing of news -fake or real -about politics on social media.",
          "The prevalence of false information on social media may strengthen the role of accuracyoriented motivations and the dimension of veracity for understanding news sharing behavior. People may worry about damaging their reputation by accidentally sharing fake news (Altay, Hacquin and Mercier 2019). To the extent such reputational worries induce accuracy-oriented social media behavior, we should expect that sharing fake news stories is a rare phenomenon relative to sharing real news, but also that the predictors of sharing fake versus real news will differ. Successfully discerning between fake and real news is difficult, first, because fake news is specifically designed to appear real and, second, because the fast-paced social media platforms deplete attention and cognitive resources (Pennycook et al. 2020). Accordingly, individual differences in the ability to detect cues about information veracity should largely determine whether people share fake news (Pennycook and Rand 2019b). In essence, from this perspective, fake news sharing happens because people are ignorant that the stories they share are false and, accordingly, below we refer to this argument as the ignorance theory. Goal-oriented motivations, however, may also critically shape sharing of political news on social media. As noted above, goal-oriented motivations affect information sharing during periods of social conflict. Politics is conflictual, but political conflict in the United States in recent years has been deepened by an increasing political polarization (Mason 2018). Goaloriented motivations put a premium on the usefulness of information. Sharing a political news story (fake or real) can be used to signal one's political affiliation, by denigrating political opponents or by mobilizing politically like-minded peers. In this regard, fake news stories can be particularly useful: as fake news is not constrained by reality, they can be excessive in their negative portrayal of political opponents (Acerbi 2019). Accordingly, to the extent goal-oriented motivations dominate social media users' interactions with news stories, the same psychological motivations should drive the sharing of real and fake news. People will share useful news that fits their political goals, and, in the extreme case, they will not pay attention to the veracity dimension of the information at all. In current American politics, two types of social or political goals could be served by sharing extreme information such as fake news. First, sharing news stories could reflect motivations to help one's preferred party against opposing parties in the increasingly polarized electoral competition (Iyengar, Sood andLelkes 2012, Lelkes andWestwood 2017). We will refer to this as the polarization theory as it focuses on partisan feelings rather than ideological disagreements between partisans of different stripes. The theory is prominent in current discussions about political events, like elections and referenda, and it has a long research history in political science (Mason 2018, Taber andLodge 2006). The theory has also informed studies on partisans' beliefs in political conspiracies (Miller, Saunders and Farhart 2015). Second, sharing news stories could be fueled by deep-seated discontent with the status quo and serve a desire to disrupt the existing social and political order. For example, Tucker et al. (2017, 53) convincingly argue that \"social media can lend a voice to ... anti system forces that actively seek to undermine liberal democracy.\" Below, we will refer to this as the disruption theory. Finally, there are also multiple scenarios for the intermediate (and likely) case where both veracity and usefulness considerations are relevant for social media users' sharing behavior. People may first evaluate the veracity of news and then, conditional on it being accurate, consider its usefulness. Or people may prioritize usefulness but still pay some attention to the veracity. Hence, the sharing of blatantly false information may, for example, complicate mobilization processes by derailing discussions about whether events really happened (Petersen, Osmundsen and Tooby 2020). Thus, when assessing usefulness, people may factor in veracity and selectively share information that is as extreme as they believe they can get away with. To elucidate whether and how the ignorance, polarization, and disruption theories account for the sharing of fake news, we use two approaches: First, we examine the role of individual differences, deducing for each theory who should be most likely to share fake news and, second, we examine the role of news story content, deducing what is most likely to be shared.",
          "The accuracy-oriented ignorance theory assumes people want to share accurate information, but end up sharing falsehoods because they lack the cognitive reflection or motivation to discern between true and false information. One line of evidence for the ignorance theory comes from Pennycook and Rand (2019b), who show that people who perform better on a cognitive reflection test can better tell if news story headlines are true or false. Another line of evidence comes from Guess, Nagler and Tucker (2019), who demonstrate that older people are more likely to share fake news on Facebook, presumably because they are less digitally literate than their younger peers. Building on this, we test the ignorance theory by focusing on four individual-level predictors of fake news sharing on Twitter: age, cognitive reflection, factual political knowledge, and digital literacy. The goal-oriented polarization theory focuses on partisanship as a key driver of fake news sharing. Currently, there is mixed evidence on how partisan motivations influence fake news sharing. In general, partisans seem to share information from ideologically similar sources (Barber√° et al. 2015) and to treat articles shared by political opponents with greater suspicion (Lee, Kim and Coe 2018), suggesting that considerations about the political usefulness of information matters. Guess, Nagler and Tucker (2019, 3) find that partisanship also affects fake news sharing in that conservatives and Republicans were more likely to share fake news on Facebook in 2016 than liberals and Democrats, a partisan asymmetry attributed to \"the overwhelming pro-Trump orientation in both the supply and consumption of fake news during that period.\" Pennycook and Rand (2019b), however, tone down the effect of partisanship. They conclude that \"people fall for fake news because they fail to think; not because they think in a motivated or identity-protective way\" (ibid., 48). To test the polarization theory, we examine whether Democratic partisans are willing to share politically congruent fake news from *pro*-Democratic sources or if the partisanship asymmetry identified by Guess, Nagler and Tucker (2019) and others prevails. Second, an important question in the literature on affective polarization is whether in-party love (Lelkes and Westwood 2017) or out-party hatred (Abramowitz and Webster 2018) sparks polarization. We provide the first test of which of these dynamics explain fake news sharing by relying on two types of individual differences as predictors of news sharing: 1) standard party identification measures and 2) batteries of positive versus negative feelings towards Democrats and Republicans. Finally, we turn to the goal-oriented disruption theory. A central focus in media discussions and psychology has been on so-called \"online trolls\" who take pleasure in misleading and harassing others online (Buckels, Trapnell and Paulhus 2014). Trolling experts speculated that sharing sensational and often implausible fake news reflects a specific form of apolitical online trolling. However, political cynicism may also drive fake news sharing. For example, a recent study showed that Americans and Danes who are discontent with political elites and the political system report substantially greater willingness to circulate conspiracy theories online (Petersen, Osmundsen and Arceneaux 2018). Here, we provide the first behavioral test of this theory and zoom in on two individual differences as predictors of fake news sharing: a psychological measure of trolling behavior and a measure of political cynicism.",
          "To elucidate the psychological motivations underlying fake news sharing, we also need to understand what news content people share. Specifically, to adjudicate between the three theories, the analyses differentiate news stories along two dimensions: (1) Whether a news story originates from a fake news source (i.e., a source containing registered debunked stories) or a real news source (i.e., a generally credible source), and (2) whether a news story comes from a *pro*-Republican or a *pro*-Democratic source. The three theories make very different predictions about which dimension matters for people's sharing preferences. The ignorance theory predicts that the veracity dimension matters most and that people with high cognitive abilities will be less willing to share fake news stories and more likely to share real news stories, irrespective of the political slant of the stories. The latter two theories make the opposite prediction: they posit that people place less value on whether stories are true or false as long as the stories are politically useful. Specifically, the disruption theory implies that disruptive individuals should selectively share as many fake news as possible to sow confusion and uncertainty. For the polarization theory, however, the content-related predictions are more complex, and so it is these predictions we unfold now. The polarization theory holds that fake news sharing is not an accident caused by ignorance, it is partisan business as usual. The media landscape in the US is politically fragmented, with some news sources catering to Republicans while others offer content that fit Democrats' taste (Groeling 2013). From the perspective of the polarization theory, fake news is an extreme version of traditional partisan news that polarized partisans may find attractive due to its rarity and extremity. Accordingly, partisans motivated to selectively share hyper-partisan content will tend to view the online media landscape of news sources as a one-dimensional continuum reflecting the political usefulness of the stories they provide. For a polarized Republican social media user, the order of the usefulness of news sources goes from *pro*-Republican fake news sources, to *pro*-Republican real news sources over centrist sources to *pro*-Democratic real news sources, and finally, to *pro*-Democratic fake news. For Democrats, the order reverses. This insight, we contend, leads to three observable implications about the content of shared news. First, partisan social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. Accordingly, we expect that sharing fake news favoring one party over the other should be best predicted by sharing real news strongly favoring the same party. Conversely, sharing fake news favoring one party should correlate negatively with sharing fake news favoring the other party. Second, if news sources differ systematically in how strongly they cater to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Specifically, fake news should show the strongest partisan sorting in terms of who wants to share from these sources. Third, and finally, the negativity of news source content should change as we move across the partisan news continuum. If partisans selectively share stories with content that fits their political tastes, the observable implication is that stories shared from news sources located towards the Democratic extreme on the partisan news continuum should portray Republican elites negatively while stories from the Republican extreme should cast Democratic elites in a negative light.",
          "",
          "We commissioned the YouGov survey agency to recruit U.S. Twitter users to complete a 20-minute online survey fielded from mid-December 2018 to mid-January 2019. The survey contained questions measuring our set of potential predictors of fake news sharing (see below) and a range of demographic variables. The survey also asked participants to provide their Twitter ID and asked for permission to scrape their publicly available Twitter data and link this data to their confidential survey responses. See SI Section 1 for the invitation letter and link to the consent form. We scraped respondents' Twitter accounts by early March, 2019. Our data includes tweets and retweets posted by the panelists before that period. YouGov invited over 27,000 U.S. Twitter users from their standing web panel to participate in our study. 8,741 participants accepted the invitation and answered all questions before the final prompt to give consent to link their public Twitter data to the survey. A majority of these respondents (5,725 people) did not share their Twitter IDs at the end of the survey. Of the 2,976 respondents who did attempt to share their Twitter handles, another 639 participants presented IDs that we could not access via Twitter's Application Programming Interface. Consequently, our final sample size is N = 2,337. Because of high and non-random attrition, our sample is unlikely to perfectly reflect any welldefined population. Still, our study participants' demographic characteristics resemble the characteristics obtained from participants in other high-quality Twitter studies. Relying on a recent Pew study (Wojcik and Hughes 2019), SM Section 2a demonstrates that our sample is comparable to the Twitter-population on race, gender, income, education, and age. It also shows that participants in our final sample were demographically very similar to the 8,741 participants who accepted to participate in the survey but failed to provide their Twitter handle. In our final sample, 50% of participants are female, 70% are white/Caucasian, the median age is 40 years old (SD = 14), the median highest level of completed education is \"4-year college degree,\" and the median family income level is \"$60,000-$69,999.\" We scraped Twitter data from all our 2,337 participants. In total, we extracted 2,709,052 tweets and retweets posted by our panelists until mid-January 2019. The median number of (re)tweets for our participants was 488 (Min #of tweets = 1, Max #of tweets = 6433). Importantly, a large majority of the tweets -around 70% -were posted after the 2016 US presidential election; the period that has been the focus of previous work. This allows us to extend earlier findings and test how they hold up in periods characterized by less intense electoral competition.",
          "We follow best practices (Guess, Nyhan and Reifler 2020, Grinberg et al. 2019, Lazer et al. 2018) and operationalize \"sharing of fake news sources\" as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news. To this end, we cross-referenced all tweeted URLs against a list of 608 fake news sources, constructed by journalists and scholars (for details, see SM Section 10 and Guess et al. 2019, Allcott, Gentzkow andYu 2019). The list was compiled to document the prevalence of fake news during the 2016 U.S. presidential election but has been updated to account for fake news publishers that have emerged since. To the extent possible, the list creators have coded the most popular fake news sources for partisan slant: 12 of the most popular were *pro*-Democratic fake news sources while 30 were *pro*-Republican sources. While this number may seem low, note that almost 86% of all shared fake news sources originated from one of these 42 domains for which we have partisan ratings. (See SM Section 6 for the complete list of fake news domains and Figure SM 2b for the 20 fake news domains that were shared the most.) As a robustness test, supplementary analyses discussed below group the fake news sources according to a more fine-grained coding scheme proposed by Grinberg and colleagues (2019) that distinguishes among three classes of fake news sources differing in their likelihood of publishing misinformation. In SM Section 8, we also replicate the main results using a different, but partially overlapping list of fake news sources. Our measure of fake news sharing records actual online behavior and thus does not suffer from biases that can plague self-reported measures of online activity. Further, while the present study is certainly not the first to couple survey and social media data (e.g., Guess, Nagler and Tucker 2019), it has the most detailed set of psychological measures; see below. Still, it is important to keep in mind when interpreting our results that we cannot tell if the specific stories people share are true or false: Our fake news measure is at the publisher level, not the story level. To measure the \"sharing of real news sources,\" we cross-referenced participants' tweets against a list of real news publishers obtained from the AllSides organization. AllSides seeks to help citizens navigate the online media landscape by providing \"balanced news and civil discourse.\" Their web page (www.allsides.com/media-bias/) contains a list of 260 real news publishers with ideological ratings: \"Left\" (labeled \"Strong Democratic\" below, n = 45), \"Lean Left\" (\"Leaning Democratic\", n = 63), \"Center/Mixed\" (n = 88), \"Lean Right\" (\"Leaning Republican\", n = 29) or \"Right\" (\"Strong Republican\", n = 39). While AllSides combines different methods for estimating ideological bias (e.g., editorial reviews, third-party judgments), SM Section 7 shows that their ratings correlate highly -Pearson's r = .90with network-based measures of ideological alignment of media sites (Bakshy, Messing and Adamic 2015). See SM Section 7 and Figure SM 2b for the 20 most shared real news domains. Our measures of real and fake news sharing account for ideological and partisan slant at the source level and are thus useful for understanding the content people care about when sharing news. To gain further leverage on testing the relevance of content for sharing behavior, we also built two unique data sets of news headlines (see SM Section 11 and 12 for details). We focus on headlines because these are more accessible than full texts and because prior research demonstrates that simply seeing a headline can exert large effects on decisions to share news on social media (Effron and Raj 2019). The first data set was based on the headlines of stories shared by our participants, which gives us a clear representation of the content they deemed sufficiently important to share. To construct the data set, we first retrieved the full URLs from all 85,084 tweeted news links shared by our participants. Based on these URLs, we parsed the news domains' HTML code and successfully extracted the headlines of 75,560 news articles (89% of all article headlines). To construct our second data set, we used the Archive.org web page -an indexing system for the Internet's web pages for historical archiving -to scrape headlines that had appeared on the daily front pages of the most popular (among our participants; see Figure 5 below) news sites in the period 2016-2019 (i.e., covering roughly the same period for which we have tweets from our participants). Compared to our Twitter headlines, this data set contains a much larger number of news story headlines (we successfully scraped a little over 500,000 headlines). More importantly, the front page headlines offer a glimpse of the universe of news stories our participants could potentially see and read before deciding which stories to share. In this way, it sidesteps issues of \"selection\" biases in sharing decisions due to psychologically motivated reasoning; a point we return to during the analyses. We subjected these headlines to two types of automated analyses. To measure the sentiment of the headlines, we relied on the sentimentR R package, which includes a sophisticated algorithm for calculating text polarity sentiment. This algorithm improves the performance of more traditional dictionary-based methods by calculating sentiment on the sentence level and considering valence shifters (e.g. not, very, hardly). To identify the political affiliations of those mentioned in the headlines, we created two custom dictionaries of prominent Republican and Democratic elites. Our primary dictionary is based on YouGov's list of the 100 most famous Democrats and Republicans. 2 We replicate our results with an alternative dictionary listing all members of the US Congress, Senate and the Trump Administration in the supplemental materials. Finally, we matched these dictionaries to the headline data sets and recorded whether a given headline mentions anyone from the Republican or Democratic party. We use these more detailed content measures in the final part of the analysis, whereas the initial parts focus on the crude distinction between fake and real news sources.",
          "All participants responded to the same survey questions, designed to measure the potential predictors of fake news sharing derived from the three theories discussed earlier: the ignorance theory, the disruption theory, and the polarization theory. SM Section 1c lists the questions asked and specific wordings.  (Pennycook and Rand 2019b). Second, we included participant age to test if we could replicate earlier findings that older social media users share more fake news stories than younger users. Finally, we included measures of factual political knowledge and digital media literacy to test the claims about why older social media users share more \"fake news.\" We included a 5-item battery from the American National Election Studies (ANES) to measure factual political knowledge (e.g., \"How many members of the U.S. Supreme Court are there?\"). To measure digital media literacy in the domain of politics, we created a 10-item battery based on previous research (Feezell 2016, Gil   2 Accessible at https://today.yougov.com/ratings/politics/fame/Democrats/all and https:// today.yougov.com/ratings/politics/fame/Republicans/all de Z√∫√±iga, Molyneux and Zheng 2014) asking participants how often they engage in specific online political behaviors (\"Use the Internet to participate in volunteer activities related to a political or social campaign\") (Œ± = .93). As this measure also taps political engagement and online civic skills, we adjust for political interest in the analyses. In addition, SM Section 4 replicates the main analysis using a more narrow, validated measure of digital literacy from Hargittai and Hsieh (2011). The disruption theory. We included two measures of disruptive motivations. To measure apolitical trolling behavior, we solicited the \"cyber-bullying\" literature and implemented a 4-item trolling scale (Buckels, Trapnell and Paulhus 2014), asking participants whether they agreed or disagreed with statements like \"I like to troll people in forums or the comments section of websites\" (Œ± = .86). We measured political cynicism via an 8-item survey instrument (Dekker and Meijerink 2012), asking participants whether they agreed or disagreed with statements like \"Politics in the United States is sick\" (Œ± = .74). The polarization theory. We included two measures of political polarization: Partisanship and feelings towards Republicans and Democrats. We used a standard ANES question to measure partisanship on a seven-point scale, ranging from \"Strong Democrat\" to \"Strong Republican\" identifier. To measure partisan feelings, we asked participants how strongly they felt various positive (hope, enthusiasm, pride) and negative (anger, frustration, fear) emotions when thinking about Republicans and Democrats. We created two feelings scales to test how in-party love and out-group hate related to news sharing by aggregating responses and taking the averages for each participant (the negative emotion items were flipped). The key models presented below analyze motivations to share news from *pro*-Republican and *pro*-Democratic news sources separately. Accordingly, we coded the \"in-party\" emotion scale such that higher values always correspond to more positive emotions towards the political side supported by the news source (i.e., higher values indicate the participant has more positive feelings towards Republicans (Democrats) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). Analogously, we constructed the \"out-party\" emotion scale such that higher values indicated more negative feelings towards the political side opposed by the news source (i.e., higher values indicate the participant has more nega-tive feelings towards Democrats (Republicans) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). SM Section 2b presents summary statistics for all main variables.",
          "",
          "Before testing the key hypotheses about the psychological motivations for fake news sharing, we provide descriptive analyses of the data. Overall, we find that news sharing is a small part of our sample's activity on Twitter. (See Figure SM2c for an overview.) Of the 2.7 million tweets in our data, only 85,344 (‚âà 3%) contain links to national news websites, fake or real. Of the tweeted news links, 3,269 (‚âà 4%) come from websites with a history of publishing fake news (see Grinberg et al. 2019, for slightly larger estimates during the 2016 US presidential election). Of these stories, a large majority -2,563 stories -originates from *pro*-Republican fake news publishers. In contrast, over twice as many real news stories come from *pro*-Democratic sources (60%) than from *pro*-Republican sources (23%). Together, these two observations imply that exposure to fake news depends heavily on the ideological leanings of one's Twitter network. In *pro*-Democratic networks, only a small fraction of the news diet is likely to come from fake news sources, whereas *pro*-Republican networks are likely to display a substantially higher fake-to-real news ratio. While these aggregate numbers suggest fake news sharing could be widespread within certain social networks, they also mask large heterogeneity in sharing behavior. The left-hand panel in Figure 1 below makes clear that sharing content from fake news sources flourishes mostly among small segments of the population: 1% of the panelists are responsible for sharing about 75% of all links to fake news publishers and 11% of them are responsible for sharing all fake news sources. Meanwhile, the top 1% of real news \"supersharers\" are only responsible for sharing about 30% of all real news sources. Figure 1's two other panels show similar results when splitting the data based on the partisan slant of the news sources. Importantly, not all fake news is equally troublesome. SM Section 8 uses Grinberg and colleagues' (2019) fine-grained categorization of fake news sources and find that about 30% of the fake news links come from sources \"publishing almost exclusively fabricated stories\" (i.e. \"black\" sites). Another 5% come from sites that \"spread falsehoods that clearly reflected a flawed editorial process\" (i.e. \"red\" sites). Finally, two-thirds of the links referred to fake news sources \"where annotators were less certain that the falsehoods stemmed from a systematically flawed process\" (i.e. \"orange\" sites). While this means that the number of individual stories containing blatant falsehoods is likely smaller than 3,325 in our data, we note that sharing news stories from websites with a record of disseminating falsehoods can still be problematic. Given the still limited penetration of fact-checking, sharers most likely do not know whether a certain news story is true or false when deciding to share it. Instead, they must make informed guesses based on the reputation of the news source. Thus, decisions to share news from these sources still reflect either ignorance or lack of concern about such matters.  Note. The x-axes show percentage of participants responsible for sharing a given percentage of all news links (y-axes). The left-hand panel gives results for sharing links to fake (black line) versus real (grey line) news publishers (N Fake News = 3,269; N Real News = 78,400). The middle and right-hand panels display results for sharing links to *pro*-Democratic (blue line) versus *pro*-Republican (red line) fake and real news sources, respectively. For real news sources, we collapse the \"Leaning\" and \"Strong\" categories. The dashed black lines denote a hypothetical uniform distribution of news sharing. (N Fake News, pro-Rep = 2,563; N Fake News, pro-Dem = 263; N Real News, pro-Rep = 16,081; N Real News, pro-Dem = 46,385.)",
          "We now turn to the analysis of the psychological motivations for sharing fake news sources. Figure 2 gives average marginal effects (AME) 3 from logistic regression models estimating the association between our main predictors and the probability of sharing at least one news story from a fake news (left panel) or a real news source (right panel). To increase power and simplify the analyses, we here combine the real news sources that Allsides rate as strongly partisan and leaning partisan (e.g., we combine news stories from \"Strong Democratic\" and \"Leaning Democratic\" real news sources). As our main interest is in whether people share fake news or not, we dichotomize the dependent variables, with 1 indicating that a participant As shown in the lower right-hand panel, the partisan factors predict sharing from real news sources in an almost identical fashion. The motivations underlying fake news and real news sharing are, psychologically speaking, highly overlapping. One difference compared to sharing of fake news sources is that our estimates are obtained with greater precision due to the larger number of participants sharing real news sources. Another difference relates to the partisan asymmetry: Concerning real news sharing, we find no asymmetry between Democrats and Republicans; both are equally partisan and strongly motivated by negative feelings towards political opponents. Altogether, these results fit well with predictions by theories on partisan motivations: Partisans share politically congenial news, primarily because of hostile feelings towards the out-party. Further, the lack of evidence for the ignorance theory suggests that these sharers pay more attention to the political usefulness of news rather the information quality. Below, we put the polarization theory to an even stronger test by considering the role of news content in sharing decisions. ",
          "For the sake of clarity, our main analysis employs crude binary distinctions between fake and real news as well as between those who share any news and those who do not share news at all. This section tests the robustness of our results with respect to these distinctions. Most importantly, we operationalize the sharing of fake news as the sharing of all links to news sources with a history of publishing falsehoods. However, some fake news sources produce more misinformation than others. It is possible that when it comes to blatant falsehoods, ignorant rather than polarized people are the main culprits. To test this possibility, we rely on the fake news classification of Grinberg et al. ( 2019) described above and re-run our analysis separately for each of three fake news categories. Our results reported in SM Section 8 show high consistency across the three categories. People who share links from sources that publish \"almost exclusively fabricated stories\" are also more polarized, but not more ignorant, and only slightly more disruptive than those who do not share any such links. We also replicate our results relying on expert ratings of the trustworthiness of 60 news sources (20 mainstream news sources, 20 hyperpartisan websites, and 20 fake news sources) based on Pennycook and Rand (2019a). Whereas this conceptualization offers an even more fine-grained look into the quality of news sources, it comes at a cost of dropping more than 90% of all news sources from our analysis. Following Pennycook and Rand (2019a) we calculate the average trustworthiness of news shared for each individual in our sample and use it as our dependent variable. SM Section 9 demonstrates that the findings reflect those presented in the main text: Political variables -partisanship and feelings towards Republicans and Democrats -correlate strongly with the trustworthiness of shared news sources whereas \"ignorance variables\" -CRT and political knowledge -do not. Finally, we replicate our analysis by modeling the actual count of fake and real news shared instead of binary variables of sharing versus not sharing. This involves a change from logistic regression to Quasi-Poisson models. The two models have important substantive differences. After all, sharing a single fake news link amidst dozens of real news links could be an honest mistake. Relying on the count variables, in contrast, put more weight on respondents with higher numbers of shares within a category. SM section 5 indicates this alternative modelling procedure does not affect our main conclusions. While the effect of political cynicism is significantly reduced, partisan motivation -especially negative affect towards the out-party -remains the primary predictor of fake news sharing. 4 The Role of News Content: Fake news as the Extreme Ends of a",
          "News source sharing patterns across the partisan news continuum. If partisan motives dictate both fake and real news sharing, social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. To test this, Figure 3 plots a heatmap of correlations based on news sharing across the seven news source types. The emerging patterns firmly favor a partisan story: People who share the most articles from ideologically biased real news sources are also most likely to share fake news sources with a similar political bias. In contrast, the off-diagonal correlations show that people shy away from sharing politically opposed sources. Importantly, the plot also reveals a partisan asymmetry. Whereas sharing *pro*-Republican real news correlates highly with sharing *pro*-Republican fake news, the correlation is substantially weaker between sharing *pro*-Democratic fake and real news. Instead, participants sharing *pro*-Democratic real news often share centrist news as well. Thus, while social media users generally favor partisan news sources, the effect is most pronounced in the Republican end of the continuum. 4 In SM Section 4, we present a series of additional robustness tests. First, we show that our results hold after removing 61 panelists who followed -or were being followed by -at least one other panelist. Second, we demonstrate that applying YouGov sample-matching weights do not alter our results. Third, we re-run models that control for age and partisanship; this does not change our results. Fourth, we leverage a potentially better measure of digital literacy (Hargittai and Hsieh 2011); using this measure does not change our substantive conclusions. Fifth, we show that our results are robust to an alternative coding scheme that treats InfoWars as a *pro*-Republican fake news source rather than a real news source. Further, SM Section 8 shows that our conclusions remain intact when using an alternative list of fake news sources compiled by Grinberg et al. (2019). The predictive power of partisanship across the partisan news continuum. If news sources differ systematically in how strongly their content caters to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Figure 4 tests this assertion. The left-hand panel plots estimated logistic regression coefficients from models that examine the association between a seven-point partisanship scale (re-scaled to range from 0 = Democratic Identifier to 1 = Republican Identifier) and sharing content from the seven news source types, while the right-hand panel gives raw counts of shared links stacked by participants' partisanship (where, to ease interpretation, we trichotomize the partisanship scale). Overall, Figure 4 provides more evidence in favor of the partisan logic of news sharing: Both panels show that Republicans are more likely than Democrats to share extreme *pro*-Republican news    Left-hand panels: Headlines shared on Twitter by panelists (upper panel) and headlines scraped from front pages of news sources (lower panel). Red dots: Headlines that mention Republican politicians. Blue dots: Headlines that mention Democratic politicians. In the lower panel, we scraped headlines from the following news sources: Fake Republican: babylonbee.com, dailywire.com, ilovemyfreedom.org, theconservativetreehouse.com, iotwreport.com; Strong Republican: breitbart.com, dailycaller.com, freebeacon.com, nypost.com, townhall.com; Lean Republican: foxnews.com, hotair.com, telegraph.co.uk, washingtonexaminer.com, washingtontimes.com; Center: thehill.com, usatoday.com, npr.com, bbc.com, wsj.com; Lean Democratic: politico.com, nbcnews.com, nytimes.com, theguardian.com, washingtonpost.com; Strong Democratic: alternet.org, dailykos.com, huffingtonpost.com, rawstory.com, vox.com; Fake Democratic: bipartisanreport.com, indiatimes.com, newspunch.com, palmerreport.com, themindunleashed.com. Righthand panels: Headline counts from news sources, conditional on whether headline mentions Republican elites, Democratic elites, or both. Note that headlines mentioning both Republicans and Democrats counted twice in the sentiment analysis. See SM Section 11 and 12 for details on the analysis. are negative towards Democrats. We can go further. For partisans, a critical consideration when promoting news sources on social media is not just the level of negativity expressed towards the out-party. Instead, the most politically useful news source is one that is negative towards the out-party and positive towards the in-party. This implies that we should also examine, for each type of news source, the difference in negativity between mentions of Republican and Democratic elites in Figure 5. Recall that Democrats are more likely to share centrist news sources while Republican partisans prefer fake news sources. The net differences in headline sentiment when mention- We suspect this explains why Republican partisans find fake news more appealing than do Democratic partisans. To get a steady supply of news that caters to their political tastes, Republicans must turn to more extreme news sources, including those known to publish falsehoods. Democrats, in contrast, will have to look no further than to Centrist and *pro*-Democratic real news sources to meet their partisan goals. Importantly, this explanation cannot easily be dismissed by a \"self-selection\" story in which Republican panelists simply decide to share more extreme stories. Focusing on the front page headlines of stories journalists and editors have deemed sufficiently newsworthy to write and publish leads to the same conclusion. Still, whether this speculation is warranted, the combined observations point to an astonishing overlap in the asymmetries in (1) the sources shared by Democrat and Republican partisans, respectively, and (2) the tone of the stories shared from these sources when covering Democratic and Republican elites, respectively. Together, these three analyses corroborate that Republicans approach news source types as if they are placed on a one-dimensional partisan continuum. Democratic partisans, to be sure, do too, but with the very important exception that *pro*-Democratic fake news sources are less attractive to highly polarized Democrats when we compare (a) Democrats to Republicans and (b) *pro*-Democratic fake news to traditional *pro*-Democratic news sources. As we turn to below, this overall pattern suggests that the sharing of fake news is This suggests that fake news sharing is \"business as usual,\" dictated by the same logic partisans use to process other types of information, including real news. From a partisan motivated perspective, fake news is not categorically different from other sources of political information. As the analyses of the news content showed, partisans' decisions to share both fake and real news sources depend on how politically useful they are in derogating the out-party. Under some circumstances, this goal is best achieved with stories from fake news websites. Under other circumstances, this can best be achieved by stories from more credible websites. To the partisan mind, fake news is the extreme end of a news source continuum where the news sources differ in how well they cater to partisan goals. Accordingly, when deciding to share a story from a fake news website, most people do not prioritize whether the story is true or not. This helps explain why cognitive reflection is not related to fake news sharing even though it correlates with the ability to discern fake news from real news. These abilities are not employed in the context of real-life sharing decisions. At the same time, the sum of findings does suggest a dual involvement of accuracy-oriented and goal-oriented motivations. First, fake news sharing is a relatively rare phenomenon that flourishes only among small segments of the population. Like others, and in contrast to what some commentators have feared, we find that only 11% of our participants shared stories from fake news sources during our period of study while half of the participants shared stories from real news web domains. Second, as discussed below, Republicans were more likely than Democrats to share fake news (e.g., Guess, Nagler and Tucker 2019), even though fake news could help Democrats achieve their partisan goals. This suggests that social media users prioritize the usefulness of information when making sharing decisions but prefer useful information that is true rather than false; they care about the veracity of news as long as it does not compromise its usefulness. Two observations merit further discussion. First, it is relevant to discuss the finding that negative feelings towards political opponents rather than positive feelings towards the inparty are the strongest predictor of fake news sharing. There is strong evidence that partisans generally prefer helping the in-party rather than hurting the out-party (Lelkes and Westwood 2017). A hostile political climate, however, may shift the balance. Research suggests that the danger of \"symbolic\" threats against the moral values of one's group can make it \"necessary to strike back, to undermine the credibility of the opposition, and to 'hit them where it hurts\"' (Amira, Wright and Goya-Tocchetto 2019). Twitter -with its fast diffusion of moralizing and emotion-laden political content (Brady et al. 2017) -may have fostered such a hostile environment, making considerations of hurting the out-party loom larger when deciding which news stories to share. Second, we found a significant partisan asymmetry: Republicans were more likely than Democrats to share fake news sources. This could reflect that Republicans are less motivated by accuracy-motivations than Democrats. Yet, accuracy motivations have low explanatory power overall and additional analyses presented in the replication code show that that the associations between ignorance-related variables and fake news sharing do not consistently differ between Democrats and Republicans. Accordingly, other explanations may be at play. One possibility highlights differences in the supply of news (Grinberg et al. 2019): Democrats and Republicans may be equally motivated by accuracy goals but Republicans end up sharing more falsehoods because their Twitter networks offer a greater supply of fake news sources to share from. Another -complementary -supply-side explanation shifts the focus from accuracy-to goal-oriented motivations: Democrats and Republicans are equally motivated to share information that is useful for furthering their political agenda but the supply of useful real news stories is lower for Republicans, propelling them to turn to fake news sites for material. The sentiment analyses of news headlines are consistent with this explanation. Only *pro*-Republican fake news sources were more negative towards Democrats than towards Republicans. This pattern was consistent across news stories shared by the panelists, both partisans and Independents (see SM section 11 for analyses of Independents), and the news stories from the news site front pages. This at least suggests that this asymmetry reflects a difference in the supply of content. A Republican scanning the Internet for materials to share must turn to fake news sources to find news that, on average, satisfy their partisan need to derogate Democrats. Democrats operating in the same manner, in contrast, can stop their search at posts linking to *pro*-Democratic real news websites or even centrist websites. Without individual-level data on news exposure, it is very difficult to tease apart these explanations (see Eady et al. (2019)) and, most likely, all are at play. Still, if the latter explanations are at least partly correct, the partisan asymmetry in fake news sharing may have less to do with psychological differences between Democrats and Republicans and more to do with differences in the supply of useful information (in a partisan sense). This would dovetail with studies demonstrating that Democrats and Republicans are equally motivated by partisan goals (Brandt et al. 2014). Also, we want to note that it is unclear whether a supply-oriented explanation is evidence of actual media bias. Bias is notoriously difficult to establish. Some prior studies have found similar evidence of a liberal bias in legacy media (Groseclose and Milyo 2005), but the general negative coverage of leading Republicans could also reflect a \"bias in reality\" (Groeling 2013) during an unusual period in US politics. Overall, these conclusions have important implications for how to intervene successfully against the circulation of \"fake news.\" If people care primarily about a story's ability to hurt political enemies, we should not be surprised if fact-checking fails to reduce sharing. In fact, our findings suggest that offline realities shape online sharing of \"fake news.\" Fake news sharing relates to the increasingly polarized political debates in the United States, and it may be difficult to fix the problem of fake news without fixing the larger problem of political polarization. Unfortunately, this is much harder to accomplish than adding automatic factchecks on social media platforms. This does not imply that it is impossible to demotivate sharing of \"fake news.\" Evidence is emerging that fake news sharing decreases other people's trust in the sharer (Altay, Hacquin and Mercier 2019). Reputational concerns may explain the low prevalence of fake news. This implies that successful interventions could work by reminding people to \"not make a fool of themselves\" when they are about to share information from non-credible sources. Yet, even if such interventions can drive down sharing of genuine fake news (Pennycook et al. 2019), the underlying problem remains. Polarized partisans can often find plenty of negative content on ordinary news sites. While each individual news story may be true, the overall picture painted by exclusively sharing derogatory news stories may not be. In this perspective, the real problem is the tendency to selectively engage with derogatory news"
        ],
        "ground_truth_definitions": {
          "fake news": {
            "definition": "fabricated information that mimics news media content in form but not in organizational process or intent",
            "context": "With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Pe√±a, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Pe√±a's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fern√°ndez, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fern√°ndez, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fern√°ndez, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4d65bab6f9b1fd61885fe187e6014a89592df187",
        "sections": [
          "Openness and transparency have long been considered key pillars of the scientific ethos (Merton 1973). Yet there is growing awareness that current research practices often deviate from this ideal, and can sometimes produce misleading bodies of evidence (Miguel et al. 2014). As we survey in this article, there is growing evidence documenting the prevalence of publication bias in economics and other scientific fields, as well as specification searching, and widespread inability to replicate empirical findings. Though peer review and robustness checks aim to reduce these problems, they appear unable to solve the problem entirely. While some of these issues have been widely discussed within economics for some time (for instance, see Leamer 1983;Dewald, Thursby, and Anderson 1986;DeLong and Lang 1992), there has been a notable recent flurry of activity documenting these problems, and also generating new ideas for how to address them. The goal of this piece is to survey this emerging literature on research transparency and reproducibility, and synthesize the insights emerging in economics as well as from other fieldsawareness of these issues has also recently come to the fore in political science (Gerber, Green, and Nickerson 2001;Franco, Malhotra, and Simonovits 2014), psychology (Simmons, Nelson, and Simonsohn 2011;Open Science Collaboration 2015), sociology (Gerber and Malhotra 2008a), finance (Harvey, Liu, and Zhu 2015), and other research disciplines as well, including medicine (Ioannidis 2005). We also discuss productive avenues for future work. With the vastly greater computing power of recent decades and the ability to run a nearly infinite number of regressions (Sala-i-Martin 1997), there is renewed concern that nullhypothesis statistical testing is subject to both conscious and unconscious manipulation. At the same time, technological progress has also facilitated various new tools and potential solutions, including by streamlining the online sharing of data, statistical code, and other research materials, as well as the creation of easily accessible online study registries, data repositories, and tools for synthesizing research results across studies. Data-sharing and replication activities are certainly becoming more common within economics research. Yet, as we discuss below, the progress to date is partial, with some journals and fields within economics adopting new practices to promote transparency and reproducibility and many others not (yet) doing so. The rest of the paper is organized as follows: Section 2 focuses on documenting the problems, first framing them with a simple model of the research and publication process (subsection 2.1), then discussing publication bias (subsection 2.2), specification searching (subsection 2.3), and the inability to replicate results (subsection 2.4). Section 3 focuses on possible solutions to these issues: improved analytical methods (subsection 3.1), study registration (subsection 3.2) and pre-analysis plans (subsection 3.3), disclosure and reporting standards (subsection 3.4), and open data and materials (subsection 3.5). Section 4 discusses future directions for research as well as possible approaches to change norms and practices, and concludes.",
          "Multiple problems have been identified within the body of published research results in economics. We focus on three that have come under greater focus in the recent push for transparency: publication bias, specification searching, and an inability to replicate results. Before describing them, it is useful to frame some key issues with a simple model.",
          "A helpful model to frame some of the issues discussed below was developed in the provocatively titled \"Why Most Published Research Findings Are False\" by Ioannidis (2005), which is among the most highly cited medical research articles from recent years. Ioannidis develops a simple model that demonstrates how greater flexibility in data analysis may lead to an increased rate of false positives and thus incorrect inference. Specifically, the model estimates the positive predictive value (PPV) of research, or the likelihood that a claimed empirical relationship is actually true, under various assumptions. A high PPV means that most claimed findings in a literature are reliable; a low PPV means the body of evidence is riddled with false positives. The model is similar to that of Wacholder et al. (2004), which estimates the closely related false positive report probability (FPRP). 2For simplicity, consider the case in which a relationship or hypothesis can be classified in a binary fashion as either a \"true relationship\" or \"no relationship\". Define Ri as the ratio of true relationships to no relationships commonly tested in a research field i (e.g., development economics). Prior to a study being undertaken, the probability that a true relationship exists is thus Ri/(Ri+1). Using the usual notation for statistical power of the test (1-Œ≤) and statistical significance level (Œ±), the PPV in research field i is given by: (eqn. 1) PPVi = (1-Œ≤)Ri‚ÅÑ((1-Œ≤)Ri + Œ±) Clearly, the better powered the study, and the stricter the statistical significance level, the closer the PPV is to one, in which case false positives are largely eliminated. At the usual significance level of Œ± = 0.05 and in the case of a well-powered study (1 -Œ≤=0.80) in a literature in which half of all hypotheses are thought to be true ex ante (Ri = 0.5), the PPV is relatively high at 89%, a level that would not seem likely to threaten the validity of research in a particular economics subfield. However, reality is considerably messier than this best case scenario and, as Ioannidis describes, this could lead much high rates of false positives in practice due to the presence of underpowered studies, specification searching and researcher bias, and the possibility that only a subset of the analysis in a research literature is published. We discuss these extensions in turn. We start with the issue of statistical power. Doucouliagos and Stanley (2013), Doucouliagos, Ioannidis, and Stanley (2016), and others have documented that many empirical economics studies are actually quite underpowered. With a more realistic level of statistical power for many studies, say at 0.50, but maintaining the other assumptions above, the PPV falls to 83%, which is beginning to potentially look like more of a concern. For power = 0.20, fully 33% of statistically significant findings are false positives. This concern, and those discussed next, are all exacerbated by bias in the publication process. If all estimates in a literature were available to the scientific community, researchers could begin to undo the concerns over a low PPV by combining data across studies, effectively achieving greater statistical power and more reliable inference, for instance, using meta-analysis methods. However, as we discuss below, there is growing evidence of a pervasive bias in favor of significant results, in both economics and other fields. If only significant findings are ever seen by the researcher community, then the PPV is the relevant quantity for assessing how credible an individual result is likely to be. Ioannidis extends the basic model to account for the possibility of what he calls researcher bias. Denoted by u, researcher bias is defined as the probability that a researcher presents a non-finding as a true finding, for reasons other than chance variation in the data. This researcher bias could take many forms, including any combination of specification searching, data manipulation, selective reporting, and even outright fraud; below we attempt to quantify the prevalence of these behaviors among researchers. There are many checks in place that attempt to limit this bias, and through the lens of empirical economics research, we might hope that the robustness checks typically demanded of scholars in seminar presentations and during journal peer review manage to keep the most extreme forms of bias in check. Yet we believe most economists would agree that there remains considerable wiggle room in the presentation of results in practice, in most cases due to behaviors that fall far short of outright fraud. Extending the above framework to incorporate the researcher bias term (ui) in field i leads to the following expression: (eqn. 2) PPVi = ((1 -Œ≤)Ri + uiŒ≤Ri)/( (1 -Œ≤)Ri + Œ± + uiŒ≤Ri + ui(1-Œ±)) Here the actual number of true relationships (the numerator) is almost unchanged, though there is an additional term that captures the true effects that are correctly reported as significant only due to author bias. The total number of reported significant effects could be much larger due to both sampling variation and author bias. If we go back to the case of 50% power, Ri = 0.5, and the usual 5% significance level, but now assume that author bias is low at 10%, the PPV falls from 83% to 65%. If 30% of authors are biased in their presentation of results, the PPV drops dramatically to 49%, meaning that nearly half of reported significant effects are actually false positives. In a further extension, Ioannidis examines the case where there are ni different research teams in a field i generating estimates to test a research hypothesis. Once again, if only the statistically significant findings are published, so there is no ability to pool all estimates, then the likelihood that any published estimate is truly statistically significant can again fall dramatically. In Table 1 (a reproduction of Table 4 from Ioannidis (2005), we present a range of parameter values and the resulting PPV. Different research fields may have inherently different levels of the Ri term, where presumably literatures that are in an earlier stage and thus more exploratory presumably have lower likelihoods of true relationships. This simple framework brings a number of the issues we deal with in this article into sharper relief, and contains a number of lessons. Ioannidis (2005) himself concludes that the majority of published findings in medicine are likely to be false, and while we are not prepared to make a similar claim for empirical economics research -in part because it is difficult to quantify some of the key parameters in the model -we do feel that this exercise does raise important concerns about the reliability of findings in many literatures. First off, literatures characterized by statistically under-powered (i.e., small 1-Œ≤) studies are likely to have many false positives. A study may be under-powered both because of small sample sizes, and if the underlying effect sizes are relatively small. A possible approach to address this concern is to employ larger datasets or estimators that are more powerful. Second, the hotter a research field, with more teams (ni) actively running tests and higher stakes around the findings, the more likely it is that findings are false positives. This is both due to the fact that multiple testing generates more false positives (in absolute numbers) and also because author bias (ui) may be greater when the stakes are higher. Author bias is also a concern when there are widespread prejudices in a research field, for instance, against publishing findings that contradict core theoretical concepts or assumptions. Third, the greater the flexibility in research design, definitions, outcome measures, and analytical approaches in a field, the less likely the research findings are to be true, again due to a combination of multiple testing concerns and author bias. One possible approach to address this concern is to mandate greater data sharing so that other scholars can assess the robustness of results to alternative models. Another is through approaches such as pre-analysis plans that effectively force scholars to present a certain core set of analytical specifications, regardless of the results. With this framework in mind, we next present empirical evidence from economics and other social science fields regarding the extent of some of the problems and biases we have been discussing, and then in section 3 turn to potential ways to address them.",
          "Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors. If the research community is unable to track the complete body of statistical tests that have been run, including those that fail to reject the null (and thus are less likely to be published), then we cannot determine the true proportion of tests in a literature that reject the null. Thus it is critically important to understand how many tests have been run. The term \"file drawer problem\" was coined decades ago (Rosenthal 1979) to describe this problem of results that are missing from a body of research evidence. The issue was a concern even earlier, see, for example, Sterling (1959) which warned of \"embarrassing and unanticipated results\" from type I errors if not significant results went unpublished. Important recent research by Franco, Malhotra, and Simonovits (2014) affirms the importance of this issue in practice in contemporary social science research. They document that a large share of empirical analyses in the social sciences are never published or even written up, and the likelihood that a finding is shared with the broader research community falls sharply for \"null\" findings, i.e., that are not statistically significant (Franco, Malhotra, and Simonovits 2014). Cleverly, the authors are able to look inside the file drawer through their access to the universe of studies that passed peer review and were included in a nationally representative social science survey, namely, the NSF-funded Time-sharing Experiments in the Social Sciences, or TESS3 . TESS funded studies across research fields, including in economics, e.g., Walsh, Dolfin, and DiNardo (2009) and Allcott and Taubinsky (2015), as well as political science, sociology and other fields. Franco, Malhotra, and Simonovits successfully tracked nearly all of the original studies over time, keeping track the nature of the empirical results as well as the ultimate publication of the study, across the dozens of studies that participated in the original project. They find a striking empirical pattern: studies where the main hypothesis test yielded null results are 40 percentage points less likely to be published in a journal than a strongly statistically significant result, and a full 60 percentage points less likely to be written up in any form. This finding has potentially severe implications for our understanding of findings in whole bodies of social science research, if \"zeros\" are never seen by other scholars, even in working paper form. It implies that the positive predictive value (PPV) of research is likely to be lower than it would be otherwise, and also has negative implications for the validity of meta-analyses, if null results are not known to the scholars attempting to draw broader conclusions about a body of evidence. Figure 1 reproduces some of the main patterns from Franco, Malhotra, and Simonovits (2014), as described in (Mervis 2014b). Consistent with these findings, other recent analyses have documented how widespread publication bias appears to be in economics research. Brodeur et al. (2016)  (i.e., above 1.96 for t-statistics). This is a strong indication that some combination of author bias and publication bias is fairly common. Using a variety of possible underlying distributions of test statistics, and estimating how selection would affect these distributions, they estimate the residual (\"the valley and the echoing bump\") and conclude that between 10 to 20% of marginally significant empirical results in these journals are likely to be unreliable. They also document that the proportion of misreporting appears to be lower in articles without \"eye-catchers\" (such as asterisks in tables that denote statistical significance), as well as in papers written by more senior authors, including those with tenured authors. A similar pattern strongly suggestive of publication bias also appears in other social science fields including political science, sociology, psychology, as well as in clinical medical research. Gerber and Malhotra (2008a) have used the caliper test, which compares the frequency of test statistics just above and below the key statistical significance cutoff, which is similar in spirit to a regression discontinuity design. Specifically, they compare the number of z-scores lying in the interval [1.96 -ùëãùëã%, 1.96] to the number in (1.96, 1.96 + ùëãùëã%], where X is the size of the caliper, and they examine these differences at 5%, 10%, 15%, and 20% critical values.4  These caliper tests are used to examine reported empirical results in leading sociology journals (the American Sociological Review, American Journal of Sociology, and The Sociological Quarterly) and reject the hypothesis of no publication bias at the 1 in 10 million level (Gerber and Malhotra 2008a). Data from two leading political science journals (the American Political Science Review and American Journal of Political Science) reject the hypothesis of no publication bias at the 1 in 32 billion level (Gerber and Malhotra 2008b). Psychologists have recently developed a related tool called the \"p-curve,\" describing the density of reported p-values in a literature, that again takes advantage of the fact that if the null hypothesis were true (i.e., no effect), p-values should be uniformly distributed between 0 and 1 (Simonsohn, Nelson, and Simmons 2014a). Intuitively, under the null of no effect, a p-value <0.08 should occur 8% of the time, a p-value <0.07 occurs 7% of the time, etc., meaning a pvalue between 0.07 and 0.08, or between any other 0.01-wide interval, should occur 1% of the time. In the case of true non-zero effects, the distribution of p-values should be right-skewed (with a decreasing density), with more low values (0.01) than higher values (0.04) (Hung et al. 1997). 5 In contrast, in bodies of empirical literature suffering from publication bias, or \"phacking\" in their terminology, in which researchers evaluate significance as they collect data and only report results with statistically significant effects, the distribution of p-values would be leftskewed (assuming that researchers stop searching across specifications or collecting data once the desired level of significance is achieved). To test whether a p-curve is right or left skewed, one can construct what the authors call a \"pp-value,\" or p-value of the p-value-the probability of observing a significant p-value at least as extreme if the null were true-and then aggregate the pp-values in a literature with Fisher's method and test for skew with a œá 2 test. The authors also suggest a test of comparing whether a p-curve is flatter than the curve that would result if studies were (somewhat arbitrarily) powered at 33%, and interpret a p-curve that is significantly flatter or left skewed than this as lacking in evidentiary value. The p-curve can also potentially be used to correct effect size estimates in literatures suffering from publication bias; corrected estimates of the \"choice overload\" literature exhibit a change in direction from standard published estimates (Simonsohn,   Nelson, and Simmons 2014b). 6   Thanks to the existence of study registries and ethical review boards in clinical medical research, it is increasingly possible to survey nearly the universe of studies that have been undertaken, along the lines of Franco, Malhotra, and Simonovits (2014). Easterbrook et al. (1991) reviewed the universe of protocols submitted to the Central Oxford Research Ethics Committee, and both Turner et al. (2008) and Kirsch et al. (2008) employ the universe of tests of certain anti-depressant drugs submitted to the FDA, and all found significantly higher publication rates when tests yield statistically significant results. Turner et al. found that 37 of 38 (97%) of trials with positive, i.e., statistically significant, results were published, while only 8 of 24 (33%) with null (or negative) results were published; for a meta-meta-analysis of the latter two studies, see Ioannidis (2008). A simple model of publication bias described in McCrary, Christensen, and Fanelli (2015) suggests that, under some relatively strong assumptions regarding the rate of nonpublication of statistically non-significant results, readers of research studies could potentially adjust their significance threshold to \"undo\" the distortion by using a more stringent t-test statistic of 3.02 (rather than 1.96) to infer statistical significance at 95% confidence. They note that approximately 30% of published test statistics in the social sciences fall between these two cutoffs. It is also possible that this method would break down and result in a \"t-ratio arms race\" if all researchers were to use it, so it is mostly intended for illustrative purposes. As an aside, it is also possible that publication bias could work against rejection of the null hypothesis in some cases. For instance, within economics in cases where there is a strong theoretical presumption among some scholars that the null hypothesis of no effect is likely to hold (e.g., in certain tests of market efficiency) the publication process could be biased by a preference among editors and referees for non-rejection of the null hypothesis of no effect. This complicates efforts to neatly characterize the nature of publication bias, and may limit the application of the method in McCrary, Christensen, and Fanelli (2015). Taken together, a growing body of evidence indicates that publication bias is widespread in economics and many other scientific fields. Stepping back, these patterns do not appear to occur by chance, but are likely to indicate some combination of selective editor (and referee) decision-making, the file-drawer problem alluded to above, and/or widespread specification searching (the focus of the next subsection), which is closely related to what the Ioannidis (2005) model calls author bias.",
          "Scholars working in several specific literatures within economics have argued for the presence of considerable publication bias, including labor economics literatures on minimum wage impacts and on the value of a statistical life, and we discuss both briefly here, as well as several other bodies of evidence in economics. Card and Krueger (1995) conducted a meta-analysis of the minimum wage and unemployment literature, and test for the \"inverse-square-root\" relationship between sample size and t-ratio that one would expect if there was a true effect and no publication bias, since larger samples should generally produce more precise estimates (for a given research design). 7 They find that t-statistics from the 15 studies using quarterly data available at the time of writing are actually negatively correlated with sample sizes. A possible explanation is that a structural change in the effect of the minimum wage (a decline over time) has taken place, but the authors consider publication bias and specification searching a more likely explanation. Neumark and Wascher (1998) construct an alternative test for publication bias, which produces an attenuation of the effect size with larger sample sizes (as sample sizes increased over time) that is qualitatively similar to that in Card and Krueger (1995), but Neumark and Wascher thus place more emphasis on the structural change explanation (i.e., actual effects declined over time) and Several studies have also documented the presence of publication bias in the literature estimating the value of a statistical life (VSL). As government regulations in health, environment and transportation are frequently based on this value, accurate estimation is of great public importance, but there is growing consensus that there is substantial publication bias in this literature, leading to a strong upward bias in reported estimates (Ashenfelter and Greenstone 2004). Using the collection of 37 studies in Bellavance, Dionne, and Lebeau (2009), Doucouliagos, Stanley, and Giles (2012) find that correcting for publication bias reduces the estimates of VSL by 70-80% from that produced by a standard meta-analysis regression. Similar analysis shows that, correcting for publication bias, the VSL also appears largely inelastic to individual income (Doucouliagos, Stanley, and Viscusi 2014). An updated analysis of publication bias in the VSL literature (Viscusi 2015) shows that although publication bias is large and leads to meaningfully inflated estimates, he argues much of it may stem from early studies in the literature that used voluntary reporting of occupational fatalities, while more recent studies estimates employing the Census of Fatal Occupational Injuries (CFOI) suffer from less measurement error and tend to produce larger estimates. Evidence for publication bias has been documented in many other economics research literatures, although not in all. See Longhi, Nijkamp, and Poot (2005) and Knell and Stix (2005) for notable examples. Table 2 describes a number of related publication bias studies that might be of interest to readers, but for reasons of space they are not discussed in detail here. In the most systematic approach to date (to our knowledge), (Doucouliagos and Stanley 2013) carry out a meta-meta-analysis of 87 meta-analysis papers (many of which are reported in Table 2), and find that over half of the literatures suffer from \"substantial\" or \"severe\" publication bias, with particularly large degrees of bias in empirical macroeconomics and in empirical research based on demand theory, and somewhat less publication bias in subfields with multiple contested economic theories. The Journal of Economic Surveys has published many meta-regression papers, including a special issue devoted to meta-regression and publication bias (Roberts 2005). The statistical techniques for assessing publication bias are summarized in Stanley (2005), and many of these are applied in the articles listed in Table 2. One common data visualization approach is the use of funnel graphs, see Stanley and Doucouliagos (2010), Light and Pillemer (1984), and our discussion in section 3 below.",
          "Another important issue related to publication bias and null hypothesis testing is the reporting of the magnitude of effect sizes. Although it appears that economics may fare somewhat better than other social science disciplines in this regard, since economics studies typically report regression coefficients and standard errors while articles in some other disciplines (e.g., psychology) have historically only reported p-values, there is some evidence that under-reporting of effect magnitudes is still a concern. In a review in this Journal, McCloskey and Ziliak (1996) find that 70% of full-length American Economic Review articles did not distinguish between statistical and practical significance. Follow-up reviews in 2004 and 2008 conclude that the situation had not meaningfully improved (Ziliak and McCloskey 2004;Ziliak and McCloskey 2008). DeLong and Lang (1992) is an early contribution that addresses the issue of publication of null findings and effect sizes. They show that only 78 of 276 null hypotheses tested in empirical papers published in leading economics journals at the time were not rejected. However, using the uniform distribution of p-values under a true null hypothesis, and the startling lack of published p-values close to 1, they conclude it is likely that practically all economic hypotheses are indeed false. They also conclude that the null results that actually do get published in journals may also result from publication bias: a null result is arguably more interesting if it contradicts previous statistically significant results. DeLong and Lang go on to suggest that since almost all economic hypotheses are false, empirical evidence should pay more attention to practical significance and effect size rather than statistical significance alone, as is too often the case.",
          "While publication bias implies a distortion of a body of multiple research studies, bias is also possible within any given study (for instance, as captured in the author bias term u in Ioannidis (2005)). In the 1980's and 1990's, expanded access to computing power led to rising concerns that some researchers were carrying out growing numbers of analyses and selectively reporting econometric analysis that supported pre-conceived notions -or were seen as particularly interesting within the research community -and ignoring, whether consciously or not, other specifications that did not. One the most widely cited articles from this period is Leamer's (1983), \"Let's Take the Depending on which set of control variables, or covariates, were included (among state median income, unemployment, percent population nonwhite, percent population 15-24 years old, percent male, percent urban, percent of two-parent households, and several others), the main coefficient of interest-the number of murders estimated to be prevented by each executionranges widely on both sides of zero, from 29 lives saved to 12 lives lost. Of the five ways of classifying variables as important or doubtful that Leamer evaluated, three produced a range of estimates that included zero, suggesting that inference was quite fragile in this case. Leamer's recommendation that observational studies employ greater sensitivity checks, or extreme bounds analysis (EBA), was not limited to testing the effect of including different combinations of covariates, as in Leamer (1983). More detailed descriptions of EBA in Leamer (1978) and Leamer and Leonard (1983)  This flexibility was highlighted in Lovell (1983), who shows that with a few assumptions regarding the variance of the error terms, searching for the best k of c explanatory variables means that a coefficient that appears to be significant at the level ùõºùõº ÔøΩ is actually only significant at the level 1 -(1 -ùõºùõº ÔøΩ) ùëêùëê/ùëòùëò . In the case of ùëòùëò = 2 and 5 candidate variables, this risks greatly overstating significance levels, and the risk is massive if there are, say, 100 candidate variables. Lovell (1983) goes on to argue for the same sort of transparency in analysis as Leamer (1983). Denton (1985) expands on Lovell's work and shows that data mining can occur as a collective phenomenon even if each individual researcher tests only one pre-stated hypothesis, if there is selective reporting of statistically significant results, an argument closely related to the file drawer publication bias discussion above (Rosenthal 1979). Related points have been made in other social science fields in recent years. In psychology, Simmons, Nelson, and Simonsohn \"prove\" that listening to the Beatles' song \"When I'm Sixty-Four\" made listeners a year and a half younger (Simmons, Nelson, and Simonsohn 2011). The extent and ease of this \"fishing\" in analysis is also described in political science by Humphreys, Sierra, and Windt (2013)  The greater use of extra robustness checks in applied economics is designed to limit the extent of specification search, and is a shift in the direction proposed by Leamer (1983), but it is unclear how effective these changes are in reducing bias in practice. As noted above, the analysis of 641 articles from three top economics journals in recent years presented in Brodeur et al. (2016) still shows a disturbing two-humped distribution of p-values, with relatively few p-values between 0.10 and 0.25 and far more just below 0.05. Their analysis also explores the correlates behind this pattern, and finds that this apparent misallocation of p-values just below the accepted statistical significant level was less pronounced for articles written by tenured authors, and tentatively find it less pronounced among studies based on randomized controlled trials (suggesting that improved research design itself may partially constrain data mining), but they did not detect any discernible differences in the pattern based on whether the authors had publicly posted the study's replication data in the journal's public archive.",
          "One area of analytical flexibility that appears particularly important in practice is sub-group analysis. In many cases, there are multiple distinct interaction effects that could plausibly be justified by economic theory, and current datasets have a growing richness of potential covariates. Yet it is rare for applied economics studies to mention how many different interaction effects were tested, increasing the risk that only statistically significant false positives are reported. While there are few systematic treatments of this issue in economics, there has been extensive discussion of this issue within medical research, where the use of non-prespecified subgroup analysis is strongly frowned upon. The FDA does not use subgroup analysis in its drug approval decisions (Maggioni et al. 2007). An oft repeated, and humorous, case comes from a trial of aspirin and streptokinase use after heart attacks conducted in a large number of patients (N=17,187). Aspirin and streptokinase were found to be beneficial, except for patients born under Libra and Gemini, for whom there was a harmful (but not statistically significant) effect (ISIS-2 COLLABORATIVE GROUP 1988). The authors included the zodiac subgroup analysis because journal editors had suggested that 40 subgroups be analyzed, and the authors relented under the condition that they could include a few subgroups of their own choosing to demonstrate the unreliability of such analysis (Schulz and Grimes 2005).",
          "",
          "There have been longstanding concerns within economics over the inability to replicate the results of specific published papers. The pioneering example is a project undertaken by the Journal of Money, Credit, and Banking (JMCB) (Dewald, Thursby, and Anderson 1986). The journal launched the JMCB Data Storage and Evaluation Project with NSF funding in 1982, which requested data and code from authors who published papers in the journal. 9 Despite the adoption of an explicit policy of data sharing by the JMCB during the project, only 78% of authors provided data within six months after multiple requests, although this was certainly an improvement over the 34% data sharing rate in the control group, namely, those who published before the new journal policy went into effect. Of the papers that were still under review by the JMCB at the time of the requests for data, one quarter did not even respond to the request, despite the request coming from the same journal considering their paper. The data that was submitted was often an unlabeled and undocumented mess, a problem that has persisted with recent data sharing policies, as discussed below. Dewald, Thursby, and Anderson (1986) attempted to replicate nine empirical papers, and despite extensive assistance from the original authors, they were often unable to reproduce the papers' published results. Little changed for a long time after the publication of this landmark article. A decade later, in a follow-up piece to the JMCB Project published in the Federal Reserve Bank of St. Louis Review, Anderson and Dewald (1994)  failed to comply with the policy of providing data and code, editor Ben Bernanke made the data and code sharing policy mandatory in 2004 (Bernanke 2004;McCullough 2007). The current AER data policy states: \"It is the policy of the American Economic Review to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide to the Review, prior to publication, the data, programs, and other details of the computations sufficient to permit replication. These will be posted on the AER Web site. The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\"10  In addition to all the journals published by the American Economic Association in April 201611 . Table 3 summarizes journal policies regarding data sharing, publication of replications or comments, and funding or conflict of interest disclosures at twelve of the top economics and finance journals (according to Scientific Journal Rankings). There has clearly been considerable progress along all of these dimensions over the past decade, but journal policies remain a mixed bag. Among these leading journals, most but not all now have some data sharing requirements, and are officially open to publishing papers that could be considered \"replications.\"12 There is also greater use of disclosure statements. The AER conducted a self-review and found relatively good, though still incomplete, compliance with its data sharing policy (Glandon 2010). Despite this positive self-assessment, others observers believe that much work remains to ensure greater access to replication data in economics. Recent studies document that fewer than 15 of over 150 articles in the JMCB archive could be replicated; there is typically little to no verification that the data and code submitted to journals actually generate the published results; and the majority of economics journals still have no explicit data sharing requirements ( McCullough, McGeary, and Harrison 2006;Anderson et al. 2008;McCullough 2009). The uneven nature of progress along these dimensions across economics journals is ",
          "The American Economic Association's journal data sharing policy -which has been adopted by several other journals and organizations nearly verbatim, as shown in Table 3 -allows for some exceptions, importantly, for proprietary data. In particular, the policy reads: \"The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\" In practice, this exemption is requested fairly often by empirical researchers, and the rate is increasing over time. During the past decade, the May American Economic Review Papers & Proceedings issue has featured a \"Report of the Editor\" which details the number of submissions to the journal, as well the number of papers published, those with data, and those that were granted exemptions. Figure 2 presents the percentage of papers in each issue of the AER since 2005 (when information becomes available) through 2015. A few patterns are noteworthy. First, proportion of papers that include data has risen over time, starting at roughly 60% and since increasing into the 70-80% range, capturing the shift towards empirical research in the discipline as a whole. During this period, the proportion of papers using data that received exemptions from 22 the data-sharing policy has risen rapidly, from roughly 10% to 40% over time. Thus replication data is in practice not available in practice for nearly half of all empirical papers published in the AER in recent years. There are many common sources of proprietary or otherwise non-sharable data driving this trend. One of the most common is U.S. government data. There are currently 23 Federal Statistical Research Data Centers (RDC), which provide researchers access to sensitive federal government data that cannot simply be shared publicly on a journal website, typically due to individual or corporate privacy concerns (e.g., IRS tax records). 16 We do not believe that research conducted with this data should be penalized in any way, and in fact, studies employing administrative data may be particularly valuable both intellectually and in terms of public policy decisions. However, despite the exemption from data sharing, it would still be useful for researchers (and journals) to make their work as reproducible as possible given the circumstances, for instance, by at least posting the associated statistical code and providing details about how other scholars could gain similar access to the data. Beyond government data, there are, of course, also an increasing number of proprietary datasets created by corporations or other entities that are willing to share sensitive commercial data with researchers, but not with the public at large where similar issues arise. Beyond commercially proprietary or legally restricted government data, there is also the important issue of norms regarding the sharing of original data collected by scholars themselves. Given the years of effort and funding that goes into creating an original dataset, what special intellectual property rights (if any) do scholars involved in generating data have? Economists should be aware of the incentives created by temporary monopoly rights to intellectual property, and in many ways the issues regarding original data collection are closely linked to traditional arguments around granting private patents. Such monopoly rights, even if temporary, could be socially beneficial if they help to drive the creation of innovative new data sources, such as the explosion of original new survey datasets in development economics over the past two decades. Yet we know of no empirical research that discusses the optimal length of such \"research dataset\" patents; this is an area that demands further attention, especially around the optimal length of exclusive access afforded to originators of new data.17  The increasingly common requirement to share data at the time of journal publication is a cause for concern in some fields. For example, in response to a proposal from the International Committee of Medical Journal Editors to require data sharing within six months after the publication of an article (Taichman et al. 2016) an editorial in the leading New England Journal of Medicine caused an outcry when they responded by describing those who do secondary analysis without the co-authorship and cooperation of the original data collecting author as \"research parasites\" (Longo and Drazen 2016). The journal re-affirmed their commitment to data-sharing (Drazen 2016) and published a supporting piece by Senator Elizabeth Warren (Warren 2016), but also a separate piece calling for a longer embargo period after publication: \"2 years after publication of the primary trial results and an additional 6 months for every year it took to complete the trial, with a maximum of 5 years before trial data are made available to those who were not involved in the trial\" (The International Consortium of Investigators for Fairness in Trial Data Sharing 2016). Presumably the increasing \"patent length\" here for each additional year it took to complete data collection is an attempt to reward research effort in collecting unusually rich longitudinal data. Yet these sorts of rules regarding timeframes seem quite ad hoc (to us, at least), further highlighting the need for a more serious examination of how best to balance the research community's right to replicate and extend existing research with scholars' incentives to invest in valuable original data. In political science, many journals have recently adopted policies similar to the AEA policy described above. For example, the current policy of the American Journal of Political Science states: \"In some limited circumstances, an author may request an exemption from the replication and verification policy. This exemption would allow the author to withhold or limit public access to some or all of the data used in an analysis. All other replication materials (e.g., software commands, etc.) still must be provided. The primary reasons for such exemptions are restricted access datasets and human subjects protection.\" 18 We lack data on how often this exemption is granted, however. Additionally, this journal goes much further than economics journals in one important way: instead of simply collecting and publishing data and code from authors, the editors use a third party research center (namely, the Odum Institute for Research in Social Science, at the University of North Carolina, Chapel Hill for quantitative analysis and the Qualitative Data Repository (QDR), at Syracuse University for qualitative analyses) to verify that the data and statistical code produce the published results.",
          "There given examples such as those discussed below in which there are discrepancies between the original published findings and later replication results, it remains possible that the high-profile cases of failed replication may simply be the tip of the iceberg. Thankfully, a few recent papers have begun to provide some evidence on this question, which we highlight below. We ourselves are no strangers to replication and re-analysis debates: papers by one of the authors of this article, described below, have been part of lively debates on replication and reanalysis using data that we shared publicly. These debates have led us to appreciate the great promise of replication research, as well as its potential pitfalls: exactly like original research studies, replication studies have their own particular strengths and weaknesses, and may serve to either advance the intellectual debate or could obscure particular issues. Yet there is no doubt in our minds that an overall increase in replication research will serve a critical role in establishing the credibility of empirical findings in economics, and in equilibrium, will create stronger incentives for scholars to generate more reliable results. Further complicating matters, an imprecise definition of the term \"replication\" itself often leads to confusion. A taxonomic proposal in Hamermesh (2007) distinguished between \"pure\", \"statistical\", and \"scientific\" replications, while a more recent effort (Clemens 2015) uses the terms \"verification,\" \"reproduction,\" \"reanalysis,\" and \"extension\" to distinguish between replications (the first two) and robustness exercises (the latter two). We first present some existing evidence on the replicability of economics and social science research in the next subsection, and then provide examples of each of Clemens' categories.",
          "The articles in the 1986 Journal of Money Credit and Banking project and the 1994 St. Louis Federal Reserve follow-up mentioned above provided some of the first attempts at systematic replication in economics, with fairly discouraging results. Have things improved in the last few decades? New evidence is emerging about the reliability of empirical economics research. One of the most important recent studies is Camerer et al. (2016), which repeated 18 behavioral economics lab experiments originally published between 2011 and 2014 in the American Economic Review and the Quarterly Journal of Economics to assess their replicability. Figure 3 below reproduces a summary of their findings. Their approach is similar in design to a large scale replication of 100 studies in psychology known as the \"Replication Project: Psychology\", which we discuss in detail below. The replication studies were designed with sample sizes that aimed to have 90% power to detect the original effect size at the 5% significance level. In all, the estimated effects were statistically significant with the same sign in 11 of the 18 replication studies (61.1% Other examples of replication failures abound. Clemens (2015) provides a useful taxonomy, and we provide an example of from each of the categories there to help distinguish between them, namely the two types of replication he discusses (verification and reproduction), and the two types of robustness exercises (reanalysis and extension). Of course not all papers fit easily into one of these categories as most tend to include elements from multiple categories.",
          "Perhaps the most straightforward type of replication in economics involves using the same specification, the same sample, and the same population. Essentially, this is running the same code on the same data and testing if you get the same results. Hamermesh (2007) referred to this as a \"pure replication.\" We believe this basic standard should be expected of all published economics research, and hope this expectation is universal among researchers. One tiny tweak to the definition of verification is that it also includes errors in coding. If an author describes a statistical test in the paper, but the code indisputably does not correctly carry out the test as described, this is also considered a verification failure. One of the earliest cases of quantitative economics research failing a verification test comes from an investigation of the effect of social security on private savings. Feldstein (1974) estimates a life cycle model showing that social security reduces private savings by as much as 50%. There were significant theoretical challenges to carrying out this exercise related to assumptions about the intergenerational transfer of wealth, but Leimer and Lesnoy (1982) discovered that a flaw in Feldstein's computer program that overestimated the growth rate of Social Security wealth for widows led to larger effects of Social Security wealth than when the mistake was corrected. Feldstein replied to the critique saying he was grateful for having the error corrected, but that the central conclusion of the study remains largely unchanged (namely, that Social Security decreased private savings by 44%) (Feldstein 1982). Much of the change in coefficients in the replication exercise resulted from Leimer and Lesnoy including an expanded time series of data-this is not a failure of verification, but rather an extension, which we discuss below.  3). 20 In many (but not all) cases discussed in Clemens, the original authors clearly admit to the failure of verification, but there is vigorous, and we think, healthy scholarly debate about how important those mistakes are and whether the results are still significant -statistically and/or practicallywhen the code or data are corrected. Of course, authors whose papers are subject to replication debates should be commended for providing other scholars with access to their data and code public in the first place, especially for these earlier articles published before journal data-sharing requirements were established.",
          "The other type of replication in Clemens' taxonomy is a reproduction. This approach uses the same analytical specification and the same population, but a different sample. Hamermesh (2007) refers to this as a statistical replication. In economics, this approach would be exhibited in a study that generated a certain set of results using a 5% sample of the census, while a different 5% census sample produced different results, or an experimental economics lab study that produced one set of results with a certain sample while the reproduction study analyzed a different sample from broadly the same population (e.g., U.S. university students). There is, of course, some  2007), the possibility is discussed that one factor contributing to lack of verification is that intermediary datasets constructed from raw data were over-written when the raw data was updated, as sometimes happens with U.S. government data. The work of one of the authors of this paper could be included on this list; see Miguel and Kremer (2004), Aiken et al. (2015) and Hicks, Kremer and Miguel (2015) on the impact of school-based deworming in Kenya. Camerer et al. (2016) could be better classified as failures of reproduction, as long as the samples were in fact collected in broadly the same manner (i.e., in person vs. online). Reproduction failures are perhaps more precisely defined in the hard sciences where experimenters routinely attempt to do the exact same physical process as another lab, albeit with a different sample of molecules, or in the biological sciences where experiments may employ a different sample of animal subjects. For instance, in defining reproduction, Clemens mentions the infamous case of the \"discovery\" of cold fusion by Fleischmann and Pons (1989), which failed to reproduce in Lewis et al. (1989).",
          "Robustness exercises come in two varieties, reanalysis and extensions. Reanalysis uses a different analytical specification on the same population (with either the same or a different sample). Many economics replication studies include both a verification aspect as well as some re-analysis. For instance, Davis (2013) conducts a successful verification of Sachs and Warner (1997), but concludes that reanalysis shows the estimates are somewhat sensitive to different statistical estimation techniques. Other well-known recent reanalysis debates in empirical economics include Miguel, Satyanath, and Sergenti (2004), Ciccone (2011) and Miguel and Satyanath (2011) on civil conflict and GDP growth using rainfall as an instrumental variable; and Acemoglu, Johnson, and Robinson (2001), Albouy (2012), and Acemoglu, Johnson, and Robinson (2012) on institutions and GDP growth with settler mortality as an instrumental variable. The debates over these and other studies makes it clear that reanalysis does not typically settle all key research questions, and the exercise often reveals that empirical economists have considerable flexibility in their analytical choices. This insight makes the development of methods to account for -and possibly constrain -this flexibility, which we discuss below in section 3, all the more important.",
          "Under Clemens' classification system, an extension uses the same analytical specification as an original study but a different population and a different sample. Most often this would be conducting the same analysis carried out in a different time or place. A well-known example of an extension involves Burnside and Dollar (2000), which showed that foreign aid seemed to be effective in increasing GDP if the recipient country was well-governed. However, using the exact same regression specification but including additional countries and years to the dataset, Easterly, Levine, and Roodman (2004) do not obtain the same result. Burnside and Dollar (2004) discuss the differences between the findings and conclude that they occur largely because of the additional countries, rather than lengthening the time series. One widely debated topic in economics that has features of both replication and robustness exercises is the topic of minimum wage impacts on unemployment. In early work, Welch (1974) concluded that early minimum wage legislation decreased teenage employment, increased the cyclicality of teenage employment with respect to the business cycle, and shifted teenage employment towards sectors not covered by the law. However, in the course of using Welch's data, Siskind (1977) discovered that Welch had used data for teenagers 16-19 years old instead of 14-19 years old for certain years, and once this was corrected, the minimum wage did not appear to reduce teenage employment. This was a fairly easy mistake to understand since the Current Population Survey was undergoing changes at the time, and table headings for unpublished data had not even been updated. Welch graciously acknowledged the error, and used the corrected data to extend the analysis to probe impacts by industry sector (Welch 1977). Scholars working on this important topic have, for several decades now, continued to find significant room for disagreement on key issues of sampling, data sources, and statistical analysis methods 21 , matters on which well-intended researchers may well disagree. In this and other similarly contentious debates, we believe that the use of pre-specified research designs and analysis plans could be useful for advancing scientific progress, a point we return to below.",
          "Though we believe (or at least, would prefer to believe) that most instances in which economics studies cannot be replicated are due to inadvertent human error or analytical judgment calls, fraud cannot be completely discounted in all cases. Popular books such as Broad and Wade's Betrayers of the Truth (1983) make it clear that scientists are not always saints. A survey of 234 economists at the 1998 ASSA/AEA meeting investigated falsification of research, inappropriate inclusion or omission of co-authors, and exchange of grades for gifts, money, or sexual favors (List et al. 2001). Both a randomization coin-toss technique to elicit true responses to sensitive questions, as well as a more standard question design, indicate that 4% of respondents admit to having at some time falsified research data, 7-10% of respondents admit to having committed one of four relatively minor research infractions, while up to 0.4% admitted to exchange of grades for gifts, money, or sexual favors. Given the seriousness of some of these offenses, an obvious concern is that these figures understate the actual incidence of fraudulent research practices. A more recent survey of members of the European Economics Association described in Necker ( 2014) asks individuals about the justifiability of certain practices as well as their behavior regarding those practices. Necker shows that 2.6% of researchers admit to having falsified data, while 94% admit to at least one instance of a practice considered inappropriate by the majority of the survey, and there is a clear positive correlation between justifiability and behavior, as well as between perceived professional publication pressures and questionable research practices. Similar surveys in other fields such as Anderson, Martinson, and Vries (2007), which surveyed researchers across disciplines funded by the U.S. National Institutes of Health, and John, Loewenstein, and Prelec (2012) in psychology, as well as a meta-analysis of 18 surveys of academic misbehavior, do not paint a very rosy picture, with 2% of respondents admitting to data fabrication, and 34% admitting to lesser forms of academic misconduct (Fanelli 2009). We are not aware of a recent case in economics that received media attention similar to the Michael Lacour fraud scandal uncovered by Broockman, Kalla, and Aranow (2015) in political science, or the case of Diedrick Stapel (see Carey 2011; Bhattacharjee 2013) in psychology. However, there is considerable evidence of plagiarism and other forms of research malpractice in economics. This Journal itself published the results of a survey sent to 470 economics journal editors, which revealed significant problems (Enders and Hoover 2004). Among the 127 editors who responded, only 19% claimed that their journal had a formal policy on plagiarism, and 42 cases of plagiarism were discovered in an average year, with nearly 24% of editors encountering at least one case. A follow-up survey of rank-and-file economists revealed a general lack of consensus on how to respond to cases of alleged plagiarism (Enders and Hoover 2006). 22Article retraction is another useful indicator of research misconduct. A search of four popular article databases for terms related to article retractions identified by Karabag and Berggren (2012)   and Stapel (2009), which was retracted in 2015 27 , clearly reads \"THIS PAPER HAS BEEN RETRACTED,\" the title has been altered to begin with \"Retracted: ,\" and the pdf features an obvious RETRACTED watermark on every page. This is also the case with all six of the retractions in Karabag and Berggren (2012), as well as other notable recent retractions such as LaCour and Green (2014), which was retracted by Marcia McNutt (2015). Innovation and New Technology for re-publication in Kyklos (Frey, Frey, and Eichenberger 1999). The most recent incident that seemed to attract significant attention was the submission of a substantively identical article to multiple journals within economics, which is also a serious lapse (\"Correspondence: David H Autor and Bruno S. Frey\" 2011). Even if plagiarism of this manner would seem significantly easier to catch in the Internet age, the proliferation of journals partially counteracts this ease. 23 https://plagiarism.repec.org/index.html 24 Although note that journals may present these policies online as opposed to formally publishing them in the journal; for instance, see the The bottom line is that there is little reason to believe that economists are inherently more ethical than other social scientists or researchers in other disciplines, so policies regarding fraud and retraction from other disciplines might potentially be beneficially applied to economics.",
          "This section discusses several new methods and tools that have emerged in economics research over the past two decades -and more forcefully over the past ten years -to address the concerns discussed in Section 2. These approaches have in common a focus on greater transparency and openness in the research process. They include improved research design (including experimental designs and meta-analysis approaches), study registration and pre-analysis plans, strengthened disclosure and reporting practices, and new norms regarding open data and materials. We discuss each in turn.",
          "There have been a number of different responses within economics to the view that pervasive specification searching and publication bias was affecting the credibility of empirical literatures. As mentioned above, there has been a shift towards a greater focus on prospective research design in several fields of applied economics work. Experimental (Duflo, Glennerster, and Kremer 2007) and quasi-experimental (Angrist and Pischke 2010) research designs arguably place more constraints on researchers relative to earlier empirical approaches, since there are natural ways to present data using these designs that researchers are typically compelled to present by colleagues in seminars and by journal referees and editors. Prospective experimental studies also tend to place greater emphasis on adequately powering an analysis statistically, which may help to reduce the likelihood of publishing only false positives (Duflo, Glennerster, and Kremer 2007). There is also suggestive evidence that the adoption of experimental and quasiexperimental empirical approaches is beginning to address some concerns about specification search and publication bias: Brodeur et al. (2016) present tentative evidence that the familiar spike in p-values just below the 0.05 level is less pronounced in randomized control trial studies than in studies utilizing non-experimental methods. Yet improved research design alone may not solve several other key threats to the credibility of empirical economics research, including the possibility that null or \"uninteresting\" findings never become known within the research community.",
          "In addition to improvements in research design, Leamer (1983) argued for greater disclosure of the decisions made in analysis, in what became known as \"extreme bounds analysis\" (described in Section 2). Research along these lines has dealt with model uncertainty by employing combinations of multiple models and specifications, as well as comparisons between them. Leamer himself has continued to advance this agenda (see Leamer 2016). We describe several related approaches here.",
          "A natural way to deal with statistical model uncertainty is through Bayesian model averaging. In this approach, each model in the space of plausible models is assigned a probability of being true based on researcher priors and goodness of fit criteria. Averaging the resulting estimates generates a statistic incorporating model uncertainty: The weighted average they generate implies a large but imprecisely estimated deterrent effect of executions on homicides in the U.S. Of course, even without employing explicit probability weights, simply visualizing the distribution of estimates across the entire space of statistical models can also be quite informative on its own. Two well-cited examples of model averaging engage in a thorough investigation of the determinants of cross-country economic growth. Sala-i-Martin's (1997) famous \"I Just Ran Two Million Regressions\" article uses model weights proportional to the integrated likelihoods of each model, picks all possible three-variable combinations out of 60 covariates that have been reported as being significantly related to economic growth, and finds that only about one-third of the 60 variables can be considered robustly positively correlated with economic growth across models. Sala-i-Martin, Doppelhofer, and Miller ( 2004) conduct what they call Bayesian Averaging of Classical Estimates (BACE), weighting estimates using an approach analogous to Schwarz's BIC, and find that just 18 of 67 variables are significantly and robustly partially correlated with economic growth, once suggesting that many findings reported in the existing empirical literature may be spuriously generated by specification searching and selective reporting.",
          "While specification searching or data mining often has a negative connotation in applied economic research, some scholars have taken a more favorable view of it, as long as the data mining is carried out appropriately (Pagan 1987;Phillips 1988). Advocates of this approach have sometimes been known as the \"LSE school\" of econometrics, or the general-to-specific modelling approach (Gilbert 1989;Hendry 1987;Hendry 1995). This approach is related in spirit to the idea of \"encompassing,\" which is the principle that one statistical model can account for, or explain, another (Mizon and Richard 1986;Bontemps and Mizon 2008). To our knowledge, this approach is more often applied in time series econometrics than in applied microeconomics, but perhaps a closer consideration is warranted. There have also been recent calls within applied economics for greater application of machine learning methods and other data science techniques that share some features with these approaches (Kleinberg, Ludwig, et al. 2015;Kleinberg, Lakkaraju, et al. 2015). For a wide set of views on data mining more broadly and the LSE approach specifically, see the Journal of Economic Methodology, which devoted a special issue to the topic (Backhouse and Morgan 2000).",
          "Simonsohn, Simmons, and Nelson (2015b) propose a method, which they call the \"specification curve,\" that is similar in spirit to Leamer's extreme-bounds analysis, but recommends researchers test the exhaustive combination of analytical decisions, not just decisions about which covariates to include in the model. If the full exhaustive set is too large to be practical, a random subset can be used. After plotting the effect size from each of the specifications, researchers can assess how much the estimated effect size varies, and which combinations of decisions lead to which outcomes. Using permutation tests (for treatment with random assignment) or bootstrapping (for treatment without random assignment), researchers can generate shuffled samples with no true effect by construction, and compare the specification curves from these placebo samples to the specification curve from the actual data. Many comparisons are possible, but the authors suggest comparing the median effect size, the share of results with predicted sign, and share of statistically significant results with predicted sign. A key comparison, which is analogous to the traditional p-value, is the percent of the shuffled samples with as many or more extreme results. Bakkensen and Larson 2014). The specification curve shows that 46% of curves from permuted data show at least as large a median effect size as the original, 16% show at least as many results with the predicted sign, and 85% show at least as many significant results with the predicted sign. This indicates that the results are likely to have been generated by chance. The Bertrand and Mullainathan (2004) specification curve, on the other hand, shows that fewer than 0.2% of the permuted curves generate as large a median effect, 12.5% of permuted curves show at least as many results with the predicted sign, and less than 0.2% of permuted curves show at least as many significant results with the predicted sign, providing evidence that the results are very unlikely to have been generated by chance.",
          "There have been significant advances in the methodological literature on quantifying the extent of publication bias in a given body of literature. Early methods mentioned above include Rosenthal's (1979) method (the \"fail-safe N\"), while Galbraith (1988) advocated for radial plots of log odds ratios, and Card and Krueger (1995) tested for relationships between study sample sizes and t-statistics. Statisticians have developed methods to estimate effect sizes in meta-analyses that control for publication bias (Hedges 1992;Hedges and Vevea 1996). The tools most widely used by economists tend to be simpler, including the widely used funnel plot, which is a scatter plot of some measure of statistical precision (typically the inverse of the standard error), versus the estimated effect size. Estimates generated from smaller samples should usually form the wider base of an inverted funnel, which should be symmetric around more precise estimates in the absence of publication bias. The method is illustrated with several economics examples in Stanley and Doucouliagos (2010), and two of these are reproduced in Figure 4. In addition to scrutinizing the visual plot, a formal test of the symmetry of this plot can be conducted using data from multiple studies and regressing the relevant t-statistics on inverse standard errors: (eqn. 4) The resulting t-test on ùõΩùõΩ 0 , referred to as the Funnel Asymmetry Test (FAT) (Stanley 2008), captures the correlation between estimated effect size and precision, and thus tests for publication bias. Using the FAT, Doucouliagos and Stanley (2009) find evidence of publication bias in Card and Krueger's (1995) sample of minimum wage studies (ùõΩùõΩ 0 ‚â† 0), consistent with their own interpretation of the published literature at that time. ùõΩùõΩ 1 here can also be interpreted as the true effect (called the precision effect test, PET) free of publication bias, and Doucouliagos and Stanley (2009) find no evidence of a true effect of the minimum wage on unemployment. The authors also conduct the FAT-PET tests with 49 additional more recent studies in this literature and find the same results: evidence of significant publication bias and no evidence of an effect of the minimum wage on unemployment. Additional meta-analysis methods, including this \"FAT-PET\" approach, are summarized in Stanley and Doucouliagos (2012).",
          "Other applied econometricians have recently called for increasing the use of multiple testing corrections in order to generate more meaningful inference in study settings with many research hypotheses (Anderson 2008;Fink, McConnell, and Vollmer 2014). The practice of correcting for multiple tests is already widespread in certain scientific fields (e.g., genetics) but has yet to become the norm in economics and other social sciences. Simply put, since we know that pvalues fall below traditional significance thresholds (e.g., 0.05) purely by chance a certain proportion of the time, it makes sense to report adjusted p-values that account for the fact that we are running multiple tests, since this makes it more likely that at least one of our test statistics has a significant p-value simply by chance. There are several multiple testing approaches, some of which are used and explained by Anderson (2008), namely, reporting index tests, controlling the family-wise error rate (FWER), and controlling the false discovery rate (FDR). These are each discussed in turn below.",
          "One option for scholars in cases where there are multiple related outcome measures is to forego reporting the outcomes of numerous tests, and instead standardize the related outcomes and combine them into a smaller number of indices, sometimes referred to as a mean effect. This can be implemented for a family of related outcomes by making all signs agree (i.e., allowing positive values to denote beneficial outcomes), demeaning and dividing by the control group standard deviation, and constructing a weighted average (possibly using the inverse of the covariance matrix to weight each standardized outcome). This new index can be used as a single outcome in a regression model and evaluated with a standard t test. Kling, Liebman, and Katz (2007) implement an early index test in the Moving to Opportunity field experiment using methods developed in biomedicine by O'Brien (1984). This method addresses some concerns regarding the multiplicity of statistical tests by simply reducing the number of tests. A potential drawback is that the index may combine outcomes that are only weakly related, and may obscure impacts on specific outcomes that are of interest to particular scholars, although note that these specific outcomes could also be separately reported for completeness.",
          "The family-wise error rate (FWER) is the probability that at least one true hypothesis in a group is rejected (a type I error, or false positive). This approach is considered most useful when the \"damage\" from incorrectly claiming any hypothesis is false is high. There are several ways to implement this approach, with the simplest method being the Bonferroni correction of simply multiplying every original p-value by the number of tests carried out (Bland and Altman 1995), although this is extremely conservative, and improved methods have also been developed. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher discount factors (Holm 1979). A related and more efficient recent method is the free step-down resampling method, developed by Westfall and Young (1993), which when implemented by Anderson (2008) implies that several highly cited experimental pre-school interventions (namely, the Abecedarian, Perry, and Early Training Project studies) exhibit few positive long-run impacts for males. Another recent method improves on Holm by incorporating the dependent structure of multiple tests. Lee and Shaikh (2014) apply it to reevaluate the Mexican PROGRESA conditional cash transfer program and find that overall program impacts remain positive and significant, but are statistically significant for fewer subgroups (e.g., by gender, education) when controlling for multiple testing. List, Shaikh, and Xu (2016) propose a method of controlling the FWER for three common situations in experimental economics, namely, testing multiple outcomes, testing for heterogeneous treatment effects in multiple subgroups, and testing with multiple treatment conditions.28 ",
          "In situations where a single type I error is not considered very costly, researchers may be willing to use a somewhat less conservative method than the FWER approached discussed above, and trade off some incorrect hypothesis rejections in exchange for greater statistical power. This is made possible by controlling the false discovery rate (FDR), or the percentage of rejections that are type I errors. Benjamini and Hochberg (1995) details a simple algorithm to control this rate at a chosen level under the assumption that the p-values from the multiple tests are independent, though the same method was later shown to also be valid under weaker assumptions (Benjamini and Yekutieli 2001). Benjamini, Krieger, and Yekutieli (2006) describes a two-step procedure with greater statistical power, while Romano, Shaikh, and Wolf (2008) propose the first methods to incorporate information about the dependence structure of the test statistics. Multiple hypothesis testing adjustments have recently been used in finance (Harvey, Liu, and Zhu 2015) to re-evaluate 316 factors from 313 different papers that explain the cross-section of expected stock returns. The authors employ the Bonferroni; Holm (1979);and Benjamini, Hochberg, and Yekutieli (2006) methods to account for multiple testing, and conclude that tstatistics greater than 3.0, and possibly as high as 3.9, should be used instead of the standard 1.96, to actually conclude that a factor explains stock returns with 95% confidence. Index tests and both the FWER and FDR multiple testing corrections are also employed in Casey, Glennerster, and Miguel (2012) to estimate the impacts of a community driven development program in Sierra Leone using a dataset with hundreds of potentially relevant outcome variables.",
          "Another method that controls for, or acts as a reality check, for data snooping or data mining was developed in White (2000). The testing of multiple hypotheses, or repeated use of the same data, is a particularly central problem with time series data used over and over again by multiple scholars, such as data on stock returns, which makes this research quite important in empirical finance. Like the model averaging approach described above, the reality check requires a researcher to estimate the entire space of plausible models, but now compares the performance of the preferred model to a benchmark model (e.g., a model for stock market predictions based on the efficient market hypothesis), and does so repeatedly with bootstrapped samples. To assess whether a certain preferred model actually outperforms the benchmark after accounting for snooping with multiple models, the econometrician first calculates the performance of the preferred model (using mean squared error improvement over the benchmark, or relative profit of the strategy). She then selects a bootstrap sample (with replacement), and calculates the mean squared error improvement (or profit) with the new sample for all of the different plausible statistical models, recording the best mean squared error improvement (or profit) across all the models. This approach can then be repeated 1,000 (or more) times, gathering the 1,000 best mean squared errors (or profits). In the final step, one must compare the original preferred model's mean squared error to the best performance from each of the 1,000 bootstraps. The p-value is the fraction of bootstrapped best fits that outperform the preferred model. (A truly predictive model would have returns higher than 95% of the best performing models from each of the bootstrapped samples.) This method was implemented on a large number of trading rules in Sullivan, Timmermann, and White (1999) and a similar method that addresses the presence of poorly performing or irrelevant alternatives was developed in Hansen (2005).",
          "A leading proposed solution to the problem of publication bias is the registration of empirical studies in a public registry. This would ideally be a centralized database of all attempts to conduct research on a certain question, irrespective of the nature of the results, and such that even null (not statistically significant) findings are not lost to the research community. Top medical journals have adopted a clear standard of publishing only medical trials that are registered (De Angelis et al. 2004). The largest clinical trial registry is clinicaltrials.gov, which helped to inspire the most high-profile study registry within economics, the AEA Randomized Controlled Trial Registry (Katz et al. 2013), which was launched in May 2013. 29   While recent research in medicine finds that the clinical trial registry has not eliminated all under-reporting of null results or other forms of publication bias and specification searching (Laine et al. 2007;Mathieu et al. 2009), they do allow the research community to quantify the extent of these problems and over time may help to constrain inappropriate practices. It also helps scholars locate studies that are delayed in publication, or are never published, helping to fill in gaps in the literature and thus resolving some of the problems identified in Franco, Malhotra, and Simonovits (2014). Though it is too soon after the adoption of the AEA's trial registry to measure its impact on research practices and the robustness of empirical results, it is worth noting that the registry is already being used by many empirical researchers: since inception in 2013, over 850 studies conducted in over 100 countries have been registered, and the pace of registrations continues to rise rapidly. allowing users to create a frozen time-stamped web URL with associated digital object identifier (DOI) for any materials uploaded to OSF. Several popular data storage options (including Dropbox, Dataverse, and GitHub) can also be synced with the OSF and its storage, creating a flexible way for researchers to register their research and materials. As of October 2016, over 7,300 public registrations have been created on OSF since the service launched in 2013.",
          "In addition to serving as a useful way to search for research findings on a particular topic, most supporters of study registration also promote the pre-registration of studies, including preanalysis plans (PAPs) that can be posted and time stamped even before analysis data are collected or otherwise available (Miguel et al. 2014). Registration is now the norm in medical research for randomized trials, and registrations often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the U.S. template may be particularly useful to researchers themselves when developing their own preanalysis plans, and instructors may find it useful in their courses. Building on, and modifying, the FDA's 1998 checklist with insights from these other recent treatments of pre-analysis plans, there appears to be a growing consensus that pre-analysis plans in economics should consider discussing at least the following list of ten issues: Pre-analysis plans are relatively new to economics, and this list is likely to evolve in the coming years as researchers explore the potential, and possible limitations, of this new tool. For those concerned about the possibility of \"scooping\" of new research designs and questions based upon a publicly posted pre-analysis plan or project description, several of the social science registries allow temporary embargoing of project details. For instance, the AEA registry allows an embargo until a specific date or project completion. At the time of writing, the OSF allows a four-year embargo until the information is made public. 31",
          "Recent examples of economics papers based on experiments with PAPs include Casey, Glennerster, and Miguel (2012) and Finkelstein et al. (2012), among others. Casey, Glennerster, and Miguel (2012) discuss evidence from a large-scale field experiment on community driven development (CDD) projects in Sierra Leone. The project, called GoBifo, was intended to make local institutions in post-war Sierra Leone more democratic and egalitarian. GoBifo funds were spent on a variety of local public goods infrastructure (e.g., community centers, schools, latrines, roads), agriculture, and business training projects, and were closely monitored to limit leakage. The analysis finds significant short-run benefits in terms of the \"hardware\" aspects of infrastructure and economic wellbeing: the latrines were indeed built. However, a larger goal of the project, reshaping local institutions, making them more egalitarian, increasing trust, improving local collective action, and strengthening community groups, which the researchers call the \"software effects,\" largely failed. There are a large number of plausible outcome measures along these dimensions, hundreds in total, which the authors analyze using a mean effects index approach for nine different families of outcomes (with multiple testing 31 See http://help.osf.io/m/registrations/l/524207-embargoes. Accessed October 10, 2016. adjustments). The null hypothesis of no impact cannot be rejected at 95% confidence for any of the nine families of outcomes. Yet Casey et al (2012) go on to show that, given the large numbers of outcomes in their dataset, and the multiplicity of ways to define outcome measures, finding some statistically significant results would have been relatively easy\". In fact, the paper includes an example of how, if they had had the latitude to define outcomes without a pre-analysis plan, as has been standard practice in most empirical economics studies (and in other social science fields), the authors could have reported either statistically significant and positive effects, or significantly negative effects, depending on the nature of the \"cherry-picking\" of results. We reproduce their results here as Table 4, where Panel A presents the statistically significant positive impacts identified in the GoBifo data and Panel B highlights negative effects. This finding begs the question: how many empirical economics papers with statistically significant results are, unbeknownst to us, really just some version of either Panel A or Panel B? This is important because, as in Casey et al (2012), the researchers tested a large number of outcomes: hospital admissions through the emergency room (ER) and not through the ER; hospital days; procedures; financial strain (bankruptcy, judgments, liens, delinquency, medical debt, and non-medical debt, measured by credit report data); self-reported health from survey data, and so on. When running such a large number of tests, the researchers again could have discovered some \"significant\" effects simply by chance. The pre-analysis plan, in conjunction with multiple hypothesis testing adjustments, give us more confidence in the main results of the study: that recipients did not improve significantly in terms of physical health measurements, but they were more likely to have health insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Additional studies that have resulted from the experiment have also employed preanalysis plans, and they show that health insurance increased emergency department use (Taubman et al. 2014), had no effect on measured physical health outcomes after two years, but did increase health care use and diabetes management, as well as leading to lower rates of depression and financial strain (Baicker et al. 2013). The health care expansion had no significant effect on employment or earnings (Baicker et al. 2014). Other prominent early examples of economics studies that have employed pre-analysis plans include poverty targeting programs in Indonesia, an evaluation of the Toms shoe company donation program, and a job training program in Turkey, among many others (Olken, Onishi, and Wong 2012;Alatas et al. 2012;Wydick, Katz, and Janet 2014;Hirshleifer et al. 2015). The PAP tool is also spreading to other social sciences beyond economics. For instance, in psychology, a pre-specified replication of an earlier paper that had found a link between female conception risk and racial prejudice failed to find a similar effect (Hawkins, Fitzgerald, and Nosek 2015). One issue that arises for studies that did register a pre-analysis plan is the question of characterizing the extent to which the analysis conforms to the original plan, or if it deviates in important ways from the plan. To appreciate these differences, scholars will need to compare the analysis to the plan, a step that could be seen as adding to the burden of journal editors and referees. Even if the analysis does conform exactly to the PAP, there is still the possibility that authors are consciously or unconsciously emphasizing a subset of the pre-specified analyses in the final study. Berge et al. (2015) develop an approach to comparing the distribution of p-values in the paper's main tables versus those in the PAP in order to quantify the extent of possibly selective reporting between the plan and the paper. The Finkelstein et al (2012) study is a model of transparency regarding the presentation of results. To the authors' credit, all analyses presented in the published paper that were not prespecified are clearly labeled as such; in fact, the exact phrase \"This analysis was not prespecified.\" appears in the paper six times. Tables in the main text and appendix that report analyses that were not pre-specified are labelled with a \"^\" character to set them apart, and are clearly labeled as such.",
          "There remain many open questions about whether, when, and how pre-analysis plans could and should be used in economics research, with open debates about how useful they are in different subfields of the discipline. Olken (2015), for example, highlights both their \"promises and perils.\" On the positive side, pre-analysis plans bind the hands of researchers and greatly limit specification searching, allowing them to take full advantage of the power of their statistical tests (even making one-sided tests reasonable). A further advantage of the use of pre-analysis plans is that they are likely to help shield researchers from pressures to affirm the policy agenda of donors and policymakers, in cases where they have a vested interest in the outcome, or when research focuses on politically controversial topics (such as health care reform). This is especially the case if researchers and their institutional partners can agree on the pre-analysis plan, as a sort of evaluation contract. On the negative side, PAPs are often complex and take valuable time to write. Scientific breakthroughs often come at unexpected times and places, often as a result of exploratory analysis, and the time spent writing PAPs may thus lead less time to spend on less structured data exploration. Coffman and Niederle (2015) argue that there is limited upside from PAPs when replication (in conjunction with hypothesis registries) is possible. In experimental and behavioral economics, where lab experiments utilize samples of locally recruited students and the costs of replicating an experiment are relatively low, they argue that replication could be a viable substitute for pre-analysis plans. Yet there does appear to be a growing consensus, endorsed by Coffman and Niederle, that pre-analysis plans can significantly increase the credibility of reporting and analysis in large-scale randomized trials that are expensive or difficult to repeat, or when a study that relies on a particular contextual factor makes it impossible to replicate. For instance, Berge et al. (2015) carry out a series of lab experiments timed to take place just before the 2013 Kenya elections. Replication of this lab research is clearly impossible due to the unique context, and thus use of a pre-analysis plan is valuable. Olken (2015) as well as Coffman and Niederle (2015) discuss another potential way to address publication bias and specification search: results-blind review. Scholars in psychology have championed this method; studies that are submitted to such review are often referred to as \"registered reports\" in that discipline. Authors write a detailed study protocol and pre-analysis plan, and before the experiment is actually run and data is collected, submit the plan to a journal. Journals review the plan for the quality of the design and the scientific value of the research question, and may choose to give \"in-principle acceptance\". This can be thought of as a kind of revise and resubmit that is contingent on the data being collected and analyzed as planned. If the author follows through on the proposed design, and the data are of sufficiently high quality (e.g., with sufficiently low sample attrition rates in a longitudinal study, etc.), the results are to be published regardless of whether or not they are statistically significant, and whether they conform to the expectations of the editor or referees, or to the conventional wisdom in the discipline. Several psychology journals currently have begun using results-blind review, either regularly or in special issues (Chambers 2013;Chambers et al. 2014;Nosek and Lakens 2014). 32A forthcoming issue of Comparative Political Studies will be the first to feature results-blind review in political science (Findley et al. 2016), and it will include both experimental and observational research studies. In our view, it would also be useful to experiment with results-blind review and registered reports in economics journals. The rise in experimental studies and pre-analysis plans in economics, as evidenced by the rapid growth of the AEA registry, is likely to facilitate the eventual acceptance of this approach.",
          "An important open question is how widely the approach of study registration and hypothesis prespecification could be usefully applied in non-prospective and non-experimental studies. This issue has been extensively discussed in recent years within medical research but consensus has not yet been reached in that community. It actually appears that some of the most prestigious medical research journals, which typically publish randomized trials, are even more in favor of the registration of observational studies than the editors of journals that publish primarily in non-experimental research (see the dueling editorial statements in Epidemiology 2010; The Lancet 2010;Loder, Groves, and MacAuley 2010;Dal-R√© et al. 2014). A major logical concern with the pre-registration of non-prospective observational studies using pre-existing data is that there is often no credible way to verify that pre-registration took place before analysis was completed, which is different than the case of prospective studies in which the data has not yet been collected or accessed. In our view, proponents of the preregistration of observational work have not formulated a convincing response to this obvious concern. A more recent study bases its analysis on Neumark's exact pre-specified tests estimate the effect of minimum wages in Canada and found larger unemployment effects, but they had access to the data before estimating their models and did not have an agreement with the journal, so the value of this \"pre-specification\" is perhaps less clear (Campolieti, Gunderson, and Riddell 2006). In political science, a pre-specified observational analysis measured the effect of the immigration stances of Republican representatives on their 2010 election outcomes (Monogan 2013). It is difficult to see how a researcher could reach Neumark's level of pre-specified detail with a research question with which they were not already intimately familiar. It seems more likely that in a case where the researcher was less knowledgeable they might either pre-specify with an inadequate level of detail, or choose an inappropriate specification; this risk makes it important that researchers should not be punished for deviating from their pre-analysis plan in cases where the plan omits important details or contains errors, as argued in Casey et al (2012). It seems likely to us that the majority of observational empirical work in economics will continue largely as is for the foreseeable future. However, for important, intensely debated, and well-defined questions, it would be desirable in our view for more prospective observational research to be conducted in a pre-specified fashion, following the example in Neumark (2001). Although pre-specification will not always be possible, the fact that large amounts of government data are released to the public on regular schedules, and that many policy changes are known to occur well in advance (such as in the case of the anticipated federal minimum wage changes discussed above, with similar arguments for future elections), will make it possible for the verifiable pre-specification of research analysis to be carried out in many settings.",
          "Another frontier topic in this realm is the use of pre-specified algorithms, including machine learning approaches, rather than exact pre-analysis plans for prospective studies. For instance, the exact procedure to be used to determine which covariates should be included in order to generate the most statistically precise estimates can be laid out in advance, even if those covariates are unknown (and unknowable) before the data has been collected. This approach has not yet been widely adopted in economics (to our knowledge), but has begun to be used in medical trials and biostatistics (van der Laan et al. 2007;Sinisi et al. 2007). A proposal related to, but slightly different than, pre-analysis plans is Nobel Prize winning physicist Saul Perlmutter's suggestion for the social sciences to use \"blind analysis\" (MacCoun and Perlmutter 2015). In blind analysis, researchers add noise to the data while working with it and running the analysis, thus preventing them from knowing which way the results are turning out, and thus either consciously or unconsciously biasing their analysis, until the very end, when the noise is removed and the final results are produced. This technique is apparently quite common in experimental physics (Klein and Roodman 2005), but we are not aware of its use in economics or other social sciences. Major differences are also beginning to emerge in the use of pre-analysis plans, and in the design and interpretation of experimental evidence more broadly, among economists versus scholars in other fields, especially health researchers, with a much greater role of theory in the design of economics experiments. Economists often design experiments to shed light on underlying theoretical mechanisms, to inform ongoing theoretical debates, and measure and estimate endogeneous behavioral responses. These behavioral responses may shed light on broader issues beyond the experimental intervention at hand, and thus could contribute to greater external validity of the results. As a result, pre-analysis plans in economics are often very detailed, and make explicit reference to theoretical models. For example, Bai et al. (2015) preregistered the theoretical microeconomic model and detailed structural econometric approach that they planned to apply to a study of commitment contracts in the Indian health sector. This distinction between the types of studies carried out by medical researchers versus economists (including those working on health topics) has a number of important implications for assessing the reliability of evidence. One has to do with the quality standards and perceptions of the risk of bias in a particular design. For medical trialists accustomed to the CONSORT standards or other medical efficacy trial reporting guidelines (described below), studies that do not feature double-blinding, and thus run the risk of endogeneous behavioral responses to the medical intervention, are considered less reliable than those studies that employ double-blinding (for a detailed discussion, see Eble, Boone, and Elbourne 2014). While a few studies conducted by economists do feature double-blinding (e.g., Thomas et al. 2003;Thomas et al. 2006), in nearly all settings blinding participants to their status is either logistically difficult (for instance, if government partners are unwilling to distribute placebo treatments to some of their population) or even impossible. To illustrate, how would you provide a placebo treatment in a study investigating the impact of the distribution of cash transfers on household consumption patterns? Even in settings that might seem promising for placebo treatments, such as the community-level deworming treatments discussed in Miguel and Kremer (2004), blinding participants to their status is basically impossible: deworming generates side effects (mainly gastrointestinal distress) in roughly 10% of those who take the pills, so community members in a placebo community would quickly deduce that they were in fact not receiving real deworming drugs if there are few or no local cases of side effects. As noted above, endogeneous behavioral responses are often exactly what we economists (and other social scientists) set out to measure and estimate in our field experiments, as described in our pre-analysis plans, and thus are to be embraced rather than rejected as symptomatic of a \"low quality\" research design that is at \"high risk of bias\". Taken together, it is clear to us that the experimental literature in economics (and increasingly in other social sciences such as political science) often has very different objectives than medical, public health and epidemiological research, and thus different research methodologies are often called for. Despite the value of learning from recent experience in biomedical research, and the inspiration that the experience of medical research has played to the rise of new experimental research methods in the social sciences, economists have not simply been able to import existing medical trial methods wholesale, but are developing new and tailored approaches to pre-registration, preanalysis plans, reporting standards, and transparency more broadly.",
          "Another approach to promoting transparency is to establish detailed standards for the disclosure of information regarding study design, data, and analysis. These could serve to limit at least some forms of data mining and specification searching, or at least might make them more apparent to the reader. Detailed reporting standards have become widespread in medical research for both experimental and observational research. Most notably for clinical trials, the Consolidated Standards of Reporting Trials (CONSORT) was developed (Begg C et al. 1996). A before-andafter comparison showed improvement in some measures of study reliability (Moher D et al. 2001), and the standards have been twice revised (Moher, Schulz, and Altman 2001;Schulz et al. 2010) and since extended to at least ten specific types of research designs, interventions, or data. Among others, and possibly particularly relevant for some types of economics research, these include cluster randomized trials (Campbell, Elbourne, and Altman 2004;Campbell et al. 2012), non-pharmacological treatment interventions (Boutron et al. 2008), and patient-reported outcomes (Calvert et al. 2013) In psychology, researchers have created an extension of CONSORT for social and psychological interventions (CONSORT-SPI) (Montgomery et al. 2013;Grant et al. 2013). Others psychologists have proposed that an effective way to reform reporting and disclosure norms within their discipline is for referees to enforce desirable practices when reviewing articles (Simmons, Nelson, and Simonsohn 2011). These authors recommended six conditions for referees to consider include the following: 1. Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. 2. Authors must collect at least 20 observations per cell or else provide a compelling costof-data-collection justification. Recently, we, the authors of this article, were part of an interdisciplinary group of researchers that developed a detailed set of journal guidelines called the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015). This modular set of guidelines for journals features eight categories, namely: citation standards, data transparency, analytic methods (code) transparency, research materials transparency, design and analysis transparency, preregistration of studies, preregistration of analysis plans, and replication -with four levels (0-3) of transparency that journals could choose to endorse or require. For example, with regards to data transparency, the level zero standard is that the journal either encourages data sharing or says nothing, while the level three standard is that \"data must be posted to a trusted repository, and reported analyses will be reproduced independently prior to publication\"; levels 1 and 2 fall somewhere in between. Journals could choose to adopt higher standards in some categories than others, as they feel most appropriate for their research community. In the six months after the guidelines were published in Science, 538 journals and 57 organizations across a wide variety of scientific disciplines, including many in the social sciences, expressed their support for the standards and agreed to evaluate them for potential adoption. Science has now announced that it will be implementing the standards, effective  2016), which argues that differences in implementation between original and replication experiments were inappropriate and introduces noise in addition to the expected sampling error. When taking this into account, one should actually expect the relatively low reported replication rate, and they thus argue there is no replication crisis. Some of the original RPP authors respond that differences between original and replication studies were in fact often endorsed by original study authors and take issue with the statistical analysis in Gilbert et al. (Anderson et al. 2016). Simonsohn (2015) engages in further discussion of how one should evaluate replication results, suggesting that powering a replication based on the effect size of the original study is problematic, and to distinguish the effect size from zero, replications (at least in psychology, with their typically small sample and effect sizes) should have a sample at least 2.5 times as large as the original. An optimistic take by Patil, Peng, and Leek (2016) suggests that researchers should compare the effect in the replication study to a \"prediction interval\" defined as ùëüùëü√¥ùëú ùëúùëúùëñùëñùëúùëú ¬± ùëßùëß 0.975ÔøΩ 1 where ùëüùëüÃÇùëúùëú ùëúùëúùëñùëñùëúùëú is the correlation estimate in the original study, ùëõùëõ ùëúùëúùëúùëúùëñùëñùëúùëú and ùëõùëõ ùëúùëúùê∏ùê∏ùëüùëü are the sample sizes in the original and replication studies, respectively; and ùëßùëß 0.975 is the 97.5% quantile of the normal distribution, which incorporates uncertainty in the estimates from both the original and replication study. Applying this approach leads to much higher estimates of study replication (75%) for the RPP. Economists may be interested to know that the researchers behind the RPP also included a prediction market in their project, and the market did a fairly good job of predicting which of the effects studies would ultimately be reproduced (Dreber et al. 2015). Unlike the prediction market in Camerer et al. (2016), the RPP prediction market outperformed a survey of researcher beliefs. 43 43 For related research on expert predictions, see DellaVigna and Pope (2016) . Other psychology researchers have tried another way to crowdsource replication: instead of bringing different research groups together to all independently run the same classic experiment, other researchers have independently analyzed the same observational dataset and attempted to answer the same question, in this case, the question of whether darker skintoned soccer players receive more red cards as a result of their race, conditional on other factors (Silberzahn and Uhlmann 2015). For instance, it has been shown in multiple instances that it is often trivially easy to identify individuals in purportedly \"de-identified\" and anonymous datasets using publicly available information. In one dramatic illustration, MIT computer science Ph.D. student Latanya Sweeney sent then-Massachusetts Governor William Weld his own complete personal health records only days after anonymized state health records were released to researchers (Sweeney 2002). A new focus of computer science theorists has been developing algorithms for \"differential privacy\" that simultaneously protect individual privacy while allowing for robust analysis of datasets. They have established that there is inherently a trade-off between these two objectives (Dwork and Smith 2010;Heffetz and Ligett 2014), though few actionable approaches to squaring this circle are currently available to applied researchers, to our knowledge.",
          "The rising interest in transparency and reproducibility in economics reflects broader global trends regarding these issues, both among academics and beyond. As such, we argue that \"this statements, and open data and materials lead to improved research quality in a way that can be credibly measured and assessed? To this point, the presumption among advocates (including ourselves, admittedly) is that these changes will indeed lead to improvements, but rigorous evidence on these effects, using meta-analytic approaches or other methods, will be important in determining which practices are in fact most effective, and possibly in building further support for their adoption in the profession. There are many potential avenues for promoting the adoption of new and arguably preferable practices, such as the data sharing, disclosure and pre-registration approaches described at length in this article. One issue that this article does not directly address is how to  Notes: Reproduced from Table 4 of Ioannidis (2005) 65 Notes: Table shows a sample of recent papers conducting meta-analyses and testing for publication bias in certain literatures in economics. Positive evidence for publication bias indicated by '+', evidence for no publication bias with '-', and mixed evidence with '~'. The number of papers and total estimates used in the meta-analysis are also shown.  Longhi, Nijkamp, and Poot (2005) Immigration and wages -18 (348) Publication bias is not found to be a major factor. The negative effect of immigration is quite small (0.1%) and varies by country. Knell and Stix (2005) Income elasticity of money demand -50 (381) Publication bias does not significantly affect the literature. Income elasticities for narrow money range from 0.4 to 0.5 for the US and 1.0 to 1.3 for other countries. Doucouliagos and Laroche (2003) Union productivity effects + 73 (73) Publication bias is not considered a major issue. Negative productivity associations are found in the UK, with positive associations in the US. Gorg and Strobl (2001) Multi-national corporations and productivity spillovers + 21 (25) Study design affects results, with cross-sectional studies reporting higher coefficients than panel data studies. There is also some evidence of publication bias. Ashenfelter, Harmon, and Oosterbeek (1999) Returns to Education + 27 (96) Publication bias is found, and controlling for it significantly reduces the differences between types of estimates of returns to education.  Duvendack, Palmer-Jones, and Reed (2015) or The Replication Network list (http://replicationnetwork.com/replication-studies/) as well as journal websites. Since \"replication\" is an imprecise term, this categorization is perhaps subject to some debate.      S1 shows a non-normalized version). Fourteen studies have a significant effect in the same direction as the original study in the meta-analysis [77.8%;95% CI = (56.5%,99.1%)]."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "certain types of statistical results are more likely to be published than other results, conditional on the research design and data used.",
            "context": "2.2 Publication Bias Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0090023afc66cd2741568599057f4e82b566137c",
        "sections": [
          "Machine learning algorithms have penetrated every aspect of our lives. Algorithms make movie recommendations, suggest products to buy, and who to date. They are increasingly used in high-stakes scenarios such as loans [113] and hiring decisions [19,39]. There are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored [45,119], and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions \"unfair\" [6,121]. In the context of decision-making, fairness is the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics. Thus, an unfair algorithm is one whose decisions are skewed toward a particular group of people. A canonical example comes from a tool used by courts in the United States to make pretrial detention and release decisions. The software, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), measures the risk of a person to recommit another crime. Judges use COMPAS to decide whether to release an offender, or to keep him or her in prison. An investigation into the software found a bias against African-Americans: 1  COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or recidivism. Similar findings have been made in other areas, such as an AI system that judges beauty pageant winners but was biased against darker-skinned contestants, 2 or facial recognition software in digital cameras that overpredicts Asians as blinking. 3 These biased predictions stem from the hidden or neglected biases in data or algorithms. In this survey we identify two potential sources of unfairness in machine learning outcomesthose that arise from biases in the data and those that arise from the algorithms. We review research investigating how biases in data skew what is learned by machine learning algorithms, and nuances in the way the algorithms themselves work to prevent them from making fair decisions-even when the data is unbiased. Furthermore, we observe that biased algorithmic outcomes might impact user experience, thus generating a feedback loop between data, algorithms and users that can perpetuate and even amplify existing sources of bias. We begin the review with several highly visible real-world cases of where unfair machine learning algorithms have led to suboptimal and discriminatory outcomes in Section 2. In Section 3, we describe the different types and sources of biases that occur within the data-algorithms-users loop mentioned above. Next, in Section 4, we present the different ways that the concept of fairness has been operationalized and studied in the literature. We discuss the ways in which these two concepts are coupled. Last, we will focus on different families of machine learning approaches, how fairness manifests differently in each one, and the current state-of-the-art for tackling them in Section 5, followed by potential areas of future work in each of the domains in Section 6.",
          "With the popularity of AI and machine learning over the past decades, and their prolific spread in different applications, safety and fairness constraints have become a significant issue for researchers and engineers. Machine learning is used in courts to assess the probability that a defendant recommits a crime. It is used in different medical fields, in childhood welfare systems [35], and autonomous vehicles. All of these applications have a direct effect in our lives and can harm our society if not designed and engineered correctly, that is with considerations to fairness. [123] has a list of the applications and the ways these AI systems affect our daily lives with their inherent biases, such as the existence of bias in AI chatbots, employment matching, flight routing, and automated legal aid for immigration algorithms, and search and advertising placement algorithms. [67] discusses examples of how bias in the real world can creep into AI and robotic systems, such as bias in face recognition applications, voice recognition, and search engines. Therefore, it is important for researchers and engineers to be concerned about the downstream applications and their potential harmful effects when modeling an algorithm or a system.",
          "COMPAS is an exemplar of a discriminatory system. In addition to this, discriminatory behavior was also evident in an algorithm that would deliver advertisements promoting jobs in Science, Technology, Engineering, and Math (STEM) fields [88]. This advertisement was designed to deliver advertisements in a gender-neutral way. However, less women compared to men saw the advertisement due to gender-imbalance which would result in younger women being considered as a valuable subgroup and more expensive to show advertisements to. This optimization algorithm would deliver ads in a discriminatory way although its original and pure intention was to be gender-neutral. Bias in facial recognition systems [128] and recommender systems [140] have also been largely studied and evaluated and in many cases shown to be discriminative towards certain populations and subgroups. In order to be able to address the bias issue in these applications, it is important for us to know where these biases are coming from and what we can do to prevent them. We have enumerated the bias in COMPAS, which is a widely used commercial risk assessment software. In addition to its bias, it also contains performance issues when compared to humans. When compared to non-expert human judgment in a study, it was discovered to be not any better than a normal human [46]. It is also interesting to note that although COMPAS uses 137 features, only 7 of those were presented to the people in the study. [46] further argues that COMPAS is not any better than a simple logistic regression model when making decisions. We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples' lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools. In another similar study, while investigating sources of group unfairness (unfairness across different groups is defined later), the authors in [145] compared SAVRY, a tool used in risk assessment frameworks that includes human intervention in its process, with automatic machine learning methods in order to see which one is more accurate and more fair. Conducting these types of studies should be done more frequently, but prior to releasing the tools in order to avoid doing harm.",
          "An interesting direction that researchers have taken is introducing tools that can assess the amount of fairness in a tool or system. For example, Aequitas [136] is a toolkit that lets users to test models with regards to several bias and fairness metrics for different population subgroups. Aequitas produces reports from the obtained data that helps data scientists, machine learning researchers, and policymakers to make conscious decisions and avoid harm and damage toward certain populations. AI Fairness 360 (AIF360) is another toolkit developed by IBM in order to help moving fairness research algorithms into an industrial setting and to create a benchmark for fairness algorithms to get evaluated and an environment for fairness researchers to share their ideas [11]. These types of toolkits can be helpful for learners, researchers, and people working in the industry to move towards developing fair machine learning application away from discriminatory behavior.",
          "Most AI systems and algorithms are data driven and require data upon which to be trained. Thus, data is tightly coupled to the functionality of these algorithms and systems. In the cases where the underlying training data contains biases, the algorithms trained on them will learn these biases and reflect them into their predictions. As a result, existing biases in data can affect the algorithms using the data, producing biased outcomes. Algorithms can even amplify and perpetuate existing biases in the data. In addition, algorithms themselves can display biased behavior due to certain design choices, even if the data itself is not biased. The outcomes of these biased algorithms can then be fed into real-world systems and affect users' decisions, which will result in more biased data for training future algorithms. For example, imagine a web search engine that puts specific results at the top of its list. Users tend to interact most with the top results and pay little attention to those further down the list [92]. The interactions of users with items will then be collected by the web search engine, and the data will be used to make future decisions on how information should be presented based on popularity and user interest. As a result, results at the top will become more and more popular, not because of the nature of the result but due to the biased interaction and placement of results by these algorithms [92]. The loop capturing this feedback between biases in data, algorithms, and user interaction is illustrated in Figure 1. We use this loop to categorize definitions of bias in the section below.",
          "",
          "",
          "",
          "",
          "",
          "",
          "Bias can exist in many shapes and forms, some of which can lead to unfairness in different downstream learning tasks. In [144], authors talk about sources of bias in machine learning with their categorizations and descriptions in order to motivate future solutions to each of the sources of bias introduced in the paper. In [120], the authors prepare a complete list of different types of biases with their corresponding definitions that exist in different cycles from data origins to its collection and its processing. Here we will reiterate the most important sources of bias introduced in these two papers and also add in some work from other existing research papers. Additionally, we will introduce a different categorization of these definitions in the paper according to the data, algorithm, and user interaction loop.",
          "In this section we talk about biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes. (1) Measurement Bias. Measurement, or reporting, bias arises from how we choose, utilize, and measure particular features [144]. An example of this type of bias was observed in the recidivism risk prediction tool COMPAS, where prior arrests and friend/family arrests were used as proxy variables to measure level of \"riskiness\" or \"crime\"--which on its own can be viewed as mismeasured proxies. This is partly due to the fact that minority communities are controlled and policed more frequently, so they have higher arrest rates. However, one should not conclude that because people coming from minority groups have higher arrest rates therefore they are more dangerous as there is a difference in how these groups are assessed and controlled [144]. (2) Omitted Variable Bias. Omitted variable bias 4 occurs when one or more important variables are left out of the model [38,114,131]. An example for this case would be when someone designs a model to predict, with relatively high accuracy, the annual percentage rate at which customers will stop subscribing to a service, but soon observes that the majority of users are canceling their subscription without receiving any warning from the designed model. Now imagine that the reason for canceling the subscriptions is appearance of a new strong competitor in the market which offers the same solution, but for half the price. The appearance of the competitor was something that the model was not ready for; therefore, it is considered to be an omitted variable. (3) Representation Bias. Representation bias arises from how we sample from a population during data collection process [144]. Non-representative samples lack the diversity of the population, with missing subgroups and other anomalies. Lack of geographical diversity in datasets like ImageNet (as shown in Figures 3 and4) results in demonstrable bias towards Western cultures. (4) Aggregation Bias. Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders. Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore, a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population [144]. This is true even when they are represented equally in the training data. Any general assumptions about subgroups within the population can result in aggregation bias.  (a) Simpson's Paradox. Simpson's paradox is a type of aggregation bias that arises in the analysis of heterogeneous data [18]. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups (Fig. 2(a)). One of the better-known examples of the type of paradox arose during the gender bias lawsuit in university admissions against UC Berkeley [16]. After analyzing graduate school admissions data, it seemed like there was bias toward women, a smaller fraction of whom were being admitted to graduate programs compared to their male counterparts. However, when admissions data was separated and analyzed over the departments, women applicants had equality and in some cases even a small advantage  over men. The paradox happened as women tended to apply to departments with lower admission rates for both genders. Simpson's paradox has been observed in a variety of domains, including biology [37], psychology [81], astronomy [109], and computational social science [91]. (b) Modifiable Areal Unit Problem is a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation [56]. This bias results in different trends learned when data is aggregated at different spatial scales. (5) Sampling Bias. Sampling bias is similar to representation bias, and it arises due to nonrandom sampling of subgroups. As a consequence of sampling bias, the trends estimated for one population may not generalize to data collected from a new population. For the intuition, consider the example in Figure 2. The left plot represents data collected during a study from three subgroups, which were uniformly sampled (Fig. 2(a)). Suppose the next time the study was conducted, one of the subgroups was sampled more frequently than the rest (Fig. 2(b)). The positive trend found by the regression model in the first study almost completely disappears (solid red line in plot on the right), although the subgroup trends (dashed green lines) are unaffected. (6) Longitudinal Data Fallacy. Researchers analyzing temporal data must use longitudinal analysis to track cohorts over time to learn their behavior. Instead, temporal data is often modeled using cross-sectional analysis, which combines diverse cohorts at a single time point. The heterogeneous cohorts can bias cross-sectional analysis, leading to different conclusions than longitudinal analysis. As an example, analysis of bulk Reddit data [10] revealed that comment length decreased over time on average. However, bulk data represented a cross-sectional snapshot of the population, which in reality contained different cohorts who joined Reddit in different years. When data was disaggregated by cohorts, the comment length within each cohort was found to increase over time. (7) Linking Bias. Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users [120]. In [104] authors show how social networks can be biased toward low-degree nodes when only considering the links in the network and not considering the content and behavior of users in the network. [153] also shows that user interactions are significantly different from social link patterns that are based on features, such as method of interaction or time. The differences and biases in the networks can be a result of many factors, such as network sampling, as shown in [59,111], which can change the network measures and cause different types of problems. 3.1.2 Algorithm to User. Algorithms modulate user behavior. Any biases in algorithms might introduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic outcomes and affect user behavior as a consequence. (1) Algorithmic Bias. Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm [9]. The algorithmic design choices, such as use of certain optimization functions, regularizations, choices in applying regression models on the data as a whole or considering subgroups, and the general use of statistically biased estimators in algorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of the algorithms. (2) User Interaction Bias. User Interaction bias is a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction [9]. This type of bias can be influenced by other types and subtypes, such as presentation and ranking biases. (a) Presentation Bias. Presentation bias is a result of how information is presented [9]. For example, on the Web users can only click on content that they see, so the seen content gets clicks, while everything else gets no click. And it could be the case that the user does not see all the information on the Web [9]. (b) Ranking Bias. The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others. This bias affects search engines [9] and crowdsourcing applications [93]. (3) Popularity Bias. Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation-for example, by fake reviews or social bots [117]. As an instance, this type of bias can be seen in search engines [71,117] or recommendation systems where popular objects would be presented more to the public. But this presentation may not be a result of good quality; instead, it may be due to other biased factors. (4) Emergent Bias. Emergent bias occurs as a result of use and interaction with real users. This bias arises as a result of change in population, cultural values, or societal knowledge usually some time after the completion of design [53]. This type of bias is more likely to be observed in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of prospective users by design [53]. This type of bias can itself be divided into more subtypes, as discussed in detail in [53]. (5) Evaluation Bias. Evaluation bias happens during model evaluation [144]. This includes the use of inappropriate and disproportionate benchmarks for evaluation of applications such as Adience and IJB-A benchmarks. These benchmarks are used in the evaluation of facial recognition systems that were biased toward skin color and gender [24], and can serve as examples for this type of bias [144]. 3.1.3 User to Data. Many data sources used for training ML models are user-generated. Any inherent biases in users might be reflected in the data they generate. Furthermore, when user behavior is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in the data generation process. Here we list several important types of such biases. (1) Historical Bias. Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection [144]. An example of this type of bias can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5% of Fortune 500 CEOs were woman-which would cause the search results to be biased towards male CEOs [144]. These search results were of course reflecting the reality, but whether or not the search algorithms should reflect this reality is an issue worth considering. (2) Population Bias. Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population [120]. Population bias creates non-representative data. An example of this type of bias can arise from different user demographics on different social platforms, such as women being more likely to use Pinterest, Facebook, Instagram, while men being more active in online forums like Reddit or Twitter. More such examples and statistics related to social media use among young adults according to gender, race, ethnicity, and parental educational background can be found in [64]. (3) Self-Selection Bias. Self-selection bias4 is a subtype of the selection or sampling bias in which subjects of the research select themselves. An example of this type of bias can be observed in an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic supporters are more likely to complete the poll. (4) Social Bias. Social bias happens when others' actions affect our judgment. [9]. An example of this type of bias can be a case where we want to rate or review an item with a low score, but when influenced by other high ratings, we change our scoring thinking that perhaps we are being too harsh [9,151]. (5) Behavioral Bias. Behavioral bias arises from different user behavior across platforms, contexts, or different datasets [120]. An example of this type of bias can be observed in [108], where authors show how differences in emoji representations among platforms can result in different reactions and behavior from people and sometimes even leading to communication errors. (6) Temporal Bias. Temporal bias arises from differences in populations and behaviors over time [120]. An example can be observed in Twitter where people talking about a particular topic start using a hashtag at some point to capture attention, then continue the discussion about the event without using the hashtag [120,146]. (7) Content Production Bias. Content Production bias arises from structural, lexical, semantic, and syntactic differences in the contents generated by users [120]. An example of this type of bias can be seen in [118] where the differences in use of language across different gender and age groups is discussed. The differences in use of language can also be seen across and within countries and populations. Existing work tries to categorize these bias definitions into groups, such as definitions falling solely under data or user interaction. However, due to the existence of the feedback loop phenomenon [36], these definitions are intertwined, and we need a categorization which closely models this situation. This feedback loop is not only existent between the data and the algorithm, but also between the algorithms and user interaction [29]. Inspired by these papers, we modeled categorization of bias definitions, as shown in Figure 1, and grouped these definitions on the arrows of the loop where we thought they were most effective. We emphasize the fact again that these definitions are intertwined, and one should consider how they affect each other in this cycle, and address them accordingly.",
          "There are multiple ways that discriminatory bias can seep into data. For instance, using unbalanced data can create biases against underrepresented groups. [170] analyzes some examples of the biases that can exist in the data and algorithms and offer some recommendations and suggestions toward mitigating these issues.",
          "Learning Data. In [24], the authors show that datasets like IJB-A and Adience are imbalanced and contain mainly light-skinned subjects-79.6% in IJB-A and 86.2% in Adience. This can bias the analysis towards dark-skinned groups who are underrepresented in the data. In another instance, the way we use and analyze our data can create bias when we do not consider different subgroups in the data. In [24], the authors also show that considering only malefemale groups is not enough, but there is also a need to use race to further subdivide the gender groups into light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. It's only in this case that we can clearly observe the bias towards dark-skinned females, as previously dark-skinned males would compromise for dark-skinned females and would hide the underlying bias towards this subgroup. Popular machine-learning datasets that serve as a base for most of the developed algorithms and tools can also be biased-which can be harmful to the downstream applications that are based on these datasets. For instance, ImageNet [135] and Open Images [86] are two widely used datasets in machine-learning. In [142], researchers showed that these datasets suffer from representation bias and advocate for the need to incorporate geographic diversity and inclusion while creating such datasets. In addition, authors in [105] write about the existing representational biases in different knowledge bases that are widely used in Natural Language Processing (NLP) applications for different commonsense reasoning tasks.",
          "Applications. These data biases can be more dangerous in other sensitive applications. For example, in medical domains there are many instances in which the data studied and used are skewed toward certain populations-which can have dangerous consequences for the underrepresented communities. [98] showed how exclusion of African-Americans resulted in their misclassification in clinical studies, so they became advocates for sequencing the genomes of diverse populations in the data to prevent harm to underrepresented populations. Authors in [143] studied the 23andMe genotype dataset and found that out of 2,399 individuals, who have openly shared their genotypes in public repositories, 2,098 (87%) are European, while only 58 (2%) are Asian and 50 (2%) African. Other such studies were conducted in [54] which states that UK Biobank, a large and widely used genetic dataset, may not represent the sampling population. Researchers found evidence of a \"healthy volunteer\" selection bias. [150] has other examples of studies on existing biases in the data used in the medical domain. [157] also looks at machine-learning algorithms and data utilized in medical fields, and writes about how artificial intelligence in health care has not impacted all patients equally.",
          "Similar to bias, discrimination is also a source of unfairness. Discrimination can be considered as a source for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes, which may happen intentionally or unintentionally, while bias can be considered as a source for unfairness that is due to the data collection, sampling, and measurement. Although bias can also be seen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic fairness literature it is more intuitive to categorize them as such according to the existing research in these areas. In this survey, we mainly focus on concepts that are relevant to algorithmic fairness issues. [99,133,152] contain more broad information on discrimination theory that involve more multidisciplinary concepts from legal theory, economics, and social sciences which can be referenced by the interested readers.",
          "Differences in treatment and outcomes amongst different groups can be justified and explained via some attributes in some cases. In situations where these differences are justified and explained, it is not considered to be illegal discrimination and hence called explainable [77]. For instance, authors in [77] state that in the UCI Adult dataset [7], a widely used dataset in the fairness domain, males on average have a higher annual income than females. However, this is because on average females work fewer hours than males per week. Work hours per week is an attribute that can be used to explain low income which needs to be considered. If we make decisions, without considering working hours, such that males and females end up averaging the same income, we will lead to reverse discrimination since we would cause male employees to get lower salary than females. Therefore, explainable discrimination is acceptable and legal as it can be explained through other attributes like working hours. In [77], authors present a methodology to quantify the explainable and illegal discrimination in data. They argue that methods that do not take the explainable part of the discrimination into account may result in non-desirable outcomes, so they introduce a reverse discrimination which is equally harmful and undesirable. They explain how to quantify and measure discrimination in data or a classifier's decisions which directly considers illegal and explainable discrimination.",
          "In contrast to explainable discrimination, there is unexplainable discrimination in which the discrimination toward a group is unjustified and therefore considered illegal. Authors in [77] also present local techniques for removing only the illegal or unexplainable discrimination, allowing only for explainable differences in decisions. These are preprocessing techniques that change the training data such that it contains no unexplainable discrimination. We expect classifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination. Unexplainable discrimination consists of direct and indirect discrimination. (1) Direct Discrimination. Direct discrimination happens when protected attributes of individuals explicitly result in non-favorable outcomes toward them [164]. Typically, there are some traits identified by law on which it is illegal to discriminate against, and it is usually these traits that are considered to be \"protected\" or \"sensitive\" attributes in computer science literature. A list of some of these protected attributes is provided in Table 3 as specified in the Fair Housing and Equal Credit Opportunity Acts (FHA and ECOA) [30]. (2) Indirect Discrimination. In indirect discrimination, individuals appear to be treated based on seemingly neutral and non-protected attributes; however, protected groups, or individuals still get to be treated unjustly as a result of implicit effects from their protected attributes (e.g., the residential zip code of a person can be used in decision making processes such as loan applications. However, this can still lead to racial discrimination, such as redlining, as despite the fact that zip code appears to be a non-sensitive attribute, it may correlate with race because of the population of residential areas.) [130,164].",
          "(1) Systemic Discrimination. Systemic discrimination refers to policies, customs, or behaviors that are a part of the culture or structure of an organization that may perpetuate discrimination against certain subgroups of the population [40]. [132] found that employers overwhelmingly preferred competent candidates that were culturally similar to them, and shared similar experiences and hobbies. If the decision-makers happen to belong overwhelmingly to certain subgroups, this may result in discrimination against competent candidates that do not belong to these subgroups. (2) Statistical Discrimination. Statistical discrimination is a phenomenon where decision-makers use average group statistics to judge an individual belonging to that group. It usually occurs when the decision-makers (e.g., employers, or law enforcement officers) use an individual's obvious, recognizable characteristics as a proxy for either hidden or more-difficult-to-determine characteristics, that may actually be relevant to the outcome [124].",
          "Fighting against bias and discrimination has a long history in philosophy and psychology, and recently in machine-learning. However, in order to be able to fight against discrimination and achieve fairness, one should first define fairness. Philosophy and psychology have tried to define the concept of fairness long before computer science. The fact that no universal definition of fairness exists shows the difficulty of solving this problem [138]. Different preferences and outlooks in different cultures lend a preference to different ways of looking at fairness, which makes it harder to come up with just a single definition that is acceptable to everyone in a situation. Indeed, even in computer science, where most of the work on proposing new fairness constraints for algorithms has come from the West, and a lot of these papers use the same datasets and problems to show how their constraints perform, there is still no clear agreement on which constraints are the most appropriate for those problems. Broadly, fairness is the absence of any prejudice or favoritism towards an individual or a group based on their intrinsic or acquired traits in the context of decision-making [139]. Even though fairness is an incredibly desirable quality in society, it can be surprisingly difficult to achieve in practice. With these challenges in mind, many fairness definitions are proposed to address different algorithmic bias and discrimination issues discussed in the previous section.",
          "In [17], authors studied fairness definitions in political philosophy and tried to tie them to machinelearning. Authors in [70] studied the 50-year history of fairness definitions in the areas of education and machine-learning. In [149], authors listed and explained some of the definitions used for fairness in algorithmic classification problems. In [139], authors studied the general public's perception of some of these fairness definitions in computer science literature. Here we will reiterate and provide some of the most widely used definitions, along with their explanations inspired from [149]. Definition 1. (Equalized Odds). The definition of equalized odds, provided by [63], states that \"A predictor ≈∂ satisfies equalized odds with respect to protected attribute A and outcome Y, if ≈∂ and A are independent conditional on Y. P( ≈∂=1|A=0,Y =y) = P( ≈∂=1|A=1,Y =y) , y‚àà{0,1}\". This means that the probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members [149]. In other words, the equalized odds definition states that the protected and unprotected groups should have equal rates for true positives and false positives. Definition 2. (Equal Opportunity). \"A binary predictor ≈∂ satisfies equal opportunity with respect to A and Y if P( ≈∂=1|A=0,Y=1) = P( ≈∂=1|A=1,Y=1)\" [63]. This means that the probability of a person in a positive class being assigned to a positive outcome should be equal for both protected and unprotected (female and male) group members [149]. In other words, the equal opportunity definition states that the protected and unprotected groups should have equal true positive rates. Definition 3. (Demographic Parity). Also known as statistical parity. \"A predictor ≈∂ satisfies demographic parity if P( ≈∂ |A = 0) = P( ≈∂|A = 1)\" [48,87]. The likelihood of a positive outcome [149] should be the same regardless of whether the person is in the protected (e.g., female) group. (1) Individual Fairness. Give similar predictions to similar individuals [48,87]. (2) Group Fairness. Treat different groups equally [48,87]. (3) Subgroup Fairness. Subgroup fairness intends to obtain the best properties of the group and individual notions of fairness. It is different than these notions but uses them in order to obtain better outcomes. It picks a group fairness constraint like equalizing false positive and asks whether this constraint holds over a large collection of subgroups [79,80]. It is important to note that according to [83], it is impossible to satisfy some of the fairness constraints at once except in highly constrained special cases. In [83], the authors show the inherent incompatibility of two conditions: calibration and balancing the positive and negative classes. These cannot be satisfied simultaneously with each other unless under certain constraints; therefore, it is important to take the context and application in which fairness definitions need to be used into consideration and use them accordingly [141]. Another important aspect to consider is time and temporal analysis of the impacts that these definitions may have on individuals or groups. In [95] authors show that current fairness definitions are not always helpful and do not promote improvement for sensitive groups-and can actually be harmful when analyzed over time in some cases. They also show that measurement errors can also act in favor of these fairness definitions; therefore, they show how temporal modeling and measurement are important in evaluation of fairness criteria and introduce a new range of trade-offs and challenges toward this direction. It is also important to pay attention to the sources of bias and their types when trying to solve fairness-related questions.",
          "There have been numerous attempts to address bias in artificial intelligence in order to achieve fairness; these stem from domains of AI. In this section we will enumerate different domains of AI, and the work that has been produced by each community to combat bias and unfairness in their methods. Table 2 provides an overview of the different areas that we focus upon in this survey. While this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories: (1) Pre-processing. Pre-processing techniques try to transform the data so that the underlying discrimination is removed [43]. If the algorithm is allowed to modify the training data, then pre-processing can be used [11]. (2) In-processing. In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process [43]. If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model-either by incorporating changes into the objective function or imposing a constraint [11,14]. (3) Post-processing. Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model [43]. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase [11,14]. Examples of some existing work and their categorization into these types is shown in Table 4. These methods are not just limited to general machine learning techniques, but because of AI's popularity, they have expanded to different domains such as natural language processing and deep learning. From learning fair representations [42,97,112] to learning fair word embeddings [20,58,169], debiasing methods have been proposed in different AI applications and domains. Most of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. In addition, some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes during the training phase but avoiding them during prediction time [94]. A list of protected or sensitive attributes is provided in Table 3. They point out what attributes should not affect the outcome of the decision in housing loan or credit card decision-making [30] according to the law. Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision. Different bias-mitigating methods and techniques are discussed below for different domains-each targeting a different problem in different areas of machine learning in detail. This can expand the horizon of the reader on where and how bias can affect the system and try to help researchers carefully look at various new problems concerning potential places where discrimination and bias can affect the outcome of a system.",
          "Every dataset is the result of several design decisions made by the data curator. Those decisions have consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In order to mitigate the effects of bias in data, some general methods have been proposed that advocate having good practices while using data, such as having datasheets that would act like a supporting document for the data reporting the dataset creation method, its characteristics, motivations, and its skews [13,55]. [12] proposes a similar approach for the NLP applications. A similar suggestion has been proposed for models in [110]. Authors in [66] also propose having labels, just like nutrition labels on food, in order to better categorize each data for each task. In addition to these general techniques, some work has targeted more specific types of biases. For example, [81] has proposed methods to test for cases of Simpson's paradox in the data, and [3,4] proposed methods to discover Simpson's paradoxes in data automatically. Causal models and graphs were also used in some work to detect direct discrimination in the data along with its prevention technique that modifies the data such that the predictions would be absent from direct discrimination [163]. [62] also worked on preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. Other pre-processing approaches, such as messaging [74], preferential sampling [75,76], disparate impact removal [51], also aim to remove biases from the data. ",
          "To address this issue, a variety of methods have been proposed that satisfy some of the fairness definitions or other new definitions depending on the application.",
          "Since classification is a canonical task in machine learning and is widely used in different areas that can be in direct contact with humans, it is important that these types of methods be fair and be absent from biases that can harm some populations. Therefore, certain methods have been proposed [57,78,85,106] that satisfy certain definitions of fairness in classification. For instance, in [147] authors try to satisfy subgroup fairness in classification, equality of opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2,159], and equalized odds in [154]. Other methods try to not only satisfy some fairness constraints but to also be stable toward change in the test set [69]. The authors in [155], propose a general framework for learning fair classifiers. This framework can be used for formulating fairness-aware classification with fairness guarantees. In another work [25], authors propose three different modifications to the existing Naive Bayes classifier for discrimination-free classification. [122] takes a new approach into fair classification by imposing fairness constraints into a Multitask learning (MTL) framework. In addition to imposing fairness during training, this approach can benefit the minority groups by focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy as a whole without attention to accuracy across different groups. In a similar work [49], authors propose a decoupled classification system where a separate classifier is learned for each group. They use transfer learning to reduce the issue of having less data for minority groups. In [73] authors propose to achieve fair classification by mitigating the dependence of the classification outcome on the sensitive attributes by utilizing the Wasserstein distance measure. In [75] authors propose the Preferential Sampling (PS) method to create a discrimination free train data set. They then learn a classifier on this discrimination free dataset to have a classifier with no discrimination. In [102], authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for classification and that can provide interpretability.  4. Algorithms categorized into their appropriate groups based on being pre-processing, inprocessing, or post-processing.",
          "Regression. [14] proposes a fair regression method along with evaluating it with a measure introduced as the \"price of fairness\" (POF) to measure accuracy-fairness trade-offs. They introduce three fairness penalties as follows: Individual Fairness: The definition for individual fairness as stated in [14], \"for every cross pair (ùë•, ùë¶) ‚àà ùëÜ 1 , (ùë• ‚Ä≤ , ùë¶ ‚Ä≤ ) ‚àà ùëÜ 2 , a model ùë§ is penalized for how differently it treats ùë• and ùë• ‚Ä≤ (weighted by a function of |ùë¶ -ùë¶ ‚Ä≤ |) where ùëÜ 1 and ùëÜ 2 are different groups from the sampled population.\" Formally, this is operationalized as Group Fairness: \"On average, the two groups' instances should have similar labels (weighted by the nearness of the labels of the instances)\" [14]. Hybrid Fairness: \"Hybrid fairness requires both positive and both negatively labeled cross pairs to be treated similarly in an average over the two groups\" [14]. In addition to the previous work, [1] considers the fair regression problem formulation with regards to two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision trees to satisfy disparate impact and treatment in regression tasks in addition to classification.",
          "In [167], authors studied the semantic role-labeling models and a famous dataset, imSitu, and realized that only 33% of agent roles in cooking images are man, and the rest of 67% cooking images have woman as agents in the imSitu training set. They also noticed that in addition to the existing bias in the dataset, the model would amplify the bias such that after training a model5 on the dataset, bias is magnified for \"man\", filling only 16% of cooking images. Under these observations, the authors of the paper [167] show that structured prediction models have the risk of leveraging social bias. Therefore, they propose a calibration algorithm called RBA (reducing bias amplification); RBA is a technique for debiasing models by calibrating prediction in structured prediction. The idea behind RBA is to ensure that the model predictions follow the same distribution in the training data. They study two cases: multi-label object and visual semantic role labeling classification. They show how these methods amplify the existing bias in data.",
          "In [137] authors show that vanilla PCA can exaggerate the error in reconstruction in one group of people over a different group of equal size, so they propose a fair method to create representations with similar richness for different populations-not to make them indistinguishable, or to hide dependence on a sensitive or protected attribute. They show that vanilla PCA on the labeled faces in the wild (LFW) dataset [68] has a lower reconstruction error rate for men than for women faces, even if the sampling is done with an equal weight for both genders. They intend to introduce a dimensionality reduction technique which maintains similar fidelity for different groups and populations in the dataset. Therefore, they introduce Fair PCA and define a fair dimensionality reduction algorithm. Their definition of Fair PCA (as an optimization function) is as follows, in which ùê¥ and ùêµ denote two subgroups, ùëà ùê¥ and ùëà ùêµ denote matrices whose rows correspond to rows of ùëà that contain members of subgroups ùê¥ and ùêµ given ùëö data points in ùëÖ ùëõ : And their proposed algorithm is a two-step process listed below: (1) Relax the Fair PCA objective to a semidefinite program (SDP) and solve it. (2) Solve a linear program that would reduce the rank of the solution.",
          "Embedding/Clustering. Inequalities in online communities and social networks can also potentially be another place where bias and discrimination can affect the populations. For example, in online communities users with a fewer number of friends or followers face a disadvantage of being heard in online social media [104]. In addition, existing methods, such as community detection methods, can amplify this bias by ignoring these low-connected users in the network or by wrongfully assigning them to the irrelevant and small communities. In [104] authors show how this type of bias exists and is perpetuated by the existing community detection methods. They propose a new attributed community detection method, called CLAN, to mitigate the harm toward disadvantaged groups in online social communities. CLAN is a two-step process that considers the network structure alongside node attributes to address exclusion bias, as indicated below: (1) Detect communities using modularity values (Step 1-unsupervised using only network structure). ( 2) Train a classifier to classify users in the minor groups, putting them into one of the major groups using held-out node attributes (Step 2-supervised using other node attributes). Fair methods in domains similar to community detection are also proposed, such as graph embedding [22] and clustering [8,31].",
          "Causal Approach to Fairness. Causal models can ascertain causal relationships between variables. Using causal graphs one can represent these causal relationships between variables (nodes of the graph) through the edges of the graph. These models can be used to remove unwanted causal dependence of outcomes on sensitive attributes such as gender or race in designing systems or policies [96]. Many researchers have used causal models and graphs to solve fairness-related concerns in machine learning. In [33,96], authors discuss in detail the subject of causality and its importance while designing fair algorithms. There has been much research on discrimination discovery and removal that uses causal models and graphs in order to make decisions that are irrespective of sensitive attributes of groups or individuals. For instance, in [164] authors propose a causal-based framework that detects direct and indirect discrimination in the data along with their removal techniques. [165] is an extension to the previous work. [160] gives a nice overview of most of the previous work done in this area by the authors, along with discussing system-, group-, and individual-level discrimination and solving each using their previous methods, in addition to targeting direct and indirect discrimination. By expanding on the previous work and generalizing it, authors in [116] propose a similar pathway approach for fair inference using causal graphs; this would restrict certain problematic and discriminative pathways in the causal graph flexibly given any set of constraints. This holds when the path-specific effects can be identified from the observed distribution. In [32] authors introduce the path-specific counterfactual fairness definition which is an extension to counterfactual fairness definition [87] and propose a method to achieve it further extending the work in [116]. In [115] authors extended a formalization of algorithmic fairness from their previous work to the setting of learning optimal policies that are subject to constraints based on definitions of fairness. They describe several strategies for learning optimal policies by modifying some of the existing strategies, such as Q-learning, value search, and G-estimation, based on some fairness considerations. In [162] authors only target discrimination discovery and no removal by finding instances similar to another instance and observing if a change in the protected attribute will change the outcome of the decision. If so, they declare the existence of discrimination. In [82], authors define the following two notions of discrimination-unresolved discrimination and proxy discrimination-as follows: Unresolved Discrimination: \"A variable V in a causal graph exhibits unresolved discrimination if there exists a directed path from A to V that is not blocked by a resolving variable, and V itself is non-resolving\" [82]. Proxy Discrimination: \"A variable V in a causal graph exhibits potential proxy discrimination, if there exists a directed path from A to V that is blocked by a proxy variable and V itself is not a proxy\" [82]. They proposed methods to prevent and avoid them. They also show that no observational criterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal reasoning framework needs to be incorporated. In [127], Instead of using the usual risk difference ùëÖùê∑ = ùëù 1 -ùëù 2 , authors propose a causal risk difference ùëÖùê∑ ùëê = ùëù 1 -ùëù ùëê 2 for causal discrimination discovery. They define ùëù ùëê 2 to be: s‚ààùëÜ ùë§ (s) ùëÖùê∑ ùëê not close to zero means that there is a bias in decision value due to group membership (causal discrimination) or to covariates that have not been accounted for in the analysis (omitted variable bias). This ùëÖùê∑ ùëê then becomes their causal discrimination measure for discrimination discovery. [161] is another work of this type that uses causal networks for discrimination discovery.",
          "",
          "Learning fair representations and avoiding the unfair interference of sensitive attributes has been introduced in many different research papers. A well-known example is the Variational Fair Autoencoder introduced in [97]. Here,they treat the sensitive variable as the nuisance variable, so that by removing the information about this variable they will get a fair representation. They use a maximum mean discrepancy regularizer to obtain invariance in the posterior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into the lower bound of their VAE architecture satisfies their proposed model for having the Variational Fair Autoencoder. Similar work, but not targeting fairness specifically, has been introduced in [72]. In [5] authors also propose a debiased VAE architecture called DB-VAE which learns sensitive latent variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE using these latent variables to debias systems like facial detection systems. In [112] authors model their representation-learning task as an optimization objective that would minimize the loss of the mutual information between the encoding and the sensitive variable. The relaxed version of this assumption is shown in Equation 1. They use this in order to learn fair representation and show that adversarial training is unnecessary and in some cases even counter-productive. In Equation 1, c is the sensitive variable and z the encoding of x. In [42], authors introduce flexibly fair representation learning by disentanglement that disentangles information from multiple sensitive attributes. Their flexible and fair variational autoencoder is not only flexible with respect to downstream task labels but also flexible with respect to sensitive attributes. They address the demographic parity notion of fairness, which can target multiple sensitive attributes or any subset combination of them.",
          "In [90] authors present a framework to mitigate bias in models learned from data with stereotypical associations. They propose a model in which they are trying to maximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary to predict the protected or sensitive variable (stereotyping variable z). The model consists of two parts-the predictor and the adversary-as shown in Figure 6. In their model, the predictor is trained to predict Y given X. With the help of a gradient-based approach like stochastic gradient descent, the model tries to learn the weights W by minimizing some loss function LP( ≈∑, y). The output layer is passed to an adversary, which is another network. This network tries to predict Z. The adversary may have different inputs depending on the fairness definition needing to be achieved. For instance, in order to satisfy Demographic Parity, the adversary would try to predict the protected variable Z using only the predicted label ≈∂ passed as an input to it, while preventing the adversary from learning this is the goal of the predictor. Similarly, to achieve Equality of Odds, the adversary would get the true label Y in addition to the predicted label ≈∂ . To satisfy Equality of Opportunity for a given class y, they would only select instances for the adversary where Y=y. [156] takes an interesting and different direction toward solving fairness issues using adversarial networks by introducing FairGAN which generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. The architecture of their FairGAN model is shown in Figure 5. FairGAN consists of two components: a generator ùê∫ ùê∑ùëíùëê which generates the fake data conditioned on the protected attribute ùëÉ ùê∫ (ùë•, ùë¶, ùë†) = ùëÉ ùê∫ (ùë•, ùë¶|ùë†)ùëÉ ùê∫ (ùë†) where ùëÉ ùê∫ (ùë†) = ùëÉ ùëëùëéùë°ùëé (ùë†), and two discriminators ùê∑ 1 and ùê∑ 2 . ùê∑ 1 is trained to differentiate the real data denoted by ùëÉ ùëëùëéùë°ùëé (ùë•, ùë¶, ùë†) from the generated fake data denoted by ùëÉ ùê∫ (ùë•, ùë¶, ùë†). Fig. 5. Structure of FairGAN as proposed in [156]. Fig. 6. The architecture of adversarial network proposed in [90] ¬© Brian Hu Zhang. In addition to that, for achieving fairness constraints, such as statistical parity, ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 1) = ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 0), the training of ùê∑ 2 is such that it emphasizes differentiation of the two types of synthetic (generated by the model) samples ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 1) and ùëÉ ùê∫ (ùë•, ùë¶|ùë† = 0) indicating if the synthetic samples are from the unprotected or protected groups. Here s denotes the protected or the sensitive variable, and we adapted the same notation as in [156].",
          "5.4.1 Word Embedding. In [20] authors noticed that while using state-of-the-art word embeddings in word analogy tests, \"man\" would be mapped to \"computer programmer\" and \"woman\" would be mapped to \"homemaker.\" This bias toward woman triggered the authors to propose a method to debias word embeddings by proposing a method that respects the embeddings for gender-specific words but debiases embeddings for gender-neutral words by following these steps: (Notice that Step 2 has two different options. Depending on whether you target hard debiasing or soft debiasing, you would use either step 2a or 2b) (1) Identify gender subspace. Identifying a direction of the embedding that captures the bias [20]. (2) Hard debiasing or soft debiasing: (a) Hard debiasing (neutralize and equalize). Neutralize puts away the gender subspace from gender-neutral words and makes sure that all the gender-neutral words are removed and zeroed out in the gender subspace [20]. Equalize makes gender-neutral words to be equidistant from the equality set of gendered words [20]. (b) Soft bias correction. Tries to move as little as possible to retain its similarity to the original embedding as much as possible, while reducing the gender bias. This trade-off is controlled by a parameter [20]. Following on the footsteps of these authors, other future work attempted to tackle this problem [169] by generating a gender-neutral version of (Glove called GN-Glove) that tries to retain gender information in some of the word embedding's learned dimensions, while ensuring that other dimensions are free from this gender effect. This approach primarily relies on Glove as its base model with gender as the protected attribute. However, a recent paper [58] argues against these debiasing techniques and states that many recent works on debiasing word embeddings have been superficial, that those techniques just hide the bias and don't actually remove it. A recent work [23] took a new direction and proposed a preprocessing method for the discovery of the problematic documents in the training corpus that have biases in them, and tried to debias the system by perturbing or removing these documents efficiently from the training corpus. In a very recent work [166], authors target bias in ELMo's contextualized word vectors and attempt to analyze and mitigate the observed bias in the embeddings. They show that the corpus used for training of ELMo has a significant gender skew, with male entities being nearly three times more common than female entities. This automatically leads to gender bias in these pretrained contextualized embeddings. They propose the following two methods for mitigating the existing bias while using the pretrained embeddings in a downstream task, coreference resolution: (1) train-time data augmentation approach, and (2) test-time neutralization approach.",
          "Resolution. The [168] paper shows that coreference systems have a gender bias. They introduce a benchmark, called WinoBias, focusing on gender bias in coreference resolution. In addition to that, they introduce a data-augmentation technique that removes bias in the existing state-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques. Their general approach is as follows: They first generate auxiliary datasets using a rule-based approach in which they replace all the male entities with female entities and the other way around. Then they train models with a combination of the original and the auxiliary datasets. They use the above solution in combination with word2vec debiasing techniques to generate word embeddings. They also point out sources of gender bias in coreference systems and propose solutions to them. They show that the first source of bias comes from the training data and propose a solution that generates an auxiliary data set by swapping male and female entities. Another case arises from the resource bias (word embeddings are bias), so the proposed solution is to replace Glove with a debiased embedding method. Last, another source of bias can come from unbalanced gender lists, and balancing the counts in the lists is a solution they proposed. In another work [134], authors also show the existence of gender bias in three state-of-the-art coreference resolution systems by observing that for many occupations, these systems resolve pronouns in a biased fashion by preferring one gender over the other.",
          "In [21] authors introduce a metric for measuring gender bias in a generated text from a language model based on recurrent neural networks that is trained on a text corpus along with measuring the bias in the training text itself. They use Equation 2, where ùë§ is any word in the corpus, ùëì is a set of gendered words that belong to the female category, such as she, her, woman, etc., and ùëö to the male category, and measure the bias using the mean absolute and standard deviation of the proposed metric along with fitting a univariate linear regression model over it and then analyzing the effectiveness of each of those metrics while measuring the bias. In their language model, they also introduce a regularization loss term that would minimize the projection of embeddings trained by the encoder onto the embedding of the gender subspace following the soft debiasing technique introduced in [20]. Finally, they evaluate the effectiveness of their method on reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise on perplexity. They also point out the effectiveness of word-level bias metrics over the corpus-level metrics.",
          "In [100] authors extend the research in detecting bias in word embedding techniques to that of sentence embedding. They try to generalize bias-measuring techniques, such as using the Word Embedding Association Test (WEAT [26]) in the context of sentence encoders by introducing their new sentence encoding bias-measuring techniques, the Sentence Encoder Association Test (SEAT). They used state-of-the-art sentence encoding techniques, such as CBoW, GPT, ELMo, and BERT, and find that although there was varying evidence of human-like bias in sentence encoders using SEAT, more recent methods like BERT are more immune to biases. That being said, they are not claiming that these models are bias-free, but state that more sophisticated bias discovery techniques may be used in these cases, thereby encouraging more future work in this area.",
          "In [52] authors noticed that when translating the word \"friend\" in the following two sentences from English to Spanish, they achieved different results-although in both cases this word should be translated the same way. \"She works in a hospital, my friend is a nurse.\" \"She works in a hospital, my friend is a doctor.\" In both of these sentences, \"friend\" should be translated to the female version of Spanish friend \"amiga,\" but the results were not reflecting this expectation. For the second sentence, friend was translated to \"amigo,\"-the male version of friend in Spanish. This is because doctor is more stereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it in its performance. To solve this, authors in [52] build an approach that leverages the fact that machine translation uses word embeddings. They use the existing debiasing methods in word embedding and apply them in the machine translation pipeline. This not only helped them to mitigate the existing bias in their system, but also boosted the performance of their system by one BLUE score. In [126] authors show that Google's translate system can suffer from gender bias by making sentences taken from the U.S. Bureau of Labor Statistics into a dozen languages that are gender neutral, including Yoruba, Hungarian, and Chinese, translating them into English, and showing that Google Translate shows favoritism toward males for stereotypical fields such as STEM jobs. In [148] authors annotated and analyzed the Europarl dataset [84], a large political, multilingual dataset used in machine translation, and discovered that with the exception of the youngest age group (20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30), which represents only a very small percentage of the total amount of sentences (0.71%), more male data is available in all age groups. They also looked at the entire dataset and showed that 67.39% of the sentences are produced by male speakers. Furthermore, to mitigate the gender-related issues and to improve morphological agreement in machine translation, they augmented every sentence with a tag on the English source side, identifying the gender of the speaker. This helped the system in most of the cases, but not always, so further work has been suggested for integrating speaker information in other ways. 5.4.6 Named Entity Recognition. In [101], authors investigate a type of existing bias in various named entity recognition (NER) systems. In particular, they observed that in a context where an entity should be tagged as a person entity, such as \"John is a person\" or \"John is going to school\", more female names as opposed to male names are being tagged as non-person entities or not being tagged at all. To further formalize their observations, authors propose six different evaluation metrics that would measure amount of bias among different genders in NER systems. They curated templated sentences pertaining to human actions and applied these metrics on names from U.S census data incorporated into the templates. The six introduced measures each aim to demonstrate a certain type of bias and serve a specific purpose in showing various results as follows: ‚Ä¢ Error Type-1 Unweighted: Through this type of error, authors wanted to recognize the proportion of entities that are tagged as anything other than the person entity in each of the male vs female demographic groups. This could be the entity not being tagged or tagged as other entities, such as location. ùëõ ‚ààùëÅ ùëì ùêº (ùëõ ùë° ùë¶ùëùùëí ‚â† ùëÉùê∏ùëÖùëÜùëÇùëÅ ) |ùëÅ ùëì | ‚Ä¢ Error Type-1 Weighted: This type of error is similar to its unweighted case except authors considered the frequency or popularity of names so that they could penalize if a more popular name is being tagged wrongfully. , where ùëì ùëüùëíùëû ùëì (‚Ä¢) indicates the frequency of a name for a particular year in the female census data. Likewise, ùëì ùëüùëíùëû ùëö (‚Ä¢) indicates the frequency of a name for a particular year in the male census data. ‚Ä¢ Error Type-2 Unweighted: This is a type of error in which the entity is tagged as other entities, such as location or city. Notice that this error does not count if the entity is not tagged. where ‚àÖ indicates that the name is not tagged. ‚Ä¢ Error Type-2 Weighted: This error is again similar to its unweighted case except the frequency is taken into consideration. ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ ùë° ùë¶ùëùùëí ‚àâ {‚àÖ, ùëÉùê∏ùëÖùëÜùëÇùëÅ }) ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ) ‚Ä¢ Error Type-3 Unweighted: This is a type of error in which it reports if the entity is not tagged at all. Notice that even if the entity is tagged as a non-person entity this error type would not consider it. ‚Ä¢ Error Type-3 Weighted: Again, this error is similar to its unweighted case with frequency taken into consideration. ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ ùë° ùë¶ùëùùëí = ‚àÖ) ùëõ ‚ààùëÅ ùëì ùëì ùëüùëíùëû ùëì (ùëõ) Authors also investigate the data that these NER systems are trained on and find that the data is also biased toward female gender by not including as versatile names as there should be to represent female names.",
          "The field of algorithmic fairness is a relatively new area of research and work still needs to be done for its improvement. With that being said, there are already papers that propose fair AI algorithms and bias mitigation techniques and compare different mitigation algorithms using different benchmark datasets in the fairness domain. For instance, authors in [65] propose a geometric solution to learn fair representations that removes correlation between protected and unprotected features. The proposed approach can control the trade-off between fairness and accuracy via an adjustable parameter. In this work, authors evaluate the performance of their approach on different benchmark datasets, such as COMPAS, Adult and German, and compare them against various different approaches for fair learning algorithms considering fairness and accuracy measures [65,72,158,159]. In addition, IBM's AI Fairness 360 (AIF360) toolkit [11] has implemented many of the current fair learning algorithms and has demonstrated some of the results as demos which can be utilized by interested users to compare different methods with regards to different fairness measures.",
          "While there have been many definitions of, and approaches to, fairness in the literature, the study in this area is anything but complete. Fairness and algorithmic bias still holds a number of research opportunities. In this section, we provide pointers to outstanding challenges in fairness research, and an overview of opportunities for development of understudied problems.",
          "There are several remaining challenges to be addressed in the fairness literature. Among them are: (1) Synthesizing a definition of fairness. Several definitions of what would constitute fairness from a machine learning perspective have been proposed in the literature. These definitions cover a wide range of use cases, and as a result are somewhat disparate in their view of fairness. Because of this, it is nearly impossible to understand how one fairness solution would fare under a different definition of fairness. Synthesizing these definitions into one remains an open research problem since it can make evaluation of these systems more unified and comparable. having a more unified fairness definition and framework can also help with the incompatibility issue of some current fairness definitions. (2) From Equality to Equity. The definitions presented in the literature mostly focus on equality, ensuring that each individual or group is given the same amount of resources, attention or outcome. However, little attention has been paid to equity, which is the concept that each individual or group is given the resources they need to succeed [60,103]. Operationalizing this definition and studying how it augments or contradicts existing definitions of fairness remains an exciting future direction. (3) Searching for Unfairness. Given a definition of fairness, it should be possible to identify instances of this unfairness in a particular dataset. Inroads toward this problem have been made in the areas of data bias by detecting instances of Simpson's Paradox in arbitrary datasets [3]; however, unfairness may require more consideration due to the variety of definitions and the nuances in detecting each one. ",
          "In this work we have taxonomized and summarized the current state of research into algorithmic biases and fairness-with a particular focus on machine learning. Even in this area alone, the research is broad. Subareas, from natural language processing, to representation learning, to community detection, have all seen efforts to make their methodologies more fair. Nevertheless, every area has not received the same amount of attention from the research community. Figure 7 provides an overview of what has been done in different areas to address fairness-categorized by the fairness definition type and domain. Some areas (e.g., community detection at the subgroup level) have received no attention in the literature, and could be fertile future research areas.",
          "In this survey we introduced problems that can adversely affect AI systems in terms of bias and unfairness. The issues were viewed primarily from two dimensions: data and algorithms. We illustrated problems that demonstrate why fairness is an important issue. We further showed examples of the potential real-world harm that unfairness can have on society-such as applications in judicial systems, face recognition, and promoting algorithms. We then went over the definitions of fairness and bias that have been proposed by researchers. To further stimulate the interest of readers, we provided some of the work done in different areas in terms of addressing the biases that may affect AI systems and different methods and domains in AI, such as general machine learning, deep learning and natural language processing. We then further subdivided the fields into a more fine-grained analysis of each subdomain and the work being done to address fairness constraints in each. The hope is to expand the horizons of the readers to think deeply while working on a system or a method to ensure that it has a low likelihood of causing potential harm or bias toward a particular group. With the expansion of AI use in our world, it is important that researchers take this issue seriously and expand their knowledge in this field. In this survey we categorized and created a taxonomy of what has been done so far to address different issues in different domains regarding the fairness issue. Other possible future work and directions can be taken to address the existing problems and biases in AI that we discussed in the previous sections.",
          "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR0011890019. We would like to thank the organizers, speakers and the attendees at the IVADO-Mila 2019 Summer School on Bias and Discrimination in AI. We would like to also thank Brian Hu Zhang and Shreya Shankar.",
          "",
          "Aside from the existence of bias in datasets, there are datasets that are specifically used to address bias and fairness issues in machine learning. There are also some datasets that are introduced to target the issues and biases previously observed in older existing datasets. Below we list some of the widely known datasets that have the characteristics discussed in this survey. 9.1.1 UCI Adult Dataset. UCI Adult dataset, also known as \"Census Income\" dataset, contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people's annual incomes, or various other studies [7]. 9.1.2 German Credit Dataset. The German Credit dataset contains 1000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status etc. It can be used in studies about gender inequalities on credit-related issues [47].  [28] 4,753 crime records Social Diversity in faces dataset [107] 1 million images Facial images Table 5. Most widely used datasets in the fairness domain with additional information about each of the datasets including their size and area of concentration."
        ],
        "ground_truth_definitions": {
          "data to algorithm bias": {
            "definition": "biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes",
            "context": "3.1.1 Data to Algorithm. In this section we talk about biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes. (1) Measurement Bias.",
            "type": "explicit"
          },
          "measurement bias/reporting bias": {
            "definition": "how we choose, utilize, and measure particular features",
            "context": "Measurement Bias. Measurement, or reporting, bias arises from how we choose, utilize, and measure particular features [144]. An example of this type of bias was observed in the recidivism risk prediction tool COMPAS, where prior arrests and friend/family arrests were used as proxy variables to measure level of \"riskiness\" or \"crime\"--which on its own can be viewed as mismeasured proxies.",
            "type": "explicit"
          },
          "omitted variable bias": {
            "definition": "one or more important variables are left out of the model",
            "context": "Omitted Variable Bias. Omitted variable bias4 occurs when one or more important variables are left out of the model [38, 114, 131]. An example for this case would be when someone designs a model to predict, with relatively high accuracy, the annual percentage rate at which customers will stop subscribing to a service, but soon observes that the majority of users are canceling their subscription without receiving any warning from the designed model.",
            "type": "explicit"
          },
          "representation bias": {
            "definition": "how we sample from a population during data collection process",
            "context": "Representation Bias. Representation bias arises from how we sample from a population during data collection process [144]. Non-representative samples lack the diversity of the population, with missing subgroups and other anomalies.",
            "type": "explicit"
          },
          "aggregation bias": {
            "definition": "false conclusions are drawn about individuals from observing the entire population.",
            "context": "Aggregation Bias. Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools.",
            "type": "explicit"
          },
          "simpson's paradox": {
            "definition": "a type of aggregation bias that arises in the analysis of heterogeneous data. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups.",
            "context": "Simpson's Paradox. Simpson's paradox is a type of aggregation bias that arises in the analysis of heterogeneous data [18]. The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups (Fig. 2(a)). One of the better-known examples of the type of paradox arose during the gender bias lawsuit in university admissions against UC Berkeley [16].",
            "type": "explicit"
          },
          "modifiable areal unit problem": {
            "definition": "a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation",
            "context": "Modifiable Areal Unit Problem is a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation [56]. This bias results in different trends learned when data is aggregated at different spatial scales.",
            "type": "explicit"
          },
          "sampling bias": {
            "definition": "nonrandom sampling of subgroups.",
            "context": "Sampling Bias. Sampling bias is similar to representation bias, and it arises due to nonrandom sampling of subgroups. As a consequence of sampling bias, the trends estimated for one population may not generalize to data collected from a new population",
            "type": "explicit"
          },
          "linking bias": {
            "definition": "network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users",
            "context": "Linking Bias. Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users [120]. In [104] authors show how social networks can be biased toward low-degree nodes when only considering the links in the network and not considering the content and behavior of users in the network.",
            "type": "explicit"
          },
          "algorithm to user bias": {
            "definition": "a result of algorithmic outcomes and affect user behavior as a consequence.",
            "context": "Any biases in algorithms might introduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic outcomes and affect user behavior as a consequence. (1) Algorithmic Bias.",
            "type": "explicit"
          },
          "algorithmic bias": {
            "definition": "when the bias is not present in the input data and is added purely by the algorithm.",
            "context": "Algorithmic Bias. Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm [9]. The algorithmic design choices, such as use of certain optimization functions, regularizations, choices in applying regression models on the data as a whole or considering subgroups, and the general use of statistically biased estimators in algorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of the algorithms.",
            "type": "explicit"
          },
          "user interaction bias": {
            "definition": "a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction.",
            "context": "User Interaction Bias. User Interaction bias is a type of bias that can not only be observant on the Web but also get triggered from two sources-the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction [9]. This type of bias can be influenced by other types and subtypes, such as presentation and ranking biases.",
            "type": "explicit"
          },
          "ranking bias": {
            "definition": "The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others.",
            "context": "Ranking Bias. The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others. This bias affects search engines [9] and crowdsourcing applications [93].",
            "type": "explicit"
          },
          "popularity bias": {
            "definition": "Items that are more popular tend to be exposed more.",
            "context": "Popularity Bias. Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation-for example, by fake reviews or social bots [117]. As an instance, this type of bias can be seen in search engines [71, 117] or recommendation systems where popular objects would be presented more to the public. But this presentation may not be a result of good quality; instead, it may be due to other biased factors.",
            "type": "explicit"
          },
          "emergent bias": {
            "definition": "a result of change in population, cultural values, or societal knowledge usually some time after the completion of design",
            "context": "Emergent Bias. Emergent bias occurs as a result of use and interaction with real users. This bias arises as a result of change in population, cultural values, or societal knowledge usually some time after the completion of design [53]. This type of bias is more likely to be observed in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of prospective users by design [53].",
            "type": "explicit"
          },
          "user to data bias": {
            "definition": "Any inherent biases in users might be reflected in the data they generate.",
            "context": "Many data sources used for training ML models are user-generated. Any inherent biases in users might be reflected in the data they generate. Furthermore, when user behavior is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in the data generation process.",
            "type": "explicit"
          },
          "historical bias": {
            "definition": "the already existing bias and socio-technical issues in the world.",
            "context": "Historical Bias. Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection [144]. An example of this type of bias can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5% of Fortune 500 CEOs were woman-which would cause the search results to be biased towards male CEOs [144]",
            "type": "explicit"
          },
          "population bias": {
            "definition": "when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population",
            "context": "Population Bias. Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population [120]. Population bias creates non-representative data.",
            "type": "explicit"
          },
          "self-selection bias": {
            "definition": "a subtype of the selection or sampling bias in which subjects of the research select themselves.",
            "context": "Self-Selection Bias. Self-selection bias4 is a subtype of the selection or sampling bias in which subjects of the research select themselves. An example of this type of bias can be observed in an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic supporters are more likely to complete the poll.",
            "type": "explicit"
          },
          "behavioral bias": {
            "definition": "different user behavior across platforms, contexts, or different datasets.",
            "context": "Behavioral Bias. Behavioral bias arises from different user behavior across platforms, contexts, or different datasets [120]. An example of this type of bias can be observed in [108], where authors show how differences in emoji representations among platforms can result in different reactions and behavior from people and sometimes even leading to communication errors.",
            "type": "explicit"
          },
          "temporal bias": {
            "definition": "differences in populations and behaviors over time.",
            "context": "Temporal Bias. Temporal bias arises from differences in populations and behaviors over time [120]. An example can be observed in Twitter where people talking about a particular topic start using a hashtag at some point to capture attention, then continue the discussion about the event without using the hashtag [120, 146].",
            "type": "explicit"
          },
          "content production bias": {
            "definition": "structural, lexical, semantic, and syntactic differences in the contents generated by users.",
            "context": "Content Production Bias. Content Production bias arises from structural, lexical, semantic, and syntactic differences in the contents generated by users [120]. An example of this type of bias can be seen in [118] where the differences in use of language across different gender and age groups is discussed.",
            "type": "explicit"
          },
          "discrimination": {
            "definition": "human prejudice and stereotyping based on the sensitive attributes",
            "context": "Similar to bias, discrimination is also a source of unfairness. Discrimination can be considered as a source for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes, which may happen intentionally or unintentionally, while bias can be considered as a source for unfairness that is due to the data collection, sampling, and measurement. Although bias can also be seen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic fairness literature it is more intuitive to categorize them as such according to the existing research in these areas.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "a7b52fa3ff2606013a6ab9ba294743d7de637e6a",
        "sections": [
          "According the United States Department of Agriculture (USDA), food insecurity \"is defined as a household-level economic and social condition of limited or uncertain access to adequate food\". 1 Recent data indicate that approximately 12.3% or 15.6 million households in the United States (U.S.) were food insecure at least some time during the last year. 1 The adverse social, physical, and psychological outcomes associated with food insecurity are well documented, particularly in households with children including higher rates of diabetes and hypertension, self-reported fair or poor health, maternal depression, behavioral problems/developmental delays in early life, and poor academic achievement. [2][3][4][5][6][7][8] These outcomes are not only detrimental to the health and well-being of individual children and families but also negatively impact broader society. Based on a combination of lower worker productivity, higher costs of public education, greater health care costs, and the cost associated with emergency food distribution, the economic burden associated with food insecurity has been estimated to be over $167.5 billion annually. 9 For over 20 years, food insecurity has been assessed and monitored by the USDA at the federal level. Although levels of food insecurity have declined and risen over this period, one trend that has continued to persist is the gap in the prevalence of food insecurity between people of color and whites. 1 An analysis examining trends in food insecurity from 2001 to 2016 found that food insecurity rates for both non-Hispanic black and Hispanic households were at least twice that of non-Hispanic white households (See Figure 1). 1 Moreover, while race/ethnic specific rates from the USDA are not available for all subgroups, other studies assessing food insecurity among American Indian/Alaska Native (AI/AN) report similar results. 10,11 Using the Current Population Survey-Food Security Supplement, Jernigan et al., 2017 found that from 2000 to 2010, 25% of AI/ANs remained consistently food insecure and AI/ANs were twice as likely to be food insecure compared to whites. 12 The relationship between race/ethnicity and food insecurity is complex and is clearly intertwined with other established determinants of food insecurity including poverty, unemployment, incarceration, and disability. Concentration of social and economic disadvantage among people of color over the life course is clearly a significant driver of higher rates of food insecurity. Substantial gaps in the wealth of people of color and whites have long existed in the U.S. For example, recent data reported by the Pew Research Center indicate that the wealth of white households was 13 times the median wealth of black and 10 times greater than Hispanic households. 13 Nevertheless, despite the intersectionality of race/ ethnicity and other social/economic determinants, some evidence suggests that the higher risk of food insecurity among people of color continue even when these other social and economic factors are removed. [14][15][16][17][18] ",
          "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
          "Several promising solutions to address food insecurity have been proposed including to strengthen and make it easier for families to connect to the existing federal food and nutrition safety net (e.g. WIC, SNAP); expand and increase access to federal income support benefits such as Earned Income Tax Credit (EITC); increase opportunities for employment and ability for individuals to earn a living wage; train health care providers to screen for food insecurity and refer individuals and families to appropriate resources and more recently, the need to take a human rights approach. 8,[31][32][33][34] Still, to address persistent racial disparities in food insecurity, food security advocates and researchers need to also examine the potential impact of utilizing approaches that address structural racism and discrimination more broadly (see Gee and Ford, 2011 and Bailey et al., 2017). 22,30 These efforts will likely pave the way for the emergence of policy and programmatic strategies that promote equity in food access and health by addressing the legacy of racial, ethnic, and class inequality. Source: Calculated by USDA, Economic Research Service, using Current Population Survey Food Security Supplement data. https://www.ers.usda.gov/topics/food-nutrition-assistance/ food-security-in-the-us/"
        ],
        "ground_truth_definitions": {
          "structural racism": {
            "definition": "the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems",
            "context": "A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\"30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0f7eda998bbce003745ff2fdbcaa1d9a8119368b",
        "sections": [
          "as corrupt and untrustworthy. Many of us have started to wonder: are we trapped in echo chambers of our own making? 1The recent conversation, however, has blurred two distinct, but interrelated, social epistemic phenomena, which I will call epistemic bubbles and echo chambers. Both are problematic social structures that lead their members astray. Both reinforce ideological separation. But they are different in their origins, mechanisms for operation, and avenues for treatment. Both are structures of exclusion -but epistemic bubbles exclude through omission, while echo chambers exclude by manipulating trust and credence. However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation. We seek to stay in touch with our friends, who also tend to have similar political views. But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation. According to Jamieson and Capella, an echo chamber's members share beliefs which include reasons to distrust those outside the echo chamber. Echo chambers work by systematically isolating their members from all outside epistemic sources (Jamieson and Cappella 2008, 163-236). This mechanism bears a striking resemblance to some accounts of cult indoctrination. By discrediting outsiders, echo chambers leave their members overly dependent on approved inside sources for information. In epistemic bubbles, other voices are merely not heard; in echo chambers, other voices are actively undermined. (This is a conceptual distinction; a community can practice both forms of exclusion to varying degrees.) The contemporary discussion has been mostly focused on the phenomenon of epistemic bubbles. Cass Sunstein's famous discussions of group polarization, extremism, and the Internet largely focus on matters of constricted information flow and omitted viewpoints (Sunstein 2001(Sunstein , 2009b(Sunstein , 2009a)). Eli Pariser's The Filter Bubble focuses entirely on filtration effects from personalization technology, as in Google searches, and self-selected informational networks, as in Facebook (Pariser 2011). Popular conversation has tended to follow Pariser's focus on technologically-mediated filtration. The term \"echo chamber\" has, in recent usage, been reduced to a synonym for such bubbles and their constricted information flow. When the specifically trust-oriented manipulations of echo chambers are discussed, they are usually lumped in with epistemic bubbles as part of one unified phenomenon. This is a mistake; it is vital to distinguish between these two phenomena. An epistemic bubble is an epistemic structure emerging from the informational architecture of communities, social networks, media, and other sources of information and argument. It is an impaired informational topology -a structure with poor connectivity. An echo chamber, on the other hand, is an epistemic structure created through the manipulation of trust; it can exist within a healthy informational topology by adding a superstructure of discredit and authority. I hope to show, contra the recent focus on epistemic bubbles, that echo chambers pose a significant and distinctive threat -perhaps even a more dangerous one -that requires a very different mode of repair. Furthermore, echo chambers can explain what epistemic bubbles cannot: the apparent resistance to clear evidence found in some groups, for example, climate change deniers and anti-vaccination groups. It may be tempting to treat members of echo chambers as mere sheep, and accuse them of problematic acquiescence to epistemic authority. But that accusation relies on an unreasonable expectation for radical epistemic autonomy. Contemporary epistemology, especially social epistemology, has taught us that trust in others is necessary and ineradicable. We are, as John Hardwig says, irredeemably epistemically dependent on each other (Hardwig 1985(Hardwig , 1991)). Echo chambers prey on our epistemic interdependence. Thus, in some circumstances, echo chamber members do not have full epistemic responsibility for their beliefs. Once one is trapped in an echo chamber, one might follow good epistemic practices and still be led further astray. And some people can be trapped in echo chambers because of circumstances beyond their control -for example, they can be raised in them. Which leads to the most important questions: how can one tell if one is in an echo chamber? And how can one escape? I will argue that, for those trapped within an echo chamber, prospects for detection are poor and the escape path daunting. Detecting and escaping from echo chambers will require a radical restructuring of a member's relationship to their epistemic past, which may be more than we can reasonably expect of one another.",
          "Let's start with a simple picture of how many of us conduct our epistemic lives right now. I get much of my news via Facebook. I have selected most of my Facebook friends for social reasons; they are my friends and colleagues. A significant conduit for my learning about events in the world is by people re-posting articles that they have found newsworthy or interesting. When I go outside of Facebook, I usually turn to sources which, by and large, are affiliated with my own political beliefs and intellectual culture. This process imposes a filter on my uptake of information. Selection and exclusion are not bad in and of themselves -the world is overstuffed with supposed sources of information, many of them terrible. The better my filter, the more it will focus my attention on relevant, useful, and reliable information. But the bare fact that each individual member of the system is themselves reliable will not guarantee any broadness or completeness of coverage. Suppose, for example, that my social network was composed entirely of intelligent, reliable professors of aesthetics whose interests were largely focused on new developments in opera, ballet, and avant-garde contemporary art. Through this system, I might learn about all the exciting new developments in the New York Art scene, but entirely miss, say, relevant developments in rap, or the fact that my country was slowly sliding into fascism. The system lacks what Goldberg calls coverage-reliability -the completeness of relevant testimony from across one's whole epistemic community (Goldberg 2011, 93-4). Bad coverage can not only leave out relevant facts and evidence; it can also fail to bring relevant arguments to our attention. Thus, bad coverage can starve us of adequate exposure to relevant arguments. Notice that bad coverage is an epistemic flaw of epistemic systems and networks, not of individuals. I can now specify my use of \"epistemic bubble\" with greater precision. An epistemic bubble is a social epistemic structure which has inadequate coverage through a process of exclusion by omission. Epistemic bubbles form by leaving out relevant epistemic sources, rather than actively discrediting them. There are at two primary forces encouraging this omission. First, there is an epistemic agent's own tendency to seek like-minded sources. This phenomenon is sometimes called \"selective exposure\" by social scientists (Nelson and Webster 2017). In many contemporary cases, such as Facebook, the process of omission can occur inadvertently. I typically put people in my Facebook feed for social reasons -because I like them or I find them funny. But social selection does not guarantee good coverage reliability; in fact, the typical bases of social selection are inimical to good coverage reliability. 2 We usually like people who are similar to us, and such similarity makes coverage gaps more likely. Friends make for good parties, but poor information networks. We now have a straightforward account of one way in which epistemic bubbles can form. We can build a structure for one set of purposes -maintaining social relations, finding -and then proceed to use it for an entirely different purpose, for which it functions badly: information gathering. Second, there are the processes by which an epistemic agent's informational landscape is modified by other agents. This might include, say, systematic censorship or media control by the state or other actors. The most worrisome of these external forces, at the moment, seems to be the algorithmic personal filtering of online experiences (Pariser 2011;Watson 2015). Internet search engines, for example, will track personal information for each particular user, and adapt their search results to suit each user's interest. Certainly, newspapers and other traditional media technologies do place external filters on their readers, but the modern instantiation is particularly powerful and troubling. As Boaz Miller and Isaac Record argue, Internet technologies create hyper-individualized, secret filters. The secrecy is particularly threatening. Many users do not know about the existence of algorithmic personal filtering. Even amongst those that do, most do not have access to the particularities of the algorithms 2 For an overview of empirical research on personal similarity and polarization, see (Sunstein 2009a, 83-5). Curiously, Sunstein notes the group polarization literature has thought relatively little about the impact of personal similarity. doing the filtering; thus, the very opacity of the process makes it harder for a user to successfully evaluate and epistemically compensate for such filtering (Miller and Record 2013). Thus, most users significantly underestimate the degree to which their exposure to information, via search results, has already been tailored to present search results to which the user will already be amenable. Both the agent-driven process of selective exposure, and the externalities of algorithmic filtering, contribute to the creation of epistemic bubbles. I introduce the term \"epistemic bubble\" here to indicate a broader set of phenomena. Pariser introduced the term \"filter bubble\" to refer specifically to technologically mediated filtering, especially via algorithmic matching. Epistemic bubbles are those structures which omit relevant voices by any means, technological or otherwise. Epistemic bubbles include filter bubbles, but also nontechnological selection processes, such as physically sorting oneself into neighborhoods of like-minded people (Bishop 2009). The account I've given of epistemic bubbles focuses on the way they omit relevant information, but that omission can also threaten us with bootstrapped corroboration. Users of social networks and personalized search technologies will encounter agreement more frequently and so be tempted to over-inflate their epistemic self-confidence. This danger threatens because, in general, corroboration is often a very good reason to increase one's confidence in the relevant beliefs (Nguyen 2010(Nguyen , 2018a)). But corroboration ought to only have weight if it adds something epistemically, rather than being a mere copy. To borrow an example from Wittgenstein: imagine looking through a stack of identical newspapers and treating each next newspaper headline saying p as a reason to increase your belief that p (Wittgenstein 2010, 100). This is clearly a mistake; the fact that a newspaper claims that p has some epistemic weight, but the number of copies of that newspaper one encounters ought not add any extra weight. Similarly, imagine speaking to a bunch of acolytes of Guru Jane, who repeat anything that Guru Jane says without any further thought. The fact that all these acolytes repeat Guru Jane's testimony should add no extra weight. So long as the disciplines repeat anything Guru Jane says -so long as they are mere conduits for information, rather than sources of information -they are simply another sort of copy. But copying isn't the only route to a problematic form of non-independence. Suppose I believe that the Paleo diet is the best diet. I proceed to assemble a body of peers who I trust precisely because they also believe that Paleo is the best diet. In that case, the existence of perfect agreement on Paleo's amazingness throughout that group ought to count for far less than it might for other groups that I had not assembled on that basis. Even if all the group members arrived at their beliefs independently, their agreement is already guaranteed by my selection principle. To the degree that I have pre-selected the members in my epistemic network from agreement with some set of beliefs of mine, then their agreement with that set of beliefs and any other beliefs that it entails ought to be epistemically discounted. 3 If we fail to so discount, we are ignoring a pernicious hidden circularity in our corroborative process. It is easy to forget to discount because the bootstrap here is obscured by time and interface. But we must actively adjust for the increased likelihood of agreement inside our bubbles, or we will unwarrantedly bootstrap our confidence levels. 4To summarize: an epistemic bubble is an epistemic network that has inadequate coverage through a process of exclusion by omission. That omission need not be malicious or even intentional, but members of that community will not receive all the relevant evidence, nor be exposed to a balanced set of arguments.",
          "Luckily for us, epistemic bubbles are relatively fragile. Relevant sources have simply been left out; they have not been discredited. It is possible to pop an epistemic bubble by exposing a member to relevant information or arguments that they have missed. Echo chambers, on the other hand, are significantly more robust. My analysis here combines empirical work and analysis from Jamieson and Cappella on the nature of right-wing echo-chambers with recent insights from social epistemology. Jamieson and Cappella studied echo chambers built around particular charismatic personalities -Rush Limbaugh and the news team of Fox News, and certain other members of conservative talk radio. Their data and analysis suggest that Limbaugh uses methods to actively isolate his community of followers from other epistemic sources. Limbaugh's consistent attacks on the \"mainstream media\" serve to discredit all potential sources of knowledge or testimony besides Limbaugh and a select inner-cadre of other approved sources (Jamieson and Cappella 2008, 140-76). Limbaugh also develops what they call a private language, full of alternate meanings for familiar terms and new jargon (for example, \"SJWs\"), in order to exaggerate the insularity and separateness of the in-group. Finally, Limbaugh provides counter-explanations of all contrary views, intended not only to attack each particular view, but also to undermine the general trustworthiness and integrity of anybody expressing a contrary view. The resulting world-view is one of highly opposed forces; once one has subscribed to Limbaugh's view, one has reason to think that anybody who does not also subscribe is actively opposed to the side of right, and thereby morally unsound and so generally untrustworthy (177-90). Jamieson and Cappella suggest that this makes a follower dependent on a single source or group of sources, and makes them highly resistant to any outside sources. They offer the following definition of an echo chamber: an echo chamber is a bounded and enclosed group that magnifies the internal voices and insulates them from rebuttal (76). I will use the term \"echo chamber\" here following their analysis, but I adapt the definition slightly for philosophical use. I use \"echo chamber\" to mean an epistemic community which creates a significant disparity in trust between members and non-members. This disparity is created by excluding non-members through epistemic discrediting, while simultaneously amplifying insider members' epistemic credential. Finally, echo chambers are such that in which general agreement with some core set of beliefs is a pre-requisite for membership, where those core beliefs include beliefs that support that disparity in trust. By \"epistemic discrediting\", I mean that non-members are not simply omitted or not heard, but are actively assigned some epistemic demerit, such as unreliability, epistemic maliciousness, or dishonesty. By \"amplifying epistemic credentials\", I mean that members are assigned very high levels of trust. Of course, these two processes can feedback into one another. So long as an echo chamber's trusted insiders continue to claim that outsiders are untrustworthy, then the inner trust will reinforce the outward distrust. And so long as outsiders are largely distrusted, then the insiders will be insulated from various forms of counter-evidence and rebuff, thus increasing their relative credence. Once a sufficient disparity in credence between insiders and outsiders has been established, so long as trusted insiders continue to hold and espouse epistemically dismissive beliefs towards outsiders, then the echo chambers' beliefs system will be extremely difficult to dislodge. Compare this process of credence manipulation to the process of omission found in epistemic bubbles. In one standard scenario, I add others as trusted members of my epistemic network based on agreement. I am then less likely to encounter an outside voicebut when I do actually have such an encounter with an outsider, I have no background reason to dismiss them. Bubbles restrict access to outsiders, but don't necessarily change their credibility. Echo chambers, on the other hand, work by offering a pre-emptive discredit towards any outside sources. 5The result is a rather striking parallel to the techniques of isolation typically practiced in cult indoctrination. The standard techniques of cult indoctrination, by a traditional account, are the aggressive emotional isolation of cult members from all non-cult members, which amplifies indoctrinated member's dependency on the cult (Singer 1979;Langone 1994;Lifton 1991). 6 New cult members are brought to distrust all non-cult members, which provides an epistemic buffer against any attempts to extract the indoctrinated person from the cult. This is nothing like how epistemic bubbles work. Epistemic bubbles merely leave their members ignorant, but ignorance can be fixed with simple exposure. The function of an echo chamber, on the other hand, is to credentially isolate its members by a manipulation of trust. By this, I mean that members are not just cut off, but are actively alienated from any of the usual sources of contrary argument, consideration, or evidence. Members have been prepared to discredit and distrust any outside sources; thus, mere exposure to relevant outside information will have no effect. In fact, echo chambers can avail themselves of another epistemic protective mechanism: they can contain what I'll call a disagreement-reinforcement mechanism. Members can be brought to hold a set of beliefs such that the existence and expression of contrary beliefs reinforces the original set of beliefs and the discrediting story. A toy example: suppose I am a cult leader, and I have taught my followers to believe that every human except the members of our group has been infested and mind-controlled by alien ghosts from Mars. I also teach my followers that these alien ghosts from Mars hate our group for knowing the truth, and so will constantly seek to undermine our knowledge of their existence through mechanisms like calling us a 'cult' and calling us lunatics. Endre Begby has offered a careful analysis of this particular sort of disagreement-reinforcement mechanism, which he calls \"evidential preemption.\" Suppose that I tell my followers to expect outsiders to falsely claim that there are no ghosts from Mars. When my followers do confront such contrary claims from outsiders, those contrary claims are exactly what they expected to hear. Thus, new contrary testimony is neutralized, because it was predicted by past beliefs. This, says Begby, functions as a kind of epistemic inoculation. There is also a secondary effect. When my followers hear exactly what I predicted, then my claims have been verified, and so my followers will have some reason to increase their trust in me. Thus, the echo chamber's belief system not only neutralizes the epistemic impact of exposure to outsiders with contrary beliefs; the existence of those contrary beliefs will actively corroborate the pre-emptor and so increase the credence level of the entire echo chamber (Begby 2017). This creates a feedback mechanism within the echo chamber -in making undermining predictions about contrary testimony, inside authorities not only discredit that contrary testimony, but increase their trustworthiness for future predictions. Once such a system of beliefs is set up, it can be very difficult to dislodge -it is selfreinforcing, bounded, and built to discount any contrary input. In fact, though my definition of echo chambers is conceptually separable from such a disagreement-reinforcement mechanism, every plausible real-world candidate for an echo chamber I've ever encountered includes some version of a disagreement-reinforcement mechanism. For a depressing realworld example, consider Pizzagate. Pizzagate is a conspiracy theory that boiled out of a rightwing online forum on Reddit, which included beliefs that Comet Ping Pong, a pizza restaurant, was the site of a child sex trafficking ring owned by a liberal conspiracy involving Hillary Clinton and Barack Obama. Eventually, Edgar Welch, a member of that forum, forcibly entered the pizza parlor armed with an assault rifle to investigate; when he satisfied himself that the restaurant contained no child slaves, he gave himself up to the police. The online forum, however, did not take this as contrary evidence. Instead, they leaned on their belief that the liberal conspiracy had total control of the mainstream media, and was willing to stage fake events to discredit the right-wing. The forum took Welch's claims that there was no sex trafficking ring as evidence that Welch was a paid actor, and thus as further confirmation of the existence of a powerful cabal of liberal child sex traffickers (Mengus 2016;Vogt and Goldman 2016). Conspiracy theories function here in a fascinating inversion to corroborative bootstrapping. In corroborative bootstrapping, the mistake is to treat problematically dependently selected insiders as if they were independent, and thus overweight their testimony. When an echo chamber uses a conspiracy theory in this manner, they are attributing a problematic form of non-independence to outsiders who are actually independent, and thereby underweighting outside testimony. An echo chamber here works by discrediting the apparent independence of, say, different climate change scientists by claiming that all their various testimonies are problematically derived from a single source. Incidentally, I am not claiming here that conspiracy theories are always or necessarily incorrect or the product of epistemic vice. As others have argued, believing in conspiracy theories isn't bad per se, because some conspiracy theories are true (Coady 2012, 110-137;Dentith 2017). But the fact that conspiracy theories can function to reinforce the boundaries of echo chambers -though they do not necessarily do so -might explain part of conspiracy theories' bad rap. Because of their effectiveness in setting up disagreement-reinforcement mechanisms, conspiracy theories are often conscripted as a powerful tool in the bad epistemic behavior of certain groups. It is important to note that the epistemic mechanisms by which echo chambers work, though problematic, are not sui generis. They are perversions of natural, useful, and necessary attitudes of individual and institutional trust. The problem isn't that we trust and distrust groups and institutions. In fact, we must do so. Eljiah Millgram calls it the problem of hyperspecialization. Human knowledge has splintered into a vast set of specialized fields that depend on each other. No one human can manage that information; we are forced to trust each other (Millgram 2015, 27-44). 7 None of us is in a position to reliably identify an expert in 7 Though this paper relies on insights from modern work in the epistemology of testimony, I have tried to rely only on uncontroversial claims from that field, and not on the technical details of any particular view. In particular, I have attempted to construct my analysis so as to be independent of the debate on whether or not testimony is a basic source of knowledge. I have also attempted to make the paper compatible with the major accounts of trust. most specialist fields outside of our own. I am, on my own, helpless to evaluate the virtues of antibiotics or the expertise of a particular doctor or surgeon. I am, instead, reliant on a vast network of institutional licensing practices in order to choose my health care sourcesincluding journal peer review, medical board exams, university hiring practices, and the like (Nguyen 2017a). Often, I trust via what Philip Kitcher calls indirect calibration -I trust mechanical engineers because they make things that work, but I know that mechanical engineers trust applied physicists, and I know that applied physicists trust theoretical physicists, so I acquire trust through a long chain of field-wide links (Kitcher 1993, 320-3). I even use litmus tests: the fact that any person or group is in favor of, say, sexual orientation conversion therapy is enough for me to discredit them on any social or moral topics. We must resort to such tactics in order to navigate the hyper-specialized world (Nguyen forthcoming). Echo chambers function parasitically on these practices by applying discredits without regard for the actual epistemic worth of the discredited institutions or individuals. The discredit is instead applied strategically and defensively, towards all outsiders solely on the basis of their being outsiders. Once the discrediting beliefs are in place, the ensuing beliefs and action the echo chambers' members are surprisingly close to rational. In fact, we can easily imagine alternative scenarios in which a very similar set of beliefs were appropriate and veristic. If I was an anti-Nazi in Germany during the rise of the Nazi party, I would do well to maintain the beliefs that the most people were corrupt, untrustworthy, and out to maliciously undermine my own true beliefs. But if such beliefs become implanted in an inappropriate context, they can lead their believers entirely astray. Epistemic bubbles can easily form accidentally. But the most plausible explanation for the particular features of echo chambers is something more malicious. Echo chambers are excellent tools to maintain, reinforce and expand power through epistemic control. Thus, it is likely (though not necessary) that echo chambers are set up intentionally, or at least maintained for this functionality. My account thus bears some resemblance to some work on testimonial injustice and the epistemology of ignorance, but it is importantly distinct. Miranda Fricker has argued for a kind of testimonial injustice, based on a gap between actual reliability and perceived credibility. For example, says Fricker, being white and being male are both bonuses to credibility. Since credibility is a source of power, anybody with credibility will seek to increase it, using that very credibility. Thus, says Fricker, credibility gaps can be turned into epistemic tools of social oppression (Fricker 2011). Similarly, Charles Mills argues that there is an active practice of ignorance among members of oppressive groups, such as white Americans. It is to the benefit of those in power to actively ignore many aspects of the existence of oppressed groups (Mills 2007;Alcoff 2007, 47-57). My account is compatible with, but independent from, Fricker's and Mills' accounts. Echo chambers can and surely are used to maintain social oppression through enhancing credibility gaps and supporting practices of active ignorance. The systematic mistrust of an echo chambers is a powerful tool for perpetuating epistemic injustice and active ignorance. However, the concept of an echo chamber does not require that they be deployed only in political contexts, nor does it require that they only be deployed only in the service of oppression. Echo chambers could potentially exist among the oppressed, and surely exist in apolitical contexts. I believe I have witnessed echo chambers forming around topics such as anti-vaccination, multi-level marketing programs, particular diets, exercise programs, liberal activism, therapeutic methodologies, philosophies of child-rearing, particular academic subdisciplines, and Crossfit (Weathers 2014).",
          "It has often been claimed, during and after the American political season of 2016, that we have entered a 'post-truth era'. Not only do some political figures seem to speak with a blatant disregard for the facts, their supporters seem unswayed by reason or contrary evidence. To many, it seems as if a vast swath of the electorate has become entirely unmoored from any interest in facts or evidence. Call this the \"total irrationality\" explanation of the post-truth phenomenon. But echo chambers offer an alternative explanation for the apparent post-truth mood. It seems likely that there is at least one vast partisan echo-chamber present in the political landscape. Jamieson and Cappella's study is a decade old, but sources like Breitbart and Alex Jones' Infowars seem like clear extensions of the same right-wing echo chamber. (Other echo chambers surely exist elsewhere on the political spectrum, though, to my mind, the left-wing echo chambers have been unable to exert a similar level of political force.) In that case, the account of echo chambers I've offered has significant explanatory force. The apparent \"posttruth\" attitude can be explained, at least in part, as the result of credence manipulations wrought by echo chambers. In healthy epistemic communities, there is something like an upper ceiling on the credence level accorded to any individual. A healthy epistemic network will supply a steady stream of contrary evidence and counterarguments; thus, no single individual or group will ever go unchallenged. Epistemic bubbles make the discovery of mistakes significantly less likely, and so tend to exaggerate the credence levels of epistemic sources inside the bubble. But when an echo chamber is in place and all outside sources have been effectively discredited, that ceiling disappears categorically. Echo chambers can create runaway credence levels for approved individuals. By removing disconfirmations and discorroboration from the system through the systematic discrediting of outsiders, echo chambers can create exceptionally high -one is tempted to say unnaturally high -levels of trust. That potential for runaway credence is built right into the foundations of any echo chamber, and arises from an interaction between the two main components of any echo chamber. First, an echo chamber involves a significant disparity of trust between the insiders and the outsiders. Second, an echo chamber involves beliefs, espoused by the insiders, reinforcing that disparity. The essential features of echo chambers seem designed to selfreinforce their peculiar arrangement of trust. Notice that epistemic bubbles alone cannot explain the post-truth phenomenon. Since epistemic bubbles work only via coverage gaps, they offer little in the way of explanation for why an individual would reject clear evidence when they actually do encounter it. Coverage gaps cannot explain how somebody could, say, continue to deny the existence of climate change when actually confronted with the overwhelming evidence. One would be tempted, then, to accuse climate change deniers of some kind of brute error. But echo chambers offer an explanation of the phenomenon without resorting to attributions of brute irrationality. Climate change deniers have entered an epistemic structure whereby all outside sources of evidence have been thoroughly discredited. Entering that epistemic structure might itself involve various epistemic mistakes and vices -but here the story can be one of the slow accumulation of minor mistakes, which gradually embed the believer in a self-reinforcing, internally coherent, but ultimately misleading epistemic structure. Similarly, some have suggested that, in the post-truth era, many people's interest in the truth has evaporated. Once again, this account of echo chambers suggests a less damning and more modest explanation. An echo chamber doesn't erode a member's interest in the truth; it merely manipulates their credence levels such that radically different sources and institutions will be considered proper sources of evidence. This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak -deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth. But by my account, we should expect discourse within echo chambers to be entirely differentwe should expect such discourse to be crisp and clear, and to present unambiguous claims about what is the case, what secret conspiracies are in place, and which sources are to be entirely distrusted. And this is precisely what we find (Jamieson and Cappella 2008, 3-41,140-176). Consider, for example, Breitbart's attacks on other media sources. One article begins: \"Mainstream media outlets continue to print false and defamatory descriptions of Breitbart News in a nakedly political effort to marginalize a growing competitor\" (Pollak 2017). This is not the double-speak of administrators and bureaucrats-this is a clear, strident, and unambiguously worded discredit. One might be tempted to say: but just give them the real evidence! You can't discredit neutral evidence! But this response radically underestimates the degree of trust and social processing involved in most presentations of evidence. Except for empirical evidence I myself have gathered, all other presentations of evidence rely on trust. My belief in the reality of climate change depends on enormous amounts of institutional trust. I have not gathered the climate change evidence myself; I mostly just trust science journalists who, in turn, trust institutional credentialing systems. Even if I had been on, say, a core sampling expedition to the Arctic, I would be unable to process that information for myself, or even vet whether somebody else has properly processed it. Even the climatologist who actually processes that information must also depend on trusting a vast array of other experts, including statisticians, chemists, and the programmers of their data analysis software. Most so-called \"neutral evidence\" depends on long and robust chains of trust (Millgram 2015, 27-44). Members of an echo chamber have acquired beliefs which break the usual arrangements of trust. But despite their evident explanatory force, echo chambers have been largely neglected by recent empirical research. Much of the recent research on causes of belief polarization focuses on the causal role of individual psychology, such as the tendency towards laziness in the scrutiny of one's own beliefs (Trouche et al. 2016). Similarly, recent studies on climate change denial focus on studying the relationship between an individual's stated political beliefs and their reactions to climate change information, without inquiring into the social epistemic structures in which the individuals are embedded (Corner, Whitmarsh and Xenias 2012). Famously, Dan Kahan and Donald Braman argue for the cultural cognition thesisthat is, that cultural commitments are prior to factual beliefs, and that non-evidentially formed cultural values inform which future presentations of evidence will be admitted as weighty (Kahan and Braman 2006). Though the values may originally come from an individual's culture, Kahan and Braman focus their analysis on how those acquired values function in individual reasoning to create polarization. They pay little attention to the continuing role of the contingent social structures in which the individual is embedded. The direct literature on echo chambers and epistemic bubbles is new and relatively small, compared to the sizable literature on individual belief polarization. Unfortunately, even in that literature, echo chambers and epistemic bubbles have often been confused. They are usually addressed in the popular media together, and the terms 'epistemic bubble' and 'echo chamber' are typically used interchangeably (El-Bermawy 2016). The same blurring has occurred in the treatment of the phenomena in academic epistemology in the surprisingly small literature on echo chambers. For example, Bert Baumgaertner, in his analysis of echo chambers via computer modeling, lumps together under the heading 'echo chamber' both Though he professes to cover both filter bubbles and echo chambers, his work focuses almost entirely on epistemic bubble effects: constricted information flow, lack of exposure to alternate arguments, and bootstrapped corroboration (Sunstein 2009b(Sunstein , xi,19-06, 2009a, 1-98), 1-98). The point here is about more just than his choice of words: his subjects of analysis include, among other things, Facebook friend groups, hate groups, extremist online political forums, conspiracy theorists, and terrorist groups (99-125, 2009b, 46-96). Clearly, this list includes prime candidates for both epistemic bubbles and echo chambers. But his analysis focuses almost entirely on the effects of bootstrapped corroboration and lack of exposure. For Sunstein, the primary mechanism driving polarization and extremism is the loss of truly public forums, because technology has over-empowered people's tendency to self-select sources offering familiar views. Thus, his solution is to re-create, in the new media environment, the kind of general public forums where people might be more likely to serendipitously encounter contrary views and arguments. His solutions include governmentfunded public news websites with diverse coverage and voluntary work by corporations and individuals to burst their bubbles. His recommendations for repair largely have to do with increasing exposure (Sunstein 2009a(Sunstein , 135-48, 2009b, 19-45,190-211), 19-45,190-211). But, again, if what's going on is actually an echo chamber effect, exposure is useless or worse. The blurring of the two concepts has also lead to some problematic dismissals of the whole cluster of phenomena. A number of recent articles in social science, communications, and media studies have argued that the whole set of worries about bubbles and echo chambers is wildly overstated. These articles share the same argumentative pattern. First, they use the terms \"filter bubble\" and \"echo chamber\" interchangeably, and address themselves to the same cluster of phenomena as Sunstein, treating them as singular. In fact, James Nelson and James Webster conflate Jamieson and Cappella's analysis of echo chambers and Pariser's analysis of filter bubbles, and erroneously attribute to Jamieson and Cappella the view that political partisans only seek out and encounter media from sources with matching political alignments -that is, Nelson and Webster attribute to an epistemic bubbles account to Jamieson and Cappella, where Jamieson and Cappella's actual text is clearly an echo chambers account (Nelson and Webster 2017, 2). More importantly, these recent articles proceed to argue against the existence of filter bubbles and echo chambers by demonstrating that, through the analysis of empirical data about media consumption, most people in fact expose themselves to media from across the political spectrum. Nelson and Webster, for example, argue against Jamieson and Capella, claiming that filter bubbles and echo chambers don't exist. Nelson and Webster support their claim with data showing that both liberals and conservatives visit the same media sites and spend comparable amounts of time at those sites (6-7). Again, this misses the mark -this is evidence only against the existence of epistemic bubbles, and not against the existence of echo chambers. Similarly, Seth Flaxman et al seeks to problematize the existence of filter bubbles and echo chambers with data that social media platforms seem to actually increase people's exposure to media from across the political divide (Flaxman, Goel and Rao 2016). Again, these data only concern the exposure and omission, and only weigh against the existence of epistemic bubbles. They say nothing about whether echo chambers exist. Echo chambers, recall, are structures of strategic discrediting, rather than bad informational connectivity. Echo chambers can exist even when information flows well. In fact, echo chambers should hope that their members are exposed to media from the outside; if the right disagreement reinforcement mechanisms are in place, that exposure will only reinforce the echo chambers' members' allegiance. We ought not conclude then, from data that epistemic bubbles do not exist, that echo chambers also do not exist. We can see now crucial it is to keep these two categories distinct. Epistemic bubbles are rather ramshackle -they go up easily, but they are easy to take down. Since there is no systematic discrediting of outsiders, simple exposure to excluded voices can relieve the problem. Echo chambers, on the other hand, are much harder to escape. Echo chambers can start to seem almost like living things -the belief systems provide structural integrity and resilience. Mere exposure will be ineffective. Jamieson and Cappella offer evidence of this effect: once listeners are caught in Rush Limbaugh's language, framing, and discredentialing of the mainstream media, their beliefs can survive frequent contact with contrary viewpoints. Limbaugh's technique, say Jamieson and Cappella, serves to insulate and inoculate his audience from being affected by exposure to contrary viewpoints (Jamieson and Cappella 2008, 163-190). In fact, if the appropriate disagreement-reinforcement mechanisms are in place, exposure will simply strengthen the attacked belief systems. Thus, an outsider's attempt to break an echo chamber as if it were a mere bubble is likely to backfire and reinforce the echo chamber's grip. 9",
          "So what, then, are the epistemic responsibilities of an agent to discover whether they are in one of these social epistemic traps, and what are their prospects for actually discovering their predicament and successfully escaping? To answer this, we must consider two distinct questions: The escape route question: Is there any way out of an echo chamber or epistemic bubble? The escape responsibility question: Could one behave epistemically virtuously, and yet still remain caught within an echo chamber or epistemic bubble? In other words, to what degree is an epistemic agent embedded within such a structure blameworthy, or blameless, for the faultiness of their beliefs? The first question asks about the possible existence of an escape route. The second asks whether there is an escape route that we might reasonably expect an epistemically virtuous agent to discover and enact. These are distinct questions, because an escape route might turn 9 Sunstein does briefly note the empirical data for the disagreement-reinforcement effect in passing, but then seems to ignore it in proposing his solutions (Sunstein 2009a, 54-5) out to be possible, but so difficult to discover or use that it was beyond what we might reasonably expect of an agent of moderate epistemic virtue. For epistemic bubbles, the answers are straightforward. As I've argued, epistemic bubbles are quite easy to shatter. One just needs exposure to excluded information. Insofar as that information is available, but simply not part of one's standard network, then members of epistemic bubbles are failing to live up to their epistemic duties, which include proactively gathering relevant data. To translate into contemporary terms: if you're subject to an epistemic bubble because you get all your news from Facebook and don't bother to look at other sources, you are, indeed, blameworthy for that failure. If one finds the language of epistemic virtues and vices appealing, then we can say that members of epistemic bubbles are committing the vice of epistemic laziness. Answering these two questions is much more difficult for echo chambers. Recall: where encountering excluded voices and evidence will shatter an epistemic bubble, such encounters are likely to reinforce an echo chamber. Let's grant that intentionally constructing an echo chamber, as Jamieson and Cappella claim that Rush Limbaugh did, is epistemically (and morally) blameworthy. Furthermore, actively entering an echo chamber seems epistemically blameworthy in many circumstances. For agent in full possession of a wide range of informational sources, to abandon most of them and place their trust in an echo chamber for, say, an increased sense of comfort and security, is surely some form of epistemic vice. There is some evidence that this may be the case; Jamieson and Cappella suggest that people enter echo chambers for the sake of the community bonds and the sense of belonging to an ingroup (Jamieson and Cappella 2008, 180). But there are many cases in which the agent seems plausibly blameless. Imagine a person raised in an echo chamber. Their earliest epistemic contacts -let's say their parents, relatives, and close family friends -are all firmly committed members of the echo chamber. Suppose that the child is either home-schooled by those echo chamber members or sent to a school that reinforces the beliefs of that particular echo chamber. I take it that it is reasonable for a child to trust their parents and those of seeming epistemic authority, and that a child is epistemically blameless for having done so (Goldberg 2013). Thus, when that child eventually comes into contact with the larger epistemic world -say, as a teenager -the echo chamber's beliefs are fully in place, such that the teenager discredits all sources outside of their echo chamber. It seems, at first glance, that our teenager could be acting very much like a reasonable epistemic agent. They could, in fact, be epistemically voracious: seeking out new sources, investigating them, and evaluating them using their background beliefs. They investigate the reliability of purported experts and discredit experts when they have apparently good reason to do so, using their background beliefs. Our teenager seems, in fact, to be behaving with many epistemic virtues. They are not at all lazy; they are proactive in seeking out new sources. They are not blindly trusting; they investigate claims of epistemic authority and decide for themselves, using all the evidence and beliefs that they presently accept, whether to accept or deny the purported expertise of others. They have theories, which they have acquired by reasonable methods, predicting the maliciousness of outsiders; they increase their trust in those theories when their predictions are confirmed. 10The worry here is that agents raised within an echo chamber are, through no fault of their own, epistemically trapped -their earnest attempts at good epistemic practices are transformed into something epistemically harmful by the social structure into which they have been embedded and which they have ingested. Paul Smart has argued for the possibility of a transformative social epistemic phenomenon which he dubs \"mandevillian intelligence,\" in honor of Bernard Mandeville. Mandeville argued that, in the right social context, individual vices could lead to collective economic prosperity. For a certain kind of economic theorist, capitalism is such a transformative structure -individuals act selfishly, but the structure of the market transforms that selfishness into virtuous collective action. According to Smart, there is an epistemic analog: the mandevillian intelligence, which transforms the individual epistemic vices of its members into a collective epistemic virtue by virtue of the social structure into which they are embedded (Smart 2017). Intellectual stubbornness, for example, might be an intellectual vice for individuals. But set those stubborn individuals in a properly arranged social structure (like, perhaps, academia) and you might get a collective system that properly explores every relevant nook and cranny with optimal thoroughness. But echo chambers are the very opposite; they are reverse-mandevillian intelligences. Echo chambers are social epistemic structures which convert individually epistemically virtuous activity into collective epistemic vice. In fact, the reverse-mandevillian nature contributes to the stickiness of the echo chamber trap. If our teenager self-reflects on their epistemic practices, what they will see might be rather gratifying. Their epistemic behavior might very well be earnest, vigorous, and engaged. It is their external context -the social epistemic system into which they have been unluckily raised -which makes such behavior problematic. Contrast this account with Quassim Cassam's treatment of Oliver, his fictional 9/11 conspiracy theorist. Oliver believes that the collapse of the twin towers was an inside job, and he is happy to provide reasons and point to supporting evidence from a great many conspiracy theorist websites. Says Cassam: Oliver is obviously mistaken -Oliver relies on outrageous, baseless claims from clearly discredited sources. The best explanation for Oliver's beliefs is in terms of epistemic vice -that is, in terms of Oliver's bad intellectual character traits. Oliver is \"gullible, cynical, and prejudiced,\" says Cassam. Oliver is gullible with regard to his conspiracy theorist sites, cynical with regard to the mainstream media, and his prejudice consists of, among other things, intellectual pride, wishful thinking, closedmindedness, and a lack of thoroughness (Cassam 2016, 162-4). And I certainly grant that such epistemic vices can lead to these sorts of beliefs. But the story of our hapless teenager offers an alternate epistemic path to such beliefs and such narrowcasted trust -one in which epistemically virtuous character traits have been wrong-footed by the social epistemic structure in which the agent has been embedded. The crucial difference between the reversemandevillian account and Cassam's account is where the brunt of the responsibility lies. In Cassam's account, the responsibility lies with the individual, and their own intellectual habits and practices. 11 In a reverse-mandevillian account, a significant part of the responsibility lies with the social structure in which the actors are embedded. The epistemic vice is a feature of the collective intelligence, rather than of the individual. Or, if one is averse to thinking in terms of collective intelligences, here's a conceptually minimal way to put the claim: echo chambers are local background conditions that turn generally good epistemic practices into locally unreliable ones. But the possibility of a truly faultless epistemic agent, wholly misled by an echo chamber, also depends on the lack of an accessible escape route. So: are there escape routes from an 11 Note, however, that Cassam distinguishes between epistemic responsibility and epistemic blameworthiness, and does not take blameworthiness to necessarily follow from responsibility (168-9). Cassam leaves room for the view that the individual's intellectual vices were epistemically responsible for their bad beliefs, but that the individual wasn't blameworthy for those vices, because the vices were inculcated in them at an early age. However, my complaint still stands, for I contest Cassam's claim that the responsibility is in the individual. echo chamber, and how reasonable is it to expect echo chamber members to discover and make use of them? Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass. Thus, their critical reflection is likely to reinforce previously held beliefs. Kelly notes that the belief polarization violates the Commutativity of Evidence Principle: The Commutativity of Evidence Principle: to the extent that what it is reasonable for one to believe depends on one's total evidence, historical facts about the order in which that evidence is acquired make no difference to what it is reasonable for one to believe. (Kelly 2008, 616) In short, belief polarization makes it matter what order they received the evidence, but the historical ordering of evidence ought not matter. Note that our epistemically hapless teenager has also violated the Commutativity of Evidence Principle. For them, it matters very much what order that they received the evidence. If they had been raised outside the echo chamber and fed a broader diet of epistemic sources before encountering the echo chamber, then they would likely have found the echo chamber's world-view to be problematic. But since our teenager encountered the echo chamber and assimilated its beliefs first, their use of background beliefs to vet new sources leads them to continually increase their trust in the echo chamber and their distrust of outsiders. Even if our echo chambered teenager eventually came to encounter all the same evidence as their epistemically free-range counterpart, their early education within the echo chamber would be decisive. So long as each new piece of evidence is assessed using the currently held set of beliefs, then early education in an echo chamber becomes domineeringly powerful. However, the Commutativity of Evidence Principle suggests a way out. In order to free themselves of the echo chamber's grip, our teenager needs to undo the influence of the historical ordering of their encounters with the evidence. How could they possibly do this? Our teenager would have to suspend belief in all their particular background knowledge and restart the knowledge-gathering process, treating all testimony as equally viable. They would need to, in a sense, throw away all their beliefs and start over again. This suggests a process that, in its outlines, might sound awfully familiar. Our escape route turns out to be a something like a modified version of Descartes' infamous method. What proceeds from this point is admittedly something of a fantasy, but perhaps it is a fantasy from which we can eventually draw some sort of moral. The story of the history of Western epistemology might be cartoonishly summarized thusly: Descartes had a dream of radical intellectual autonomy. By his accounting, he came to realize that many of the beliefs he had acquired in his early life were false, and that those early false beliefs might have infected any number of other beliefs. His response was that famed method: to get rid of his beliefs and start over again, trusting no-one and nothing and only permitting those beliefs of which he was entirely certain. Call this the Cartesian epistemic reboot. But if recent epistemology has taught us anything, it's that this total reboot is nothing but a pipe dream. Any sort of reasonable epistemic life is essentially impossible without trusting the testimony of others (Burge 1993;Faulkner 2000;Goldberg 2010;Zagzebski 2012;Hardwig 1985Hardwig , 1991)). But recall that the reason Descartes wanted to discard everything and start over from scratch -the motivation for his project, and not the method -was explained in the very first line of \"Meditation 1\": He was worried by the falsehoods he had learned in childhood and the shakiness of the edifice that had been built from those falsehoods (Descartes 1984, 24). Our teenager faces a problem quite similar in structure. The credentialing structure of their upbringing is flawed; that credentialing structure has influenced any number of their other beliefs, and the degree of that influence is impossible to track. Furthermore, these later beliefs, approved by the echo chambers' credentialed sources, will often serve to reinforce that credentialing structure. The pernicious effect of an echo chamber cannot be attacked one belief at a time. Any single belief that our teenager re-considered would come under the influence of the network of the flawed background beliefs that sustains an echo chamber. What they need is some way to start over. In order to undo the influence of historical ordering, an epistemic agent will have to temporarily suspend belief in all their beliefs, in particular their credentialing beliefs, and start from scratch. But when they start from scratch, they need not disregard the testimony of others, nor need they hold to Descartes' stringent demand for certainty. Let's call this procedure the social epistemic reboot. In the social epistemic reboot, the agent is permitted, during the belief re-acquisition process, to trust that things are as they seem and to trust in the testimony of others. But they must begin afresh socially, by re-considering all testimonial sources with presumptive equanimity, without deploying their previous credentialing beliefs. Furthermore, they must discard all their other background beliefs, because those potentially arose from the flawed credential structure of the echo chamber, and very likely have been designed to support and reinforce that very credential structure. Our rebooter must take on the social epistemic posture that we might expect of a cognitive newborn: one of tentative, but defeasible, trust in all apparent testimonial sources (Burge 1993) (Nguyen 2011). This method will, if successfully applied, undo the historical dependence of our epistemic agent and remove the undue influence of the echo chamber. The social epistemic reboot is, theoretically at least, the escape route we've been searching for. 12This reboot, described in such clinical terms, might seem rather fantastical. But it is not, I think, utterly unrealistic. Consider the stories of actual escapees from echo chambers. Take, for example, the story of Derek Black, who was raised by a neo-Nazi father, groomed from childhood to be a neo-Nazi leader, and who became a teenaged breakout star of white nationalist talk radio. When Black left the movement, he went through years-long process of self-transformation. He had to completely abandon his belief system, and he spent years rebuilding a world-view of his own, immersing himself broadly and open-mindedly in everything he'd missed -pop culture, Arabic literature, the pronouncements of the mainstream media and the US government, rap -all with an overall attitude of trust (Saslow 2016). Of course, all we have shown so far is that the social epistemic reboot would, if pulled off, undo the effects of an echo chambered upbringing. Whether or not an epistemic agent might reasonably be expected to reboot, or blameworthy for failing to reboot, is a separate and significantly more difficult question. First, a social epistemic reboot might be psychologically impossible, or at least beyond what we could reasonably expect of normal epistemic agents. Second, what reason would an epistemic agent have to undertake a social epistemic reboot? Such an undertaking would be justified only if the agent had a significant reason to think that their belief system was systematically flawed. But echo chamber members don't seem likely to have access to any such apparent reason. After all, they have clear and coherent explanations for all the evidence and testimony they encounter. If this is all right then we arrive at a worrying conclusion: that echo chambers may, theoretically, be escapable, but we have little reason to expect members of echo chambers realize that they are members of something that needs escaping. What could hope do we have, then, of motivating a reboot? Derek Black's own story gives us a hint. Black went to college and was shunned by almost everyone in his college community. But then Matthew Stevenson, a Jewish fellow undergraduate, began to invite Black to his Shabbat dinners. Stevenson was unfailingly kind, open, and generous, and he slowly earned Black's trust. This eventually lead to a massive upheaval for Black -a slow dawning realization of the depths to which he had been systematically misled. Black went through a profound transformation, and is now an anti-Nazi spokesperson. The turning point seems to be precisely that Stevenson, an outsider, gained Black's trust. And this is exactly where we should expect the turning point to be. Since echo chambers work by building distrust towards outside members, then the route to unmaking them should involve cultivating trust between echo chamber members and outsiders. In order to motivate the social epistemic reboot, an echo chamber member needs to become aware of how in the echo chamber's grip they are, and forming a trust relationship with an outsider might could mediate that awareness. But how that trust could be reliably cultivated is a very difficult matter, and a topic for future investigation. We have, however, arrived at a tentative moral of the story. Echo chambers work by a manipulation of trust. Thus, the route to undoing their influence is not through direct exposure to supposedly neutral facts and information; those sources have been preemptively undermined. It is to address the structures of discredit --to"
        ],
        "ground_truth_definitions": {
          "epistemic bubble": {
            "definition": "a social epistemic structure in which some relevant voices have been excluded through omission.",
            "context": "However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation.",
            "type": "explicit"
          },
          "echo chamber": {
            "definition": "a social epistemic structure in which other relevant voices have been actively discredited.",
            "context": "But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation.",
            "type": "explicit"
          },
          "belief polarization": {
            "definition": "the tendency of individuals, once they believe that p, to increase their belief that p.",
            "context": "Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass.",
            "type": "explicit"
          },
          "orwellian double speak": {
            "definition": "deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker.",
            "context": "This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak - deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 œÄ r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP Œ± for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return Œ± (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
