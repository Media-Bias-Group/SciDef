{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  ",
        "reasoning": "The provided section contains a description of capabilities related to accessing, processing, interpreting, communicating, and acting on health information, but it does not explicitly specify the term that is being defined. Since no term is named and defined within the text, no term-definition pairs can be extracted.",
        "extracted_terms": []
      },
      {
        "augmented": true,
        "section": "Of narratives that put a strain on the relationship between narrative discourse and story, the most commonly cited is not an experimental twentieth-century novel but the extraordinarily enjoyable eighteenth-century comic novel Life and Opinions of Tristram Shandy (1759-67) by Laurence Sterne. There are numerous examples of modernist novels that place considerable demands on the reader's quest for story, among them, Virginia Woolf's Jacob 's Room (1922), James Joyce's Finnegans Wake (1940), Samuel Beckett's The Unnamable (1952), Alain Robbe-Grillet's In the Labyrinth (1959), William Burroughs's Naked Lunch (1959), Thomas Pynchon's The Crying of Lot 49 (1966), and J. G. Ballard's The Atrocity Exhibition (1970). As for narratives that go from end to beginning, few are so thoroughgoing at the molecular level in the manner of Martin Amis's Time's Arrow (mentioned above). One that comes close is the short story \"Journey to the Source\" (1944)  ",
        "reasoning": "The paragraph contains only examples and descriptive comparisons of various novels and does not provide explicit definitions of any terms. No sentences state a term followed by an explanation or equivalent that would qualify as a definition per the specified format.",
        "extracted_terms": []
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Peña, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Peña's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fernández, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fernández, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fernández, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.1.2",
      "cloudpickle": "3.1"
    }
  }
}
