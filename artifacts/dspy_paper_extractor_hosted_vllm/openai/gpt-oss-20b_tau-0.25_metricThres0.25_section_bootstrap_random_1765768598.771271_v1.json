{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The text provides explicit definitions for four terms: “racial discrimination,” “disparate treatment,” “disparate impact,” and “structural racism.” Each term is accompanied by a single clear sentence stating its meaning, making them suitable for extraction. Their definitions are directly quoted or closely paraphrased within the section, with enough surrounding context to satisfy the requirement of one preceding and one following sentence where available.",
        "extracted_terms": [
          {
            "term": "racial discrimination",
            "definition": "the unequal treatment of persons or groups on the basis of their race or ethnicity.",
            "context": "There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) 'differential treatment on the basis of race that disadvantages a racial group' (disparate treatment) and ( 2) 'treatment on the basis of inadequately justified factors other than race that disadvantages a racial group' (disparate impact). 25",
            "type": "explicit"
          },
          {
            "term": "disparate treatment",
            "definition": "differential treatment on the basis of race that disadvantages a racial group",
            "context": "The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) 'differential treatment on the basis of race that disadvantages a racial group' (disparate treatment) and ( 2) 'treatment on the basis of inadequately justified factors other than race that disadvantages a racial group' (disparate impact). 25 Related to the first component, 'disparate treatment', studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27",
            "type": "explicit"
          },
          {
            "term": "disparate impact",
            "definition": "treatment on the basis of inadequately justified factors other than race that disadvantages a racial group",
            "context": "The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) 'differential treatment on the basis of race that disadvantages a racial group' (disparate treatment) and ( 2) 'treatment on the basis of inadequately justified factors other than race that disadvantages a racial group' (disparate impact). 25 The later component, 'disparate impact', also has implications for creating racial/ethnic disparities in food insecurity.  ",
            "type": "explicit"
          },
          {
            "term": "structural racism",
            "definition": "the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)",
            "context": "While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Sexist ideologies maintain as well as reflect societal gender inequality; therefore, across nations, HS and BS means should be negatively correlated with national indicators of gender equality. Admittedly, our data are imperfect for these cross-cultural comparisons, given that (a) people in all but one of our samples cannot be presumed to be representative of their country and (b) we have data from only 19 nations (which, given that nation becomes the unit of analysis, means that our N is low). These factors should, however, work against the possibility of finding significant crosscultural correlations between the ASI and gender inequality rather than privilege our hypothesis. 8 7 Women's mean HS and BS scores, which are correlated in the .80 to .90 range with men's HS and BS means, were also strongly related to the gender gap in BS, though not to the gender gap in HS. Women's HS and BS means,respectively, to the gender gap in BS, but only .08 (ns) and .39 (p < .10) to the gender gap in HS. 8 Some readers may wonder whether correlations of the ASI scales to the United Nations measures might be an artifact of the types of samples taken in each country. If (a) more diverse (nonstodent) community samples produced higher means and (b) diversity of sample was confounded with traditionality of country, the correlations between the ASI scales and United Nations indices might be inflated. This scenario is unlikely, given that type of sample and national sexism scores were not confounded in this manner. The country with the highest proportion of nonstudents was the Netherlands (highly egalitarian), and about 50% of the sample from another of the most egalitarian nations, Australia, were older adults. Among the countries scoring highest in sexism, the samples from Botswana, Cuba, Colombia, South Africa, and Turkey were exclusively, and the samples from Nigeria and Chile were predominantly (75-80%), composed of university students. About 40% of the respondents in the South Korean sample were working adults, but their means were similar to the students' means. We correlated men's and women's means on HS and BS with the two United Nations indices of gender equality. It is important to recall that the GEM assesses women's presence in elite, highstatus jobs in business (e.g., as managers) and government (e.g., as parliament members), whereas the GDI assesses women's level of development in education, longevity, and standard of living. Because nations with higher human development more generally are likely to have higher GDI scores (simply because the nation as a whole is wealthier), we controlled for national HDI scores when correlating HS and BS to the GDI. The correlations are reported in Table 4. Because men are the dominant group, their scores might be expected to be most predictive of gender inequality. Despite the limitations of our sampling, men's national means on both HS and BS tended to be negatively correlated with the GDI and the GEM. 9 Although the correlation of men's mean BS scores with the GDI and GEM were only marginally significant, the magnitude of these correlations was roughly similar to the significant correlations of men's HS scores to the United Nations statistics. Women's mean HS and BS scores were (marginally significantly) related only to the GEM, not to the GDI. Overall, the results suggest that both HS and BS are predictive of gender inequality.",
        "reasoning": "The passage contains explicit definitions of the two United Nations gender equality indices: the Gender Equality Monitor (GEM) and the Gender Development Index (GDI). These are defined in the sentence that explains “the GEM assesses women's presence in elite, high‑status jobs in business (e.g., as managers) and government (e.g., as parliament members), whereas the GDI assesses women's level of development in education, longevity, and standard of living.” This sentence provides clear, standalone definitions for each term. No other terms are defined explicitly in the section. Therefore, only those two terms are extracted.",
        "extracted_terms": [
          {
            "term": "GEM",
            "definition": "women's presence in elite, highstatus jobs in business (e.g., as managers) and government (e.g., as parliament members)",
            "context": "Because men are the dominant group, their scores might be expected to be most predictive of gender inequality. the GEM assesses women's presence in elite, highstatus jobs in business (e.g., as managers) and government (e.g., as parliament members), whereas the GDI assesses women's level of development in education, longevity, and standard of living. Overall, the results suggest that both HS and BS are predictive of gender inequality.",
            "type": "explicit"
          },
          {
            "term": "GDI",
            "definition": "women's level of development in education, longevity, and standard of living",
            "context": "Because men are the dominant group, their scores might be expected to be most predictive of gender inequality. the GEM assesses women's presence in elite, highstatus jobs in business (e.g., as managers) and government (e.g., as parliament members), whereas the GDI assesses women's level of development in education, longevity, and standard of living. Overall, the results suggest that both HS and BS are predictive of gender inequality.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "It may look like there is a story out there that pre-exists the narrative discourse and therefore is \"mediated\" by it. But isn't this an illusion? After all, as we noticed above, the story only comes to life when it is narrativized. For Jonathan Culler, there is at bottom an ambiguity here which will never be resolved. He calls it the \"double logic\" of narrative, since at one and the same time story appears both to precede and to come after narrative discourse. On the one hand, a story does seem to have a separate existence, lying out in some virtual realm while the narrative discourse endeavors to communicate it. This effect is especially powerful in stories that are narrated in the past tense, since the narration seems to start at a point after the completion of the story. On the other hand, before the narrative discourse is expressed, there is no story. 6 Tolstoy, for example, recounted that, when he was writing Anna Karenina, he found that, after Vronsky and Anna had finally made love and Vronsky had returned to his lodging, he, Tolstoy, discovered to his amazement that Vronsky was preparing to commit suicide. He wrote on, always in the past tense, but faster and faster, to see how the story would turn out. 7 In other words, without first creating the narrative discourse, he would never know the story. One thing that strengthens the sense that stories are always mediated is that they can be adapted. Cinderella, for example, or the Faust story are not bound by any particular discourse but can travel from one set of actors or film or prose rendition to another, and yet still remain recognizably the same story. As Claude Bremond puts it, a story is \"neither words, nor images, nor gestures, but the events, situations, and behaviors signified by the words, images, and gestures.\" 8  But then what exactly is this story that travels? If we never know it except as it is mediated in one way or another, how can we say for sure that a story is a particular story and not some other story? We've all seen the Cinderella story in many different versions. A diligent scholar in the nineteenth century dug up roughly 1100 versions of Cinderella (and that was long before Disney's 1950 animated feature film). Leaving a film, I might say: \"That was a Cinderella story,\" and people might agree. But what if they disagree? How would we settle the dispute? Leaving a production of King Lear, I might say: \"That was a Cinderella story,\" and find that some people strenuously disagree. I would then point out how it features a beautiful, honest, virtuous sister (Cordelia) who, because of her wicked, selfish, dishonest sisters (Goneril and Regan), is neglected and cut off from the family fortune. My opponents, though, would point out quite rightly all the differences: that Cordelia is not forced to work as a scullery maid, that there is no fairy godmother, no coach, no ball, no glass slipper, and for that matter no happy ending. Moreover, most of our attention is devoted to events involving other people, like Lear and Gloucester. I would probably lose the argument, but in the process we would have raised an interesting question. What is necessary for the story of Cinderella to be the story of Cinderella? Between the traditional fairy tale and King Lear, when does the story of Cinderella stop being Cinderella and start being something else? Is a magical transformation of Cinderella necessary? Is the ball necessary? Is the Prince's search for Cinderella necessary? Is the happy ending necessary? This is a question that can never be answered with precision, in part because each of us reads differently. But for that reason, the pursuit of the issue can still be interesting, if only to explore these differences. In the dispute above, it may be that the Cordelia story dominates my perceptions of Lear, and for that reason I am more inclined to see the framework of a Cinderella variant in this tragedy than others. And this perceptual bias of mine might be interesting (at least for people who are interested in me or the kind of people I might represent). But what permits these differences in reading to begin with is the condition we observed at the outset of this section: story is always mediated (constructed) by narrative discourse. We are always called upon to be active participants in narrative, because receiving the story depends on how we in turn construct it from the discourse. Are stories, then, at the mercy of the reader and how diligently he or she reads? To a certain degree this is true. But most stories, if they succeed -that is, if they enjoy an audience or readershipdo so because they have to some extent successfully controlled the process of story construction. Where differences between readings become fraught with significance is in the area of interpretation. My tendency to feature the Cinderella elements in King Lear colors the way I interpret the play.",
        "reasoning": "The section contains an explicit definition of the term *story* given by Claude Bremond. It states that a story is “neither words, nor images, nor gestures, but the events, situations, and behaviors signified by the words, images, and gestures.” This meets the criteria of an explicit definition.  \nAdditionally, Jonathan Culler's description of the “double logic” of narrative serves as a clear definition: the term “double logic” is defined as the phenomenon where a story appears both to precede and to come after narrative discourse.  \nEach term is explicitly defined, the definitions are concise, and the surrounding sentential context is retained as required.",
        "extracted_terms": [
          {
            "term": "story",
            "definition": "neither words, nor images, nor gestures, but the events, situations, and behaviors signified by the words, images, and gestures.",
            "context": "Cinderella, for example, or the Faust story are not bound by any particular discourse but can travel from one set of actors or film or prose rendition to another, and yet remain recognizably the same story. As Claude Bremond puts it, a story is 'neither words, nor images, nor gestures, but the events, situations, and behaviors signified by the words, images, and gestures.' 8 But then what exactly is this story that travels?",
            "type": "explicit"
          },
          {
            "term": "double logic",
            "definition": "story appears both to precede and to come after narrative discourse.",
            "context": "For Jonathan Culler, there is at bottom an ambiguity here which will never be resolved. He calls it the 'double logic' of narrative, since at one and the same time story appears both to precede and to come after narrative discourse. On the one hand, a story does seem to have a separate existence, lying out in some virtual realm while the narrative discourse endeavors to communicate it.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "We report on the detailed dissagreement statistics of the annotators per each fine-grained label, and highlight the difficulty of each label, in Table 5.    label count %conf %other mcl difficulty 1: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor 169 0.309 0.315 5 Medium 2: URW: Blaming the war on others rather than the invader: The West are the aggressors 260 0.377 0.390 7 Medium 3: URW: Discrediting Ukraine: Rewriting Ukraine's history 23 0.272 0.111 28 Medium * 4: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society 38 0.380 0.053 6 Medium * 5: URW: Discrediting Ukraine: Discrediting Ukrainian military 316 0.314 0.217 1 Medium * 6: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies 502 0.215 0.253 8 Easy 7: URW: Discrediting Ukraine: Ukraine is a puppet of the West 219 0.235 0.165 28 Easy * 8: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities 127 0.368 0.243 6 Medium * 9: URW: Discrediting Ukraine: Ukraine is associated with nazism 97 0.189 0.080 6 Easy * 10: URW: Discrediting Ukraine: Situation in Ukraine is hopeless 107 0.303 0.220 6 Medium * 11: URW: Russia is the Victim: The West is russophobic 167 0.343 0.427 2 Medium 12: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence 130 0.518 0.273 1 Hard * 13: URW: Russia is the Victim: UA is anti-RU extremists 34 0.477 0.406 11 Hard * 14: URW: Praise of Russia: Praise of Russian military might 466 0.140 0.346 5 Easy 15: URW: Praise of Russia: Praise of Russian President Vladimir Putin 100 0.157 0.348 17 Easy 16: URW: Praise of Russia: Russia is a guarantor of peace and prosperity 257 0.352 0.385 1 Medium 17: URW: Praise of Russia: Russia has international support from a number of countries and people 228 0.160 0.245 16 Easy 18: URW: Praise of Russia: Russian invasion has strong national support 20 0.333 0.250 17 Medium * 19: URW: Overpraising the West: NATO will destroy Russia 11 0.263 0.400 20 Medium 20: URW: Overpraising the West: The West belongs in the right side of history 36 0.263 0.133 28 Medium * 21: URW: Overpraising the West: The West has the strongest international support 24 0.254 0.308 7 Medium 22: URW: Speculating war outcomes: Russian army is collapsing 51 0.313 0.476 23 Medium 23: URW: Speculating war outcomes: Russian army will lose all the occupied territories 10 0.312 0.200 22 Medium * 24: URW: Speculating war outcomes: Ukrainian army is collapsing 93 0.242 0.118 5 Easy * 25: URW: Discrediting the West, Diplomacy: The EU is divided 105 0.164 0.435 28 Easy 26: URW: Discrediting the West, Diplomacy: The West is weak 145 0.301 0.267 14 Medium * 27: URW: Discrediting the West, Diplomacy: The West is overreacting 34 0.395 0.294 11 Medium * 28: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests 211 0.381 0.262 7 Medium * 29: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work 109 0.550 0.434 28 Hard * 30: URW: Discrediting the West, Diplomacy: West is tired of Ukraine 43 0.583 0.171 28 Hard * 31: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire 79 0.338 0.723 12 Medium 32: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe 12 0.000 NaN Easiest 33: URW: Distrust towards Media: Western media is an instrument of propaganda 104 0.180 0.452 11 Easy 34: URW: Distrust towards Media: Ukrainian media cannot be trusted 23 0.500 NaN 6 Hard 35: URW: Amplifying war-related fears: By continuing the war we risk WWIII 135 0.285 0.339 37 Medium 36: URW: Amplifying war-related fears: Russia will also attack other countries 133 0.211 0.270 2 Easy 37: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed 292 0.259 0.486 2 Medium 38: URW: Amplifying war-related fears: NATO should/will directly intervene 63 0.305 0.273 36 Medium * 39: CC: Criticism of climate policies: Climate policies are ineffective 74 0.407 0.174 44 Hard * 40: CC: Criticism of climate policies: Climate policies have negative impact on the economy 96 0.337 0.333 45 Medium * 41: CC: Criticism of climate policies: Climate policies are only for profit 58 0.508 0.069 68 Hard * 42: CC: Criticism of institutions and authorities: Criticism of the EU 54 0.275 NaN 45 Medium 43: CC: Criticism of institutions and authorities: Criticism of international entities 104 0.434 0.196 44 Hard * 44: CC: Criticism of institutions and authorities: Criticism of national governments 225 0.375 0.274 45 Medium * 45: CC: Criticism of institutions and authorities: Criticism of political organizations and figures 190 0.487 0.172 44 Hard * 46: CC: Climate change is beneficial: CO2 is beneficial 19 0.071 NaN 51 Easiest 47: CC: Climate change is beneficial: Temperature increase is beneficial 13 0.266 0.500 54 Medium 48: CC: Downplaying climate change: Climate cycles are natural 36 0.294 0.300 50 Medium 49: CC: Downplaying climate change: Weather suggests the trend is global cooling 13 0.406 0.308 70 Hard * 50: CC: Downplaying climate change: Temperature increase does not have significant impact 5 0.736 0.286 56 Hardest * 51: CC: Downplaying climate change: CO2 concentrations are too small to have an impact 18 0.500 0.143 59 Hard * 52: CC: Downplaying climate change: Human activities do not impact climate change 34 0.387 0.083 60 Medium * 53: CC: Downplaying climate change: Ice is not melting 18 0.117 NaN 62 Easy 54: CC: Downplaying climate change: Sea levels are not rising 2 0.500 0.500 47 Hard 55: CC: Downplaying climate change: Humans and nature will adapt to the changes 5 1.000 0.400 50 Hardest * 56: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty 51 0.333 0.118 59 Medium * 57: CC: Questioning the measurements and science: Data shows no temperature increase 10 0.562 0.333 56 Hard * 58: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change 5 0.750 1.000 Hardest 59: CC: Questioning the measurements and science: Scientific community is unreliable 45 0.540 0.185 56 Hard * 60: CC: Criticism of climate movement: Climate movement is alarmist 67 0.619 0.386 61 Hardest * 61: CC: Criticism of climate movement: Climate movement is corrupt 35 0.869 0.463 60 Hardest * 62: CC: Criticism of climate movement: Ad hominem attacks on key activists 78 0.234 0.217 60 Easy * 63: CC: Controversy about green technologies: Renewable energy is dangerous 18 0.421 0.250 65 Hard * 64: CC: Controversy about green technologies: Renewable energy is unreliable 42 0.344 0.333 45 Medium * 65: CC: Controversy about green technologies: Renewable energy is costly 27 0.512 0.100 45 Hard * 66: CC: Controversy about green technologies: Nuclear energy is not climate friendly 3 1.000 0.750 45 Hardest * 67: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites 47 0.438 0.217 68 Hard * 68: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives 74 0.548 0.094 45 Hard * 69: CC: Amplifying Climate Fears: Earth will be uninhabitable soon 51 0.432 0.188 70 Hard * 70: CC: Amplifying Climate Fears: Amplifying existing fears of global warming 864 0.124 0.477 71 Easy 71: CC: Amplifying Climate Fears: Doomsday scenarios for humans 104 0.435 0.239 70 Hard * 72: CC: Amplifying Climate Fears: Whatever we do it is already too late 18 0.264 0.111 70 Medium * 73: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative 7 0.785 0.273 68 Hardest * 74: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism 10 0.375 0.333 68 Medium * Table 5: Label difficulty, %conf is the percentage of annotation with that label that results in an inconsistency, while %other is the percentage that resulted in an inconsistency with label other. \"mcl\" is the most common label with which a specific label is confused. \"difficulty\" make assess the difficulty of the label based on %conf: Easiest (≤ .1), Easy (≤ .25), Medium (≤ .4), and Hard (≤ .4) or Hardest (> 0.6), a star indicate that there is more confusion within the labels of the taxonomy than with the Other class, meaning that it is extra difficult for the annotator",
        "reasoning": "I examined the provided section and found no explicit term definitions. The text consists mainly of a table listing labels, counts, percentages, and difficulty ratings. All label names are given as strings (e.g., \"URW: Blaming the war on others rather than the invader: Ukraine is the aggressor\"), but none of them are accompanied by a definition clause such as \"means...\" or an explanatory sentence that qualifies as a definition. Therefore, there are no term-definition pairs to extract from this section.",
        "extracted_terms": []
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4eb5a9745dfc892cb42ed427a5f08e6abf8df7e3",
        "sections": [
          "T he rise of affective polarization-most notably, the tendency for partisans to dislike and distrust those from the other party 1 -is one of the most striking developments of twenty-first-century US politics 2,3 . Affective polarization has wide-ranging implications for our social and economic lives. It plays a role in how much time we spend with our families, where we want to work and shop and whom we want to date and marry 4 . But what does it mean for our politics? The answer is surprisingly unclear, as Iyengar and colleagues note (p. 139): \"little has been written on this topic (that is, the political effects), as most studies have focused on the more surprising apolitical ramifications\" 4 . Here, we take up a crucial dimension of that question: how are individuals' issue positions related to their level of affective polarization? We argue that the two are strongly connected in ways not addressed in previous research. Partisans with high levels of animus toward the other party are more motivated to distinguish themselves from their political opponents. They do so by taking positions on new issues that differ from the other (disliked) party and match those of their own preferred party. While this argument-that previous levels of partisan animus play a role in subsequent issue positions-is straightforward, testing it is difficult given the inherent endogeneity between policy beliefs, affective polarization and elite issue positions: if scholars find that those who harbour the most animus toward the other party also hold more extreme beliefs, is that due to animus driving those particular beliefs, to policy beliefs driving animus 5 or due to elite issue polarization simultaneously driving both the public's out-party animus 6 and policy beliefs 7 ? The emergence of the novel COVID-19 pandemic in the winter of 2020 presents us with the conditions needed to overcome some of the endogeneity that limits existing work. We collected data on respondents' levels of affective polarization in 2019, before the emergence of the coronavirus. We therefore have a measure of affective polarization that is exogenous to the pandemic: we can examine how pre-existing levels of partisan animus correlate with subsequent responses to COVID-19 without concern that the responses to the pandemic are, in fact, shaping affective polarization (and, more directly, out-party animus). Put another way, this design allows us to rule out the aforementioned possibilities that individuals' or elites' policy beliefs drive affective polarization (and hence any relationships between polarization and beliefs). Although our approach cannot isolate causal effects-given that we use observational data without a clear causal identification strategy-it does allow us to overcome the endogeneity identified above, which has been the key limitation encountered in previous work. We find a strong association between out-party animus and subsequent responses to the pandemic, offering evidence that policy beliefs reflect affective feelings toward the other party rather than just the issues at hand. That said, however, our findings also highlight how local context matters, as this relationship is muted among those who live in areas with particularly severe outbreaks of COVID-19. In these locations, even those with high levels of partisan animus have good reason to be concerned about the virus-it is personally salient to them. This highlights how real-world conditions affect citizens' issue positions, and suggests a potential limit to the types of partisan-motivated reasoning that probably underlie our results. The implications of our work go beyond political ramifications; we demonstrate that partisan hostility combined with conflicting elite cues can intersect with national efforts and can, quite literally, mean the difference between life and death 8 . To explicate our argument, we start conceptually by connecting affective polarization with partisanship 1 . Partisanship is a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party) 9 . This process gives rise to two of the underlying components of affective polarization: in-group favouritism and out-group animosity 4 . Over-time shifts in affinity for one's own party and animosity toward the other party have not been symmetric 2,4,10,11 . Indeed, out-party animus has increased dramatically in recent years 2,4 while in-party warmth has, if anything, slightly declined over the same time period 10 . Consistent with evidence of increasing out-party animosity, individuals report that they are less likely to date those from the",
          "NATUre HUmAN BeHAVIOUr other party 12 , they would pay out-partisans less for the same work 13 and they would prefer not to have out-partisans as roommates 14 . Furthermore, those with higher levels of out-party animosity report engaging in more discriminatory behaviour against those from the other party (for example, they do not want to work with those from the other party) 15 . Out-party animus, rather than in-party favouritism, is key to these associations in the literature 11 . Partisan identity alone, however, is not enough to explain out-group animus 2,16 -one must also account for other changes in the political and media environment 2,4 . The partisan-ideological sorting of liberals to the Democratic Party and conservatives to the Republican Party 7 , as well as the social sorting that has led to more demographically homogenous parties 17 , have both contributed to partisan animosity. Also at work are other changes in elite behaviours 18 and increasing elite polarization 6,19 . Moreover, changes in the information environment, such as the rise of partisan media 20,21 , increasingly negative campaigns 22 and new social media outlets, contribute to out-party animosity 23 . Given that out-party animus has elevated the partisan cue in social contexts, it may also have affected people's responses to elite political cues. As Pierce and Lau argue, for example (p. 9), \"strong affective reactions to a politician may themselves engender awareness of and like or dislike for certain policies. For instance, a visceral aversion to a candidate may lead a voter to reject positions associated with that politician\" 24 . To this end, people are motivated to do the opposite of what the other, disliked, party endorses [25][26][27] . They do this because the out-party animus is so strong that they want to differentiate themselves from that disliked party. And, importantly, it follows that those with greater out-party animus (that is, stronger affective reactions) will be most motived to hold distinctive views 28,29 , taking positions opposite to those put forth by out-party elites (for example, elected officials) and in line with those of their own party's elites. This response to cues may be especially apparent when the difference between in-party and out-party cues is stark 30 , as it is in the case of COVID-19 (refs. 31,32 ). Demonstrating that affective polarization (and its key underlying component, out-party animus) relates to policy beliefs, however, is surprisingly complicated 33 . There is an empirical relationship between alignment in issue positions and partisan animus but it is difficult to identify the original source of this relationship 34 . Indeed, theoretically, the relationship between issue beliefs and out-party animus could stem from three possible scenarios: (1) animus driving cue taking on issues (as just explained), (2) issue position extremity causing greater partisan animus 5,34 or (3) elite issue polarization leading separately to both public issue divides 7 and out-party animus among the public 6,35 . As a result, it is difficult to determine how animus connects to political views-that is, whether policy positions are undergirded by affective dislike beyond substantive considerations. Although it is difficult to address this issue fully without manipulating affective polarization, one approach that would allow us to address a part of this problem is a measure of out-party animus taken before the emergence of an issue. This allows us to record levels of animus (at time t -1) before the existence of those issue positions (at time t). This means that elite polarization on the issue at time t cannot have affected earlier measures of partisan animus taken at time t -1, or that attitudes measured at time t are the cause of this time t -1 animus. Nevertheless, the persistence of existing issues on the policy agenda, and the unpredictability of new issues emerging onto the agenda, make it extremely difficult to use an ex ante measure (and, to our knowledge, this has not been done). The COVID-19 pandemic, however, allows us to consider an issue as it emerges. To do this, we need measures of partisan animus taken before COVID-19 began to spread in the United States. If we instead used a measure taken after COVID-19 entered the agenda, the issue itselfand politicians' reactions to it-could shape those recorded levels ",
          "NATUre HUmAN BeHAVIOUr of partisan animus. For example, Democrats' levels of partisan animus might reflect not only their underlying hostility toward President Trump, but also how he specifically responded to COVID-19 (that is, downplaying its severity, refusing to acknowledge its existence in the United States for several weeks, and so on). If we see that this measure of animus is related to attitudes about the pandemic, then, it could simply reflect politicians' reactions to it. We therefore use pre-pandemic measures of partisan animus (from August 2019)-paired with attitudes toward the pandemic measured once it emerged (from April 2020)-to study the relationship between the two. The partisan difference in elite responses to the pandemic suggests why affective polarization, and specifically out-party animus, may play a key role in driving issue positions here. From the beginning of the outbreak, Democratic politicians, relative to Republican ones, expressed greater concern about the virus, implored the public to take more precautions and supported more restrictive policies 36 . President Trump-with his dismissal of the virus, demands to reopen the economy and refusal to wear a mask-is the apotheosis of this trend, but is far from the only example of it, as Democratic governors typically took swifter and more public actions to combat the virus than did most Republican governors 37 . Moreover, these partisan debates and polarization on the issue were reflected in the media coverage 38 . The fact that the two parties behaved as mirror opposites in response to the pandemic is especially notable here, as it means that citizens simultaneously received distinct information about how members of both partisan groups should behave, making the elite cues especially clear 7 . This makes for clear cues, but it also means we cannot empirically differentiate the relative impact of in-party versus out-party cues. Future work would benefit from looking at situations with cues from only one party 32 , although it is a situation that is increasingly rare 39 . We would expect that, in such situations, animus would drive reactions from the out-party cue alone and (possibly) the in-party cue alone since partisans want to distinguish themselves. In line with the aforementioned theoretic logic that affective polarization-and especially its key ingredient, out-party partisan animus-may increase the motivation to follow cues, we expect the following pattern: as out-party animus increases, Democrats will express more concern about the virus, be more willing to take actions to prevent its spread (for example, wash their hands more, avoid large crowds, cancel travel and so on) and be more supportive of policies to stop the virus (for example, stay-at-home orders) (hypothesis 1a). Conversely, among Republicans we expect that as out-party animus increases, worries about COVID-19 will decrease, there will be a lower likelihood of taking actions to prevent its spread and less support for policies to stop the spread of the virus (hypothesis 1b). Our argument is not simply that partisan gaps have emerged: that point has been thoroughly documented elsewhere 8,40,41 . Instead, our argument is that it is the animus component of affective polarization, at least partially, that drives these gaps. Our argument implicitly invokes partisan-motivated reasoning since we posit that partisans have a directional motivation in forming opinions 42 . Partisan-motivated reasoning means partisans process information and form attitudes with the goal of confirming their partisan identities and differentiating themselves from the other party (this contrasts with issue-based motivated reasoning where the goal is to confirm a standing issue belief) 43 . While directional partisan reasoning predominates in highly political situations 44 , it can shift when particular issues rise in salience 45 . Of ",
          "NATUre HUmAN BeHAVIOUr particular relevance are conditions that prompt partisans to shift from having a directional motivation to an accuracy motivation. In this latter case, individuals assess information based on the 'best' available evidence rather than to affirm an identity [46][47][48] . In the case of COVID-19, this will occur as the direct threat of the virus increases and is captured by the number of cases in one's local area. An increase in cases can alter personal experiences (for example, increasing the likelihood that someone you know personally has been infected) which, in turn, vitiates partisan reasoning 47 . We thus predict that, as the number of COVID-19 cases in one's area increases, the impact of out-party animus will decrease and the partisan gap will similarly decrease (hypothesis 2). In short, partisan animus matters, but so too does the geography of the COVID-19 outbreak in the United States. Broadly, then, our study suggests the possibility that partisan-motivated reasoning is conditional and may be shaped by context. Following a similar logic, we also might expect the partisan animus effect to decline among those who have had, or are vulnerable to, COVID-19, but at the time of our data collection the number of such individuals in our sample was too small to test that possibility. We use a multi-wave, nationally representative survey. In the summer of 2019, 3,345 respondents answered a set of questions (for an unrelated survey) that provide our pre-COVID-19 measure of partisan animosity. These participants were re-interviewed in April 2020 as the coronavirus spread throughout the nation; a total of 2,484 respondents who answered our 2019 questionnaire completed  ",
          "NATUre HUmAN BeHAVIOUr our re-interview, for a re-contact rate of 74% (more details on the sample are given in Supplementary Methods). In this re-interview, we measured participant reactions to the COVID-19 outbreak focusing on three relevant dimensions: (1) how worried they are about the virus, both for themselves and for the nation as a whole, measured by a range of items assembled into an index (α = 0.89); (2) which behaviours (from a list of 14) they are taking to avoid becoming infected with COVID-19 (that is, washing their hands more, cancelling travel and so on); and (3) their support for various policies to limit the spread of COVID-19 (that is, stay-at-home orders, business closures and so on), again analysed as an index (α = 0.73). All analyses treat these three measures as dependent variables, and the pre-pandemic measure of animosity is our key explanatory variable. More information on the survey is provided in Methods and Supplementary Methods.",
          "Figure 1 shows kernel density plots (separately for Democrats and Republicans) for each of the three dependent variables: (1) worry about COVID-19, (2) behaviours being taken to avoid becoming infected with COVID-19 and (3) support for various policies to limit the spread of COVID-19; see Methods for details on the coding of these and all other variables. The plots show that the average Democrat is more worried, is more likely to have changed behaviours and is more supportive of polices to stop the spread of infections than the average Republican, consistent with other analyses showing partisan gaps in these areas 40,41 . There is, however, substantial overlap in the attitudes of Republicans and Democrats, which suggests the possibility that something moderates the relationship between partisanship and COVID-19 attitudes. Figure 2 contains scatter plots for each of the dependent variables (on the y axes) along with the number of cases in the respondent's county (on the x axes), as well as a locally weighted scatterplot smoothing (LOWESS) smoother to show the nonparametric bivariate relationship between the two variables. It is clear that, as case numbers increase, values on all dependent variables also increase. Because the relationship is nonlinear, especially for low-infection areas, and there is a long right-tail of cases (that is, a small number of areas, primarily in New York City, with extremely high rates of infection), we use the natural log of cases in all of our models. We next turn to our quantities of interest-the relationship between partisanship, partisan animosity and responses to the COVID-19 pandemic-and estimates of uncertainty around those effects. To ensure the robustness of our results, we estimate a series of models with additional controls added in each model (Methods and Supplementary Methods). The results we present are robust to changes in estimation approach, and to the inclusion of a variety of controls including a measure of partisan affect and strength of identity. We present the results of our main models in Figs. 34567. These are represented as plots since our models rely on interactions, and the coefficient estimates on interactions and their constitutive terms do not easily translate to our quantities of interest; as a result, the significance levels of these coefficients may not be informative in terms of testing our hypotheses [49][50][51] . Relevant here is the slope of the outcome variable at various levels of other covariates, a quantity termed the 'marginal effect'; this term is not intended to signal a causal relationship 52 . By definition, all tests of the statistical significance of this effect are two-tailed. We begin with plots from a model that includes an interaction between partisanship and partisan animosity, while controlling for the number of cases. The top panels of Fig. 3 present the marginal effect of out-party animus for Democrats and Republicans for each dependent variable, while the bottom panels show the marginal effect of Republican partisanship for various levels of animus. Hence, the top parts directly test hypotheses 1a and 1b for each dependent variable, while the bottom parts plot the partisan gap as out-party animus increases. Beginning ",
          "NATUre HUmAN BeHAVIOUr see a similar relationship between partisan animus and how worried Democrats are about COVID-19. When we move to the middle panels of Fig. 3 examining behaviours, here we see that Democrats with high levels of animus report engaging in more behaviours to combat COVID-19 than do Democrats who do not hold as much animus. With this dependent variable, however, there is no similar, statistically significant result among Republicans. Finally, with regards to policy, we see that out-party animus is associated with greater support for policies to combat COVID-19 among Democrats while Republicans with high levels of animus are less supportive of the same policies than Republicans with less animus. Hence we see support for the partisan animus hypothesis for at least one party across all three variables. As the bottom panels of Fig. 3 show, there is a partisan gap for each dependent variable and the size of that gap grows as out-party animus increases. Since the model controls for the number of cases in the respondent's county, the partisan gaps are not the result of areas with many Democrats having more severe outbreaks than areas with many Republicans. Recall, however, we argue that severe outbreaks might mitigate the role of partisan animosity. The figures now described will look at the models with triple interaction, including cases. In Figs. 456, we present the marginal effect of the Republican dummy variable at different levels of out-party animus-that is, the difference in the expected value in the dependent variable for Republicans minus the expected value in the dependent variable for Democrats. For each dependent variable, we include separate plots for a low number of cases (the 25th percentile) and a high number of cases (the 75th percentile). If our argument is correct, then we should find that as out-party animus increases, the gap between the parties increases as well (that is, the marginal effect of partisanship increases; this is the test of hypothesis 1). However, we should also find that this relationship is muted in areas with large numbers of cases, as all citizens are more concerned about the virus. Simply put, we should see a steeper slope (larger marginal effect) in areas with low cases relative to high cases if hypothesis 2 is correct. In Fig. 4 we present the marginal effect of being a Republican (as opposed to a Democrat) on worry, as partisan animus increases. In the left-hand panel, which presents the relationship between partisanship and out-party animus in areas with few cases, we see that as partisan animus increases, the partisan gap emerges: when animus is low, partisans are indistinguishable from one another but, when animus is high, partisans significantly diverge. In contrast, in the right-hand panel, depicting the pattern in areas with high levels of cases, there no significant partisan gap among those with high levels of animus (that is, the confidence interval overlaps zero). We do see small partisan differences for moderate levels of out-party animus (probably because the majority of our respondents have moderate levels of animus), but these gaps are distinctly smaller than the partisan gaps among those who live in areas with few cases. Figure 5 presents the same analysis for the behaviour dependent variable. Partisan animus again has a clear correlation with political outcomes, as we observe partisan divides on COVID-19 behaviours. The difference, however, is that we see the same increasing partisan gap regardless of the number of cases in the county. Higher numbers of cases correlate with more preventative behaviours overall, but higher partisan gaps in behaviour emerge alongside animus regardless of the number of cases. The reason for this is that, while individuals with low or moderate levels of animus are responsive to the number of cases, Democrats and Republicans with high levels of animus are not. Why do those with such animus not change their behaviour as the number of cases increases? The answer is probably different for Republicans and Democrats. Republicans with high animus took low-cost actions (for example, hand washing) in low-case areas and were forced to avoid certain behaviours (like going to restaurants) due to local restrictions. As cases increased, they may have believed they were already doing enough. Democrats with high levels of animus are probably less responsive because they are engaging in more behaviours even in counties with few cases. Nevertheless, because the group is already changing behaviours in low-case areas, they are ",
          "NATUre HUmAN BeHAVIOUr unlikely (or perhaps, even unable) to take on more behaviours in areas with more severe outbreaks. Turning next to Fig. 6, we see that when it comes to policy support there is once again a relationship between partisan animus and opinions. Here, much as with worry, the number of cases moderates the relationship. Although there is a significant partisan difference among partisans with high animus when cases are low, there is no statistically significant difference between Republicans and Democrats in those counties with high numbers of cases, regardless of the level of animus. We note that support for policies to prevent the spread of infections is high; among both Democrats and Republicans, a majority supported these policies at the time of the re-interview. As a result, it probably should not be a surprise that, in areas with a significant outbreak of COVID-19 infections, partisan gaps disappear. Otherwise, like worry but unlike behaviours, expressing policy support is not a costly behaviour per se. It is worth noting that Republicans with high animus and in high-case areas appear to be more supportive of government policy intervention than of engaging in relevant behaviours, and this is an area for future work to explore more carefully. In Fig. 7, we use the same models to present a different perspective on the results, now focusing on the marginal effect of out-party animus for Democrats and Republicans in low-and high-case counties. The goal here is to see which party is the root of the partisan gaps at higher levels of partisan animus, by considering the relationship between a unit increase in animus and the likelihood of worrying, changing behaviour and supporting policy. For worry about COVID-19 and support for COVID-19 policies, the marginal effect of animus is significant and negative for Republicans in counties with few cases; the confidence intervals for the other marginal effects overlap with zero. Increases in animus are statistically significant only for Republicans in counties with low cases, suggesting that, for worry and support, partisan gaps are largely a function of Republicans with considerable animus towards Democrats. When it comes to behaviours, however, the only statistically significant marginal effect for partisan animus is among Democrats in counties with few cases-these individuals are engaging in preventative behaviours despite the low community spread of COVID-19. At the same time, however, in counties with large outbreaks, the absolute size of the marginal effects for both Democrats and Republicans is still equivalent to a full behaviour in both cases-even if the confidence intervals overlap zero. That might explain why the partisan gap remains in those counties: the difference within parties between those with high animus is smaller but, since the parties move in opposite directions, the partisan gap remains the same. Overall, the results make clear that the partisan gaps observed in the data are at least partially a function of partisan animus, suggesting that it is cue taking that divides the parties on this issue. This type of partisan reasoning, however, is blunted when real-world threats become salient: in areas with substantial numbers of infections the role of animus is more muted, as all citizens respond to the outbreak. Furthermore, we see that with regards to worry and policy support, the correlation with partisan animus in counties with low cases is most pronounced among Republicans. Given the messaging from the President, this pattern makes sense. It is understandable why Democrats, regardless of animus, and (to a lesser extent) Republicans who do not have high animus worried about the virus's impact on public health and the economy, and supported shutdowns and stay-at-home orders even when the local severity was low. On the other hand, Republicans with high animus attuned to a Republican President saying, counter to the clear Democratic message, that it will all just disappear 53 , needed an active, local outbreak to increase their concerns to the level that Democrats were feeling.",
          "While scholars, pundits and citizens alike invoke affective polarization as a factor in driving issue positions, there has, to date, been no direct evidence that it actually does. We leveraged a unique ",
          "NATUre HUmAN BeHAVIOUr data opportunity to track the association between affective polarization and, more directly, out-party animus and responses to the COVID-19 pandemic. Rhetorical differences between party elites help to produce our results: not only were Democratic elites much more likely to emphasize the threat of the virus to public health and the importance of taking appropriate precautions 54 , but President Trump downplayed the danger and advocated for treatment approaches shown to be ineffective 55 . Other Republican elected officials were similarly dismissive of the virus at the time of our study. These rhetorical divisions are associated with mass partisan divisions: as animus increases, Republicans become less concerned about COVID-19 and less willing to support policies to mitigate the threat of the virus. Real-world threat-here, a high level of infections in one's county-tempers that relationship, since, as theories of reasoning suggest, it pushes individuals away from directional partisan motivations towards an accuracy motivation-that is, a desire to rely on the best available evidence to which they have access. Still, we note that even in counties with high numbers of cases, Democrats and Republicans with high levels of animus differ in how likely they are to report engaging in actual, costly behaviours. This is because Democrats with high levels of out-party animus are already engaging in a high number of mitigating behaviours while Republicans with high levels of out-party animus remain resistant to costly behaviours as case levels increase. These findings have implications for understanding how best to combat COVID-19. Since affective polarization (particularly partisan animus) underlies partisan gaps, policymakers will need to devise different strategies to bring the parties together on these issues. Simply highlighting areas of commonality, scientific directives or economic forecasts is not enough; instead, they will need to ameliorate partisan animus to shrink the gaps. This would require, for example, correcting misperceptions about the parties 56,57 , priming superordinate identities 58 and/or fostering inter-party contact and dialogue 59 . The results also offer insights for theories of partisan reasoning, insofar as they show how such thinking can drive opinions but also how real-world threats can alter motivations. More broadly, our findings suggest that policy differences between the parties are not simply a function of different information 60,61 , or different values 62 , but possibly of partisan animus as well. This is a substantial finding insofar as a large literature documents correlations between partisan animosity and social and economic behaviours (for example, on friendships, romantic relationships, business transactions and so forth) 4 , but there is much less work examining the association between animus and political attitudes, due to the data difficulties we highlighted earlier in the paper. We show clear political consequences with respect to perhaps the most important of policies: government directives for preventing a public health and economic crisis.      ",
          "For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.",
          "The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly The statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.",
          "A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals) For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted Give P values as exact values whenever suitable. For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above.",
          "Policy information about availability of computer code Data collection no software was used.",
          "For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.",
          "All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: -Accession codes, unique identifiers, or web links for publicly available datasets -A list of figures that have associated raw data -A description of any restrictions on data availability Data availability. The data that support the findings of this study are available via Dataverse at: https://doi.org/10.7910/DVN/H7AT3N"
        ],
        "ground_truth_definitions": {
          "partisanship": {
            "definition": "a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party).",
            "context": "To explicate our argument, we start conceptually by connecting affective polarization with partisanship partisanship. Partisanship is a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party). This process gives rise to two of the underlying components of affective polarization: in-group favouritism and out-group animosity.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s40537-025-01077-x",
        "sections": [
          "Automatic information extraction from texts has been an important research topic of NLP for decades. With the rapid development of big data, and social media, the amount of available texts on the Internet has increased exponentially. It becomes an important research field to analyze and extract the informative content from texts. The main research on automatic analysis of the texts includes keyphrase extraction (KPE), phrase mining, named entity recognition (NER), hot spot detection, new word detection, etc. Effective solutions to these problems have been instrumental for facilitating downstream tasks such as machine translation [1], mention detection [2], textual themes extraction [3], lexicon construction [4], ontology learning [5], and knowledge organization [6]. Terminology extraction, referred to as terminology recognition, is an important research topic continually. It is usually considered as a sub-problem of information extraction and aims to identify terminology in the specialized texts, where terminology can be regarded as the linguistic representation of domain-specific concepts [7]. It is labour-intensive to manually extract terminologies and prone to errors due to the shifting topics [8], so ATE has attracted more and more attention. Many ATE methods, frameworks and tools have been developed continually. Traditional terminology extraction methods can be categorized into three kinds: linguistics-based approaches, statistics-based approaches, and hybrid approaches. Linguistics-based approaches [9,10] rely on dictionaries and rules to extract terminologies by matching specific patterns or vocabulary. Statistics-based approaches [11] leverage frequency information from the text to identify terminologies. Hybrid approaches [12] combine linguistic and statistical techniques, effectively utilizing domain knowledge and data statistics to enhance accuracy. Each method has its advantages, and the choice depends on specific requirements. However, these supervised approaches are solely based on the linguistic and statistical characteristics, so that the scale and quality of the annotated corpus have a great impact on the performance of terminology extraction. Researchers have gradually applied external knowledge (Wikipedia, 1 WordNet [13]), semantic information [14], graph structure [15], and topic models [16,17] to terminology extraction. Building on the advancements in the field, terminology extraction has evolved into methods based on machine learning [18,19], deep learning [20,21], graph-based approaches [22,23], and LMs [24,25]. This paper provides a comprehensive review of terminology extraction from natural language texts. It's worth noting that our review, for the first time, emphasizes the related work of terminology extraction with LMs. By delving into significant research topics, the survey encompasses formal definitions, related topics, existing approaches' taxonomy, commonly-used datasets, application scenarios, and open research issues. Serving as a valuable resource, this paper offers researchers and practitioners in the terminology extraction field an insightful introduction to key methodologies, typical features, accessible demos, ongoing challenges, and emphasizes the significance of related software tools and datasets for research and development in this domain. The subsequent sections of this paper are structured as follows. Section 2 will expound upon related concepts and the research progress in the field of terminology extraction. In Sect. 3, a comprehensive depiction of a generic and widely applicable system architecture for terminology extraction will be provided. Traditional linguistic-based and statistical-based approaches are commonly used in earlier work and recent work on terminology extraction. Recent studies also apply graph-based approaches, machine learning, deep learning and LMs. Based on this taxonomy, approaches to terminology extraction are summarized in Sect. 4. We describe the annotated datasets, open-sourced tools, and evaluation metrics for terminology extraction in Sect. 5. Section 6 analyzes several key issues in the field of terminology extraction, while Sect. 7 explores future research directions in this domain. Finally, we conclude the paper in Sect. 8.",
          "The section describes background knowledge for the core concepts of terminology extraction. We first describe and define the two important concepts of \"terminology\" and \"domain\" and then describe the characteristic related to terminology.",
          "With the in-depth development of terminology extraction, researchers have proposed different definitions of terminology, but there is still no unified standard. For example, the authors in [26] propose to separate the concept of the terminology from the common word, marking the beginning of terminology research. Wüster defines terminology as \"a concept reference system within a specialized field\". Based on Wüster's terminology theory, Wright et al. [27] argue that terminology should be organized by subject and examined for its logical, holistic, part-whole, and associative relationships. Sager et al. [28] define terminology as items with special reference significance within a particular field, in contrast to common words, which are widely used in everyday language. With ongoing research into terminology extraction methods, the strict separation between terminology and common words is gradually diminishing. The authors in [29,30] indicate the concept of terminology must be combined with the context. Regarding terminology ambiguity in the context of text, an expression is considered a terminology if it has domain-specific semantics, whereas it is classified as a common word if it lacks domain relevance. However, common word and terminology themselves cannot be regarded as completely separated because general language and specificdomain language have overlapping parts. Concurrently, the definition of terminology provided by the International Organization for Standardization (ISO) [31] emphasizes that terminology serves as a verbal designation for a general concept within a specific domain. This underscores the fundamental association between terminology and the domain to which it pertains. Therefore, the general definition [32] for terminology can be summarized as terminology is a linguistic unit that characterizes a specific domain. Since the definition of terminology clearly indicates that terminology has tight connection to domain, this leads to what constitutes a domain. For example, Cole et al. [33] point out that terminology research should be adapted to the subject field. While there exists a prevailing inclination to confine terminology research within the realms of scientific and technical domains, it is imperative to recognize that virtually all professional fields possess the potential to be the focal point of terminology research. The authors in [29] argue that the knowledge domain is a semantically much wider terminology, somewhat regarded as a discipline in its own right. These descriptions provide some concepts about the domain but do not explicitly define the boundaries of the domain. ISO delineates a domain as a realm of professional knowledge, and the delineation of its boundaries is established with a focus on purpose-related considerations. ISO provides a definition of a domain as a specialized area of professional knowledge. The demarcation of a domain's boundaries is approached from a purpose-oriented perspective according to ISO standards. ConstanzeAnna et al. [32] implicitly define the content of a domain by conducting topic-relevant web crawls or utilizing existing specialized corpora.",
          "Terminology represents the related concepts of a domain, and terminology constructs the skeleton structure of the domain knowledge. The relationship with the domain is the main feature of distinguishing domain-specific terminology from a common word, but identifying terminology is not always simple. Whether it is manual annotation or automatic extraction via a model, there exist some expressions which cannot stably conform to the definition of terminology in the domain. For example, \"Graphics Processing Unit (GPU)\" is highly relevant to the field of computing and can be directly considered a computer terminology. However, common words like \"folder\" have some association with the computing domain, but it's uncertain if they can be classified as computer terminologies. As previously mentioned, terminologies cannot be considered entirely independent of general vocabulary. Kageura et al. [34] explored the relationship between terminologies and general vocabulary, as illustrated in Fig. 1. At the intersection of language and specialized domains, the phenomenon of overlap between terminologies and common words has been a notable issue. To better address the ambiguity of terminologies, researchers often categorize terminologies into different hierarchies, ensuring precise definitions. Hoffmann et al. [35] introduce a classification of terminology into three distinct categories: discipline-specific terminology, non-discipline-specific terminology, and common word. Common word is not considered as terminology, while nondiscipline-specific terminology is shared across multiple fields. Roelcke et al. [36] introduce a more detailed framework, establishing a nuanced model that discriminates among four tiers, as illustrated in Fig. 2. Intra-subject terminology is domain-specific, tailored to a particular field, while inter-subject terminology finds application across multiple domains. Extra-subject terminology, on the other hand, Fig. 1 The relation between terminology and general vocabulary [34] Fig. 2 Tiers of terminology [36] Translated by Constanze Anna [32] lacks domain affiliation but finds utility within specific fields. Lastly, non-subject terminology encompasses all terminological items employed universally across various specialized domains. Rigouts Terryn et al. [37] introduce a two-dimensional space with two distinct attributes (lexicon-specificity and domain-specificity) that categorizes terminologies into three classes: Specific Terms, Outof -Domain Terms , and Common Terms, as illustrated in Fig. 3. Here, lexicon-specificity measures the degree to which a terminology is a specialized vocabulary, and domain-specificity indicates its relevance to a specific domain.",
          "In this section, we present a generic system architecture that summarizes the common characteristics of terminology extraction methods. Terminology extraction approaches [38][39][40][41] initiate their discussions with a preprocessing phase, mirroring a prevalent trend in contemporary applied research across various NLP tasks [21,[42][43][44]. Nakagawa et al. [45] suggest that terminology recognition algorithms [32,46,47] typically employ two steps: candidate terminology extraction, candidate terminology scoring and ranking. Q. Zadeh et al. [48] describe the Fig. 3 Term model by [37] Fig. 4 General architecture of terminology extraction approaches [48] typical terminology extraction process. Building upon Q. Zadeh's work, we summarize a preprocessing step before terminology extraction, as depicted in Fig. 4. First, the pipeline performs preprocessing tasks, such as cleaning and tokenization on the raw texts. Subsequently, linguistic filters, such as the filter using Part-of-Speech (PoS) [49], are applied to extract candidate terminologies from the texts. Then unithood 2 and termhood 3 are evaluated on the candidate terminologies to generate a sorted list.",
          "The first step in terminology extraction is preprocessing the raw texts, as shown in Fig. 5. During the preprocessing stage, the most common tasks include: • Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems. • PoS tagging: Assigning a PoS tag to each word or token in the texts to indicate the word's grammatical role and lexical category within the sentence. This task is not mandatory in preprocessing. At present, there is a wide range of preprocessing tools available, catering to both English texts, such as TweetNLP, 4 Stanford CoreNLP, 5 and NLTK, 6 and Chinese texts, including HanLP, 7 jieba, 8 and THULAC. 9",
          "The generation of a candidate terminology set is a fundamental and crucial step in ATE methods because the quality of candidate terminologies directly influences the final performance of terminology extraction. After preprocessing the initial texts, various techniques can be employed to extract candidate terminologies and generate a set of candidate terminologies, including PoS tagging, specific markers, and statistical-based heuristic rules. Currently, PoS tagging is one of the most widely used methods to solve the problem about candidate terminology extraction.",
          "2 Unithood: the degree of a string to occur as a word or a phrase. 3 Termhood: the degree of a word or a phrase to occur as a domain specific terminologyrepresenting certain concepts in that domain. 4 http:// www. cs. cmu. edu/ ~ark/ Tweet NLP/. 5 https:// stanf ordnlp. github. io/ CoreN LP/. 6 https:// www. nltk. org/. 7 https:// www. hanlp. com/. 8 https:// github. com/ fxsjy/ jieba. 9 http:// thulac. thunlp. org/. By tagging words with their respective PoS and applying shallow parsing based on predefined syntactic patterns [50][51][52], such as noun phrases or verb phrases, multiword expressions. On the other hand, specific markers within the texts can also be relied upon for candidate terminology extraction. These markers (bold, italics), help determine boundaries of terminologies, and candidate terminologies are extracted based on boundary markers, excluding texts sequences that do not contain terminologies (phrases containing verbs and pronouns). Finally, statistical methods can also be employed for candidate terminology extraction using heuristic rules. These heuristic rules, such as n-gram filtering [53], are used to retain high-frequency n-word sequences (typically 1 n 6 ). This approach can generate a large number of candidate terminologies, necessitating additional filtering steps, such as stop-word filtering or checking for the presence of special symbols.",
          "After candidate terminology extraction, a scoring procedure is employed to measure the likelihood that a candidate terminology is one we can extract, this scoring procedure usually combines termhood and unithood scores. Different terminology extraction methods utilize different terminology ranking algorithms. Terminology candidate ranking algorithms constitute the most intricate step in ATE methods [54,55]. Therefore, section 4 provides a comprehensive summary of ATE methods, categorizing them into seven distinct categories. Section 4 also provides detailed explanations of candidate terminology scoring and ranking methods. In this paper, most of ATE methods follow the framework depicted in Fig. 4. However, there are exceptions among ATE methods. ATE approaches, such as sequence tagging-based approaches [56,57], directly generate the final terminology sets but do not perform scoring and ranking within the set. Methods, such as Segphrase [53], AutoPhrase [58] and SemRe-Rank [59], generate a ranked terminology set, and then they re-rank the previously sorted results using the extra information.",
          "In this section, we divide the existing work of terminology extraction into seven categories: Linguistic-based approaches, Statistical-based approaches, Hybrid approaches, Graph-based approaches, Machine learning approaches, Deep learning approaches, LM-based approaches, and Multilingual ATE Approaches (Table 1).",
          "Initially, we can only use Linguistic-based methods for terminology extraction limited by the capability of model, the scale of annotated corpus and the performance of computing resource at that time. These methods primarily leverage linguistic knowledge (lexical patterns, morphological features, semantic information). The core idea in these methods is that terminology often appear within superficial language structures and patterns. We can extract terminologies by formulating a well-designed set of linguistic rules which can summarize the commonly-used terminology patterns. Early linguisticsbased methods mainly relied on linguistic knowledge. They identify words or phrases with specific PoS tagging patterns as potential terminologies. These methods employ rule templates, which are crafted manually by the linguistic experts, to identify domainspecific terminologies from candidate terminologies. For example, systems such as LEXTER [60], FASTR [61], MeSH [62], DP [63], TF-RBM [64] and HRB-ATA [10] define hundreds of rules, including a large number of heuristic rules, to select candidate terminologies. Moreover, Chatterjee et al. [65] employ domain-specific patterns, expertly crafted and encoded as regular expressions, to identify initial terminologies. In a similar vein, Cram et al. [9] choose candidate terminologies through the application of superficial grammars expressed as a collection of regular expressions pertaining to PoS tagging. And Marciniak et al. [66] also use shallow grammars with the additional specification of morphological values dependencies to select correct terminologies. Venugopalan et al. [14] guide the input of Latent Dirichlet Allocation (LDA) [67] model for terminology extraction with regular expression-based linguistic rules. The linguistic-based methods mentioned above require manual rule construction, resulting their poor generalization. To address this issue, scholars have proposed a model for the automatic acquisition of domain-specific linguistic rules and the extraction of PoS rules for terminologies from corpora using this model. For example, Foo et al. [68] use the supervised machine learning algorithm, Repeated Incremental Pruning to Produce Error Reduction (RIPPER) Algorithm [69], to produce rules. Liu et al. [70] introduce an automated technique for the selection and learning of rule sets, with the objective of refining the syntactical rulebased method for extracting aspects in opinion mining. In addition, Shao et al. [71] introduce an unsupervised method that relies on sentence patterns and PoS sequences. This method does not necessitate learning from annotated data but merely initializes a few patterns to extract terminology extraction. In general, linguistics-based terminology extraction methods initially relied on manually crafted rules. Over time, scholars have Table 2 Statistical-based approaches Category Method name Calculation formula Unithood PMI PMI = log p(w1w2) p(w1)p(w2) ≈ log N•f (w1w2) f (w1)f (w2) , a represents a terminology consisting of |a| words, w 1 and w 2 are the words that make up terminology a, w 1 w 2 represents a bigram candidate term, p represents probability, and p(w 1 w 2 ) represents the probability of words w 1 and w 2 occurring together. 12 represent the frequencies of w 1 , w 2 , w 1 w 2 appearing, N represents the total number of words. , O i represents the observed TF of a certain category i in the candidate terminology a, and E i represents the expected TF calculated based on the corpus. Dice Termhood TF proposed methods for automatically acquiring rules, thus improving the generalization performance. These methods hold potential for terminology extraction in various domains. However, linguistics-based terminology extraction methods depend on expert knowledge and POS taggers, and manually written rules, which cannot cover all domainspecific features. As a result, there is limited research utilizing pure linguistic methods for ATE. Instead, these methods are often used as a preprocessing step for generating sets of terminology candidates in terminology extraction.",
          "Statistical-based terminology extraction approaches assume that candidate terminology with higher frequencies are more likely to be actual terminologies. These approaches use the target corpus's distribution frequency of words/phrases to extract terminologies. The advantage of statistical approaches is that it does not depend on the language of the dataset. Statistical approaches usually attribute the terminology features to two convenient principles [12], i.e., unithood and termhood. These Statistical approaches leverage a range of techniques, including term frequency (TF), term frequencyinverse document frequency (TF-IDF), mutual information, T-Score, cosine similarity, information gain, and so on. All these techniques are shown in Table 2. Unithood is used to measure the collocation strength and adhesion degree of multi-word candidate terminologies, commonly used unithood methods include Pointwise mutual information (PMI) [72], log-likelihood ratio (LLR) [73,74], z-score [75], T-score [72] and Chisquared ( χ 2 ) [76,77]. Other many studies [53,58,[78][79][80][81]] also use unithood to measure candidate terminology. Although unithood plays an indispensable role in terminology extraction, the research of [82] and [83] show that unithood is not enough to evaluate the validity of candidate terminology and it can only determine that the word sequence is a fixed collocation. Termhood measures the relevance between candidate terminologies and domains according to the statistical features of candidate terminologies from the target corpus, such as TF and TF-IDF. TF represents the frequency of a word appearing in a text, typically used to assess the importance of a word within the text. Inverse Document Frequency (IDF) [11] measures the reciprocal of the proportion of documents within the corpus in which a candidate terminology is present relative to the total number of documents. TF-IDF extends TF by combining the importance of a word within an individual text and its importance within the entire collection of texts. TF-IDF is one of the most effective methods in domain-specific measurements. There are also some other terminology extraction methods based on termhood are proposed, such as average term frequency (ATF) [84], domain consensus (DC) [85], and ResidualIDF (RIDF) [78]. In addition, the YAKE! [86] is not constrained by the source or collection of texts. It automatically identifies and extracts the most important Keyphrases from the text by considering factors such as TF, position, and relevance. This allows it to be effective across various text collections and domains. Statistical terminology extraction methods identify terminologies by analyzing probabilistic statistics, such as TF and document frequency, within text data. This approach is applicable to text data from various domains and does not require linguistic rules or annotated data. However, it heavily relies on the scale and quality of the text data and lacks a deep understanding of the underlying semantic information of terminologies.",
          "At the early stage of terminology extraction, the hybrid approaches mostly combine linguistic methods and statistical methods, among which C-value and NC-value are the early and representative ones. C-value [50] combines linguistic and statistical methods to generate a set of candidate terminologies using language rules and then filters them using statistical information. NC-value [50] builds upon C-value and fully leverages rich contextual information of phrases. The NC-value method begins by computing the C-value scores and ranking the terminology candidates in a corpus. It then generates a context information list for each candidate terminology, where each contextual word is assigned a weight. Finally, the NC-value value is calculated based on the C-value score of the candidate terminology and the context information list. Basic [87] modifies the C-value method by expanding nested terminologies. It considers that the number of nested terminologies in long word strings should also be a part of the termhood of candidate terminologies. The Basic model emphasizes extracting frequent long word sequences but overlooks the issue of domain-specific candidate terminologies. To address this concern, the ComboBasic model [88] introduces a customized functionality for terminology scope based on the Basic model. Yao et al. [89] propose a concept extraction approach that combines linguistic analysis, statistical analysis, and semantic analysis. This method retains the language and statistical analysis components of hybrid approaches while introducing semantic analysis to capture interrelated concepts. Furthermore, some works combine statistics features and rules to extract domain terminology [90,91]. Yan et al. [91] propose an algorithm based on rules and statistics to extract water environment terminology. They use n-gram to segment the preprocessed texts, use related rules to improve mutual information and adjacency entropy to filter candidate terminology, and then use TF-IDF to select terminology about the water environment. Hybrid methods from this group use terminology extraction methods as features and attempt to learn the importance of each feature in an unsupervised or supervised setting. Hammi et al. [92] introduce a hybrid approach for aspect terminology extraction that combines linguistic-based and deep learning methods. This approach leverages a set of rules to extract a highrecall list of aspect terminologies. Additionally, it employs deep learning techniques (word embeddings) to create another list of aspect terminologies enriched with semantic information. Finally, the extraction process combines these two lists of aspect terminologies. Hybrid terminology extraction methods combine linguistic analysis, statistical analysis, and other techniques, leveraging the strengths of each to improve the precision and recall of terminology extraction [93]. Their advantages lie in enhancing domain independence and language neutrality through feature combinations, making them adaptable to various application scenarios, and enabling the extraction of domain-specific terminologies to overcome the limitations of traditional methods. However, the drawbacks of hybrid methods are also apparent. Many methods rely on linear combinations of voting or heuristic algorithms, which fail to fully account for the nonlinear relationships between features, leading to insufficient depth in feature combinations and affecting the extraction results [51]. Moreover, hybrid methods face limitations in adapting to different domains and languages, and may struggle with complex tasks.",
          "Graph-based approaches are inspired by PageRank [94]. These approaches convert documents into graphs, where nodes represent words, and connections between nodes signify the co-occurrence of those words. Graph-based approaches can fuse more vertex features. Table 3 summarizes graph-based terminology extraction methods. In 2004, Mihalcea et al. [22] first propose the KPE algorithm Textrank based on PageRank.",
          "",
          ",where σ represents the damping factor, typically set to 0.85, In(v i ) is the set of vertices that point to it, Out(v i ) is the set of vertices that vertex v i point to. w jk , w ij is either 1 or 0, with 1 indicating that v i and v j co-occur, and 0 otherwise. SingleRank, ExpandRank Using PageRank algorithm, but is the co-occurrence count of words v i and v j , sim doc (d 0 , d p ) is the similarity between documents d 0 and d p . PositionRank Improving PageRank using a non-uniform distribution prior, represents the position weight of vertex v i with respect to other vertices.",
          "Improving PageRank using a non-uniform distribution prior, but p(v i ) is a topic distribution inferred by LDA.",
          "Using TextRank Algorithm, but , where P(v i ) is the set of the word offset positions of candidate v i .",
          "Using TextRank Algorithm, but is the candidates set belonging to the same topic as v j and α is a hyperparameter.",
          "Using TextRank Algorithm, but the weight is a embedding weight w(i, j) vi , e vj are the term embeddings for v i , v j .",
          "Using personalised PageRank algorithm Pr = σ MPr + (1 -σ )v, M is a transition matrix, Pr is a vector where each element represents the score assigned to the corresponding node.",
          "Using TextRank Algorithm, but the weight is determined using either pre-trained vector similarity or WordNet-based concept similarity.",
          "Improving PageRank using a non-uniform distribution prior, but",
          "Improving PageRank using a non-uniform distribution prior, but where w vi is the weight of v i as we defined before, and deg(v i ) is the degree of v i . TextRank constructs a semantic text graph based on the co-occurrence frequency of words and calculates the importance score of each word after iteration. SingleRank [95] extends TextRank by introducing weights to the edges between nodes. On top of SingleRank, ExpandRank [95] incorporates TF-IDF information from neighboring documents. Ushio et al. [96] introduce TFIDFRank and LexRank, each of which extends SingleRank by computing word distributions using TF-IDF and lexical specificity, respectively. PositionRank [97] integrate the position information of words into a biased PageRank to score and rank keyphrases. TopicRank [98] employs hierarchical agglomerative clustering to group candidate phrases into distinct topics. As an extension of TopicRank, MultipartiteRank [23] improves the selection of candidate terminologies within a cluster by representing candidate terminologies and topics in a multipartite graph, thereby enhancing the ranking of candidate terminologies. For the first time, TopicalPageRank [16,99] integrates topic information into the formula for PageRank computation. TPR utilizes the inferred latent topic distribution from a LDA model to perform PageRank on noun phrases extracted from the document, resulting in the final set of Keyphrases. To use the semantic relevance and contextual information between words, Khan et al. [100] propose the Term Ranker based on TextRank, which learns the semantic representation of candidate terminologies by embedding. This semantic representation is used to capture the similarity and relationship strength between terminologies. Then edges with weight are added between nodes to construct an undirected weighted graph. In addition, the Term Ranker method also integrates the synonym terminologies glossary, which combines the nodes representing synonymous terminologies. It increases the number of central nodes in the semantic graph, thus improving the score of low-frequency terminologies. Pan et al. [101] propose a new graph propagation algorithm to address the issue of low-frequency terminologies in the MOOC domain. This algorithm ranks candidate terminologies based on the learned semantic similarity between vocabulary, the quantity, and quality of voting terminologies. The authors in [59,102] propose a general SemRe-Rank method to enhance the existing terminology extraction methods. First, SemRe-Rank extracts a set of candidate terminologies and scores them using existing ATE methods. Then, it constructs a graph for each document, embedding words into a personalized PageRank and iterates through the graph until convergence. This process aims to calculate revised importance scores for each candidate terminology, achieving the goal of re-ranking. Zhang et al. [103] introduce TextRankembedding and TextRank-wpath, which combine semantic similarity with TextRank for terminology ranking. However, TextRank-embedding uses cosine similarity based on word embedding, while TextRank-wpath utilizes WordNet-based concept similarity. WikiRank [104] is an unsupervised extraction method based on Wikipedia background knowledge. It constructs a semantic graph based on semantics and obtains the optimal set of key phrases by solving optimization problems on this graph. Graph-based terminology extraction methods construct a graph from the text to model semantic relationships between terminologies. This approach can incorporate various features, such as TF, position, topics, to calculate the weights of words. In comparison to statistical methods primarily based on TF, graph-based methods can handle low-frequency but significant terminologies well, while avoiding the expensive cost of manual annotation. However, the performance of this method is influenced by the size of the graph and the density of edges, so achieving fast and efficient propagation of the graph remains a challenge to be addressed.",
          "Machine learning-based terminology extraction methods are powerful techniques for automatically identifying and extracting domain-specific or topic-specific terminologies from texts. These machine learning-based extraction methods typically start with annotating a dataset and then converting training instances into a feature space. The feature space integrates various natural language features to improve the accuracy of terminology extraction. Traditional machine learning algorithms, such as Conditional Random Field (CRF) [105], CRF++ [106], Support Vector Machine (SVM) [107], Decision Tree [108], and Naive Bayes, fundamentally focus on enriching the features of input data for terminology extraction. These features can be linguistic-based features (POS patterns, the presence of special characters), statistical-based features, or a combination of both. In 1999, Frank et al. [109] propose the KPE method, which utilizes a Naive Bayes model to classify candidate terminologies. The features used in this method include the TF-IDF of words and their positional information. In 2000, Turney et al. [110] compare the performance of genetic algorithms and decision trees in the task of KPE. Ercan et al. [111] employ a Bagged decision tree to extract keyphrases by combining scores derived from WordNet with the positional information of words. Zheng et al. [112] employ a CRF model for domain-specific terminology extraction. The model used only six features (POS tags, TF-IDF, semantic information, mutual information, left entropy, and right entropy). Doan et al. [113] employ an SVM model to build a binary classifier on annotated training samples for recognizing drug entities. Haddoud et al. [114] employ the logistic regression algorithm to tackle the KPE task, utilizing features such as word length and TF. Shirakawa et al. [115] introduce an extended naive Bayes model for extracting key terminologies from texts, enabling the classification of noisy short texts. Liu et al. [53] address the limitation that the original TF cannot accurately assess the true quality of terminologies. They introduce the SegPhrase method, which combines the concept of phrase segmentation with machine learningbased terminology extraction methods, resulting in promising extraction outcomes. Bay et al. [19] propose approach of machine learning using word embeddings with shallow linguistic information. They combine the established statistical extraction method TF-IDF with Bay's new approach. They incrementally add candidate terminologies using the Word2Vec model [116] and exploit semantic features from a corpus based on domain-specific seed vocabulary. In addition to traditional machine learning methods, hybrid algorithms combine statistical machine learning and deep learning techniques, such as LSTM-CRF [117], BiLSTM-CRF [118], BiLSTM-CNNs-CRF [119], and BERT-BILSTM-CRF [120]. These methods will be discussed in detail in Sect. 4.6. Machine learning-based terminology extraction methods offer advantages in accuracy and flexibility. They effectively integrate various features [112,114], including linguistic features (POS patterns and special characters) and statistical features (TF-IDF and mutual information), improving extraction precision. These methods are widely applicable and stable across many tasks, especially in domain-specific data, where task optimization can lead to better results. However, they have limitations. These methods depend heavily on feature engineering, requiring significant manual effort and domain knowledge, making them time-consuming and costly. They also rely on high-quality annotated datasets, limiting their use in unsupervised or weakly supervised scenarios. Additionally, simply combining statistical and linguistic features often fails to capture the deep semantic information of terminologies [121], resulting in suboptimal performance in complex tasks.",
          "Deep learning methods offer a variety of solutions for NLP tasks and have garnered significant attention in the field of terminology extraction. These methods typically achieve accuracy levels close to expert performance and are widely employed for terminology extraction. Within the realm of deep learning methods, we categorize them into traditional deep learning approaches and embedding-based methods to comprehensively explore their applications and effectiveness. The early deep learning approaches regard terminology extraction as a classification problem and need to extract and select candidate terminology firstly. Caragea et al. [122] propose CeKE, a binary classification model designed to categorize candidate phrases into keyphrases and non-keyphrases. In 2016, Wang et al. [123] propose a weakly-supervised joint model with Long Short-Term Memory (LSTM) [124] classifier and CNN classifier to learn different representations of candidate terminology without manually selecting features. Inspired by Wang's research, Khosla et al. [125] employ a joint training model, which introduces character-level n-gram embeddings with CNN classifier, to extract terminology. Amjadian et al. [126] initially employ local-global embeddings and a classifier to filter terminologies. Subsequently, the classifier is used in reverse to determine whether the candidate terminologies are genuine terminologies, thereby significantly improving the results of terminologies extraction. Gao et al. [20] propose an end-to-end deep learning model, which extracts nested terminology by learning the vector representation of candidate terminology. Moreover, the majority of research based on deep learning transforms terminology extraction into a sequence labeling problem [57]. In this approach, character vectors, word vectors, PoS features, entity features, and others are assigned corresponding tags. The general steps are illustrated in Fig. 6. In [127], an LSTM-CRF model, which combines word embedding features with character-level information to generate the final word representation, is used. In 2018, Zhao et al. [56] propose the BiLSTM-CRF model, which regards terminology extraction as a sequence tagging task. This model extracts the word vector features, PoS features and entity features of each word as input and then uses CRF to map the word to one of B, I, O, E, S tags after bidirectional multilayer hiding layer. In [119], the BiLSTM-CNNs-CRF model is introduced. Building upon the BiLSTM-CRF model, it incorporates a convolutional neural network (CNN) [128,129] layer to extract local features of the current word. Kucza et al. [57] use the B, I, O, L, U tagging scheme to identify terminology by sequence tagging. At the same time, various Recurrent Neural Networks (RNN) and word embedding methods have been used for terminology extraction. Zheng et al. [120] integrate the BERT [130] pretrained model with the BiLSTM-CRF architecture for terminology extraction, called BERT-BILSTM-CRF. BERT is used to capture context-sensitive word embedding as the input of the next component, and then BiLSTM-CRF is employed to obtain sentence semantic features for identifying and extracting domain terminology. Tran et al. [131] investigate the performance of the XLMRoBERTa multilingual Transformer language model [132] in monolingual and cross-domain sequence tagging terminology extraction tasks. Fusco et al. [133] employ pseudo labels generated by a fully unsupervised annotator to fine-tune transformer-based models, thus creating an efficient transformerbased sequence tagging model for terminology extraction. With the advancement of embedding techniques, distributed vector representations of words trained by neural networks have become a major trend, and they have demonstrated significant effectiveness in the field of terminology extraction [134] (Table 4). Wang et al. [135] have confirmed the significant impact of word embedding in the field of KPE. Key2Vec [136] utilizes FastText [137] for the training of phrase embeddings and subsequently ranks candidate phrases employing a personalized PageRank algorithm with weighting. Drawing on PoS tagging, Bennani-Smires et al. [138] extract candidate phrases within the document. Subsequently, it computes the cosine similarity between the embeddings of these candidate phrases and document embeddings for ranking candidate phrases based on their similarity, and ultimately obtains the keyphrases. SIFrank utilizes PoS tags to extract noun phrases (NPs) as candidate phrases. It combines the",
          "",
          "[136] Key2Vec Generate phrase and document embeddings utilizing FastText, followed by the application of the PageRank algorithm to identify keyphrase candidates from the pool of candidate keyphrases. [138] EmbedRank Calculate the cosine similarity between phrases embeddings and document embeddings for ranking. [144] SIFRank Enhancing the static embeddings within EmbedRank involves the utilization of a pretrained language models (ELMo), and a sentence embedding model (SIF). [142] AttentionRank Calculate self-attention and cross-attention using pre-trained language models (PLMs). [143] MDERank By employing a masking strategy, the ranking of candidate documents is determined by assessing the similarity between the embeddings of the source document and the masked document. sentence embedding model SIF [139] and the autoregressive pre-trained language model ELMo [140] to obtain semantic vectors for both sentences and candidate phrases. Finally, candidate phrases are ranked by computing the similarity between the sentences and candidate phrase vectors. KeyBERT [141] introduces a method for extracting keyphrases using BERT. Initially, it employs BERT to extract document embeddings for obtaining document-level vector representations. Next, it extracts word/phrase vectors for N-grams, and finally identifies words/phrases that match the documents with cosine similarity. AttentionRank [142] also utilizes BERT for keyphrases extraction. They employ BERT to calculate the self-attention and cross-attention of candidate phrases and subsequently rank them based on their importance. MDERank [143] is the latest embedded-based keyword extraction algorithm. This research underscores the significance of keyphrases in the semantic content of documents, highlighting that a lack of keyphrases in a document often lead to a significant alteration in its semantics. To address the issue of mismatch between phrases and document length in SIFRank, this algorithm evaluates the importance of candidate keyphrases by comparing the similarity between the original document and a document with masked candidate keyphrases. A lower similarity indicates a higher importance of the candidate keyphrase. In the field of terminology extraction, embedding models play a crucial role in transforming textual information into continuous vector representations. Traditional embedding models typically include word embedding, phrase embedding, and sentence embedding. Word Embedding models like Word2Vec and GloVe [145] aim to map words into vector spaces, capturing semantic relationships between words. Phrase Embedding models attempt to represent phrases as vectors, as seen in Skip-Thought Vectors [146]. Sentence Embedding models focus on representations at the entire sentence level, examples of which include InferSent [147] and Universal Sentence Encoder [148]. However, these traditional embedding models are often unsupervised and struggle to adapt automatically to different tasks and domains. Additionally, they perform less effectively when dealing with complex syntactic and semantic structures due to their limited context awareness and shallow language understanding. In recent years, pre-trained embedding models such as ELMo, BERT, RoBERTa [149], and XLNET [150] have emerged and achieved tremendous success. ELMo employs bidirectional language models to produce context-sensitive embeddings for individual words. BERT has achieved state-of-the-art (SOTA) performance across multiple NLP tasks through pre-training, taking full advantage of contextual information. RoBERTa further enhances performance through large-scale data and improved training strategies. XLNET introduces auto-regressive properties, better capturing global dependencies in text. To address the issues of high memory usage and prolonged training time with BERT, Kumar et al. [151] introduce the Hierarchical Self-Attention Network (HSAN) for aspect terminology extraction. HSAN combines the importance of words within sentences with the dependencies between words in a sentence to effectively predict aspect terminologies within the sentence. Jerdhaf et al. [152] employ BERT to extract specialized terminology associated with the semantic domain indicating or suggesting the presence of implants in electronic medical records. Furthermore, Jia et al. [153] propose a bilingual terminology extraction model which is initialized with an e-commerce cross-lingual pretrained vectorized semantic representation. Given the terminology in the source language and the sentences in the target language, this model can distinguish whether the target sentence contains the translation of the source terminology and predict its position in the target sentence. Fan et al. [154] design a neural network architecture aimed at enhancing clinical and biomedical concept extraction while improving the efficacy of word embeddings. This framework utilizes CNN with multi-size filters to extract character-level information and employs cross-attention mechanism to incorporate both global and local information into word embeddings. The authors in [25] introduce a graph-enhanced sequence tagging framework that integrates graph embeddings [155] and PLM for the extraction of keyphrases from lengthy documents. The advantages of these pre-trained embedding models lie in their adaptability to multiple tasks without the need for complex feature engineering and their ability to provide deeper language understanding. Deep learning-based terminology extraction methods are generally accurate, as they automatically learn complex features from text [56,119], reducing the need for manual feature engineering. They perform well with diverse and complex terminologies and are sensitive to contextual information [156], especially when using PLMs like BERT and XLNet. These methods can capture semantic and syntactic relationships, helping to understand the meanings and usage of terminologies. However, they have some limitations. Deep learning models require significant computational resources, which can be challenging in resource-limited environments [157]. They also rely heavily on high-quality annotated data [158], including accurate context and term boundaries. Poor data can reduce model performance. Additionally, while these models are adaptable across domains, their ability to generalize may be limited, especially when textual structures vary significantly between domains.",
          "In recent years, the field of NLP has witnessed a revolutionary transformation, centered around LMs (including PLMs and LLMs). LMs such as GPT-4 [159], GPT-2 [160], LLaMA [161], Vicuna [162], ChatGLM3 [163], RoBERTa [149], have achieved remarkable success in various NLP tasks, providing researchers and practitioners with unprecedented tools and resources. As the advent of LMs is serving as a catalyst for a new phase of innovation and progress, the future prospects of terminology extraction can also be largely influenced. The introduction of LMs has not only shown great theoretical potential but has also demonstrated significant practical impact. Numerous studies [24,25,164] have confirmed the outstanding performance of LMs in terminology extraction tasks. Lange et al. [24] introduce a novel PLM, CLIN-X, for terminology extraction in the clinical domain. This model is designed for both English and Spanish. Initially, they perform pre-training of the CLIN-X language model on clinical documents using mask language modeling (MLM) [165]. Subsequently, they employ four techniques, including subwordbased concept extraction [166] with cross-sentence context [167], BIOSE labels, CRF and model transfer [168], to fine-tune the concept extraction corpus. Tran et al. [169] conduct an evaluation of various monolingual and multilingual transformer models (RoBERTa, XLNet, XLM-RoBERTa) in the domain of cross-domain terminology extraction. This evaluation is carried out using the ACTER dataset 10 (English, French and Dutch) and the RSDo5 dataset 11 (Slovenian). The experimental results demonstrate that, in terminology extraction tasks excluding NER, monolingual models outperform multilingual models in all languages except Dutch. To address the issue of resource scarcity in the e-commerce domain, Jia et al. [153] introduce a new task: discovering bilingual terminologies from comparable data. In order to tackle this task, they propose a bilingual terminology extraction framework. This framework first involves fine-tuning a cross-lingual PLM with a large-scale bilingual e-commerce corpus. Subsequently, using the initial settings of this model, they extract terminologies from target sentences based on deep semantic relationships between the source terminologies and the target sentences. The authors in [25] combine Graph Neural Network (GNN) representations with LLMs to enhance KPE in long texts. Fang et al. [170] introduce a concept model (conceptor) for concept extraction. The concept model employs a prompt-tuning [171] method to fine-tune PLMs (BERT, RoB-ERTa) to encode statements. Subsequently, MLM is utilized to rank candidate concepts and select the concept with the highest scores. Lu et al. [172] introduce a three-stage framework DS-MOCE, for distantly supervised concept extraction. In this framework, it initially employs prompt-based learning to categorize concepts in MOOCs. Then, it utilizes high-precision annotation to eliminate noise. Finally, it employs PLMs with concept distribution and positive unlabeled learning (PUL) to handle noise and incomplete mentions, resulting in the final course concepts. Yuan et al. [173] introduce KPCE, a concept extraction framework with knowledge-guided prompt. KPCE consists of a prompt constructor and a concept extractor. The prompt constructor employs topics obtained from a knowledge graph as a knowledge-guided prompt. These prompts are then used to train a BERT-based extractor, responsible for extracting concepts of various granularities from input texts. With the rapid advancement of LLMs, ChatGPT has emerged. Upon its introduction, ChatGPT demonstrates excellent performance in the field of NLP. Researchers have proposed a series of methods utilizing ChatGPT to assist in terminology extraction. In [174], the NER task is defined as a zero-shot or few-shot text generation task, and a comparison is made between ChatGPT method with prompt engineering and traditional fine-tuning methods in the context of rare disease NER. Overall, traditional fine-tuning models outperform ChatGPT in terminologies of performance, but in a one-shot setting, ChatGPT achieves higher accuracy. Romano et al. [164] introduce a data collection and curation framework for theme-driven KPE. This framework extracts clinically relevant keyphrases from user-generated health texts, and generates the MOUD-Keyphrase dataset. The study also quantitatively analyzes unsupervised KPE models and ChatGPT on this dataset. The findings indicate that ChatGPT demonstrates superior performance compared to unsupervised KPE models. To address the issue of data scarcity in terminology extraction, Veyseh et al. [160] propose a multi-step terminology extraction training method. Initially, they obtain optimal representations and sentences  for training data to augment the input data. Subsequently, fine-tuning is performed on the generative language model GPT-2 [175] to generate additional sentences. Song et al. [176] assess ChatGPT's ability in keyphrase generation on six benchmark datasets. The research results indicate that ChatGPT has significant potential in keyphrase generation tasks but still faces challenges in generating missing keyphrases. Building upon Song's research, the authors in [177] further assess ChatGPT's performance. In Martínez-Cruz's research, the keyphrase generation performance of ChatGPT is compared with SOTA models on six publicly available datasets from scientific articles and news domains. The results indicate that ChatGPT achieved SOTA performance across all test datasets and settings without requiring additional training or fine-tuning, solely through promptbased learning. Recently, the success of ChatGPT has spurred more researchers to explore domain-adaptive LLMs. Current research indicates that with better prompts, LLMs demonstrate exceptional terminology extraction capabilities. However, these LLMs require a substantial amount of data for pre-training and fine-tuning. The quality and quantity of the data directly impact the effectiveness of terminology extraction. Additionally, they demand significant computational resources and time. Some researchers have also investigated ChatGPT's performance in terminology extraction tasks, demonstrating its significant potential in this domain without the need for additional training and fine-tuning. Nevertheless, there is still some gap when compared to traditional PLMs.",
          "In recent years, significant progress has been made in multilingual terminology extraction methods, particularly in improving the accuracy and efficiency of cross-domain ATE. [178] proposed the TExSIS system, which generates term candidates using chunkbased alignment and combines multiple statistical filters to determine term specificity, adopting a multilingual perspective for terminology extraction, and outperforming traditional monolingual re-alignment methods. [41] introduced a unified framework that independently aligns bilingual terminologies for single-word and multi-word terminologies by improving context-based and neural network approaches, with the system adaptable to domain-specific terminology extraction. [179] introduced the TermEnsembler system, which integrates seven bilingual alignment methods using ensemble learning to automatically extract and align terminologies from English and Slovenian texts, achieving an accuracy of over 96%. [180] proposed a bilingual terminology recognition system, LUIZ, based on an \"equivalence bag\" method, which combines morphological and syntactic patterns with statistical ranking to identify cross-lingual multi-word terminologies in domains such as tourism, accounting, and military, using translation equivalent pairs. [181] introduced three Transformer-based multilingual terminology extraction methods, which enhance the performance of cross-domain ATE for English, French, and Dutch by utilizing sentence-level token classification, sequence classification, and neural machine translation models. [182] proposed using cross-lingual and cross-domain transfer learning with fine-tuned BERT models for ATE, showing its effectiveness in extracting single and multi-word terminologies across multiple languages and specialized domains. [183] presented a tool based on existing lexicons and bilingual parallel corpora, utilizing varying match functions to extract and align English-Serbian power engineering domain terminologies, generating bilingual terminology pairs. [131] treated terminology extraction as a sequence labeling task, using the Transformer-based XLM-RoBERTa model to evaluate the performance of multilingual PLMs in cross-domain tasks, achieving a significant improvement in F1 score on the Slovenian dataset. [184] proposed an unsupervised multilingual keyword extraction algorithm that selected keywords based on high TF-IDF ranks in both the target and other languages, improving accuracy by leveraging multilingual information. [185] proposed a multilingual terminology extraction method that improves cross-domain performance through cross-lingual transfer learning and a nested terminology labeling mechanism (NOBI), significantly boosting recall and F1 scores, especially in the extraction of short nested terminologies. Overall, these advancements demonstrate the growing potential of multilingual models in extracting and aligning terminologies across diverse languages and domains. The continuous improvement in both accuracy and efficiency underscores the increasing applicability of these methods in real-world, cross-lingual contexts.",
          "In the field of terminology extraction, the selection of datasets, tools, and evaluation metrics is of paramount importance. These elements form the foundation of research methodology and outcomes, providing a means to assess the efficacy of terminology extraction algorithms and driving continuous progress and innovation in the field. In this section, we delve into the datasets, tools and evaluation metrics related to terminology extraction, aiming to assist researchers in better understanding, selecting and utilizing them to advance the research in terminology extraction.",
          "This section introduces some commonly used terminology extraction datasets, which provide researchers with rich experimental materials, promoting the development and innovation of terminology extraction techniques. The selection and design of these datasets aim to cover the diversity of different domains and languages to meet various research requirements. In the following section, we present several representative terminology extraction datasets, summarizing their domains, languages, data sources, and data characteristics, as shown in Table 5.",
          "This subsection briefly introduces and compare the software and tools related to terminology extraction and Table 6 summarizes these terminology extraction tools. • TermoUDfoot_1  [211] is a language-independent terminology extraction tool that utilizes language-dependent shallow syntactic patterns to select candidate terminologies. This tool is applicable to languages with a Universal Dependencies (UD) parser. • TermoPL 12 [212,213] is a terminology extraction tool from domain corpora in Polish. It applies C-value to rank candidate terminologies as the longest identified nominal phrases or their nested sub-phrases. It can also compare two candidate terminology lists using three different coefficients showing asymmetry of terminology occurrences in this data. • D -Terminer 13 [214] is an open-access online demonstration for ATE from parallel corpora for both monolingual and multilingual purposes. Monolingual terminology extraction is based on recursive neural networks and employs a supervised approach with dependency on pre-trained embeddings. Table 5 Datasets Datasets Studies Language Domains Docs Words(K) Terms GENIA [186] English Biomedicine 193 435 93,293 FAO [187] English Agriculture 780 26,672 1554 Krapivin [188] English Informology 2304 21,189 8766 ACL [48] English Computer Science 10,922 41,202 21,543 ACL2.0 [189] English Computer Science 300 33 6818 TTC [190] Multilingual Wind Energy 103 801 287 Mobile Technology 37 305 254 Europarl [191] Multilingual Eurovoc Thesaurus 9672 63,279 15,094 SemEval [192] English Computer Science 244 2033 4002 Theses100 -English Miscellaneous 100 472 767 Wiki20 [193] English Miscellaneous 20 122 730 ACTER [194] Multilingual Corruption 44 469 6385 Dressage 89 103 10,889 Heart failure 190 46 14,011 Wind Energy 38 315 9478 CRAFT [195] English Biomedicine 67 560 10,000 Hindi Wiki [196] Hindi Education 71 12 953 Irish Wiki [197] Irish Education 11 5 864 Mathematical [198] Chinese Math 3000 31 14,500 WellXplain [199] English Math 3092 72 14,500 Hospital-EMRs [200] Swedish Medicine 48,000 71,220 48,088 JSynCC [19] German Medicine 1006 368 2583 RSDO5 [201] Slovene Miscellaneous 12 257 37,985 LDKP3K [202] English Scientific 100,000 602,710 76,110 KP20k [203] English Scientific 568,000 106,863 2,780,316 NUS [204] English Scientific 211 1605 2834 DUC-2001 [95] English News 308 227 6160 Inspec [25] English Scientific 2000 27 19,275 OpenKP [205] English Scientific 147,200 132,538 264,906 COBEC [206] English DM 20 310 477 DB 27 296 395 OS 1 4 42 IIR [207] English computer science 16 -3175 ScienceIE [208] English Computer Science, Physics, Material Science 500 -5730 BitterCorpus [209] Multilingual Information Technology 56 -874 KAS-biterm [210] Multilingual Academic writing 816 618 15,929 13 https:// lt3. ugent. be/ dterm iner/. • TEST [215] is used to automatically detect the existence of new technologies and tools in texts, and extract terminologies used to describe these new technologies. • YAKE! 14 [86] is a lightweight, multilingual, unsupervised automatic KPE method. It selects the most important keyphrases in a text based on textual statistical features extracted from a single document. • KeyBERT 15 [141] presents a toolkit designed for KPE with BERT, aligning with the paradigm of Phrase-Document (PD) based methods. • ATR 4S 16  [54], implemented in Scala, is an open-source terminology recognition toolkit. It incorporates fifteen terminology extraction algorithms, including CValue, Basic, Com-boBasic, and more. • JATE2.0 17 [216] is a Java ATE toolkit developed within the framework of Apache Solr, and an upgraded version of JATE [84]. At the same time, JATE2.0 is an open-source, highly modular, adaptable and extensible ATE library, containing dozens of implemented ATE algorithms. • TermSuite 18 [9] is an ATE tool based on Java and UIMA framework, which is extensible and capable of managing variations in terminology. This tool mainly uses the Weirdness method to sort candidate terminologies, focusing on syntax and morphological pattern to identify terminology variants.  [220] is a freely available terminology extraction tool that combines linguistic and statistical methods. It takes into account the structure of potential terminologies and their relative frequency of occurrence. • TerMine 24 [50] is an ATE tool based on the C-value, which uses linguistic and statistical information. Its primary disadvantage is that it can only extract multi-word terminology, but many critical single-word terminologies are in the domain. ",
          "Require annotated data. and R(i) respectively represent the Precision(p) and Recall(R) for the extracted i-th terminology. 21 http:// www. cs. cf. ac. uk/ flexi term. 22 https:// gate. ac. uk/ proje cts/ neon. 23 http:// termo stat. ling. umont real. ca. 24 http:// www. nactem. ac. uk/ softw are/ termi ne/. 19 https:// sourc eforge. net/ proje cts/ tbxto ols/. 20 http:// tubo. lirmm. fr/ biotex/. • TOPIA 25 is an ATE package based on python. It offers a terminology extraction approach combining PoS tagging with simple statistical metrics, such as frequency.",
          "Terminology extraction is a fundamental task in NLP and information retrieval, aiming to automatically identify domain-specific terminologies from text. This chapter focuses on three aspects of terminology extraction evaluation: evaluation methods, evaluation metrics, and other key factors. The evaluation methods are further categorized into manual evaluation and automatic evaluation. Evaluation metrics are divided into traditional metrics, and derived metrics from information retrieval. Other key factors include frameworks for evaluating software quality, considering characteristics like functionality, usability, reliability, and efficiency, providing a comprehensive perspective on terminology extraction evaluation. Manual evaluation is a traditional approach in terminology extraction, relying on domain experts to compare extracted terminologies with predefined references and assess system performance. It also allows for analysis of terminology importance, diversity, and relevance. However, due to high labor costs and potential bias, it is less suitable for large-scale assessments. In terminology extraction evaluation, automatic evaluation methods based on standard terminology tables compare the extraction results with predefined terminology sets (such as terminology dictionaries or gold standards) [221] to quickly calculate metrics like precision (P) [180] and recall (R) [222]. This approach relies on automated tools, making it suitable for large-scale task evaluation, particularly in domains where terminology sets are relatively stable. The dictionary-based method directly matches the extracted terminologies with the terminology list in the standard dictionary [178]. While simple to implement, it has limited adaptability to terminology variations. In contrast, the gold standard-based method [223,224] uses diversified terminology lists constructed by domain experts as references, enabling the capture of different expressions of terminologies and providing more fine-grained evaluation results. Although automatic evaluation based on standard terminology tables offers significant advantages in efficiency, its accuracy depends on the quality and coverage of the dictionary or gold standard, which may pose challenges in dynamically evolving domains. To measure the performance of terminology extraction systems quantitatively, various evaluation metrics are employed. Traditional metrics such as precision (P) and recall (R) are foundational in assessing the correctness of extracted terminologies. Precision (P) measures the proportion of correctly identified terminologies among all extracted terminologies, while recall (R) evaluates the proportion of correct terminologies successfully extracted from the total relevant set. The F1 score (F1), as the harmonic mean of precision and recall, provides a balanced measure of system performance. Additionally, derived metrics from information retrieval, such as Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Average Precision (AvP), offer a more nuanced assessment of ranking and retrieval quality. Mean Average Precision (MAP) [225,226] 25 https:// pypi. python. org/ pypi/ topia. terme xtract. evaluates the overall ranking quality of the extraction results by calculating the average precision over multiple queries. This metric is valuable for assessing how well the system ranks relevant terminologies among all extracted terminologies. Mean Reciprocal Rank (MRR) [96,97,227] measures the rank of the first relevant terminology in the extraction results, providing a clear indicator of how quickly the model retrieves relevant information. Average Precision (AvP) [51,82,228] computes the precision for each relevant terminology in the extraction and averages the values, offering an overall measure of the system's effectiveness in ranking and retrieving relevant terminologies from the set of extracted terminologies. Descriptions and formulas for these metrics are shown in Table 7. Existing research has proposed several key factors beyond simple performance metrics, constructing a more comprehensive framework for evaluating terminology extraction software. [229] introduced a standardized method for evaluating terminology extraction software, focusing on four characteristics: functionality, usability, reliability, and efficiency, which are further subdivided into seven sub-characteristics, such as accuracy and learnability, providing users with quantifiable software quality references. [230] designed a framework focusing on external quality, selecting three characteristicsfunctionality, usability, and efficiency-and subdividing them into four sub-characteristics, such as precision and operability. Through empirical research, they compared the quality characteristics of multiple tools, offering new insights into the evaluation framework's practical application and result interpretation.",
          "",
          "One of the major challenges currently faced in the field of terminology extraction is how to handle polysemy, poor domain adaptability, and low computational efficiency [231]. Traditional terminology extraction methods often struggle with polysemy, failing to distinguish between different meanings of the same term in various contexts. In addition, existing models typically perform well only within specific domains, with clear limitations when it comes to extracting terminology across domains, lacking sufficient generalization capability. On the other hand, while deep learning methods have improved the accuracy of terminology recognition to some extent, they are computationally inefficient due to the large amount of computational resources required for training and inference, especially during pre-training and fine-tuning [25,130,131,[232][233][234][235]. Therefore, how to improve the computational efficiency of terminology extraction methods while addressing polysemy and enhancing domain adaptability is an urgent challenge that remains to be solved.",
          "Current terminology extraction methods often rely heavily on large amounts of labeled data for training, especially in deep learning applications where data quality and quantity directly impact performance [236,237]. This reliance on labeled data creates challenges in domains where obtaining large datasets is difficult due to privacy concerns, complexity, or high labeling costs. In many cases, labeled data is scarce, limiting the effectiveness and applicability of these methods. Additionally, existing models lack flexibility to adapt to new domains or tasks without substantial retraining, leading to poor performance when labeled datasets are limited. Furthermore, these models often require extensive manual labeling and domain expert input, which is time-consuming and resourceintensive. Reducing the dependence on labeled data and improving the adaptability and generalization of terminology extraction methods are essential for their advancement, especially in low-resource and cross-domain applications.",
          "Despite advancements in terminology extraction technology, the lack of unified and effective validation standards remains a critical issue. Current evaluation methods are often focused on specific datasets or metrics tailored to particular domains, limiting their generalizability across different contexts. This lack of cross-domain and crosstask evaluation hinders the comparability of research results and the development of universally applicable methods. Deep learning-based models, which are increasingly used for terminology extraction, are complex, and traditional performance metrics like precision, recall, or F1 score fail to fully capture their capabilities in realworld applications [198,238]. These metrics often overlook factors like contextual understanding, domain-specific nuances, and the model's ability to handle ambiguity [239]. Moreover, existing validation methods don't account for the adaptability and robustness of models in dynamic, real-world environments where data quality and domain shifts are constant challenges. As a result, validation processes often don't reflect the true performance of models in diverse, practical scenarios. Developing comprehensive and effective validation methods to evaluate terminology extraction models across various application scenarios, while considering their adaptability and real-world effectiveness, remains a key challenge.",
          "",
          "Large language models (LLMs) are rapidly advancing in the field of NLP, and many studies have explored their application in terminology extraction tasks [240,241]. These models undergo extensive pre-training on large-scale text data, endowing them with a strong capability to capture rich linguistic and contextual information. This enhancement significantly improves their potential for understanding context and offers promise in addressing the challenges of polysemy, ambiguity, and contextual understanding that traditional terminology extraction methods face. Terminology extraction methods based on LLMs also have the potential to adapt to diverse domain-specific and cross-domain requirements [169,185]. These models have been trained across multiple domains and languages, creating new opportunities for cross-domain terminology extraction and a better response to domain-specific challenges. However, it is essential to note that, despite the promising prospects of terminology extraction based on LLMs, the pre-training and fine-tuning processes require substantial computational resources [242]. Researchers and practitioners should give careful consideration to this aspect to ensure effective resource management. This matter is anticipated to be among the challenges requiring attention in future research endeavors.",
          "The rise of LLMs has sparked widespread interest in the field of prompt engineering [243]. Prompt engineering is a method that leverages natural language prompts to guide models in performing specific tasks. In the domain of terminology extraction, promptbased techniques have become increasingly prominent [244]. This approach involves cleverly designed natural language prompts that enable models to more accurately extract domain-specific terminologies and phrases without requiring significant updates to the model parameters. It fully leverages the potential of LLMs (such as GPT-4, ChatGLM3, etc.) in understanding context. By embedding prompts into the model, it allows the model to gain a deeper understanding of the contextual background of terminologies, leading to more precise terminology recognition and extraction. Furthermore, prompt-based methods offer models the opportunity for pre-training on large-scale unlabeled text. By defining new prompt functions, this approach enables models to engage in few-shot or even zero-shot learning [245], allowing adaptation to new scenarios, particularly in instances where labeled data is limited or unavailable. What is particularly fascinating is that prompt-based methods exhibit exceptional flexibility, catering to various domain-specific and crossdomain needs. Researchers and practitioners can easily customize models to meet specific task requirements by simply adjusting or modifying prompts, making them more versatile and practical. This trend demonstrates the synergy between LLMs and natural language prompts, unlocking significant potential in the terminology extraction field, thereby enhancing the accuracy and applicability of information extraction.",
          "A key direction for future terminology extraction research is improving the evaluation system. Current methods primarily focus on standard metrics like precision and recall, which often don't fully reflect the practical effectiveness of these methods. Future evaluation systems should prioritize multi-dimensional assessments, including scalability, adaptability, domain independence, and language neutrality [246]. Given the performance differences across domains, languages, and datasets, a major challenge will be designing a more universal and easily applicable evaluation framework. Additionally, combining quantitative and qualitative evaluations is crucial. This involves not only automated assessments but also manual review and expert feedback to validate extraction quality. A comprehensive evaluation system will provide more accurate standards for assessing terminology extraction methods, driving further advancements in the field.",
          "Our review provides a detailed exposition of various aspects of the field, including relevant concepts, the interplay of issues, generic system architectures, terminology extraction methods, related resources and metrics, and prospective research topics. We delve into the evolution of different approaches to terminology extraction, encompassing traditional linguistics-based and statistically-based methods, as well as more recent developments such as graph-based algorithms, machine learning, deep learning, and LMs. These methods represent different research directions within the terminology extraction domain. Traditional linguistics-based and statistically-based methods, such as PoS tagging, KPE, and TF analysis, have laid a robust foundation for terminology extraction. However, they exhibit certain limitations in handling polysemy, domain adaption, and semantic relatedness. Machine learning-based approaches rely on feature engineering and classical supervised learning algorithms. They have demonstrated favorable results in terminology extraction tasks, albeit typically requiring substantial manual effort and domain knowledge. Deep learning methods, on the other hand, leverage end-to-end training of neural network models to automatically learn higher-level feature representations but often consume large-scale annotated data. Embedding techniques map text into lower-dimensional vector spaces, encoding terminology semantically and facilitating the capture of semantic relationships among terminologies. Additionally, graphbased methods construct term-concept networks, offering enhanced representations of intricate relationships among terminologies. However, one of the most remarkable developments is the rise of LMs, particularly massive LMs. Models like BERT and GPT-4 have achieved tremendous success in the field of NLP and have profoundly impacted terminology extraction. These models undergo self-training on vast amounts of textual data, endowing them with formidable language comprehension capabilities, enabling them to better understand the context and meanings of terminologies. Furthermore, their powerful representation enhances the accuracy of terminology extraction, especially when dealing with polysemy and domain-specific terminologies, as they excel in capturing linguistic nuances. In the future, as LLMs evolving and adapting to different domains, we can anticipate further improvements in terminology extraction methods. Additionally, researchers can explore how to better integrate various approaches to address the challenges in terminology extraction. Recently, many articles [247][248][249] have proposed methods that combine LLMs with knowledge graphs for knowledge extraction. The integration of LLMs with knowledge graphs can become a new trend in addressing terminology extraction tasks. LLMs can play a pivotal role in future terminology extraction research, providing us with more advanced solutions for terminology extraction, and advancing the fields of information retrieval and knowledge management. NER Named entity recognition NLP Natural language processing NPs Noun phrases PD Phrase-document PMI Pointwise mutual information PLMs Pre-trained language models PoS Part-of-Speech P Precision R Recall RIDF ResidualIDF RIPPER Repeated incremental pruning to produce error reduction RNN Recurrent neural networks SOTA State-of-the-art SVM Support vector machine TF Term frequency TF-IDF Term frequency-inverse document frequency"
        ],
        "ground_truth_definitions": {
          "Lemmatization": {
            "definition": "Transforming various forms of vocabulary into their base forms, which are lemmas or stems",
            "context": "Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems. • PoS tagging: Assigning a PoS tag to each word or token in the texts to indicate the word’s grammatical role and lexical category within the sentence. This task is not mandatory in preprocessing.",
            "type": "implicit"
          },
          "Tokenization": {
            "definition": "Segmenting into the smallest meaningful units, such as words or phrases",
            "context": "• Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems.",
            "type": "implicit"
          },
          "terminology": {
            "definition": "a concept reference system within a specialized field",
            "context": "For example, the authors in propose to separate the concept of the terminology from the common word, marking the beginning of terminology research. Wüster defines terminology as “a concept reference system within a specialized field”. Based on Wüster’s terminology theory, Wright et al. argue that terminology should be organized by subject and examined for its logical, holistic, part-whole, and associative relationships.",
            "type": "explicit"
          },
          "Cleaning": {
            "definition": "Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks",
            "context": "During the preprocessing stage, the most common tasks include: • Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases.",
            "type": "implicit"
          },
          "domain": {
            "definition": "a realm of professional knowledge",
            "context": "These descriptions provide some concepts about the domain but do not explicitly define the boundaries of the domain. ISO delineates a domain as a realm of professional knowledge, and the delineation of its boundaries is established with a focus on purpose-related considerations. ISO provides a definition of a domain as a specialized area of professional knowledge.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "dx.doi.org/https://doi.org/10.1016/j.ipm.2021.102505",
        "sections": [
          "Media has a substantial impact on public perception of events, and, accordingly, the way media presents events can potentially alter the beliefs and views of the public. One of the ways in which bias in news articles can be introduced is by altering word choice. Such a form of bias is very challenging to identify automatically due to the high context-dependence and the lack of a large-scale gold-standard data set. In this paper, we present a prototypical yet robust and diverse data set for media bias research. It consists of 1,700 statements representing various media bias instances and contains labels for media bias identification on the word and sentence level. In contrast to existing research, our data incorporate background information on the participants' demographics, political ideology, and their opinion about media in general. Based on our data, we also present a way to detect bias-inducing words in news articles automatically. Our approach is feature-oriented, which provides a strong descriptive and explanatory power compared to deep learning techniques. We identify and engineer various linguistic, lexical, and syntactic features that can potentially be media bias indicators. Our resource collection is the most complete within the media bias research area to the best of our knowledge. We evaluate all of our features in various combinations and retrieve their possible importance both for future research and for the task in general. We also evaluate various possible Machine Learning approaches with all of our features. XGBoost, a decision tree implementation, yields the best results. Our approach achieves an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79, which outperforms current media bias detection methods based on features. We propose future improvements, discuss the perspectives of the feature-based approach and a combination of neural networks and deep learning with our current system.",
          "News articles in online newspapers are considered a crucial information source that replace traditional media such as television or radio broadcasts and print media, and new sources of information such as social media (Dallmann, Lemmerich, Zoller, & Hotho, 2015). Many people consider such articles a reliable source of information about current events, even though it is also broadly believed and academically confirmed that news outlets are biased (Wolton, 2017). Given the trust readers put into news articles and the significant influence of media outlets on society and public opinion, media bias may potentially lead to the adoption of biased views by readers (Spinde, Hamborg, Donnay, Becerra, & Gipp, 2020a). However, ''unrestricted access to unbiased information is crucial for forming a well-balanced understanding of current events'' (Hamborg, Donnay, & Gipp, 2018). Highlighting media bias instances may have many positive implications and can mitigate the effects of such biases (Baumer, Elovic, Qin, Polletta, & Gay, 2015). While complete elimination of bias might be an unrealistic goal, drawing attention to its existence can not only warn readers that content is biased but also allow journalists and publishers to assess their work objectively (Dallmann et al., 2015). Furthermore, such insights could be very interesting for research projects, e.g., in social science. We want to point out that it is uncertain if and how actual news consumers would like to obtain such information. Only a few systems are already employed to help readers mitigate the consequences of media bias impact. Most of them focus on the aggregation of articles about the same event from various news sources to provide different perspectives (Lim, Jatowt, & Yoshikawa, 2018a). For example, the news aggregator Allsides 1 allows readers to compare articles on the same topic from media outlets known to have different political views. Various media bias charts, such as the Allsides media bias chart, 2 or the Ad Fontes media bias chart 3 provide up-to-date information on media outlets' political slants. The main objective of this paper is to present a prototypical system for the automated identification of bias-inducing words in news articles. In the following chapter, we will give an overview of research on the topic and show major currently existing drawbacks on the issue. They lead us to more fine-grained research contributions. Mainly, we will: 1. create a labeled data set for media bias analysis on different levels; 2. analyze and engineer features potentially indicating biased language; 3. train a classifier to detect bias-inducing words and 4. evaluate the performance. This study holds both theoretical and practical significance. We summarize all existing research to give a full overview of possible classification features for media bias. We also show the relevance of all these features. We provide a data set of media bias annotations. It is the first such data set in the field, reporting word and sentence level annotations and detailed information on annotator characteristics and background. Our current data set already significantly extends available data in this domain, providing a unique and more reliable insight into bias perception. It also offers grounds for future extension. Lastly, we train and present a classifier for biased words that outperforms existing feature-based classifiers for bias. The rest of the paper is organized as follows. Section 2 presents the literature review on media bias and its automated detection. Section 3 details the methodology, and Section 4 presents the results of this study. Finally, in Section 5 we present a discussion of the project and an outlook on future work.",
          "",
          "Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975). On the other hand, bias can also be unintentional and even unconscious (Baumer et al., 2015;Williams, 1975). Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently. In turn, an author can manipulate a reader's perception by implying a particular opinion or perspective or inducing positive or negative emotions. The following two examples present instances of media bias by word choice, respectively: 1. Practicing pro-life litigators know that Trump judges are saving lives by permitting restrictions on abortion to go into effect. 4 2. Tens of millions of children under 12 months are potentially at risk for diseases such as diphtheria and polio as the Chinese coronavirus pandemic interrupts routine vaccinations, according to data published by global public health experts on Friday. 5 In the first example, the author chooses the vaguer word ''pro-life'' to describe the very concrete ''anti-abortion'' position as highly positive. In the second example, labeling the coronavirus pandemic with the word ''Chinese'' implies China's fault in the pandemic.",
          "To the best of our knowledge, there are no tools or systems for automatic identification of media bias based on word choice. The task is a challenging one for several reasons. Firstly, while a vast amount of text data from the news is available, articles naturally are created without such sophisticated labels that could allow us to detect bias inducing words; therefore, existing data are unlabeled. Existing annotated data sets are very small. That means that currently, there is no large-scale gold standard data set for the identification of media bias by word choice (Hamborg et al., 2018). Furthermore, it has been proven that bias identification is a non-trivial task for non-experts (Lim et al., 2018a;Recasens, Danescu-Niculescu-Mizil, & Jurafsky, 2013), which can cause problems while creating such a data set. Secondly, bias words are highly dependent on the context. Therefore, simple presence of specific words is not a reliable indicator of bias (Hube & Fetahu, 2018). 1. An abortion is the murder of a human baby embryo or fetus from the uterus, resulting in or caused by its death (Hube & Fetahu, 2018). 2. In 2008, he was convicted of murder (Hube & Fetahu, 2018). In the first example, the word ''murder'' describes abortion as something highly negative. In the second example, ''murder'' is used to describe a pure fact of murder. In the first case, the word induces bias, whereas, in the second, it does not. Another good example of context dependence of bias is the bidirectionality of the epistemological bias, i.e., this bias can occur in two cases: when a truthful proposition is questioned or when a false or controversial proposition is presupposed or implicated. The word that will not cause bias in the first case, e.g., factive verb, will cause it in the second, and vice versa (Recasens et al., 2013). Finally, media bias and bias in reference works are subtle, implicit, and intricate since, in general, the news is expected to be factual and impartial (Baumer et al., 2015;Hube & Fetahu, 2018;Lim et al., 2018a;Recasens et al., 2013). As fairly noticed by Williams (1975), remarkable bias is not extreme but rather reasonable and plausible: extreme bias is obvious and does not threaten values or institutions. Despite the challenging nature of the task, several researchers attempted to annotate media bias by word choice automatically. We also derive valuable insights for this project from the research attempting to identify the biased language in the related fieldreference sources such as encyclopedias, where neutral language is also desired and required. Lim et al. (2018a) use crowdsourcing to construct a data set consisting of 1235 sentences from various news articles reporting on the same event, namely, the arrest of a black man. The data set provides labels on the articles and word level. The authors then train a Support Vector Machine on the Part of Speech (POS) tags and various handcrafted linguistic features to classify bias on the sentence level, achieving the accuracy of 70%. In related work, Lim, Jatowt, Färber, and Yoshikawa (2020) propose another media bias data set consisting of 966 sentences and containing labels on the sentence level. The data set covers various news about four different events: Donald Trump's statement about protesting athletes, Facebook data misuse, negotiations with North Korea, and a lawmaker's suicide. Baumer et al. (2015) focus on the automated identification of framing in political news and construct a data set of 74 news articles from various US news outlets covering diverse political issues and events. They then train Naïve Bayes on handcrafted features to identify whether a word is related to framing, and achieve 61% accuracy, 34% precision, 70% recall, 0.45-0.46 𝐹 1 -score. Hamborg, Zhukova and Gipp (2019) constructed a data set using content analysis. They created a codebook describing frame properties, coding rules, and examples. The data set consists of 50 news articles from various US news outlets and covers ten political events. The authors distinguish the target concept and phrases framing this concept, and define a number of framing properties, e.g., ''affection'', ''aggression'', ''other bias'', etc. The authors then automatically extract the candidates for target concepts and identify frames by looking for words semantically similar to the previously defined framing properties via exploiting word embeddings properties. Then, identified framing properties are assigned to the candidates via dependency parsing. The authors achieve an F1-score of 45.7%. Fan et al. (2019) create the data set BASIL, annotated by two experts, covering diverse events and containing lexical and informational bias. The data set allows analysis at the token level and relatively to the target, but only 448 sentences are available for lexical bias. Then, they employ BERT lexical sequence tagger to identify lexical and informational bias at the token level and achieve an 𝐹 1 -score of 25.98%. Chen, Al-Khatib, Wachsmuth, and Stein (2020) create a data set of 6964 articles covering various topics and news outlets containing political bias, unfairness, and non-objectivity labels at the article level. They then train the recurrent neural network to classify articles according to these labels. Finally, the authors conduct a reverse feature analysis and find that, at the word level, political bias correlates with such LIWC categories (Pennebaker, Boyd, Jordan, & Blackburn, 2015) as negative emotion, anger, and affect. Recasens et al. (2013) create static bias lexica based on Wikipedia bias-driven edits due to NPOV (Neutral Point of View) violations. 6 The bias lexicon and a set of various linguistic features are then fed into the logistic regression classifier to predict which words in the sentences are bias-inducing. The authors reached 34.35% to 58.70% accuracy for predicting 1 to 3 potential bias-inducing words in a sentence, respectively. Hube and Fetahu (2018) propose the semi-automated approach to extract domain-related bias words lexicon, based on the word embeddings properties. The authors then feed obtained bias words and other linguistic features into a random forest classifier to detect language bias in Wikipedia at the sentence level. The authors achieve 73% accuracy, 74% precision, 66% recall, and an 𝐹 1score 0.69 on the newly created ground truth based on Conservapedia,foot_1 and state that the approach is generalizable for Wikipedia with a precision of 66%. In their later work, Hube and Fetahu (2019) train a recurrent neural network on a combination of word embeddings and a few handcrafted features to classify bias in Wikipedia at the sentence level and achieve 81.9% accuracy, 91.7% precision, 66.8% recall, and 0.773 𝐹 1 -score. Spinde, Hamborg, andGipp (2020b, 2020c) analyze media bias in German news articles covering the refugee crisis. The three components: an IDF component, a combined dictionary-based component, and a component based on a semantically created bias dictionary, are analyzed to identify bias on the word level. The combination of the dictionary component and the topic-dependent bias word dictionary achieves an 𝐹 1 -score of 0.31, precision of 0.43, and recall of 0.26. The authors point out that considering adjectives separately increased the performance.",
          "",
          "The general workflow of a prototypical system for automated identification of bias-inducing words in news articles is presented in Fig. 1. In our work, we start from collecting the sentences and gathering annotations via a crowdsourcing process (1). We then obtain various features (3) described in more detail in Section 3.5. One of the features is a bias lexicon built semi-automatically by computing words similar to potential bias words using outlet-specific word embeddings (2). We then train a supervised classifier on our engineered features and annotated labels (4). After the best model is selected and optimized, we evaluate the performance of the feature-based approach for detection of media bias. Furthermore, we evaluate all features individually (5).",
          "One of the challenges in the automated detection of media bias is the lack of a gold standard large-scale data set with labeled media bias instances. The existing data sets described in Section 2.2 either do not allow for the analysis of media bias on the word level or can induce drawbacks due to the following limitations: (1) they only include a few topics (Lim et al., 2020(Lim et al., , 2018a)), (2) they mostly focus exclusively on framing (Baumer et al., 2015;Hamborg, Zhukova et al., 2019), (3) annotations are target-oriented (Fan et al., 2019;Hamborg, Zhukova et al., 2019), (4) annotations are not on the word level (Lim et al., 2018a), or (5) training data are too small (Fan et al., 2019). Therefore, we decided to create a diverse, robust, and extendable data set to identify media bias. We hand-picked 1.700 sentences from around 1.000 articles. According to our collection strategy, most of the sentences should contain media bias instances, while the smaller number of sentences should be neutral. However, the final annotations are made by the crowd-source annotators. The sentences equally represent the full political spectrum since we used the articles from the major left and right-wing outlets, classified by their political ideology within the media bias ratings of www.allsides.com. We covered 14 different topics (selected randomly out of a variety of possible topics), from very contentious (e.g., abortion, elections) to less contentious topics (e.g., student debt, sport). To gather annotations of the sentences and the words, we developed our own survey platform, combining classical survey functionality with text annotations, and hired participants via Amazon Mechanical Turk to complete microtasks. Annotation quality ensured by experts is often preferable, but we expressively wanted to collect a large number of annotations from non-experts. It has been shown that many complex problems can be resolved successfully through crowdsourcing if the existing crowdsourcing platforms are used in combination with appropriate management techniques and quality control (Mitrović, 2013;Mladenović, Mitrović, & Krstev, 2016)  8 . Seven hundred eighty-four annotators participated in the survey, all located in the United States. The vast majority (97.1%) of the annotators were native English speakers, 2.8% were near-native speakers. The annotators from diverse age groups participated in the survey; people from 20 to 40 years old (67.4%) prevail over other age groups. The annotators' gender is balanced between females (42.5%) and males (56.5%). The annotators have a diverse educational background; more than half have higher education. The political orientation is not well balanced: liberal annotators prevail (44.3%) over conservative annotators (26.7%) and annotators from the center (29.1%). The vast majority of the annotators read the news sometimes, more than half -one (46.4%) or more (23.1%) times per day. Each annotator received 20 randomly reshuffled sentences. We showed each sentence to ten annotators. Within our platform,foot_3 we first instruct participants about the general goals of the study. We explain the tasks in detail and ask them to leave aside their personal views. We also give them a few examples of bias and ask a control question to check whether participants understood media bias's general concept. If the control question was not answered correctly, participants had to reread the instructions. Within the annotation task itself, we provide detailed instructions on the workflow. We then ask each annotator to highlight words or phrases that induce bias according to the provided instructions. After that, we ask them to annotate the whole sentence as biased or impartial, and whether they would describe it as opinionated, factual, or mixed. To the best of our knowledge, our data set is the first in the research area to collect detailed background demographic information about the annotators, such as gender, age, education, English proficiency, but also information on political affiliation and news consumption. Overall, our data set allows for performing three different tasks: bias identification on the word level, sentence level, and a classification of the sentence as being opinionated, factual, or a mixture of both. We discuss the results of the annotation in Section 4.1.",
          "As one of our features, we present a lexicon of biased words, built explicitly for the news domain (Hube & Fetahu, 2018). Interestingly, although such a lexicon cannot serve as an exhaustive indicator of media bias due to high context-dependence (Hube & Fetahu, 2019), it can potentially serve as a useful feature of a more complex media bias detection system. To extract a biased word lexicon of high quality, we replicate the method proposed by Hube and Fetahu (2018). The authors proposed a semi-automated way to automatically extract biased words from corpora of interest using word embeddings. We present the whole pipeline of the approach in Fig. 2. We first manually create a list of words that describe contentious issues and concepts. Then, we use this list to manually select ''seed'' biased words in the two separate word embedding spaces trained on news articles potentially containing a high number of biased words. We select seed biased words among the words that have high cosine similarity to the words describing contentious issues. We publish our list of seed biased words at https://bit.ly/36guHdu. 10We assumed that news outlets with presumably stronger political ideology would use bias words when describing contentious issues with a higher likelihood than neutral mediums. To capture both liberal and conservative biases, we train word embeddings separately on the corpora of news articles from HuffPost and Breitbart, respectively. In the choice of the outlets, we relied on the information provided by Allsides: both outlets are presented at the media bias chart,foot_5 and for both outlets, the confidence level of the assigned ratings is high.foot_6 Noteworthy, these two sources are also ones of the most popular media sources that left-and right-leaning communities share respectively in Soliman, Hafer, and Lemmerich (2019). The articles from both sources, published from 2010 to 2020, are scraped from Common Crawlfoot_7 using NewsPlease (Hamborg, Meuschke, Breitinger and Gipp, 2017). We split the initial text into lower-cased tokens, remove punctuation marks and numbers, and train Word2Vec word embeddings (Mikolov, Chen, Corrado, & Dean, 2013). The chosen hyper-parameters are summarized in Table 1. Since evaluation of such an unsupervised task as word embeddings creation is quite challenging (Bakarov, 2018), we choose the hyper-parameters based on the existing research (Spinde, Rudnitckaia, Hamborg, & Gipp, 2021a). The number of dimensions is set to 300 and is not increased further due to the scarcity of the training data (Mikolov et al., 2013). The window size is set to 8 since  larger window size can capture broad topical content (Levy, Goldberg, & Dagan, 2015). We increase the number of iterations to 10 since the size of training data is small and cannot be increased. In the scope of this project, it is important to avoid unstable low-qualitative vectors, therefore words appearing less than 25 times are excluded. Finally, treating n-grams as single units may lead to better training of a given model (Camacho-Collados & Pilehvar, 2018). We use the default scoring for n-grams generation and run two passes over training data. The thresholds for n-grams inclusion are based on manual analysis of the generated n-grams. The rest of hyper-parameters are set to the default values. As a next step, we divide the set of seed biased words into random batches consisting of ten words and repeat this process ten times to create batches with various combinations of words. Then, for each batch, the average vector in the word embedding space trained on a 100 billion Google news data setfoot_8 is calculated. For each average vector, we extract the top 100 words close to this average vector. Hube and Fetahu (2018) do not reshuffle words in batches and extract the top 1000 words. However, the average cosine similarity of farthest words among the top 1000 is 0.47, whereas the average cosine similarity of farthest words among the top 100 is 0.52. Besides, extracting the top 1000 words introduces noise. Finally, we add extracted words to the resulting bias word lexicon, and remove duplicates.",
          "We define bias-inducing words detection as a binary classification problem where we have only two mutually exclusive classes: whether a word is biased (class 1) or not (class 0). With our binary classifier, and in the context of media bias by word choice, no exhaustive set of precise media bias characteristics exist. Therefore, we combine different linguistic features of biased language proposed by Recasens et al. (2013) and a variety of other syntactic and lexical features (Lim et al., 2020). As the context is highly important when distinguishing between unbiased and biased words, we attempt to capture useful information from context by including collective features adding two previous and two following words into a word's feature vector. We admit that such a way T. Spinde et al.",
          "The complete set of features used in our approach for detecting biased words.",
          "POS tags POS tag indicating the syntactic role of each word, e.g., noun, adverb, etc. Honnibal and Montani (2017).",
          "Dependencies revealing how words in the text relate to each other, e.g., whether a word is a root, object, or subject (Bird, Loper, & Klein, 2009;Honnibal & Montani, 2017).",
          "Named entities, e.g., persons, organizations, locations, etc. Bird et al. (2009) and Honnibal and Montani (2017).",
          "Norms of GloVe word embedding vectors pre-trained on the Common Crawl a Honnibal and Montani (2017). TF-IDF Frequency of the term in a document and in the whole article collection (Lim, Jatowt, & Yoshikawa, 2018b;Pedregosa et al., 2011).",
          "Word is a report/implicative/assertive/factive/positive/negative word, is strongly or weakly subjective, or a hedge (Recasens et al., 2013).",
          "Classifications as kill verb (Greene & Resnik, 2009), hyperbolic term (Chakraborty, Paranjape, Kakar, & Ganguly, 2016), boosters, and attitude markers (Hyland, 2019). LIWC features LIWC features based on psychological and psychometric analysis (Pennebaker et al., 2015). Semi-automated bias lexicon Previously described semi-automatically created bias word lexicon (Hube & Fetahu, 2018). a https://commoncrawl.org, accessed on 2020-10-31. to account for context is not optimal and requires elaboration in future. We compare different combinations of the features, and also train different machine learning classification algorithms, such as logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), complement Naïve bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), decision tree (DT), random forest (RF), XGBoost and a simple neural network -multilayer perceptron (MLP). To the best of our knowledge, we present the first detailed comparison of classifiers for word-level bias detection.",
          "We present our entire feature list in Table 2. In continuation, we describe the individual features and the intuition behind using them for our task. For POS tags, syntactic dependencies, named entity types, word vector norms, and linguistic features, we refer to the previous work on these topics as described by Hube and Fetahu (2018) and Recasens et al. (2013). TF-IDF. Lim et al. (2018b) propose detection of bias using inverse document frequency (IDF) as one of the features to detect media bias, under the assumption that more rarely occurring terms are more likely to be extreme in any direction and are hence more likely to induce bias. As our data set consists of sentences where the terms are probably rarely repeated within one sentence, we adjusted Lim et al.'s assumption (Lim et al., 2018b) slightly by calculating the TF-IDF statistic basing on the whole text of articles the sentences were collected from. LIWC Features. Linguistic Inquiry and Word Count (Pennebaker et al., 2015) is a common approach to analyzing various emotional, cognitive, and structural components in language. It identifies linguistic cues related to psychological processes such as anger, sadness, or social wording. We consider all feature categories from LIWC, as this has shown to be the most effective usage of the resource to identify bias. Additional lexical features. It is not well known which features are the most efficient indicators of media bias (Baumer et al., 2015). Therefore, we test additional features that have been used by researchers to study similar constructs but have not been applied for the detection of media bias yet. According to Greene and Resnik (2009), the so-called ''kill verbs'' together with the relevant grammatical relation (governor or dependent term) cause different sentiment perception. In the following example, the second one is perceived as more negative since it implies an intention (Yano, Resnik, & Smith, 2010): 1. Millions of people starved under Stalin. 2016) study click baits in online news and find that click bait headlines usually include hyperbolic wordswords with highly positive sentiment. We assume that hyperbolic words used in click bait titles to attract readers' attention can be used to emphasize some concepts and induce bias in news articles. Hyperbolic words are, for example, ''absolutely'', ''brilliant'', or ''impossibly''. Hyland (2019) introduces linguistic features that help authors to express their views on the discussed proposition. One such subcategory are boosters -words that express certainty about a particular position, e.g., ''believed'', ''always'', ''no doubt''. In some regard, boosters are opposite to hedges, which, on the contrary, reduce the confidence of a statement. Another subcategory are attitude markers -indicators of the author's expression of affective attitude to statements, e.g., ''fortunately'', ''shockingly'', ''disappointed'', etc. ",
          "",
          "",
          "In the final data set, classes are highly imbalanced (Section 4.1). Since accuracy does not capture the capability of a model to predict rare class correctly (Branco, Torgo, & Ribeiro, 2015), we focus on such evaluation metrics as confusion matrix, precision, recall, F1-score, and receiver operating characteristic area under curve (ROC AUC). We compare the performance of our system to several possible baselines: • Baseline 1 (B1) -a purely random classifier; • Baseline 2 (B2) -occurrence of a word in the negative sentiment lexicon; • Baseline 3 (B3) -occurrence of a word in the negative or positive lexicon; • Baseline 4 (B4) -occurrence of a word in the semi-automated bias lexicon. In our final data set, each observation corresponds to one word, its feature vector (including the collective context features), and the label. We perform 10-fold cross-validation when compare performance of different classifiers, 5-fold cross-validation when optimize hyper-parameters of the selected model, and finally, estimate the final performance on a test set of words that did not participate in training and manually investigate correctly and wrongly classified instances.",
          "",
          "To gain insights into the characteristics of our data set, we analyze it quantitatively and qualitatively. The results of sentences classification are presented in Figs. 3 and 4. The annotation results confirm our data sampling strategy: biased and non-biased statements are not balanced in the data set: biased statements prevail over non-biased statements. Besides, most media bias instances are taken from liberal and conservative news sources, whereas sources from the center were used mainly to retrieve non-biased statements. Note that this does not imply that liberal and conservative news outlets generally experience media bias by word choice and provide opinionated news more often than news outlets from the center. It is valid only due to our data collection scheme. We assigned a biased or impartial label to a sentence if more than half of respondents annotated a sentence as biased or impartial, respectively. 149 sentences could not be labeled due to a lack of agreement between annotators. Many measures for assessing interannotator agreement have been used in similar computational linguistics projects. Based on the way in which we have organized our crowdsourcing workflow, i.e. having 10 annotators per task, we have decided to use Fleiss (1971) to assess the inter-annotator agreement. It represents the task's general difficulty: for example, Hube and Fetahu (2018) reported 𝛼 = 0.124 on word-level bias, and Recasens et al. (2013) reported a 40.73% agreement when looking at only the most biased word in Wikipedia statements. The value of 0.21 that we achieved can be considered as a fair agreement. We assigned an opinionated, factual, or mixed label to a sentence if most respondents annotated a sentence as opinionated, factual, or mixed, respectively. We could not label 174 sentences due to the lack of agreement between annotators. According to our crowdsourced annotations, the data set contains an almost equal number of factual, opinionated, and mixed statements. The annotation scheme for biased words allowed respondents to highlight not only the words but also short phrases. A word was considered biased if at least four respondents highlighted it as biased. On average, a sentence that contains biased words contains two biased words. Out of 31,794 words for training, only 3018 are biased, which constitutes only 9.5% of our current data. The types of words annotated as biased are presented in Table 3.  We observe that annotators select not only extreme and emotional words that can be considered biased even without context, but also context-dependent bias words. For instance, while the word ''Chinese'' is generally not biased it can be in specific contexts, such as ''House Democrats' Chinese coronavirus relief package bails out coastal millionaires and billionaires while ensuring big businesses are able to freely hire illegal aliens and visa overstayers over unemployed Americans''. 15Albeit emphasizing in the instructions that words that are connected to very controversial topics or have very negative sentiment are not necessarily biased, some of such words were still annotated as biased. For example, the term ''neo-Nazis'' in the sentence ''For years now, Fox News has been mainstreaming arguments that used to be the province of fringe websites run by neo-Nazis and other groups who believe the U.S. is meant to be a country of white people and for white people''. 16Furthermore, we find that annotators sometimes fail to mark words as biased if a sentence contains clearly extreme and emotional words. For example, a majority of annotators marked ''cray-cray'' as biased but did not notice ''totally'' in the sentence ''Over the past few decades, RFK Jr.'s famous name has helped him get in the door to talk to important people, and it probably is not long before the person who is all jacked up to meet a Kennedy realizes the guy is totally cray-cray''. 17  As expected, we find a positive correlation between marking sentences as biased and opinionated, and factual and non-biased. Furthermore, more controversial topics are annotated as non-biased, on average, 7.4 p.p. less than less controversial topics. Interestingly, in 49.3% of the sentences labeled as non-biased, annotators still labeled some words as biased. Annotators who estimate themselves as conservative mark 3.76 p.p. more sentences as biased than others who describe themselves as being liberal -except if the sentence stems from a conservative news outlet (Yano et al., 2010). Furthermore, annotators who report that they check news at least sometimes, label sentences as biased 6.85 p.p. more than those who report to check news very rarely, and 19.95 p.p. more than those who report that they never check news.",
          "In this section, we first present the characteristics of the articles we used to train our word embedding models and the performance of the trained word embeddings. We also provide the characteristics of the pre-trained Google News embeddings. We measure semantic word similarity and word analogy (Bruni, Tran, & Baroni, 2014;Finkelstein et al., 2001;Mikolov et al., 2013). Table 4 depicts the results of our measures. Two data sets -WordSim-353 (Finkelstein et al., 2001) and MEN (Bruni et al., 2014) -allow to estimate the Pearson correlation for the semantic similarity between pairs of words in respective word embeddings and as estimated by human assessors. The Google analogy test set (Mikolov et al., 2013) allows to evaluate accuracy. Even though those evaluation data sets are not perfectly suited for our task, the comparison shows that our data sets are large enough to give comparable results to the full Google News data set. We also manually inspected the embeddings' results and confirmed that they capture bias to a reasonable extent. Second, we qualitatively investigate the lexicon of biased words resulting from the semi-automated expansion (Section 3.3). We manually inspected a random sample of 100 words and find that the vast majority (Around 69% in a random sample of 100 words) are negatively connotated, are emotional, and convey strong opinion (Fig. 5). Furthermore, the dictionary consists of disproportionally many rather uncommon words (e.g., ''teapartiers'', ''obamian'', ''eggheaded'', ''mobocracy''), that are only interpretable when knowing about the circumstances they developed in. We find only one word (''similarly'') that cannot directly be related to bias, while we ourselves would classify all 99 other words as being very likely to induce bias. Among 96 words for which POS tag can be identified unambiguously, 41.7% are nouns, 24.0% are verbs, 21.9% are adjectives, and 11.5% are adverbs. Finally, we compare the method of batches developed by Hube and Fetahu (2018) to the naive approach where close words are retrieved for a single seed bias word instead of an average of a batch. We find the employed batched extraction to be superior. Specifically, while both approaches yield a high proportion of biased words, naive approach also yields many words that are not biased but co-occur with biased words, such as ''abortion'', ''personhood'', ''immigrants'', ''mexicans'', etc. Table 5 contrasts extraction for two methods. So far, the lexicon seems to be valuable especially in finding negatively connotated neologisms and words that convey strong opinion even by themselves. Despite being more efficient than the naive approach, the method of batches, nevertheless, still cannot avoid some degree of noisy words and words falsely included as biased. Among such words are misspellings, abbreviations, and words that describe a contentious or a negative issue or concept, e.g., ''xenophobia'', ''criticize'', ''anti-Semite'', ''solipsist''. We will further evaluate the resource in future work.",
          "We first train ''quick and dirty'' models (Gron, 2017) on all available features with default parameters (as implemented in Scikit-Learn (Pedregosa et al., 2011) and XGBoost (Chen & Guestrin, 2016) libraries) and compare the performance based on scores T. Spinde et al. averaged from ten-fold cross-validation (Pedregosa et al., 2011). We compare 𝐹 1 -score, precision, recall, and ROC AUC. Since data are imbalanced (only ≈ 10% are biased), weighting of classes is employed for all methods (where possible). Table 6 shows that no model yields a high 𝐹 1 -score. Instead, best performing models yield either high precision or high recall. Since results of our method are, for now, intended to be verified by a user, we prefer recall over precision while still aiming for moderately high 𝐹 1 -score. We choose XGBoost for further optimization since it achieved both the highest ROC AUC score and the highest 𝐹 1 -score. It also has a relatively high recall: the model predicts more True Positives (biased words as biased) than False Negatives (biased words as unbiased). The model suffers from predicting many False Positives (unbiased words as biased) but to the smaller extent than other models with higher recall (Logistic regression, QDA, NB, SVM). XGBoost is ''a scalable end-to-end tree boosting system'' (Chen & Guestrin, 2016). The algorithm is based on gradient boosting -an ensemble method that adds predictors sequentially to an ensemble, each new one is fit to the residual errors made by the previous one (Gron, 2017). Thus, the final model -a combination of many weak learners -is itself a strong learner. In addition to the fact that XGBoost already achieved best results on our data set, it provides several advantages: it accounts for sparsity caused by one-hot encoding (Chen & Guestrin, 2016), allows for a fine parameter tuning using a computationally efficient algorithm (Bentéjac, Csörgo, & Martínez-Muñoz, 2019), and allows to estimate feature importance since we do not have reliable prior information about the importance of the features (Baumer et al., 2015). We fine-tune five hyper-parameters that help to control for overfitting. 18  For the fine-tuned model, we quantitatively evaluate the performance and feature importance. Table 7 shows that fine-tuning yields an insignificant performance improvement of 𝐹 1 = 1p.p. We find that the model suffers from underfitting since performance 18 We find the following values to be optimal. max-depth = 6, min-child-weight = 18, subsample = 1, colsample-bytree = 1, and eta = .2. Other hyper-parameters are set to the default values. The maximum number of boosting rounds is set to 999, and early stopping is applied if the 𝐹 1 -score for validation set does not increase within ten rounds. The model is weighting the imbalanced classes. The evaluation metric is the 𝐹 1 -score averaged on five-fold cross-validation. is also low on training (𝐹 1 = 0.51) and validation sets (𝐹 1 = 0.50). Comparing XGBoost performance to the defined baselines, we see that XGBoost significantly outperforms the random baseline (B1) but fails to significantly outperform the naive usage of the negative sentiment lexicon (B2). However, when analyzing results in a confusion matrix, we see that using just the negative dictionary, in fact, predicts 53% of biased words incorrectly as non-biased words whereas XGBoost predicts only 23% incorrectly. High 𝐹 1 -score and ROC AUC of baseline B2 is mostly due to the low number of False Positives, but this is a behavior close to simply predicting all words as non-biased. Since we do not have the prior information on which features are the most contributing into media bias detection, we first trained the classifier on the all the available features. When analyzing feature importance, we find that the most important features are the occurrence of a word in a negative lexicon (gain = 1195) and being a proper noun (470). The bias lexicon that we created semi-automatically is among the top 10 important features (gain = 106). Among linguistic features proposed by Recasens et al. (2013) as indicators of bias, only sentiment lexica, subjectivity lexica, and assertive verbs are among the top 30 important features. While report verbs and hedges still have minor importance, factive and implicative verbs have zero importance. We train several models feeding them with features that have different importance. Excluding features with low feature importance does not improve the performance (Table 7). Besides, we test how the model performs when different feature groups are not included. Thus, we train a model with all features except one particular feature or group of features. We notice that the performance drops significantly only when linguistic features are not used, most likely because of the negative sentiment lexicon's high importance. Lastly, we qualitatively investigate automatically detected bias candidates. Examples of correctly classified biased words (TPs) include mostly emotional words that can be considered biased even without context. Words that can be described as causing negative emotions occur more often than those causing positive emotions. 12.5% of TPs correctly indicate less obvious bias, which is most likely generally more rare. The following examples illustrate (1) obvious negative bias, (2) obvious positive bias, and (3) slightly more subtle bias among correctly classified words. 1. Large majorities of both parties seem to like the Green New Deal, despite efforts by Fox News to paint it as disastrous. 19  2. Right-wing media sprung into action to try to discredit her, of course, by implying that a woman who graduated summa cum laude with an economics degree is a bimbo and with Twitchy using a screenshot to make the usually genial Ocasio-Cortez somehow look like a ballbuster. 20 3. As leading 2020 Dems advocate spending big on the Green New Deal, it turns out most Americans are worried about other issues. 21   Examples of biased words incorrectly classified as non-biased (FNs) include words that are (1) parts of phrases, (2) ambivalent as to whether they are biased, (3) not generally biased but only in a particular context, (4) mistakes in the annotation, and random misclassifications: 1. By threatening the kids and their families with deportation, the administration's U.S. Citizenship and Immigration Services was effectively delivering death sentences. 22 19 https://www.alternet.org/2019/04/just-a-cover-for-sexism-and-white-nationalism-paul-krugman-explains-why-the-rights-attacks-on-new-democratic-  lawmakers-are-bogus/, accessed on 2020-10-31. 20 https://www.alternet.org/2019/01/alexandria-ocaio-cortez-is-absolutely-right-there-shouldnt-be-any-billionaires/, accessed on 2020-10-31. 21 https://fxn.ws/370GuwZ, accessed on 2020-10-31. 22 https://www.msnbc.com/rachel-maddow-show/trump-admin-backs-plan-deport-critically-ill-children-msna1280326, accessed on 2020-10-31. T. Spinde et al. 2. When the Muslim ban was first enacted, it triggered chaos at airports and prompted widespread protest and legal challenges, and it continues to impose devastating costs on families and people who wish to come to the U.S. 23 3. The specter of ''abortion regret'' has been used by lawmakers and judges alike to impose or uphold rules making it harder for people to get abortions. 24 4. Gun enthusiasts cannot admit that they like firearms because they fear black people. 25   Examples of non-biased words misclassified by the model as biased (FPs) include words that (1) are ambivalent as to whether they are biased, (2) describe negative or contentious issues, (3) are due to erroneous annotation, and (4) random misclassifications: 1. Justice Sonia Sotomayor, in her dissent, accused the majority of weaponizing the First Amendment -an unconscionable position for a person tasked with ''faithfully and impartially'' discharging the duty to protect the inherent rights of all Americans. 26 2. He also denounced the policy of Chancellor Angela Merkel and the attitude of the German media, which ''are constantly pushing'' for Europe to welcome more and more migrants, in opposition to the will of the Hungarian people. 27 3. Michelle Williams won a Golden Globe for her role in ''Fosse/Verdon'' on Sunday night, but perhaps her biggest moment came during her acceptance speech when she defended abortion rights and encouraged women to vote ''in your own self-interest''. 28 4. The case was sent back to lower courts to determine whether the gun owners may seek damages or press claims that the amended law still infringes their rights. 29",
          "One of the main contribution of our work is the creation and annotation of a robust and diverse media bias data set. Other than already existing studies on the topic, our data contain background information about the annotators, increasing our results' transparency and reliability. We perform visual analysis and observe the following findings. Topics that are less controversial are annotated as non-biased slightly more often than very controversial topics. Conservative annotators perceive statements as biased slightly more often than liberal annotators, but for both, it is only true unless the statement is from a conservative media outlet. We also find that annotators who read news never or very rarely are less likely to annotate statements as biased. Besides, our annotation results show the connection of bias and opinion. Even though our data set was developed on a small scale and cannot serve as a gold standard for the field of media bias research, it offers a complete framework for further data extension, especially in combination with our specifically developed survey platform. We plan to extend the data set in the future. While using crowdsourcing gave us an insight into the perception of bias by a broad audience, some related issues could not be resolved, e.g., the submission of random words. Furthermore, even honest workers made mistakes because the identification of media bias is, in general, not a trivial task, especially for non-experts (Lim et al., 2018a;Recasens et al., 2013). We will follow a dual strategy in future work: While extending the existing data set, we will develop an expert annotation guideline and evaluate the same sentences in cooperation with experts in the field. We will also test the difference between the annotation of single, isolated sentences, and sentences within an article's scope. Lastly, we will evaluate from a psychological perspective how different types of questioning affect the perception of bias, and have already collected around 500 varying questions on the issue. Regarding the quality of the annotations, our main strategy is to exclude noise from the final labels by setting up a threshold of the number of annotators required to label a word as biased or not. However, after manual analysis of final annotations, we find that setting up any strict threshold will introduce some noise and result in some words being omitted. The threshold of four is the most reasonable, but we admit that some words are omitted, and some non-biased words are included. We will experiment with more annotators per sentence to see whether we can reduce the percentage of errors. Our semi-automatically created bias lexicon is indeed able to find emotional words and words that convey a strong opinion. However, we conclude that while capturing emotional and negative opinionated words, the lexicon is unlikely to be exhaustive. So far, the approach lacks an additional method on how to expand the lexicon without adding non-biased words. Overall, our prototypical system achieves an 𝐹 1 -score of 0.43, precision of 0.29, recall of 0.77, and ROC AUC of 0.79. Our data set is the largest and most transparent in the area to date and our classifier is the first built on these data, making a direct comparison to other methods unfeasible. On their respective data sets, researchers who detected media bias on the word level achieved an T. Spinde et al. F1-score of 0.26 (Fan et al., 2019) and 0.31 (Spinde et al., 2020b); researchers, who detected framing on the word level, achieved an F1-score of 0.45-0.46 (Baumer et al., 2015;Hamborg, Zhukova et al., 2019). We present the most complete collection of features for classification to date, extending the work of Hube and Fetahu (2018) and Recasens et al. (2013). Especially Boosters were not used in previous research, but are among the most important features. We will continue our detailed analysis of feature importance for the overall task with our larger crowdsourced data set and the expert data. We will also improve the quality of our features. For example, for implicative verbs, Pavlick and Callison-Burch (2016) introduced a method to automatically predict implicativeness of a verb based on the known constraints on the tense of implicative verbs. We could also expand our sentiment and subjectivity lexicons by using WordNet, a de facto lexico-semantic network (Fellbaum, 1998;Miller, 1995), or SentiWordNet 3.0, a lexical resource that assigns sentiment scores to each synset of WordNet (Baccianella, Esuli, & Sebastiani, 2010). While recognizing around 77% of biased words correctly, our approach misclassifies around 20% of non-biased words. Due to the classes' imbalance, 20% of the misclassified majority class significantly decreases overall performance. In this section, we discuss the drawbacks of the implementation that lead to low performance. Especially words that are biased only in a particular context are rarely classified correctly, highlighting how media bias is usually very subtle and context-dependent. However, so far, we only accounted for context by using one collective feature for the window of four words surrounding the word. Overall, we believe that the feature-based approach is especially valuable because of its explanatory character, relating bias to specific features, which is impossible with automated feature extraction. It is also not as dependent on the amount of data as a neural network. However, we will integrate deep learning into our approach, as such an architecture especially helps to account for inter-dependencies between words. Both methods can also be combined, with our specific features giving meaning to the identification of words with automatically identified features. We will also investigate whether the classifier works better on the political left, right, or center sources. We will also determine whether we can distinguish bias for any particular ideology in our overall vocabulary of biased words. In this paper, we propose an approach to identifying media bias using an automatic feature-based classification. To evaluate our method, we also present a 1700-sentence data set of crowdsourced biased word annotations, where we show each sentence to ten survey participants. For the first time in the research area, we report each person's background and make our data more transparent and robust. We extend existing feature sets for the task and especially evaluate each feature in detail. We also experiment with different classifiers, with our final choice returning an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79. Our results slightly outperform existing methods in the area. To increase our results further, we show how we can improve our data and features in future work and how neural networks could be a suitable option to combine with our method, even though some drawbacks have to be overcome. We publish our code and current system at https://github.com/Media-Bias-Group/Automa-CRediT authorship contribution statement Timo Spinde: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Lada Rudnitckaia: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Jelena Mitrović: Helped to implement and completely financed the successful survey execution. Felix Hamborg: Wrote the discussion chapter. Michael Granitzer: Gave permanent expert feedback on our work. Bela Gipp: Gave permanent expert feedback on our work. Karsten Donnay: Helped to implement and completely financed the successful survey execution."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "slanted news coverage or internal bias, reflected in news articles.",
            "context": "2.1. Media bias Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975).",
            "type": "explicit"
          },
          "bias by word choice": {
            "definition": "when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept.",
            "context": "Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0f7eda998bbce003745ff2fdbcaa1d9a8119368b",
        "sections": [
          "as corrupt and untrustworthy. Many of us have started to wonder: are we trapped in echo chambers of our own making? 1The recent conversation, however, has blurred two distinct, but interrelated, social epistemic phenomena, which I will call epistemic bubbles and echo chambers. Both are problematic social structures that lead their members astray. Both reinforce ideological separation. But they are different in their origins, mechanisms for operation, and avenues for treatment. Both are structures of exclusion -but epistemic bubbles exclude through omission, while echo chambers exclude by manipulating trust and credence. However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation. We seek to stay in touch with our friends, who also tend to have similar political views. But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation. According to Jamieson and Capella, an echo chamber's members share beliefs which include reasons to distrust those outside the echo chamber. Echo chambers work by systematically isolating their members from all outside epistemic sources (Jamieson and Cappella 2008, 163-236). This mechanism bears a striking resemblance to some accounts of cult indoctrination. By discrediting outsiders, echo chambers leave their members overly dependent on approved inside sources for information. In epistemic bubbles, other voices are merely not heard; in echo chambers, other voices are actively undermined. (This is a conceptual distinction; a community can practice both forms of exclusion to varying degrees.) The contemporary discussion has been mostly focused on the phenomenon of epistemic bubbles. Cass Sunstein's famous discussions of group polarization, extremism, and the Internet largely focus on matters of constricted information flow and omitted viewpoints (Sunstein 2001(Sunstein , 2009b(Sunstein , 2009a)). Eli Pariser's The Filter Bubble focuses entirely on filtration effects from personalization technology, as in Google searches, and self-selected informational networks, as in Facebook (Pariser 2011). Popular conversation has tended to follow Pariser's focus on technologically-mediated filtration. The term \"echo chamber\" has, in recent usage, been reduced to a synonym for such bubbles and their constricted information flow. When the specifically trust-oriented manipulations of echo chambers are discussed, they are usually lumped in with epistemic bubbles as part of one unified phenomenon. This is a mistake; it is vital to distinguish between these two phenomena. An epistemic bubble is an epistemic structure emerging from the informational architecture of communities, social networks, media, and other sources of information and argument. It is an impaired informational topology -a structure with poor connectivity. An echo chamber, on the other hand, is an epistemic structure created through the manipulation of trust; it can exist within a healthy informational topology by adding a superstructure of discredit and authority. I hope to show, contra the recent focus on epistemic bubbles, that echo chambers pose a significant and distinctive threat -perhaps even a more dangerous one -that requires a very different mode of repair. Furthermore, echo chambers can explain what epistemic bubbles cannot: the apparent resistance to clear evidence found in some groups, for example, climate change deniers and anti-vaccination groups. It may be tempting to treat members of echo chambers as mere sheep, and accuse them of problematic acquiescence to epistemic authority. But that accusation relies on an unreasonable expectation for radical epistemic autonomy. Contemporary epistemology, especially social epistemology, has taught us that trust in others is necessary and ineradicable. We are, as John Hardwig says, irredeemably epistemically dependent on each other (Hardwig 1985(Hardwig , 1991)). Echo chambers prey on our epistemic interdependence. Thus, in some circumstances, echo chamber members do not have full epistemic responsibility for their beliefs. Once one is trapped in an echo chamber, one might follow good epistemic practices and still be led further astray. And some people can be trapped in echo chambers because of circumstances beyond their control -for example, they can be raised in them. Which leads to the most important questions: how can one tell if one is in an echo chamber? And how can one escape? I will argue that, for those trapped within an echo chamber, prospects for detection are poor and the escape path daunting. Detecting and escaping from echo chambers will require a radical restructuring of a member's relationship to their epistemic past, which may be more than we can reasonably expect of one another.",
          "Let's start with a simple picture of how many of us conduct our epistemic lives right now. I get much of my news via Facebook. I have selected most of my Facebook friends for social reasons; they are my friends and colleagues. A significant conduit for my learning about events in the world is by people re-posting articles that they have found newsworthy or interesting. When I go outside of Facebook, I usually turn to sources which, by and large, are affiliated with my own political beliefs and intellectual culture. This process imposes a filter on my uptake of information. Selection and exclusion are not bad in and of themselves -the world is overstuffed with supposed sources of information, many of them terrible. The better my filter, the more it will focus my attention on relevant, useful, and reliable information. But the bare fact that each individual member of the system is themselves reliable will not guarantee any broadness or completeness of coverage. Suppose, for example, that my social network was composed entirely of intelligent, reliable professors of aesthetics whose interests were largely focused on new developments in opera, ballet, and avant-garde contemporary art. Through this system, I might learn about all the exciting new developments in the New York Art scene, but entirely miss, say, relevant developments in rap, or the fact that my country was slowly sliding into fascism. The system lacks what Goldberg calls coverage-reliability -the completeness of relevant testimony from across one's whole epistemic community (Goldberg 2011, 93-4). Bad coverage can not only leave out relevant facts and evidence; it can also fail to bring relevant arguments to our attention. Thus, bad coverage can starve us of adequate exposure to relevant arguments. Notice that bad coverage is an epistemic flaw of epistemic systems and networks, not of individuals. I can now specify my use of \"epistemic bubble\" with greater precision. An epistemic bubble is a social epistemic structure which has inadequate coverage through a process of exclusion by omission. Epistemic bubbles form by leaving out relevant epistemic sources, rather than actively discrediting them. There are at two primary forces encouraging this omission. First, there is an epistemic agent's own tendency to seek like-minded sources. This phenomenon is sometimes called \"selective exposure\" by social scientists (Nelson and Webster 2017). In many contemporary cases, such as Facebook, the process of omission can occur inadvertently. I typically put people in my Facebook feed for social reasons -because I like them or I find them funny. But social selection does not guarantee good coverage reliability; in fact, the typical bases of social selection are inimical to good coverage reliability. 2 We usually like people who are similar to us, and such similarity makes coverage gaps more likely. Friends make for good parties, but poor information networks. We now have a straightforward account of one way in which epistemic bubbles can form. We can build a structure for one set of purposes -maintaining social relations, finding -and then proceed to use it for an entirely different purpose, for which it functions badly: information gathering. Second, there are the processes by which an epistemic agent's informational landscape is modified by other agents. This might include, say, systematic censorship or media control by the state or other actors. The most worrisome of these external forces, at the moment, seems to be the algorithmic personal filtering of online experiences (Pariser 2011;Watson 2015). Internet search engines, for example, will track personal information for each particular user, and adapt their search results to suit each user's interest. Certainly, newspapers and other traditional media technologies do place external filters on their readers, but the modern instantiation is particularly powerful and troubling. As Boaz Miller and Isaac Record argue, Internet technologies create hyper-individualized, secret filters. The secrecy is particularly threatening. Many users do not know about the existence of algorithmic personal filtering. Even amongst those that do, most do not have access to the particularities of the algorithms 2 For an overview of empirical research on personal similarity and polarization, see (Sunstein 2009a, 83-5). Curiously, Sunstein notes the group polarization literature has thought relatively little about the impact of personal similarity. doing the filtering; thus, the very opacity of the process makes it harder for a user to successfully evaluate and epistemically compensate for such filtering (Miller and Record 2013). Thus, most users significantly underestimate the degree to which their exposure to information, via search results, has already been tailored to present search results to which the user will already be amenable. Both the agent-driven process of selective exposure, and the externalities of algorithmic filtering, contribute to the creation of epistemic bubbles. I introduce the term \"epistemic bubble\" here to indicate a broader set of phenomena. Pariser introduced the term \"filter bubble\" to refer specifically to technologically mediated filtering, especially via algorithmic matching. Epistemic bubbles are those structures which omit relevant voices by any means, technological or otherwise. Epistemic bubbles include filter bubbles, but also nontechnological selection processes, such as physically sorting oneself into neighborhoods of like-minded people (Bishop 2009). The account I've given of epistemic bubbles focuses on the way they omit relevant information, but that omission can also threaten us with bootstrapped corroboration. Users of social networks and personalized search technologies will encounter agreement more frequently and so be tempted to over-inflate their epistemic self-confidence. This danger threatens because, in general, corroboration is often a very good reason to increase one's confidence in the relevant beliefs (Nguyen 2010(Nguyen , 2018a)). But corroboration ought to only have weight if it adds something epistemically, rather than being a mere copy. To borrow an example from Wittgenstein: imagine looking through a stack of identical newspapers and treating each next newspaper headline saying p as a reason to increase your belief that p (Wittgenstein 2010, 100). This is clearly a mistake; the fact that a newspaper claims that p has some epistemic weight, but the number of copies of that newspaper one encounters ought not add any extra weight. Similarly, imagine speaking to a bunch of acolytes of Guru Jane, who repeat anything that Guru Jane says without any further thought. The fact that all these acolytes repeat Guru Jane's testimony should add no extra weight. So long as the disciplines repeat anything Guru Jane says -so long as they are mere conduits for information, rather than sources of information -they are simply another sort of copy. But copying isn't the only route to a problematic form of non-independence. Suppose I believe that the Paleo diet is the best diet. I proceed to assemble a body of peers who I trust precisely because they also believe that Paleo is the best diet. In that case, the existence of perfect agreement on Paleo's amazingness throughout that group ought to count for far less than it might for other groups that I had not assembled on that basis. Even if all the group members arrived at their beliefs independently, their agreement is already guaranteed by my selection principle. To the degree that I have pre-selected the members in my epistemic network from agreement with some set of beliefs of mine, then their agreement with that set of beliefs and any other beliefs that it entails ought to be epistemically discounted. 3 If we fail to so discount, we are ignoring a pernicious hidden circularity in our corroborative process. It is easy to forget to discount because the bootstrap here is obscured by time and interface. But we must actively adjust for the increased likelihood of agreement inside our bubbles, or we will unwarrantedly bootstrap our confidence levels. 4To summarize: an epistemic bubble is an epistemic network that has inadequate coverage through a process of exclusion by omission. That omission need not be malicious or even intentional, but members of that community will not receive all the relevant evidence, nor be exposed to a balanced set of arguments.",
          "Luckily for us, epistemic bubbles are relatively fragile. Relevant sources have simply been left out; they have not been discredited. It is possible to pop an epistemic bubble by exposing a member to relevant information or arguments that they have missed. Echo chambers, on the other hand, are significantly more robust. My analysis here combines empirical work and analysis from Jamieson and Cappella on the nature of right-wing echo-chambers with recent insights from social epistemology. Jamieson and Cappella studied echo chambers built around particular charismatic personalities -Rush Limbaugh and the news team of Fox News, and certain other members of conservative talk radio. Their data and analysis suggest that Limbaugh uses methods to actively isolate his community of followers from other epistemic sources. Limbaugh's consistent attacks on the \"mainstream media\" serve to discredit all potential sources of knowledge or testimony besides Limbaugh and a select inner-cadre of other approved sources (Jamieson and Cappella 2008, 140-76). Limbaugh also develops what they call a private language, full of alternate meanings for familiar terms and new jargon (for example, \"SJWs\"), in order to exaggerate the insularity and separateness of the in-group. Finally, Limbaugh provides counter-explanations of all contrary views, intended not only to attack each particular view, but also to undermine the general trustworthiness and integrity of anybody expressing a contrary view. The resulting world-view is one of highly opposed forces; once one has subscribed to Limbaugh's view, one has reason to think that anybody who does not also subscribe is actively opposed to the side of right, and thereby morally unsound and so generally untrustworthy (177-90). Jamieson and Cappella suggest that this makes a follower dependent on a single source or group of sources, and makes them highly resistant to any outside sources. They offer the following definition of an echo chamber: an echo chamber is a bounded and enclosed group that magnifies the internal voices and insulates them from rebuttal (76). I will use the term \"echo chamber\" here following their analysis, but I adapt the definition slightly for philosophical use. I use \"echo chamber\" to mean an epistemic community which creates a significant disparity in trust between members and non-members. This disparity is created by excluding non-members through epistemic discrediting, while simultaneously amplifying insider members' epistemic credential. Finally, echo chambers are such that in which general agreement with some core set of beliefs is a pre-requisite for membership, where those core beliefs include beliefs that support that disparity in trust. By \"epistemic discrediting\", I mean that non-members are not simply omitted or not heard, but are actively assigned some epistemic demerit, such as unreliability, epistemic maliciousness, or dishonesty. By \"amplifying epistemic credentials\", I mean that members are assigned very high levels of trust. Of course, these two processes can feedback into one another. So long as an echo chamber's trusted insiders continue to claim that outsiders are untrustworthy, then the inner trust will reinforce the outward distrust. And so long as outsiders are largely distrusted, then the insiders will be insulated from various forms of counter-evidence and rebuff, thus increasing their relative credence. Once a sufficient disparity in credence between insiders and outsiders has been established, so long as trusted insiders continue to hold and espouse epistemically dismissive beliefs towards outsiders, then the echo chambers' beliefs system will be extremely difficult to dislodge. Compare this process of credence manipulation to the process of omission found in epistemic bubbles. In one standard scenario, I add others as trusted members of my epistemic network based on agreement. I am then less likely to encounter an outside voicebut when I do actually have such an encounter with an outsider, I have no background reason to dismiss them. Bubbles restrict access to outsiders, but don't necessarily change their credibility. Echo chambers, on the other hand, work by offering a pre-emptive discredit towards any outside sources. 5The result is a rather striking parallel to the techniques of isolation typically practiced in cult indoctrination. The standard techniques of cult indoctrination, by a traditional account, are the aggressive emotional isolation of cult members from all non-cult members, which amplifies indoctrinated member's dependency on the cult (Singer 1979;Langone 1994;Lifton 1991). 6 New cult members are brought to distrust all non-cult members, which provides an epistemic buffer against any attempts to extract the indoctrinated person from the cult. This is nothing like how epistemic bubbles work. Epistemic bubbles merely leave their members ignorant, but ignorance can be fixed with simple exposure. The function of an echo chamber, on the other hand, is to credentially isolate its members by a manipulation of trust. By this, I mean that members are not just cut off, but are actively alienated from any of the usual sources of contrary argument, consideration, or evidence. Members have been prepared to discredit and distrust any outside sources; thus, mere exposure to relevant outside information will have no effect. In fact, echo chambers can avail themselves of another epistemic protective mechanism: they can contain what I'll call a disagreement-reinforcement mechanism. Members can be brought to hold a set of beliefs such that the existence and expression of contrary beliefs reinforces the original set of beliefs and the discrediting story. A toy example: suppose I am a cult leader, and I have taught my followers to believe that every human except the members of our group has been infested and mind-controlled by alien ghosts from Mars. I also teach my followers that these alien ghosts from Mars hate our group for knowing the truth, and so will constantly seek to undermine our knowledge of their existence through mechanisms like calling us a 'cult' and calling us lunatics. Endre Begby has offered a careful analysis of this particular sort of disagreement-reinforcement mechanism, which he calls \"evidential preemption.\" Suppose that I tell my followers to expect outsiders to falsely claim that there are no ghosts from Mars. When my followers do confront such contrary claims from outsiders, those contrary claims are exactly what they expected to hear. Thus, new contrary testimony is neutralized, because it was predicted by past beliefs. This, says Begby, functions as a kind of epistemic inoculation. There is also a secondary effect. When my followers hear exactly what I predicted, then my claims have been verified, and so my followers will have some reason to increase their trust in me. Thus, the echo chamber's belief system not only neutralizes the epistemic impact of exposure to outsiders with contrary beliefs; the existence of those contrary beliefs will actively corroborate the pre-emptor and so increase the credence level of the entire echo chamber (Begby 2017). This creates a feedback mechanism within the echo chamber -in making undermining predictions about contrary testimony, inside authorities not only discredit that contrary testimony, but increase their trustworthiness for future predictions. Once such a system of beliefs is set up, it can be very difficult to dislodge -it is selfreinforcing, bounded, and built to discount any contrary input. In fact, though my definition of echo chambers is conceptually separable from such a disagreement-reinforcement mechanism, every plausible real-world candidate for an echo chamber I've ever encountered includes some version of a disagreement-reinforcement mechanism. For a depressing realworld example, consider Pizzagate. Pizzagate is a conspiracy theory that boiled out of a rightwing online forum on Reddit, which included beliefs that Comet Ping Pong, a pizza restaurant, was the site of a child sex trafficking ring owned by a liberal conspiracy involving Hillary Clinton and Barack Obama. Eventually, Edgar Welch, a member of that forum, forcibly entered the pizza parlor armed with an assault rifle to investigate; when he satisfied himself that the restaurant contained no child slaves, he gave himself up to the police. The online forum, however, did not take this as contrary evidence. Instead, they leaned on their belief that the liberal conspiracy had total control of the mainstream media, and was willing to stage fake events to discredit the right-wing. The forum took Welch's claims that there was no sex trafficking ring as evidence that Welch was a paid actor, and thus as further confirmation of the existence of a powerful cabal of liberal child sex traffickers (Mengus 2016;Vogt and Goldman 2016). Conspiracy theories function here in a fascinating inversion to corroborative bootstrapping. In corroborative bootstrapping, the mistake is to treat problematically dependently selected insiders as if they were independent, and thus overweight their testimony. When an echo chamber uses a conspiracy theory in this manner, they are attributing a problematic form of non-independence to outsiders who are actually independent, and thereby underweighting outside testimony. An echo chamber here works by discrediting the apparent independence of, say, different climate change scientists by claiming that all their various testimonies are problematically derived from a single source. Incidentally, I am not claiming here that conspiracy theories are always or necessarily incorrect or the product of epistemic vice. As others have argued, believing in conspiracy theories isn't bad per se, because some conspiracy theories are true (Coady 2012, 110-137;Dentith 2017). But the fact that conspiracy theories can function to reinforce the boundaries of echo chambers -though they do not necessarily do so -might explain part of conspiracy theories' bad rap. Because of their effectiveness in setting up disagreement-reinforcement mechanisms, conspiracy theories are often conscripted as a powerful tool in the bad epistemic behavior of certain groups. It is important to note that the epistemic mechanisms by which echo chambers work, though problematic, are not sui generis. They are perversions of natural, useful, and necessary attitudes of individual and institutional trust. The problem isn't that we trust and distrust groups and institutions. In fact, we must do so. Eljiah Millgram calls it the problem of hyperspecialization. Human knowledge has splintered into a vast set of specialized fields that depend on each other. No one human can manage that information; we are forced to trust each other (Millgram 2015, 27-44). 7 None of us is in a position to reliably identify an expert in 7 Though this paper relies on insights from modern work in the epistemology of testimony, I have tried to rely only on uncontroversial claims from that field, and not on the technical details of any particular view. In particular, I have attempted to construct my analysis so as to be independent of the debate on whether or not testimony is a basic source of knowledge. I have also attempted to make the paper compatible with the major accounts of trust. most specialist fields outside of our own. I am, on my own, helpless to evaluate the virtues of antibiotics or the expertise of a particular doctor or surgeon. I am, instead, reliant on a vast network of institutional licensing practices in order to choose my health care sourcesincluding journal peer review, medical board exams, university hiring practices, and the like (Nguyen 2017a). Often, I trust via what Philip Kitcher calls indirect calibration -I trust mechanical engineers because they make things that work, but I know that mechanical engineers trust applied physicists, and I know that applied physicists trust theoretical physicists, so I acquire trust through a long chain of field-wide links (Kitcher 1993, 320-3). I even use litmus tests: the fact that any person or group is in favor of, say, sexual orientation conversion therapy is enough for me to discredit them on any social or moral topics. We must resort to such tactics in order to navigate the hyper-specialized world (Nguyen forthcoming). Echo chambers function parasitically on these practices by applying discredits without regard for the actual epistemic worth of the discredited institutions or individuals. The discredit is instead applied strategically and defensively, towards all outsiders solely on the basis of their being outsiders. Once the discrediting beliefs are in place, the ensuing beliefs and action the echo chambers' members are surprisingly close to rational. In fact, we can easily imagine alternative scenarios in which a very similar set of beliefs were appropriate and veristic. If I was an anti-Nazi in Germany during the rise of the Nazi party, I would do well to maintain the beliefs that the most people were corrupt, untrustworthy, and out to maliciously undermine my own true beliefs. But if such beliefs become implanted in an inappropriate context, they can lead their believers entirely astray. Epistemic bubbles can easily form accidentally. But the most plausible explanation for the particular features of echo chambers is something more malicious. Echo chambers are excellent tools to maintain, reinforce and expand power through epistemic control. Thus, it is likely (though not necessary) that echo chambers are set up intentionally, or at least maintained for this functionality. My account thus bears some resemblance to some work on testimonial injustice and the epistemology of ignorance, but it is importantly distinct. Miranda Fricker has argued for a kind of testimonial injustice, based on a gap between actual reliability and perceived credibility. For example, says Fricker, being white and being male are both bonuses to credibility. Since credibility is a source of power, anybody with credibility will seek to increase it, using that very credibility. Thus, says Fricker, credibility gaps can be turned into epistemic tools of social oppression (Fricker 2011). Similarly, Charles Mills argues that there is an active practice of ignorance among members of oppressive groups, such as white Americans. It is to the benefit of those in power to actively ignore many aspects of the existence of oppressed groups (Mills 2007;Alcoff 2007, 47-57). My account is compatible with, but independent from, Fricker's and Mills' accounts. Echo chambers can and surely are used to maintain social oppression through enhancing credibility gaps and supporting practices of active ignorance. The systematic mistrust of an echo chambers is a powerful tool for perpetuating epistemic injustice and active ignorance. However, the concept of an echo chamber does not require that they be deployed only in political contexts, nor does it require that they only be deployed only in the service of oppression. Echo chambers could potentially exist among the oppressed, and surely exist in apolitical contexts. I believe I have witnessed echo chambers forming around topics such as anti-vaccination, multi-level marketing programs, particular diets, exercise programs, liberal activism, therapeutic methodologies, philosophies of child-rearing, particular academic subdisciplines, and Crossfit (Weathers 2014).",
          "It has often been claimed, during and after the American political season of 2016, that we have entered a 'post-truth era'. Not only do some political figures seem to speak with a blatant disregard for the facts, their supporters seem unswayed by reason or contrary evidence. To many, it seems as if a vast swath of the electorate has become entirely unmoored from any interest in facts or evidence. Call this the \"total irrationality\" explanation of the post-truth phenomenon. But echo chambers offer an alternative explanation for the apparent post-truth mood. It seems likely that there is at least one vast partisan echo-chamber present in the political landscape. Jamieson and Cappella's study is a decade old, but sources like Breitbart and Alex Jones' Infowars seem like clear extensions of the same right-wing echo chamber. (Other echo chambers surely exist elsewhere on the political spectrum, though, to my mind, the left-wing echo chambers have been unable to exert a similar level of political force.) In that case, the account of echo chambers I've offered has significant explanatory force. The apparent \"posttruth\" attitude can be explained, at least in part, as the result of credence manipulations wrought by echo chambers. In healthy epistemic communities, there is something like an upper ceiling on the credence level accorded to any individual. A healthy epistemic network will supply a steady stream of contrary evidence and counterarguments; thus, no single individual or group will ever go unchallenged. Epistemic bubbles make the discovery of mistakes significantly less likely, and so tend to exaggerate the credence levels of epistemic sources inside the bubble. But when an echo chamber is in place and all outside sources have been effectively discredited, that ceiling disappears categorically. Echo chambers can create runaway credence levels for approved individuals. By removing disconfirmations and discorroboration from the system through the systematic discrediting of outsiders, echo chambers can create exceptionally high -one is tempted to say unnaturally high -levels of trust. That potential for runaway credence is built right into the foundations of any echo chamber, and arises from an interaction between the two main components of any echo chamber. First, an echo chamber involves a significant disparity of trust between the insiders and the outsiders. Second, an echo chamber involves beliefs, espoused by the insiders, reinforcing that disparity. The essential features of echo chambers seem designed to selfreinforce their peculiar arrangement of trust. Notice that epistemic bubbles alone cannot explain the post-truth phenomenon. Since epistemic bubbles work only via coverage gaps, they offer little in the way of explanation for why an individual would reject clear evidence when they actually do encounter it. Coverage gaps cannot explain how somebody could, say, continue to deny the existence of climate change when actually confronted with the overwhelming evidence. One would be tempted, then, to accuse climate change deniers of some kind of brute error. But echo chambers offer an explanation of the phenomenon without resorting to attributions of brute irrationality. Climate change deniers have entered an epistemic structure whereby all outside sources of evidence have been thoroughly discredited. Entering that epistemic structure might itself involve various epistemic mistakes and vices -but here the story can be one of the slow accumulation of minor mistakes, which gradually embed the believer in a self-reinforcing, internally coherent, but ultimately misleading epistemic structure. Similarly, some have suggested that, in the post-truth era, many people's interest in the truth has evaporated. Once again, this account of echo chambers suggests a less damning and more modest explanation. An echo chamber doesn't erode a member's interest in the truth; it merely manipulates their credence levels such that radically different sources and institutions will be considered proper sources of evidence. This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak -deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth. But by my account, we should expect discourse within echo chambers to be entirely differentwe should expect such discourse to be crisp and clear, and to present unambiguous claims about what is the case, what secret conspiracies are in place, and which sources are to be entirely distrusted. And this is precisely what we find (Jamieson and Cappella 2008, 3-41,140-176). Consider, for example, Breitbart's attacks on other media sources. One article begins: \"Mainstream media outlets continue to print false and defamatory descriptions of Breitbart News in a nakedly political effort to marginalize a growing competitor\" (Pollak 2017). This is not the double-speak of administrators and bureaucrats-this is a clear, strident, and unambiguously worded discredit. One might be tempted to say: but just give them the real evidence! You can't discredit neutral evidence! But this response radically underestimates the degree of trust and social processing involved in most presentations of evidence. Except for empirical evidence I myself have gathered, all other presentations of evidence rely on trust. My belief in the reality of climate change depends on enormous amounts of institutional trust. I have not gathered the climate change evidence myself; I mostly just trust science journalists who, in turn, trust institutional credentialing systems. Even if I had been on, say, a core sampling expedition to the Arctic, I would be unable to process that information for myself, or even vet whether somebody else has properly processed it. Even the climatologist who actually processes that information must also depend on trusting a vast array of other experts, including statisticians, chemists, and the programmers of their data analysis software. Most so-called \"neutral evidence\" depends on long and robust chains of trust (Millgram 2015, 27-44). Members of an echo chamber have acquired beliefs which break the usual arrangements of trust. But despite their evident explanatory force, echo chambers have been largely neglected by recent empirical research. Much of the recent research on causes of belief polarization focuses on the causal role of individual psychology, such as the tendency towards laziness in the scrutiny of one's own beliefs (Trouche et al. 2016). Similarly, recent studies on climate change denial focus on studying the relationship between an individual's stated political beliefs and their reactions to climate change information, without inquiring into the social epistemic structures in which the individuals are embedded (Corner, Whitmarsh and Xenias 2012). Famously, Dan Kahan and Donald Braman argue for the cultural cognition thesisthat is, that cultural commitments are prior to factual beliefs, and that non-evidentially formed cultural values inform which future presentations of evidence will be admitted as weighty (Kahan and Braman 2006). Though the values may originally come from an individual's culture, Kahan and Braman focus their analysis on how those acquired values function in individual reasoning to create polarization. They pay little attention to the continuing role of the contingent social structures in which the individual is embedded. The direct literature on echo chambers and epistemic bubbles is new and relatively small, compared to the sizable literature on individual belief polarization. Unfortunately, even in that literature, echo chambers and epistemic bubbles have often been confused. They are usually addressed in the popular media together, and the terms 'epistemic bubble' and 'echo chamber' are typically used interchangeably (El-Bermawy 2016). The same blurring has occurred in the treatment of the phenomena in academic epistemology in the surprisingly small literature on echo chambers. For example, Bert Baumgaertner, in his analysis of echo chambers via computer modeling, lumps together under the heading 'echo chamber' both Though he professes to cover both filter bubbles and echo chambers, his work focuses almost entirely on epistemic bubble effects: constricted information flow, lack of exposure to alternate arguments, and bootstrapped corroboration (Sunstein 2009b(Sunstein , xi,19-06, 2009a, 1-98), 1-98). The point here is about more just than his choice of words: his subjects of analysis include, among other things, Facebook friend groups, hate groups, extremist online political forums, conspiracy theorists, and terrorist groups (99-125, 2009b, 46-96). Clearly, this list includes prime candidates for both epistemic bubbles and echo chambers. But his analysis focuses almost entirely on the effects of bootstrapped corroboration and lack of exposure. For Sunstein, the primary mechanism driving polarization and extremism is the loss of truly public forums, because technology has over-empowered people's tendency to self-select sources offering familiar views. Thus, his solution is to re-create, in the new media environment, the kind of general public forums where people might be more likely to serendipitously encounter contrary views and arguments. His solutions include governmentfunded public news websites with diverse coverage and voluntary work by corporations and individuals to burst their bubbles. His recommendations for repair largely have to do with increasing exposure (Sunstein 2009a(Sunstein , 135-48, 2009b, 19-45,190-211), 19-45,190-211). But, again, if what's going on is actually an echo chamber effect, exposure is useless or worse. The blurring of the two concepts has also lead to some problematic dismissals of the whole cluster of phenomena. A number of recent articles in social science, communications, and media studies have argued that the whole set of worries about bubbles and echo chambers is wildly overstated. These articles share the same argumentative pattern. First, they use the terms \"filter bubble\" and \"echo chamber\" interchangeably, and address themselves to the same cluster of phenomena as Sunstein, treating them as singular. In fact, James Nelson and James Webster conflate Jamieson and Cappella's analysis of echo chambers and Pariser's analysis of filter bubbles, and erroneously attribute to Jamieson and Cappella the view that political partisans only seek out and encounter media from sources with matching political alignments -that is, Nelson and Webster attribute to an epistemic bubbles account to Jamieson and Cappella, where Jamieson and Cappella's actual text is clearly an echo chambers account (Nelson and Webster 2017, 2). More importantly, these recent articles proceed to argue against the existence of filter bubbles and echo chambers by demonstrating that, through the analysis of empirical data about media consumption, most people in fact expose themselves to media from across the political spectrum. Nelson and Webster, for example, argue against Jamieson and Capella, claiming that filter bubbles and echo chambers don't exist. Nelson and Webster support their claim with data showing that both liberals and conservatives visit the same media sites and spend comparable amounts of time at those sites (6-7). Again, this misses the mark -this is evidence only against the existence of epistemic bubbles, and not against the existence of echo chambers. Similarly, Seth Flaxman et al seeks to problematize the existence of filter bubbles and echo chambers with data that social media platforms seem to actually increase people's exposure to media from across the political divide (Flaxman, Goel and Rao 2016). Again, these data only concern the exposure and omission, and only weigh against the existence of epistemic bubbles. They say nothing about whether echo chambers exist. Echo chambers, recall, are structures of strategic discrediting, rather than bad informational connectivity. Echo chambers can exist even when information flows well. In fact, echo chambers should hope that their members are exposed to media from the outside; if the right disagreement reinforcement mechanisms are in place, that exposure will only reinforce the echo chambers' members' allegiance. We ought not conclude then, from data that epistemic bubbles do not exist, that echo chambers also do not exist. We can see now crucial it is to keep these two categories distinct. Epistemic bubbles are rather ramshackle -they go up easily, but they are easy to take down. Since there is no systematic discrediting of outsiders, simple exposure to excluded voices can relieve the problem. Echo chambers, on the other hand, are much harder to escape. Echo chambers can start to seem almost like living things -the belief systems provide structural integrity and resilience. Mere exposure will be ineffective. Jamieson and Cappella offer evidence of this effect: once listeners are caught in Rush Limbaugh's language, framing, and discredentialing of the mainstream media, their beliefs can survive frequent contact with contrary viewpoints. Limbaugh's technique, say Jamieson and Cappella, serves to insulate and inoculate his audience from being affected by exposure to contrary viewpoints (Jamieson and Cappella 2008, 163-190). In fact, if the appropriate disagreement-reinforcement mechanisms are in place, exposure will simply strengthen the attacked belief systems. Thus, an outsider's attempt to break an echo chamber as if it were a mere bubble is likely to backfire and reinforce the echo chamber's grip. 9",
          "So what, then, are the epistemic responsibilities of an agent to discover whether they are in one of these social epistemic traps, and what are their prospects for actually discovering their predicament and successfully escaping? To answer this, we must consider two distinct questions: The escape route question: Is there any way out of an echo chamber or epistemic bubble? The escape responsibility question: Could one behave epistemically virtuously, and yet still remain caught within an echo chamber or epistemic bubble? In other words, to what degree is an epistemic agent embedded within such a structure blameworthy, or blameless, for the faultiness of their beliefs? The first question asks about the possible existence of an escape route. The second asks whether there is an escape route that we might reasonably expect an epistemically virtuous agent to discover and enact. These are distinct questions, because an escape route might turn 9 Sunstein does briefly note the empirical data for the disagreement-reinforcement effect in passing, but then seems to ignore it in proposing his solutions (Sunstein 2009a, 54-5) out to be possible, but so difficult to discover or use that it was beyond what we might reasonably expect of an agent of moderate epistemic virtue. For epistemic bubbles, the answers are straightforward. As I've argued, epistemic bubbles are quite easy to shatter. One just needs exposure to excluded information. Insofar as that information is available, but simply not part of one's standard network, then members of epistemic bubbles are failing to live up to their epistemic duties, which include proactively gathering relevant data. To translate into contemporary terms: if you're subject to an epistemic bubble because you get all your news from Facebook and don't bother to look at other sources, you are, indeed, blameworthy for that failure. If one finds the language of epistemic virtues and vices appealing, then we can say that members of epistemic bubbles are committing the vice of epistemic laziness. Answering these two questions is much more difficult for echo chambers. Recall: where encountering excluded voices and evidence will shatter an epistemic bubble, such encounters are likely to reinforce an echo chamber. Let's grant that intentionally constructing an echo chamber, as Jamieson and Cappella claim that Rush Limbaugh did, is epistemically (and morally) blameworthy. Furthermore, actively entering an echo chamber seems epistemically blameworthy in many circumstances. For agent in full possession of a wide range of informational sources, to abandon most of them and place their trust in an echo chamber for, say, an increased sense of comfort and security, is surely some form of epistemic vice. There is some evidence that this may be the case; Jamieson and Cappella suggest that people enter echo chambers for the sake of the community bonds and the sense of belonging to an ingroup (Jamieson and Cappella 2008, 180). But there are many cases in which the agent seems plausibly blameless. Imagine a person raised in an echo chamber. Their earliest epistemic contacts -let's say their parents, relatives, and close family friends -are all firmly committed members of the echo chamber. Suppose that the child is either home-schooled by those echo chamber members or sent to a school that reinforces the beliefs of that particular echo chamber. I take it that it is reasonable for a child to trust their parents and those of seeming epistemic authority, and that a child is epistemically blameless for having done so (Goldberg 2013). Thus, when that child eventually comes into contact with the larger epistemic world -say, as a teenager -the echo chamber's beliefs are fully in place, such that the teenager discredits all sources outside of their echo chamber. It seems, at first glance, that our teenager could be acting very much like a reasonable epistemic agent. They could, in fact, be epistemically voracious: seeking out new sources, investigating them, and evaluating them using their background beliefs. They investigate the reliability of purported experts and discredit experts when they have apparently good reason to do so, using their background beliefs. Our teenager seems, in fact, to be behaving with many epistemic virtues. They are not at all lazy; they are proactive in seeking out new sources. They are not blindly trusting; they investigate claims of epistemic authority and decide for themselves, using all the evidence and beliefs that they presently accept, whether to accept or deny the purported expertise of others. They have theories, which they have acquired by reasonable methods, predicting the maliciousness of outsiders; they increase their trust in those theories when their predictions are confirmed. 10The worry here is that agents raised within an echo chamber are, through no fault of their own, epistemically trapped -their earnest attempts at good epistemic practices are transformed into something epistemically harmful by the social structure into which they have been embedded and which they have ingested. Paul Smart has argued for the possibility of a transformative social epistemic phenomenon which he dubs \"mandevillian intelligence,\" in honor of Bernard Mandeville. Mandeville argued that, in the right social context, individual vices could lead to collective economic prosperity. For a certain kind of economic theorist, capitalism is such a transformative structure -individuals act selfishly, but the structure of the market transforms that selfishness into virtuous collective action. According to Smart, there is an epistemic analog: the mandevillian intelligence, which transforms the individual epistemic vices of its members into a collective epistemic virtue by virtue of the social structure into which they are embedded (Smart 2017). Intellectual stubbornness, for example, might be an intellectual vice for individuals. But set those stubborn individuals in a properly arranged social structure (like, perhaps, academia) and you might get a collective system that properly explores every relevant nook and cranny with optimal thoroughness. But echo chambers are the very opposite; they are reverse-mandevillian intelligences. Echo chambers are social epistemic structures which convert individually epistemically virtuous activity into collective epistemic vice. In fact, the reverse-mandevillian nature contributes to the stickiness of the echo chamber trap. If our teenager self-reflects on their epistemic practices, what they will see might be rather gratifying. Their epistemic behavior might very well be earnest, vigorous, and engaged. It is their external context -the social epistemic system into which they have been unluckily raised -which makes such behavior problematic. Contrast this account with Quassim Cassam's treatment of Oliver, his fictional 9/11 conspiracy theorist. Oliver believes that the collapse of the twin towers was an inside job, and he is happy to provide reasons and point to supporting evidence from a great many conspiracy theorist websites. Says Cassam: Oliver is obviously mistaken -Oliver relies on outrageous, baseless claims from clearly discredited sources. The best explanation for Oliver's beliefs is in terms of epistemic vice -that is, in terms of Oliver's bad intellectual character traits. Oliver is \"gullible, cynical, and prejudiced,\" says Cassam. Oliver is gullible with regard to his conspiracy theorist sites, cynical with regard to the mainstream media, and his prejudice consists of, among other things, intellectual pride, wishful thinking, closedmindedness, and a lack of thoroughness (Cassam 2016, 162-4). And I certainly grant that such epistemic vices can lead to these sorts of beliefs. But the story of our hapless teenager offers an alternate epistemic path to such beliefs and such narrowcasted trust -one in which epistemically virtuous character traits have been wrong-footed by the social epistemic structure in which the agent has been embedded. The crucial difference between the reversemandevillian account and Cassam's account is where the brunt of the responsibility lies. In Cassam's account, the responsibility lies with the individual, and their own intellectual habits and practices. 11 In a reverse-mandevillian account, a significant part of the responsibility lies with the social structure in which the actors are embedded. The epistemic vice is a feature of the collective intelligence, rather than of the individual. Or, if one is averse to thinking in terms of collective intelligences, here's a conceptually minimal way to put the claim: echo chambers are local background conditions that turn generally good epistemic practices into locally unreliable ones. But the possibility of a truly faultless epistemic agent, wholly misled by an echo chamber, also depends on the lack of an accessible escape route. So: are there escape routes from an 11 Note, however, that Cassam distinguishes between epistemic responsibility and epistemic blameworthiness, and does not take blameworthiness to necessarily follow from responsibility (168-9). Cassam leaves room for the view that the individual's intellectual vices were epistemically responsible for their bad beliefs, but that the individual wasn't blameworthy for those vices, because the vices were inculcated in them at an early age. However, my complaint still stands, for I contest Cassam's claim that the responsibility is in the individual. echo chamber, and how reasonable is it to expect echo chamber members to discover and make use of them? Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass. Thus, their critical reflection is likely to reinforce previously held beliefs. Kelly notes that the belief polarization violates the Commutativity of Evidence Principle: The Commutativity of Evidence Principle: to the extent that what it is reasonable for one to believe depends on one's total evidence, historical facts about the order in which that evidence is acquired make no difference to what it is reasonable for one to believe. (Kelly 2008, 616) In short, belief polarization makes it matter what order they received the evidence, but the historical ordering of evidence ought not matter. Note that our epistemically hapless teenager has also violated the Commutativity of Evidence Principle. For them, it matters very much what order that they received the evidence. If they had been raised outside the echo chamber and fed a broader diet of epistemic sources before encountering the echo chamber, then they would likely have found the echo chamber's world-view to be problematic. But since our teenager encountered the echo chamber and assimilated its beliefs first, their use of background beliefs to vet new sources leads them to continually increase their trust in the echo chamber and their distrust of outsiders. Even if our echo chambered teenager eventually came to encounter all the same evidence as their epistemically free-range counterpart, their early education within the echo chamber would be decisive. So long as each new piece of evidence is assessed using the currently held set of beliefs, then early education in an echo chamber becomes domineeringly powerful. However, the Commutativity of Evidence Principle suggests a way out. In order to free themselves of the echo chamber's grip, our teenager needs to undo the influence of the historical ordering of their encounters with the evidence. How could they possibly do this? Our teenager would have to suspend belief in all their particular background knowledge and restart the knowledge-gathering process, treating all testimony as equally viable. They would need to, in a sense, throw away all their beliefs and start over again. This suggests a process that, in its outlines, might sound awfully familiar. Our escape route turns out to be a something like a modified version of Descartes' infamous method. What proceeds from this point is admittedly something of a fantasy, but perhaps it is a fantasy from which we can eventually draw some sort of moral. The story of the history of Western epistemology might be cartoonishly summarized thusly: Descartes had a dream of radical intellectual autonomy. By his accounting, he came to realize that many of the beliefs he had acquired in his early life were false, and that those early false beliefs might have infected any number of other beliefs. His response was that famed method: to get rid of his beliefs and start over again, trusting no-one and nothing and only permitting those beliefs of which he was entirely certain. Call this the Cartesian epistemic reboot. But if recent epistemology has taught us anything, it's that this total reboot is nothing but a pipe dream. Any sort of reasonable epistemic life is essentially impossible without trusting the testimony of others (Burge 1993;Faulkner 2000;Goldberg 2010;Zagzebski 2012;Hardwig 1985Hardwig , 1991)). But recall that the reason Descartes wanted to discard everything and start over from scratch -the motivation for his project, and not the method -was explained in the very first line of \"Meditation 1\": He was worried by the falsehoods he had learned in childhood and the shakiness of the edifice that had been built from those falsehoods (Descartes 1984, 24). Our teenager faces a problem quite similar in structure. The credentialing structure of their upbringing is flawed; that credentialing structure has influenced any number of their other beliefs, and the degree of that influence is impossible to track. Furthermore, these later beliefs, approved by the echo chambers' credentialed sources, will often serve to reinforce that credentialing structure. The pernicious effect of an echo chamber cannot be attacked one belief at a time. Any single belief that our teenager re-considered would come under the influence of the network of the flawed background beliefs that sustains an echo chamber. What they need is some way to start over. In order to undo the influence of historical ordering, an epistemic agent will have to temporarily suspend belief in all their beliefs, in particular their credentialing beliefs, and start from scratch. But when they start from scratch, they need not disregard the testimony of others, nor need they hold to Descartes' stringent demand for certainty. Let's call this procedure the social epistemic reboot. In the social epistemic reboot, the agent is permitted, during the belief re-acquisition process, to trust that things are as they seem and to trust in the testimony of others. But they must begin afresh socially, by re-considering all testimonial sources with presumptive equanimity, without deploying their previous credentialing beliefs. Furthermore, they must discard all their other background beliefs, because those potentially arose from the flawed credential structure of the echo chamber, and very likely have been designed to support and reinforce that very credential structure. Our rebooter must take on the social epistemic posture that we might expect of a cognitive newborn: one of tentative, but defeasible, trust in all apparent testimonial sources (Burge 1993) (Nguyen 2011). This method will, if successfully applied, undo the historical dependence of our epistemic agent and remove the undue influence of the echo chamber. The social epistemic reboot is, theoretically at least, the escape route we've been searching for. 12This reboot, described in such clinical terms, might seem rather fantastical. But it is not, I think, utterly unrealistic. Consider the stories of actual escapees from echo chambers. Take, for example, the story of Derek Black, who was raised by a neo-Nazi father, groomed from childhood to be a neo-Nazi leader, and who became a teenaged breakout star of white nationalist talk radio. When Black left the movement, he went through years-long process of self-transformation. He had to completely abandon his belief system, and he spent years rebuilding a world-view of his own, immersing himself broadly and open-mindedly in everything he'd missed -pop culture, Arabic literature, the pronouncements of the mainstream media and the US government, rap -all with an overall attitude of trust (Saslow 2016). Of course, all we have shown so far is that the social epistemic reboot would, if pulled off, undo the effects of an echo chambered upbringing. Whether or not an epistemic agent might reasonably be expected to reboot, or blameworthy for failing to reboot, is a separate and significantly more difficult question. First, a social epistemic reboot might be psychologically impossible, or at least beyond what we could reasonably expect of normal epistemic agents. Second, what reason would an epistemic agent have to undertake a social epistemic reboot? Such an undertaking would be justified only if the agent had a significant reason to think that their belief system was systematically flawed. But echo chamber members don't seem likely to have access to any such apparent reason. After all, they have clear and coherent explanations for all the evidence and testimony they encounter. If this is all right then we arrive at a worrying conclusion: that echo chambers may, theoretically, be escapable, but we have little reason to expect members of echo chambers realize that they are members of something that needs escaping. What could hope do we have, then, of motivating a reboot? Derek Black's own story gives us a hint. Black went to college and was shunned by almost everyone in his college community. But then Matthew Stevenson, a Jewish fellow undergraduate, began to invite Black to his Shabbat dinners. Stevenson was unfailingly kind, open, and generous, and he slowly earned Black's trust. This eventually lead to a massive upheaval for Black -a slow dawning realization of the depths to which he had been systematically misled. Black went through a profound transformation, and is now an anti-Nazi spokesperson. The turning point seems to be precisely that Stevenson, an outsider, gained Black's trust. And this is exactly where we should expect the turning point to be. Since echo chambers work by building distrust towards outside members, then the route to unmaking them should involve cultivating trust between echo chamber members and outsiders. In order to motivate the social epistemic reboot, an echo chamber member needs to become aware of how in the echo chamber's grip they are, and forming a trust relationship with an outsider might could mediate that awareness. But how that trust could be reliably cultivated is a very difficult matter, and a topic for future investigation. We have, however, arrived at a tentative moral of the story. Echo chambers work by a manipulation of trust. Thus, the route to undoing their influence is not through direct exposure to supposedly neutral facts and information; those sources have been preemptively undermined. It is to address the structures of discredit --to"
        ],
        "ground_truth_definitions": {
          "epistemic bubble": {
            "definition": "a social epistemic structure in which some relevant voices have been excluded through omission.",
            "context": "However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation.",
            "type": "explicit"
          },
          "echo chamber": {
            "definition": "a social epistemic structure in which other relevant voices have been actively discredited.",
            "context": "But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation.",
            "type": "explicit"
          },
          "belief polarization": {
            "definition": "the tendency of individuals, once they believe that p, to increase their belief that p.",
            "context": "Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass.",
            "type": "explicit"
          },
          "orwellian double speak": {
            "definition": "deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker.",
            "context": "This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak - deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 π r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP α for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return α (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "26af8e81a7ca5b137912acb81d8c19f38fce0200",
        "sections": [
          "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
          "Howard's research on the \"chlorine sunset\" controversy is based on interviews and document analysis. He conducted twenty-seven semistructured interviews, lasting an hour on average, with staff members of federal regulatory agencies in the United States and Canada, staff members of the International Joint Commission (IJC), members of the Great Lakes Science Advisory Board, staff members or individuals otherwise associated with nongovernmental organizations (NGOs), academic or governmental members of the industrial ecology or green chemistry communities, and industrial chemists in industry and academia. A number of transcripts were supplemented with additional information from follow-up correspondence. Documents analyzed included (1) reports, press releases, Web documents, and other materials published by NGOs, the chemical industry, and federal agencies; (2) articles and commentaries in newspapers and popular and trade magazines; (3) research articles and commentaries in scholarly anthologies and peer-reviewed scholarly journals; (4) books written by key actors; and (5) transcripts of Congressional testimony. A little-studied controversy involving one of the major branches of industrial chemistry documents a striking example of undone science and illustrates the role it can play in structuring conflict between competing regulatory paradigms. Much of the controversy has centered on the Great Lakes region, where extensive chemical manufacturing and contamination has occurred; where scientists have documented threats to wildlife and humans from persistent, toxic, industrial chlorinated pollutants; where extensive citizen activism has emerged around this threat; and where a quasigovernmental advisory body has assumed a leadership role in addressing this concern (Botts et al. 2001). A number of environmental and health advocates have argued, based both on fundamental toxicology and on long historical experience with chlorinated synthetic chemicals (e.g., DDT and PCBs), that the entire class of thousands of such substances should be tentatively presumed dangerous and that the chemical industry accordingly should wean itself from most major uses of chlorine (Thornton 1991(Thornton , 2000;;International Joint Commission [IJC] 1992; see Howard 2004). The analysis offered here briefly considers the character and function of undone science in the debate provoked by proposals for a \"chlorine sunset.\" The chlorine sunset controversy revolves around conflict between two sharply contrasting regulatory paradigms: risk and precaution (Thornton 2000;Howard 2004). The powerful chemical industry has coevolved with, supports, and is supported by the dominant U.S. and Canadian environmental regulatory regime, which restricts chemical industry decision making only to the extent that detailed calculation of risk indicts individual chemical substances. Meanwhile, Greenpeace, a marginalized, reputedly radical environmental NGO, and the IJC, a prominent but marginalized binational advisory organization, argued for a regulatory regime based on the precautionary principle (see Tickner 2003), which in their view justified governmental action against an entire class of industrial chemicals. The dominant paradigm assumes the unit of analysis to be the individual substance and places the burden of proof on the public to prove harm; in contrast, the challenger paradigm allows, even requires, the primary unit of analysis to be the entire class of substances and places the burden of proof on corporate officials. Within this matrix of political and epistemological conflict, the political economy and political sociology of undone science can be seen to revolve around a series of three dyads, each paradigm implying parallel formulations of \"done science\" and undone science. The three dyads are summarized in Table 1. One dyad appears in the context of health impacts research. Industry and federal officials operating in the risk paradigm hold that the legitimate goal of health impacts research performed or mandated by government is ad hoc identification of individual chlorinated chemicals that cannot be safely manufactured and used. In this paradigm, chlorine chemistry itself is seen as immune to fundamental interrogation; the role of public science is limited to documenting the odd substance that can be definitively proven harmful and, on that basis, restricted. \"We've made the point over and over again that you have to look at each product's physical and chemical characteristics to draw conclusions about what it is going to do in the environment,\" argued Brad Lienhart, of the Chlorine Chemistry Council. To do otherwise would be to \"[make] non-science-or nonsense-into science\" (quoted in Sheridan 1994, 50). Beginning in the early 1990s, \"sunset\" proponents vigorously argued that such research is incapable of interrupting a long series of chlorinated \"Pandora's poisons\" from entering the environment and human tissues long before their deleterious effects are documented. Inevitably remaining undone, they argued, is science capable of systematically identifying unsafe chemicals from among tens, perhaps hundreds, of thousands of chlorinated industrial substances, by-products, and breakdown products, a scope of research that the risk paradigm is sometimes assumed to provide but, owing to the sheer enormity of the undertaking, cannot. The government's effort to identify unsafe chlorinated chemicals is ad hoc precisely because it cannot, in any meaningful sense, be systematic; not only are available resources insufficient, but the enterprise is technically infeasible. Viewed in this light, the science is undoable. The IJC argued: There is a growing body of evidence that [suggests that] these compounds are at best foreign to maintaining ecosystem integrity and quite probably persistent and toxic and harmful to health. They are produced in conjunction with proven persistent toxic substances. In practice, the mix and exact nature of these various compounds cannot be precisely predicted or controlled in production processes. Thus, it is prudent, sensible and indeed necessary to treat these substances as a class rather than as a series of isolated, individual chemicals. (IJC 1992, 29) A second dyad appears in the risk paradigm's stance on innovation. Industry has systematically pursued the development of chlorine chemistry, developing chlorinated chemicals and expanding markets for them; meanwhile, advocates of chlorine precaution have pointed to the need to systematically develop nonchlorine alternatives. This is in part science that the risk paradigm has long left undone-historical research and development trajectories that could have led to a wider range of nonchlorine chemicals and processes being available today. The implication of the historical analysis offered by a leading sunset proponent (Thornton 2000; see also Stringer and Johnston 2001) is that over the past century the technological, economic, and political momentum of chlorine chemistry has to some extent bent the overall industry research and development agenda toward chlorine and away from nonchlorine alternatives. Here undone science consists of a body of nonchlorine chemicals and processes that might now exist but for the long dominance of research and development predicated on chlorine. It is a point seemingly acknowledged by a confidential IJC informant who did not support the commission's sunset recommendation: \"There's no reason why we couldn't, as a global society, live a non-chlorine lifestyle. It's just, you know <laughs>, that ain't gonna happen, because that is not our history! We're kind of, in a way, captives of our past.\" In the risk paradigm, with its laissez-faire orientation, such research and development need not be undertaken by the industry but instead is tacitly left to whichever agency or organization might care to undertake it. Viewed from the vantage point of the industry, with its adamantine conception of chlorine chemistry as technologically and economically inevitable, the only conceivable motivation for conducting such research and development would be some kind of ideological fetish (see, e.g., Chlorine Chemistry Council n.d.). It would represent \"a veiled attempt to return to a pre-industrial Eden,\" one industry supporter suggested (Amato 1993). Crucially, although this agenda would have been and would now be technically feasible, such research would be hobbled by the absence of a sizable cadre of technoscientists devoted to the project and by a lack of financial resources to sustain the effort. A third dyad occurs within the challenger, precautionary paradigm and directly counters the values and priorities of the dominant paradigm's dyads. Paired with precaution advocates' assertion of the need for research to systematically develop nonchlorine alternatives-here seen as industry's responsibility rather than the public's-is an explicit assertion that industry should assume the burden of making the case for any specific chlorinated chemicals (or chemical processes) that can be demonstrated to be both essential (i.e., nonsubstitutable) and capable of being manufactured and used in ways that (to some as yet unstated standard) pose no significant environmental hazard. Industry's motivation for undertaking this latter effort would, of course, be profit. And owing to the presumably quite limited number of substances to be evaluated, it would be both technically feasible and, given the industry's substantial financial and technical resources, affordable. The chlorine sunset controversy is now effectively dormant. In the face of bitter industry resistance and U.S. and Canadian governmental intransigence, the IJC and Greenpeace ceased promoting their sunset recommendations in the mid-1990s (Howard 2004). Thornton's book, which appeared in 2000, reawakened (and in significant ways deepened) the debate, but it did so only briefly. The sunset proposals have not visibly shifted policy at any level in North America. A major international treaty on persistent organic pollutants signed in 2001 represented an important victory for activists, but it also underscored the lingering, unresolved character of the chlorine debate: all twelve of the \"dirty dozen\" substances it required to be phased out are chlorinated compounds, and each was targeted on the basis of its discreet, well-documented characteristics. Meanwhile, thousands of far less extensively studied chlorinated chemicals-and chlorine chemistry as a whole-remain unregulated. This analysis of the chlorine sunset controversy illustrates how regulatory regimes influence the construction and articulation of research priorities. In this case, advocates of the risk and precaution paradigms, on the basis of competing understandings of the appropriate unit of regulatory analysis and appropriate regulatory burden of proof, promote competing conceptualizations of science both done and undone. More specifically, the case suggests that done and undone science in such a controversy can be understood as occurring in dyadic pairs and that a major role for challenger discourses is making the implicit undone portion of dyads within the dominant paradigm visible and explicit. This analysis also highlights an important category of undone science in technoscience controversies-undoable sciencethat improves understanding of how regulatory regimes constrain the identification of undone science. Here, close examination of precautionary advocates' critique of the risk paradigm clarifies the process through which conventional regulatory structures veil undoable science in the form of systematic research for which insufficient resources and insufficient technical means are available.",
          "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (Löwy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] £1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center … there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life … and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't … appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
          "Kempner's research is based on an interview study that examines \"forbidden knowledge,\" a term used to capture scientists' decisions not to produce research because they believe it to be taboo, too contentious, or politically sensitive (a type of negative knowledge in the terminology introduced above). In 2002-2003, she and colleagues conducted ten pilot and forty-one in-depth, semistructured telephone interviews with a sample of researchers drawn from prestigious U.S. universities and representing a diverse range of disciplines, including neuroscience, microbiology, industrial/organizational psychology, sociology, and drug and alcohol research (Kempner, Perlis, and Merz 2005). Those fields were chosen to gauge the range, rather than the prevalence, of experiences with forbidden knowledge. Interviews lasted between thirty-five and forty-five minutes and were audiotaped, transcribed, coded, and analyzed according to the principles of grounded theory (Strauss and Corbin 1990). While many social movements organize around the identification and completion of undone science, others devote themselves to making sure that some kinds of knowledge are never produced. They are not alone. The idea that some knowledge ought to be forbidden is deeply embedded in Western cultures and appears in literature through the ages, from Adam and Eve's expulsion in Genesis to Dr. Frankenstein's struggle with a monster of his own creation (Shattuck 1996). Mertonian rhetoric aside, most people agree that some science poses unacceptable dangers to research subjects or to society at large. The widely accepted Nuremberg Code, for example, places strict limits on human experimentation, in an effort to ensure that some science-such as Nazi human experimentation in World War II-is never done again. Determining which knowledge ought to remain undone can often be contentious, as illustrated by current high-profile public debates surrounding the ethics and implications of stem cell research and cloning technologies. Nevertheless, as in research agenda-setting arenas (Hess 2007), debates and decisions about what knowledge should remain off limits to the scientific community typically occur among elites: legislators and federal agencies perennially issue guidelines and mandates regarding which research should not be conducted, setting limits on everything from reproductive and therapeutic cloning to studies of the psychological effects of Schedule I drugs, like heroin and marijuana. Scientists and the lay public both have limited opportunities to voice their opinion in these discussions. In dramatic cases, scientists have attempted to preempt mandates via self-regulation, as was the case in 1975 when scientists meeting at Asilomar called for a moratorium on certain kinds of recombinant DNA research (Holton and Morrison 1979). According to the forty-one elite researchers interviewed for this case study, these formal mechanisms account for only a portion of the limitations that can produce undone science (Kempner, Perlis, and Merz 2005). More often, researchers described how their research had been hamstrung by informal constraints-the noncodified, tacit rules of what could not be researched or written. Yet researchers were very clear about what constituted \"forbidden knowledge\" in their respective fields. The boundaries of what could not be done had been made known to them when either they or a colleague's work had been targeted for rebukein essence, their work had breached an unwritten rule. The management of forbidden knowledge, thus, worked much as Durkheim said it would: once someone's research had been identified as especially problematic by, for example, a group of activists, their work became a \"cautionary tale,\" warning others \"not to go there\" (Kempner, Bosk, and Merz 2008). In this way, social movement organizations and activists are able to play an important role in debates about what ought to remain undone, whether or not they are invited to the table. Besides their influence on shaping research agenda-setting arenas, social movements can and do influence individual researchers' decisions not to pursue particular types of studies. In recent decades, for example, animal rights organizations have had an enormous influence on the kinds of research that scientists choose not to produce. We found that the researchers in our sample who work with animal models took seriously the threat posed by those organizations. They spoke of \"terrorist-type attacks\" and told stories of colleagues who received \"razor blades in envelopes\" and \"threatening letters.\" Others faced activists who staked out at their houses. Researchers learned from these cautionary tales and, in many cases, said that they had self-censored as a result. One researcher, for example, explained that he would not work with primates-only \"lower order\" animals like mice and drosophilia because: I would like to lunatic-proof my life as much as possible … I, for one, do not want to do work that would attract the particular attention of terrorists … The paranoia was acute. One researcher refused to talk to the interviewer until she proved her institutional affiliation: \"For all I know, you are somebody from an animal rights organization, and you're trying to find out whatever you can before you come and storm the place.\" Over time, the overt interventions of animal rights organizations in the production of research have redefined the ethics of animal research, ushering in legislation like the Animal Welfare Act of 1985, which requires research institutions that receive federal funding to maintain \"Institutional Animal Care and Use Committees\" (Jasper and Nelkin 1992). However, lay groups do not need to use such directly confrontational tactics to influence researchers' decisions, especially if the groups are successful in their attempts to reframe a particular social problem. For example, substance abuse researchers argued that their research agendas were limited by the success of the Alcoholics Anonymous' campaign to define treatment for alcoholism as lifelong abstinence from drink. Although these researchers would like to conduct \"controlled drinking\" trials, in which alcoholics are taught to drink in moderation, they argued that \"There's a strong political segment of the population in the United States that without understanding the issues just considers the goal of controlled alcohol abuse to be totally taboo.\" The mere threat of interference from the grassroots was enough to keep many researchers from conducting certain studies. Several drug and alcohol researchers described great unwillingness to conduct studies on the health benefits of \"harm reduction\" programs, such as those that distribute free condoms in schools or clean needles in neighborhoods, because they might attract unwanted controversy from lay groups who oppose such public health interventions. Thus, in some contrast to the role that social movement organizations and lay experts/citizen scientists play in exposing undone science and encouraging knowledge creation in chemical, air monitoring, and breast cancer research, this study shows that the same actors can also play a powerful role in determining which knowledge is not produced. Moreover, conflict over the direction of funding streams, while critically important to the political of research agenda setting, do not solely determine what science is left undone. Rather, social movements are also effective beyond research agenda-setting processes that occur at the institutional level; this study provides evidence that they also shape the microlevel interactional cues and decision-making process of individual scientists. Although more research is needed to understand the circumstances under which researchers decide to selfcensor in response to pressure from outside groups, this case suggests that social movements may have much greater potential to thwart research than originally thought. The implications are intriguing and deserve greater attention. On one hand, disempowered groups may leverage these techniques to gain a voice in a system of knowledge from which they are typically excluded. On the other hand, it is troubling to learn that the subsequent \"chilling effect\" happens privately, often without public discussion and in response to intimidation and fear.",
          "The diverse cases provide an empirical basis for moving forward the theoretical conceptualization of undone science in relation to a new political sociology of science and that program's concern with how research agendas are established. Perhaps the most significant general observation is that the identification of undone science is part of a broader politics of knowledge, wherein multiple and competing groups-including academic scientists, government funders, industry, and civil society organizations-struggle over the construction and implementation of alternative research agendas. To a large extent, our case studies focus on attempts by civil society or quasigovernmental organizations to identify areas of research they feel should be targeted for more research. However, the identification of undone science can also involve claims about which lines of inquiry should warrant less attention than they currently receive, either because there are decreasing social returns on continued investments in heavily researched areas or because the knowledge is deemed not worth exploring and possibly dangerous or socially harmful-what Gross (2007) calls \"negative knowledge.\" Examples of the latter include the research programs and methods targeted by animal rights groups and research on chlorinated chemicals targeted by Greenpeace. There are many other cases that would fit this role for civil society organizations, including calls for research moratoria on weapons development, genetically modified food, nuclear energy, and nanotechnology. Five more specific insights follow from and add complexity to this general observation. First, while we see undone science as unfolding through conflict among actors positioned within a multiorganizational field, as Gibbons' case shows, definitions of undone science may also vary significantly within different organizational actors, coalitions, or social movements. Some portions of the movement may be captured by mainstream research, and consequently advocacy is channeled into support for the experts' prioritizations of research agendas. Thus, a research topic such as environmental etiologies of breast cancer may represent undone science to a marginalized segment of breast cancer advocates and their allies in the scientific community, but it may represent negative knowledge to the majority of breast cancer advocates and the dominant cancer research networks. To further complicate the picture, rapid developments and changes within the scientific field, such as the development of genomic research to better pinpoint environmental or epigenetic factors, may result in shifts in research priorities that can open up opportunities for research in areas of undone science. Here, one sees that internal changes and differences among both researchers and civil society advocates interact to define shifting coalitions of research priorities. Second, the dynamic nature of coalitions and alliances that emerge around undone science suggests that the articulation of research priorities is often a relatively fluid process; even when civil society groups target some areas of scientific research as deserving low or no priority, their views may in turn lead to the identification of other areas of research deserving higher priority. For example, the position of an animal rights group may begin with opposition to some types of animal research but lead to support for more \"humane\" forms of animal research that have been reviewed by animal research committees. Likewise, the position of an organization such as Greenpeace in opposition to chlorinated chemicals is linked to an articulation of the need for research on green chemistry alternatives. As these examples suggest, the identification of undone science can be viewed as multifaceted outcomes of coalitions and conflict among diverse groups representing various social categories, each promoting a mix of topics seen as deserving more, less, or no attention from the scientific community. Third, making sense of the complex processes that produce undone science involves attending to the distributions of power, resources, and opportunities that structure agenda setting within the scientific field. An important element of field structure is the role of regulatory regimes in shaping definitional conflicts over research priorities. Howard's work suggests that done and undone environmental science dyads can be a key expression of the regulatory paradigm in which they occur and intimately linked to the way expertise is conceptualized and deployed in the paradigm. Furthermore, he proposes that until mainstream science faces a challenger, important forms of undone science within the dominant paradigm can remain implicit and unarticulated. In other words, undone science may take the form of a latent scientific potential that is suppressed through \"mobilization of bias\" (Lukes 2005; see also Frickel and Vincent 2007). Ottinger (2005) also notes the important role of regulatory standards in defining opportunities for activists who attempt to get undone science done largely using their own resources. In the case of air monitoring devices, an alternative research protocol and data gathering device operated by laypeople provides a basis for challenging official assurances of air quality safety. Rather than advocate for shifts in a research agenda, they simply enact the shift. In Howard's terms, the lay research projects also dramatize the implicit and unarticulated bias in the dominant method of air quality monitoring. Ottinger's (2005) focus on the double role of standards as enabling and constraining factors in establishing both the conditions and limitations of undone science is intriguing, and it remains for future research to examine the efficacy of tactical dynamics in relation to structural constraints encountered across a range of regulatory and research contexts. Fourth, while access to financial resources is an implicit focus of efforts to identify undone science, Kempner's research demonstrates that the interaction of civil society and research priorities is not restricted to the broad issue of funding. Although civil society organizations can exert an effect on research funding allocations, as we have seen especially in environmental and health research priorities, Kempner notes that there are other mechanisms that can cause such shifts. Her work suggests that efforts to study the problem of undone science should also consider the role that a moral economy has in shaping scientists' decisions about what research programs they will and will not pursue (Thompson 1971; on moral economy in science, see Kohler 1994). Furthermore, even if scientists do not accept in principle the notion that certain knowledge should remain undone, they may simply decide not to invest in some areas of research because of intense direct pressures from civil society organizations such as animal rights groups. As a result of individual decisions not to engage in an area of research, changes in the research agendas of a field can occur even when funding is not shifting dramatically. Finally, sometimes structural constraints such as limited access to resources coincide with practical constraints to produce \"undoable science.\" In the case of the chlorine sunset provisions, precaution advocates see governmental programs for screening individual chemicals as obscuring a plain fact: the sheer number of chemicals and their complex interaction with ecological and biological systems make it impossible to predict whether a given concentration of a given chemical will in any meaningful sense be \"safe\" or whether it will be a risk. As a result of this \"wicked problem\" (Rittel and Weber 1973), the articulation of undone science as a goal for research prioritization and funding-in this case, the standard assumption of a need for ever more research on the environmental, health, and safety implications of new chemicals-turns against itself, because the call for research into specific chemicals tacitly supports a regulatory framework that systematically generates a policy failure (see Beck 1995).",
          "This study demonstrates some of the ways in which the analysis of undone science can enrich empirical understandings of research agenda-setting processes. The considerable variation we find in just four cases suggests that one promising avenue for future research lies in developing more systematic comparisons across academic, government, industry, and community settings. Doing so will further elaborate the ways in which the institutional contexts of research-including different sets of political and economic pressures, normative expectations, resource concentrations, and sizes and configurations of research networksshape the articulation of undone science and the successful or failed implementation of alternative research agendas. Our broader aim in seeking to give undone science higher visibility within STS is to broaden the foundations for a new political sociology of science. Much like feminist and antiracist science studies, the political sociology of science situates questions relating to the uneven distribution of power and resources in science at the center of the STS project while remaining attentive to how knowledge and its inverse-ignorance-is socially shaped, constructed, and contested. As we have argued here, one of the crucial sites where questions of power, knowledge, and ignorance come together is in the domain of research agenda setting, where intense coalitions and conflicts are forged to gain access to the limited resources that ultimately shape what science is done and what remains undone. ",
          ""
        ],
        "ground_truth_definitions": {
          "undone science": {
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.",
            "context": "What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "c84a169e6df175c4662012d3ba7dbf8fa1b5abc9",
        "sections": [
          "In the past couple of years, the debate concerning so-called 'fake news' has prominently surfaced in news and politicians' speeches around the world. This catchy but fuzzy tag (European Commission, 2018) has mainly been used to describe misleading content disseminating across social media, but it has also been invoked by political actors to discredit some news organizations' critical reporting (Caplan et al., 2018;Tandoc et al., 2018). Alarmed by the simplification of this buzzword, researchers have started to unpack the concept, defining diverse types of 'fake news' (Allcott and Gentzkow, 2017;Jack, 2017;Marwick and Lewis, 2017;Warde, 2017) in order to understand the implications and solutions. In common with classic studies on misleading information dealing with rumours, propaganda and conspiracy theories (Allport and Postman, 1946;Lasswell, 1927;Sunstein and Vermeule, 2009), these most recent attempts to define 'fake news' seek to differentiate between specific forms of the phenomenon with reference to the source's intent to deceive (disinformation) versus the honest mistakes, negligence, or unconscious biases (misinformation) (Fallis, 2015;Floridi, 2011). In other words, the act of creating and injecting 'fake news' in the system is a defining moment for most classic and contemporary studies. In this article, we suggest a radical change of perspective. Such change is driven by deep transformations characterizing contemporary news systems, wherein older and newer media actors -with different degrees of potential reach, epistemological authority and skills of media manipulation -operate on the basis of overlapping and competing media logics. In this 'hybrid news system' (Chadwick, 2013), judgements with regard to the falsehood and motivations of propagators (the actors who share the fake news) can easily be different from the motivations of the original creator. Such patterns mean that what happens after the 'generative act' of a piece of false news is crucial to the study of real-world cases. The radical change of perspective required by the hybrid news system thus consists of a shift from exclusive attention to producers of 'fake news' to a broader approach that also focuses on propagators and, as a result, on the dynamic and diverse processes that characterize the dissemination of problematic information through multiple chains of propagation. On a theoretical level, our proposal is informed by the conceptual framework of sociocybernetics with specific reference to 'second-order cybernetics'. According to this approach -originally developed by Heinz von Foerster, whose aphorism 'Truth: The invention of a liar' has inspired the title of this article -information is not something that can be transmitted, stored, or retrieved in itself; it only exists 'when looked upon by a human mind' (von Foerster, 2003: 103). Following what von Foerster said about truth, the title of this article retains and twists the double meaning of its original inspiration insofar as it not only indicates simply and obviously that a piece of false information is invented by a liar. It also intrinsically turns its pronouncer into a liar, i.e. someone who indirectly claims to speak the truth by asserting that something is false. In this sense, the term 'fake news' is inherently divisive and detrimental to healthy debates. Information is an eminently social process: it is 'a difference which makes a difference' for an observer (Bateson, 1972: 321). What is informative for one observer can be uninformative for another observer. Interests, backgrounds, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. We contend that, in the hybrid media system, each 'fake news' cycle can only be studied as a unique process that emerges from multiple combinations of judgements on the falsehood of the news. Such judgements are made by all the diverse actors who decide to inject and share it, following logics that are related to their perception of the source, of the story itself and of the context. In order to describe our analytical model of problematic information, in the next section we trace back the notion of 'fake news' to classic and contemporary studies on disinformation and misinformation, highlighting what we consider the weaknesses of these analytical definitions: that is, their sole focus on the initial step of the process -when someone introduces false information into the system. We then briefly describe the literature that has informed our definition of the contemporary media context in terms of the hybrid news system and the radical change in perspective that it requires: that is, analytical attention to both the generative acts of 'fake news' and to what happens in the processes of circulation. We thereafter consider this innovative perspective's roots and its focus on the observer's judgements regarding informativeness. Next, we discuss the profound consequences of this theoretical gaze on a micro, meso and macro level analysis of real-world 'fake news' news stories.",
          "Even if the notion of 'fake news' has been popularized only recently, the dangerous implications of unverified, inaccurate, defective and false information have been extensively studied by a diverse set of academic literature since the early 20th century. Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989(Habermas, [1962]]; Jack, 2017). There is, however, a lack of agreement regarding the diverse types of misleading information that fall under these two concepts of disinformation and misinformation, for example, satire, parody, and even bullshit. In the cases of satire and parody, in particular, the source intentionally produces misleading content, yet the intent is not to mislead, and the source does not benefit from being misleading (Fallis, 2015). Nevertheless recent studies have included (news) satire (content that exaggerates facts to convey a critique) and (news) parody (non-factual information produced to inject humour) among the forms of problematic information that can fool both large publics and influence intermediaries of information (Jack, 2017;Warde, 2017). Another type of misleading information that evades the classic distinction between disinformation and misinformation is what Frankfurt and Bischoff (2005) define as 'bullshit', wherein the source does not care if what he/she says is true or false but only aims to persuade or to provoke a reaction. Among the various types of problematic information, rumour has received the most systematic academic attention (Allport and Postman, 1946;Rojecki and Meraz, 2016). Rumours are forms of information that are characterized by uncertain veracity and that can be later proven false. The credibility of rumours is unrelated to direct evidence but to the fact that other people seem to believe them. Rumours are often at the origin of conspiracy theories, which are explanations for events through the causal agency of a group of individuals acting in secret (Keeley, 1999). If we had to describe conspiracy theories using the distinction between disinformation and misinformation, one could say that conspiracy theories are a form of disinformation since these theories can often easily be proven false, and it is no accident that the information is misleading (Fallis, 2015). At the same time, conspiracy theories could be identified as misinformation since some supporters of these theories are entirely sincere and believe them to be true (Sunstein and Vermeule, 2009). Another type of problematic information that has received considerable attention from researchers is propaganda. Studies on propaganda can be traced back to Lasswell (1927), who explained how manipulated significant symbols can manage collective attitudes, promoting a particular political side or perspective. The study of propaganda is obtaining renewed interest due to the increasing skills of media manipulation that have enriched online propaganda tactics, with the development of bots (Albright, 2016), which give the impression that many people are liking a piece of misleading news. Another form of disinformation that is garnering renewed academic interest is deceptive advertising, particularly advertising materials that simulate the formats of legacy news media to confer more legitimacy to a one-sided article or website, with the aim of obtaining financial gain or a positive public perception (Allcott and Gentzkow, 2017;Tandoc et al., 2018;Warde, 2017). Other types of problematic information related to new digital techniques are doctored and manipulated photos/videos (Farid, 2009), which may have no factual basis or may be factual but misappropriated. Contemporary media cultures also challenge the definition of the motivations that drive problematic information in contemporary media environments: internet trolls (Marwick and Lewis, 2017;Phillips, 2015), for example, may deliberately create misleading content for fun, with the aim of provoking outrage or fooling their audience. While helpful, most of these concepts and typologies of misleading information focus on the act of creation as the defining moment, looking at the author's judgement with regard to the falsehood and distinguishing between his/her intentions (to deceive, convey a critique, inject humour, persuade, reduce uncertainty, give an impression of events, make money, promote a particular political perspective/product, have fun). We argue that this focus represents a limit for the study of problematic information in the contemporary media environment, in which the diffusion of news depends on the multiple actors involved in the process of newsmaking and news sharing.",
          "Information cycles of the 21st century are characterized by a multitude of highly interdependent media technologies, in which numerous media actors follow newer and older, overlapping and competing media logics, producing news and other products with a news format (Chadwick, 2009(Chadwick, , 2013)). According to this vision, multiple and integrated platforms have rendered the process of newsmaking more inclusive, blurring the last century's boundaries between professional journalism and user-generated content as well as affecting the norms and values that guide journalists' practices in the (mixed) genres of journalistic information and the organization of newsrooms (Hermida and Thurman, 2008;Tandoc, 2014). In the hybrid media system, a plethora of actors -such as bots (Albright, 2016)participate in the production and dissemination of both journalists' ratified news and a wide range of other products that take on a news format. As boyd (2017) wrote, contemporary publics are increasing able to 'hack the attention economy'. These multiple actors, all involved in the hybrid processes of newsmaking and news sharing, require a radical shift in the study of misleading information, from exclusive focus on the first step of the process (the creation of news) to also considering what happens after this generative act. Judgements regarding the falsehood of news and motivations to share it can easily be different from those of the authors. We trace this shift of perspective back to second-order cybernetics.",
          "As anticipated by the review of the literature, most of the existing works emphasize that judgements on the level of facticity of online content are strongly interlinked with assessments regarding the intentions of the content creator. Understanding the intent of the creator of 'fake news' is thus considered a crucial indicator for differentiating among different types of problematic information. Nevertheless, several recent works recognize the difficulties in clearly assessing such intent (Jack, 2017). Even employing the state-of-the-art techniques of digital investigative journalism, the cues left by these actors are often inadequate for clearly differentiating between honest mistakes, deliberate deception and satire. This phenomenon is often referred to as 'Poe's Law' (Aikin, 2013). Furthermore, certain actors (e.g. trolls) deliberately sow confusion about their real intentions by framing as satire their false and/or outrageous content (Marwick and Lewis, 2017). No matter how difficult this assessment, billions of actors of the hybrid media news system are called upon every day, often multiple times a day, to quickly make such judgements on the online content to which they are exposed. Many experts both inside and outside academia are working to make this judgement process easier and less prone to error (Caplan et al., 2018). While recognizing the importance of these efforts, we take a different perspective, a perspective that includes at its core the possibility that misjudgements occur: a second-order perspective. By second-order perspective, we specifically refer to the work carried out by an interdisciplinary group of scientists (Norbert Wiener, Claude Shannon and Warren McCulloch, to name just a few) in the United States following the Second World War and developed under the programme title of 'second-order cybernetics' by a team led by Heinz von Foerster, an Austrian-American scientist combining physics and epistemology, at the Biological Computer Laboratory of the University of Illinois at Urbana-Champaign (Heims, 1991). Second-order cybernetics postulates that a system comes 'into being' when an observer acts to 'draw its boundaries ' (von Foerster, 2003). Once applied to media and communication, this approach suggests shifting attention from the source (the content creator) to the observer of the source (those who are exposed to this content). According to von Foerster (2003), a source in itself is rarely important unless someone pays attention to it. In a certain sense, sources only 'come into being' when an observer recognizes them as sources. According to Gregory Bateson (1972), an anthropologist who met von Foerster and Shannon multiple times during the Macy conferences (Heims, 1991), information is in fact 'a difference that makes a difference'. Bateson's definition of information underlines the crucial role played by the expectations of the receiver/observer. From this perspective, information is not something that can be transmitted, stored, or retrieved; it exists only 'when looked upon by a human mind' (von Foerster, 2003). What is informative for one observer may be uninformative for another observer. The observer's interests, background, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. While the intent of the content creator (first-order perspective) is often difficult to assess, the observers exposed to certain content (second-order perspective) formulate their best guess regarding both facticity and creator's intent. These judgements affecton different scales, depending on the observer's position in the system and number of connections to the actors -the entire process. On this basis, the observer defines the subsequent course of action (e.g. decides to further share a content). By extension, a researcher analysing the way an information cycle unravels from an external perspective is a third-order observer attempts to assess the intentions and goals of both first-order and second-order observers: they try to guess the intention and goals of the initiators of an information cycle, as well as the motivations of those who take part in it. At the same time, researchers also define a case of online spread of information as 'fake news', which thus represents the actual starting point of the analytical process. Given the multiple levels of observations at stake, our perspective is constructivist but not relativistic. All observers, including a researcher potentially studying the process, tend to judge the truthfulness of content from their unavoidably limited perspectives. Different actors are differently equipped to support this judgement process. Nevertheless, even professional subjects and organizations sometimes misjudge. These perspectives are different but not equal. For this reason, we introduce the distinction between 'true'/'false' (with lower case initial letters) according to the perspective of the actors involved in the process and 'True'/'False' (with capitalized initial letters) according to an external perspective, such as that of the researcher using the model. By the same token, a propagator is an actor of the hybrid media system (e.g. a journalist, news organization, citizen, politician, troll, fake account, bot) who is exposed to False information (created/shared by a creator or previous propagator) and, for numerous possible reasons, decides to share it further (thereby creating propagation). It may be the case that the propagator is exercising bad judgement (believing to be true what is in fact False), or it may be the case that she/he correctly judged the information as false and deliberately decides to share it to deceive others. In other terms, echoing Bateson's extension of Shannon's information theory (Shannon and Weaver, 1949), we suggest adopting a second-order perspective to shift attention from the source -as an entity with its own static properties (motivations, biases, drivers) -to the observer who consumes content originally published by the source and makes a judgement about its properties. We argue that, however right or wrong this judgement may be, the descriptions made by these observers shape the process of misleading information as a complex phenomenon emerging from the interplay of a multitude of actors in the hybrid news system. From this perspective, a False information cycle is rarely reducible to a single typology. A news article clearly fabricated with the aim of exploiting the logic of online advertising (and thus produced as disinformation) may become a must-read article within a community of like-minded believers of certain conspiracy theories and shared as a legitimate piece of news (misinformation). The implications of this radical change of perspective are presented and discussed in the next sections dedicated to the micro, meso and macro levels of our analytical model. By employing such terminology and analytical approach we address the call to precision that Turner (2006) has recommended to sociologists by inviting them to develop theories aimed at explaining social phenomena by paying attention to relationships between forces operating at different levels of the social universe.",
          "Multiple cycles of information constantly populate hybrid news systems. They result from the activity of various actors who produce and share information with small and large audiences that might or might not decide to actively contribute to their further circulation. Each of these cycles is grounded upon cascades of judgements (whether or not to believe a piece of information) and decisions (whether or not to share a piece of content). These actions are taken, with very different levels of sophistication, by diverse individuals, groups and organizations. Despite being guided by different logics, ethical concerns and skills, all the different actors populating these systems, we argue, ground their judgements and their decisions upon similar principles. In the following sections, we discuss the dissemination of False information within the hybrid media system by focusing on three different levels: a micro level consisting of the bases on which subjects judge the truthfulness of the information with which they engage; a meso level consisting of the matrix of expectations between actors and content at each propagation of False information; and a macro level consisting of the broad and heterogeneous process emerging from a chain of False news propagation. While these three levels of the analysis are presented, for sake of clarity, as a bottom-up structure, it is worth underlining that both the meso and the macro level are emergent phenomena.",
          "First, we focus on how single actors process information. False information is potentially an extremely 'informative' (and thereby effective) form of information (Karlova and Fisher, 2013), as unexpected novelty is a defining characteristic of information (Shannon and Weaver, 1949). Focusing on the 'informativeness' of False information allows us to employ journalism studies and literature on information sharing within digital environments to also discuss how the multiple actors of the hybrid media system make judgements and take decisions when exposed to False information. Starting from the classification proposed by Wathen and Burkell (2002) for the internet 1.0, we argue that all individuals and institutions inhabiting hybrid news systems evaluate the truthfulness of the information they manage on the basis of specific patterns regarding: (a) the source, (b) the story and (c) the context. According to the literature on digital information ecologies, two elements influence individuals in their assessments of a piece of information's truthfulness on the basis of its sources: authority (Clark and Slotta, 2000;Rieh, 2002) and proximity (Meyer, 1994). Given that the credibility of established institutions has plummeted over the past decades (Peters and Broersma, 2013), multiple scales of authority have emerged by which the status of 'influencers' is granted on the basis of the attention received from a very specific community (Marwick and boyd, 2011;Tufekci, 2013). When authority becomes strictly contextual, it is much more complicated for individuals to adopt such parameters to assess the trustworthiness of the original source of a piece of news. In this context, the (social or ideological) proximity characterizing our relationship with the node in our digital network endorsing a piece of information to which we are exposed becomes much more important than its original source (Messing and Westwood, 2014). Authority and proximity thus overlap almost completely, with the result that the source's expertise -a key element of authority -loses its centrality in how individuals evaluate the trustworthiness of a piece of information. Classic literature on newsmaking has shown that a source's capital of authority influences the credit granted to that information by journalists as well (Gans, 1979;Tuchman, 1978). Such patterns privilege institutional sources over other types of sources since institutional authority is considered a byproduct of the position of power the institution occupies within society. Digital and especially social media have offered institutional and political actors the opportunity for an uninterrupted flow of communication, within which False information can be strategically disseminated, exploiting some of the affordances of these platforms. Just as with everyday citizens, journalists' proximity is likewise indicative of a source's credibility (Gans, 1979): a frequently 'encountered' source becomes familiar and is thus more likely to be trusted. Within contemporary media ecologies, it becomes much easier for journalists to constantly engage with multiple potential sources of news through digital media, allowing them to become proximate with an increasingly diverse range of sources, including nonelite and even anonymous subjects. A second crucial element orienting individuals and media institutions in their evaluations of the truthfulness of a piece of information is its content. Social psychology literature has stressed that people tend to believe true stories that are coherent with their vision of the world and are relevant to them, terming such processes 'confirmation bias' (Nickerson, 1998). Two opposing patterns characterizing news consumption on social media -i.e. high selectivity (Prior, 2013) and incidental exposure (Fletcher and Nielsen, 2017) -both interact with confirmation bias in affecting judgements of truthfulness. Since users craft their information networks on the basis of commonalities related to specific interests and visions, tribalism can emerge (Sunstein, 2017), with in-group members judging content truthfulness on the basis of its capacity to reinforce community bonds. At the same time, since social media are 'news-finds-me' environments (Gil de Zúñiga et al., 2017), they can expose users to highly conflicting information, with confirmation bias becoming the most important principle orienting the way in which information is processed (Zollo et al., 2017). Despite professional standards, congruence (with their own or their audience's vision of the world as well as with the editorial line of their company) and relevance (for the public debate at a given moment) can also be regarded as key to the process through which journalists and news organizations evaluate a story's newsworthiness (Golding and Elliott, 1979). Finally, contextual factors influence people and institutions in their assessments of the truthfulness of a piece of information. Here we refer to the situation within which information is processed. The informational exuberance (Chadwick, 2009) characterizing contemporary information ecosystems can result in what has been described as 'information overload' (Austin et al., 2012), a condition in which content is very difficult to process, negatively impacting the benefits individuals derive from it. The result is a limited attention (boyd, 2010) to processing information, potentially influencing how individuals assess truthfulness. Contextual factors also impact the ways in which professional actors process information and assess its truthfulness. When an extremely relevant story unexpectedly breaks out, professional actors can experience an information overload due to the highly augmented density of information flowing online. In this manner, as it has been described both in the literature and in journalistic accounts (Schifferes et al., 2014), inaccurate or even fabricated information produced and shared by known and unknown sources passes professional newsmaker gatekeepers. The resulting practice has been described by Bruno (2011) as 'tweet first, verify later'.",
          "Multiple judgements (true/false) on the information circulating within the system and sharing decisions place actors in relationships with one another and with information content, generating multiple and multifaceted chains of propagation. We will discuss the possible structures of these chains as a whole in the next section; we focus here on the links in the chain, i.e. the individual propagations or the meso level of our analytical framework. At this level, the observer-dependent perspective introduced above becomes relevant. Since we aim to consider multiple, and frequently conflicting, judgements regarding the truthfulness of information produced and/or shared, recognizing the unavoidable subjectivity characterizing the whole process -including the process of the researcher studying the cycle -is very important. Indeed, it is the observer's perspective that draws the boundary of the system by determining that we are dealing with a cascade of propagations of False content (here, as we have already explained above, the capitalized initial letter distinguishes the outcome of the researcher's judgement from the outcomes of the propagators' judgement). Starting from these premises, the possible combinations of the judgement of the original author of the False information (injector) and the judgements of further propagators and between various propagators when the circulation cascade is activated generate a matrix of four possible scenarios (Table 1). In the first case (1), both actors are aware of the false nature of the information but nevertheless decide to share it. While this pattern can be observed in the propagation of humoristic and satirical content, it can also be the case of two subjects who, for strategic reasons (e.g. propaganda), deliberately produce and share false information. We define this as 'pure disinformation' propagation. Conversely, when a piece of information originally injected as true is shared by a propagator who thinks it is false (2), we witness a case in which misinformation is exploited to become disinformation. In most of the cases in which False information is produced as true, the creator unintentionally injects misleading information into the system. A third option occurs when a piece of information is devised as false by the injector but perceived as true (3) by the propagator: we are observing a case of disinformation propagated through misinformation. In this case, an actor produces information knowing it is False, but other users perceive it as true and share it as such. Gross examples of this often result in embarrassing moments for the propagator (especially when the subject is a news organization) who mistakenly understood as true fabricated false information. Finally, when False information is perceived as true by the injector and by the propagator (4), we are witnessing what we could assume to be a classic process of propagation of misinformation. It is the case with the most common conspiracy theories, which flourish thanks to the content created and shared by members of polarized online communities that firmly believe in what they communicate. Other times, False information inadvertently injected into the system by an authoritative source (e.g. a respected news media organization) keeps propagating as a legitimate piece of content until someone proves it False. Given the context described above, it should be clear that trying to understand the nature of contemporary disinformation by focusing solely on the initial step of the process -when someone introduces information into the system -is at the very least an oversimplification. Over 60 years ago, Katz and Lazarsfeld (1955) argued that mass information flows depended on the personal influence that opinion leaders exercised upon other citizens, showing how, in a process, the 'first step' is not necessarily the most important one. Testing the two-step flow theory in the context of social media, Hilbert et al. (2017: 456) empirically demonstrated how, by considering the multiple perspectives of all actors involved in information cascades, the model best describing information flows can vary from a simple single step to a 'some kind of intricate network-step flow'. Similarly, Messing andWestwood (2014: 1058) contend that, within social media ecologies, the agenda-setting power that was originally concentrated in newsrooms is now diffused across social networks. The multiple actions of propagation, guided by single judgements and decisions, have thus become the backbone of news flows within the contemporary hybrid information system. Information cycles are thus better represented by a dynamic growing tree-like structure of propagation cascades, conceptually similar to those studied in the field of complex propagation (Centola et al., 2007). Conceptualizing False information cycles in terms of propagation cascades has two major consequences. First, a cycle/cascade is very unlikely to exclusively include propagations of a single type. Every real-world cycle is most likely a combination of multiple semi-independent types of propagation (Table 1) in which individual assessments and decisions are constantly influenced but not determined by the local dynamics discussed above. This process is illustrated in Figure 1. A central consequence of this way of understanding information cycles is that, despite each information cycle being a set of diverse propagations, it is still possible to theoretically describe 'global cascades' as an emergent and thus autonomous phenomenon. Within this perspective, the emergence of spreading dynamics on a systemic level (e.g. news and misinformation propagating through viral and apparently unstoppable dynamics) is coherently built upon several individual processes, thus the study of specific cases of propagation can be used to understand underlying dynamics and how these micro and meso dynamics merge into the macro evolution of a single information cycle. Keeping this is mind, it is important to stress that while every information cycle is most probably composed of several coexisting types of propagation, it is likely that every cycle will be dominated by a single type or by a combination of types.",
          "On 15 November 2015, the Spanish newspaper La Razon published on its front page the face of one of the terrorists responsible for the attacks that had hit Paris a few days earlier. Unfortunately, the face did not belong to one of the terrorists involved in the terrible attack on the French capital but instead to Veerender Jubbal, a young Canadian man. Before the attacks, Veerender Jubbal -who is Sikh and wears a turban -had posted a selfie taken with his iPad, asking his Twitter followers to wish him 'good luck'. After the attack, someone easily edited the photograph by replacing the iPad with a Quran and Photoshopping a suicide vest onto his shirt. The fake picture went viral online and was shared by various news organizations, even making it to the front page of the print edition of La Razon, before it was finally debunked and people began listening to Veerender's complaints. The origin of the Photoshopped image is in some sense largely irrelevant to the study of the cycle of False information. The picture was created and shared with the knowledge that it was a fake, but it was quickly picked up by several sources who judged it true. The diffuse proximity allowed by social media, confirmation bias dynamics (terrorists must wear turbans, and it does not matter if it is a dastar, a Sikh turban) and the 'tweet first, verify later' logic can perhaps explain how a photograph of a smiling man from the internet can be judged as the true picture of a suicide bomber, taken just before an attack. Nevertheless, once the picture was online, it was quickly shared as a legitimate picture of a terrorist, and it took over 24 hours to stop the dissemination of the False information. On 18 November 2016, Chris Lamb wrote in the US edition of The Huffington Post a satirical piece in which he imagined the US president Donald Trump suggesting the removal of the Statue of Liberty because it encouraged immigration. Despite the obvious satirical content of the article, several Italian newspapers (including the leading Il Corriere della Sera and La Repubblica) and TV channels (including La7 and RAI News) reported the news that 'Trump attacked the Statue of Liberty'. The news got great attention in Italy and was widely shared as a story confirming the unpredictability and unorthodox behaviour of the US president, before being retracted by the same newspapers that initially shared it. On 9 November 2016, the day after Republican presidential nominee Donald Trump claimed an unexpected victory in the US presidential race, a grainy picture began circulating on social media, along with the claim that the Ku Klux Klan were openly marching in Mebane, North Carolina to celebrate the win. According to investigations by the website Snopes.com (Palma, 2016), the people marching were indeed supporters of the conservative candidate celebrating the victory, but what the original creator of the picture mistook as a robe was in fact a flag. Nevertheless, the photo circulated widely on social media and partisan websites as an evidence of the connection between the conservative president-elect and the Ku Klux Klan. These are just three minor stories that illustrate key elements of the system we have described: a. The intention of the injector does not determine the future evolution of the False information cycle. Given the contemporary hybrid media system and the complex nature of information cycles, False and misleading information need not be born as such. The real goal behind the production of a piece of False information may be largely unknown (as in the case of Veerender Jubbal), can be satirical (as in the case of Lamb's article) or may be born from an honest mistake (as in the case of the picture taken in North Carolina). b. False news can only be understood as a process. Information will be constantly assessed by a number of different actors, which will act in accordance with their individual criteria to establish its truthfulness. If and when these individuals share the information, this will generate a nearly unique chain of types of propagations, defined by the model presented in Table 1. c. Information equilibrium does not imply consensus. One consequence of the model and the first two points above is that societal consensus regarding the nature of any information is somewhat unlikely. When information is constantly assessed by a number of independent actors acting in accordance with the aforementioned principles, every operation aimed at establishing the real nature of a piece of content (e.g. debunking or fact checking a piece of news) will itself be judged as true or false and ultimately accepted only by a limited group of actors.",
          "In this article, we have proposed a new approach to studying False information. Building upon an interdisciplinary theoretical background, we have suggested moving beyond a simple focus on the initial step of the news cycle (when news is actually produced and regarding the producer's intentions) to instead observe the cycle as an emergent whole influenced but not determined by individual assessments (concerning the nature of the information) and decisions (concerning whether to propagate the information). We have suggested that this is necessary due to the hybrid nature of contemporary media systems, and we theoretically grounded our approach in a second-order perspective. We have suggested that the propagation of False information should be investigated especially at the micro level (actors' judgements) and meso level (combinations of actors' decisions), while we contend that the processes characterizing the macro level should be seen as the result of the multiple combinations characterizing the previous two levels. We have argued that, at the micro levels, all actors in the hybrid media system operate, at their very core, in the very same way (despite the different levels of perceived responsibility, ethical concern, and ultimate goals characterizing them). These key processes, when connected within a relational model, produce a set of four typologies of propagation that can explain all the various types of actual propagation, including those that have always been highly problematic for pre-existing models (e.g. satire, bullshit). We therefore suggest that 'theorizing about the propagator' is beneficial to understand, mutatis mutandis, how both False and True news circulate. Certainly, we are not the first to suggest placing more focus on propagation processes. Academic research on rumours and conspiracy theories has gone beyond the generative moment of information, looking at the process of disseminating misleading information. And yet the scenarios that emerge from this wide body of research have focused only on a type of propagation in which -at times clashing with the initial goals of the authorspeople share False information while believing it to be true. A more coherent focus on the propagation process would allow us to address a number of cases that would otherwise be difficult to explain with an appropriate theoretical model, such as the case of propagators of False information who recognize its falsehood but nevertheless decide to diffuse it. This also applies to the cases of satire and parody, which may be generated with positive intentions but may nevertheless be judged true and diffused as such as the result of an honest yet dangerous mistake. Adopting this approach allows us to observe diffusion of False information as a much more nuanced phenomenon that, as shown in our examples, does not require a single understanding of the news or assume a single possible reaction. When information is constantly assessed by a number of independent actors, who then act in accordance with their assessments, a situation of coexisting opposing beliefs is the normal status of the system rather than the exception. Finally -much like Niklas Luhmann's idea of communication as an ephemeral event -a propagation is, by definition, an event in a chain of propagations and a specific cascade of propagations with its emergent properties becomes observable as an autonomous phenomenon. As such, societal communication can reference this whole case in terms of codes other than true/false. Different systems in a functional differentiated society (Luhmann, 2012) can thus address a cascade as a whole using their own perspective. While true/false is the code used by science, the media system would deal with this case in terms of its informativeness (information/non-information) and the law system using its own binary code (legal/illegal). The term 'fake news' as it is now commonly used covers completely such reality, which is why we provocatively argue that only a liar can breezily use it."
        ],
        "ground_truth_definitions": {
          "disinformation": {
            "definition": "Misleading information deliberately aimed at deceiving others.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          },
          "misinformation": {
            "definition": "False or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "1acbc58041a408e33ba33e2f0af5808f2bc56ff1",
        "sections": [
          "C rop improvement is important to increase agricultural pro- ductivity and to contribute to food and nutrition security. The need for new crop varieties is exacerbated by climate change. Farmers need to replace crop varieties with better-adapted ones to match rapidly evolving climate conditions (1)(2)(3)(4). Where suitable modern varieties do not exist, suitable farmer varieties are needed instead (\"variety\" is applied to all cultivated materials here) (4). The variety replacement challenge has yet to be effectively addressed. One proposed solution is to increase variety supply by accelerating crop breeding, removing older varieties from the seed supply chain, and assiduously promoting new varieties for farmers (2). Supply-driven variety replacement requires that new varieties are locally adapted and acceptable, but varieties are often recommended without prior geographic analysis to determine recommendation domains (5) on the basis of trials that do not adequately represent local production conditions (6)(7)(8). Therefore, a supply-driven approach may introduce varieties that perform worse than locally grown varieties. Demandoriented approaches address this issue but also fall short of a solution. They involve farmers directly in the selection of crop varieties in on-farm experiments (6). Farmer-participatory selection stimulates local interest in new varieties and produces information on variety performance that is immediately relevant to local climate adaptation. This local focus is a strength as well as a limitation. Scaling is constrained by the resource-intensive nature of current participatory experimental methods and the incompatibility of datasets across different efforts (9). The resulting paucity of data is a problem, because variety trials need to capture spatiotemporal environmental variation to characterize climatic responses. A solution could come from a more scalable type of participatory research: citizen science using digital \"crowdsourcing\" approaches (10)(11)(12). This has already shown its potential to engage large numbers of volunteering citizen scientists who jointly generate sizable datasets that allow for geospatial analysis of climate change impact (for example, on cross-continental Significance Climate adaptation requires farmers to adjust their crop varieties over time and use the right varieties to minimize climate risk. Generating variety recommendations for farmers working in marginal, heterogeneous environments requires variety evaluation under farm conditions. On-farm evaluation is difficult to scale with conventional methods. We used a scalable approach to on-farm participatory variety evaluation using crowdsourced citizen science, assigning small experimental tasks to many volunteering farmers. We generated a unique dataset from 12,409 trial plots in Nicaragua, Ethiopia, and India, a participatory variety evaluation dataset of large size and scope. We show the potential of crowdsourced citizen science to generate insights into variety adaptation, recommend adapted varieties, and help smallholder farmers respond to climate change. bird migration) (13). In a similar way, farmer citizen scientists could provide information about crop variety performance, which would feed into a demand-driven, scalable solution to varietal climate adaptation. To test this idea, we applied a recently developed citizen science approach tricot-triadic comparisons of technologies (14,15). In tricot variety evaluation, each farmer plants seeds from a personal test package of three varieties, which are randomly assigned from a larger pool of tested varieties. Farmers' independent on-farm observations are compiled and analyzed centrally. A simple ranking-based feedback format allows even farmers with low literacy skills to contribute their evaluation data through various channels, including mobile telephones (15). Pilots with the tricot approach have established its potential to produce accurate data (16) and to engage motivated farmers as citizen scientists (17). The question that we address is if tricot trials can provide robust, actionable information on varietal climate adaptation. We organized tricot trials to obtain a dataset covering 842 plots of common bean in Nicaragua, 1,090 plots of durum wheat in Ethiopia, and 10,477 plots of bread wheat in India (Fig. 1). The trials captured environmental variation through broad sampling both spatially (many fields distributed across the landscape) and temporally (different seasons and planting dates). We linked farmers' observations via their geographic coordinates and planting dates to agroclimatic and soil variables. We modeled the influence of the environmental variables on the probability that varieties outperform the other varieties in the trials. We evaluated whether seasonal climate adequately predicts variety performance in the tricot trials. Then, we explored if climatic analysis of tricot trial data improves variety recommendations.",
          "Cross-validation showed that the tricot trials uncovered statistically robust differences in variety performance (Table 1). From a previous pilot study, we expected consistently positive, but low to moderate, pseudo-R 2 values (16). In this study, model fit was comparatively low for bread wheat in India (0.04-0.09), moderate for common bean in Nicaragua (0.15-0.20), and high for durum wheat in Ethiopia (0.39-0.48). The three case studies each provide independent confirmation of the predictive value of the tricot trials. Various factors influenced model fit, includ-  ing farmers' observation skills and environmental variation. The largest differences were between countries, which were probably due to the different levels of diversity within the sets of varieties. Indian and Nicaraguan farmers evaluated a small, carefully selected group of modern varieties with relatively homogeneous performance. In Ethiopia, farmers tested a diverse set of modern and farmer varieties drawn from a wide area and evidently found easily observable differences in performance between varieties. For each country, we modeled the environmental influence on variety performance. We were specifically interested in models with covariates derived from seasonal climatic conditions (climate in Table 1), because these covariates can potentially enhance extrapolation of variety performance predictions across time and space. In all cases, these models had indeed a better fit than the respective model without environmental covariates (no covariates in Table 1). The next question that we addressed was if the models with climatic variables captured the main environmental factors or missed important aspects. Therefore, we compared these models with two other types of models. One type of model includes covariates that represent the experimental design and are known in advance: geolocation, season, planting dates, and soil categories (design in Table 1). These models reflect how multilocation trials are often analyzed and capture variation in terms of the trial structure but not in terms of the underlying climatic causal factors, hence limiting the potential of extrapolation beyond the trial. In all cases, the models with climatic covariates slightly outperformed the models with trial design covariates. This means that the climatic covariates contain unique and substantial information explaining varietal performance. A second comparison was with models that include the climatic covariates together with additional covariates that represent geographic structure (climate + geolocation in Table 1). This comparison tested if important local factors are being overlooked that are not covered by the climatic covariates. Adding these geolocational variables did not improve the models, however, and even slightly degraded them. This implies that no large-scale geographical structure remained after accounting for seasonal climate. From this analysis, it is clear that the models with climatic covariates captured a large part of the environmental variation in variety performance. Therefore, in subsequent analyses, we focused on models with climatic covariates only. We generated generalizable models that afford extrapolation across seasons of variety performance predictions by selecting those climatic variables that contribute to predictivity across seasons. The variable selection procedure retained one climatic variable in each case (Fig. 2 and SI Appendix, Fig. S1). We discuss the results for each case study. For Nicaragua, Fig. 2 shows the Plackett-Luce tree (PLT) with the retained variable of the generalizable model for common bean. We found that bean variety performance changed when the maximum night temperature exceeded 18.7 • C. This finding corresponds to the threshold temperature for heat stress reported in the literature of 20 • C at night (18). Our estimate is slightly lower than the reported threshold but refers to land surface temperature rather than air temperature. Three For durum wheat in Ethiopia, varietal differences in performance were related to the lowest night temperature during the vegetative period (SI Appendix, Fig. S2). Performance patterns changed when at least one 8-day period had average night temperatures under 8.4 • C. This temperature corresponds to the threshold temperatures for vernalization and cold acclimation induction (19). Under warm conditions, vernalizationrequiring varieties will delay flowering. Under cold conditions, cold-sensitive varieties will reduce their yield due to chilling or frost damage. Most of the varieties tested in Ethiopia were farmer varieties and likely adapted to their original environments, which may have led to differences in adaptiveness between varieties. To test the effect of local adaptation, we compared cold-adapted varieties with cold-sensitive farmer varieties as detected by the tricot trials (Materials and Methods). Coldadapted varieties came from higher elevations (2,483 ± 113 meters above sea level) than cold-sensitive ones (2,101 ± 485 meters), a significant difference [t(594) = 16.1, P < 2.2 • 10 -16 ]. Our results indicate that cold tolerance is a main geographic adaptation factor for durum wheat in the Ethiopian highlands. For bread wheat in India, varietal performance patterns changed with the diurnal temperature range (DTR) during the vegetative period, which is the difference between minimum and maximum daily temperatures (SI Appendix, Fig. S3). Splits occurred at DTR values of 14.5 • C and 15.7 • C. Between these two values, the varieties showed very similar performance. Many varieties that performed above average under high DTR performed below average under low DTR and vice versa. Some varieties performed well under both high and low DTR, especially HD 2967. Our interpretation is that low and high ranges of DTR are related to different sets of stresses, while the middle range has relatively low stress. DTR has an impact on crop yield through several mechanisms: high DTR is associated with increased heat or cold stress, and low DTR is associated with high cloud coverage, low solar radiation, and high rainfall. Consistent with our results, a study has shown that DTR explains a substantial share of wheat yield variation in India (20). This same study found that DTR has a negative correlation with wheat yields in some areas and a positive correlation in other areas, in line with high and low DTRs having an association with different types of crop stress.",
          "We examined four ways in which climatic analysis afforded by tricot trials can improve variety recommendations. First, a potential improvement is that climatic analysis corrects the climatic sampling bias, a bias that occurs when trials are performed under unrepresentative seasonal climate conditions, thereby degrading variety recommendations. To assess the importance of climatic sampling bias, we followed the cross-validation procedure used to generate the generalizable models but did not use the seasonal climate data for predictions. Instead, we predicted variety performance for a representative 15-y base period of seasonal climate data and averaged the results (average season in Table 2). The averaged prediction had slightly higher pseudo-R 2 values than the \"no covariates\" model in all cases. This analysis shows that, even when climatic sampling bias is low, correction can help to further improve predictions. Second, climatic analysis can improve variety recommendations by incorporating seasonal forecasts. Perfect forecast in Table 2 shows that the pseudo-R 2 values increase further when observed climate information is available for prediction. The improvement gained from a perfect forecast was substantially larger than the improvement from sampling bias correction. It requires additional work to quantify the improvement of variety recommendations with a realistic climate forecast skill. It is clear, however, that variety recommendations derived from tricot trials can benefit from seasonal forecasts. Third, climatic analysis can support risk analysis. Table 3 shows the expected probability of outperforming all other varieties, which is a metric of average performance, and a risk metric, worst regret (21)-the largest underperformance of the recommended variety relative to the best variety. These two metrics produced divergent variety recommendations in all three cases (indicated in bold in Table 3). In principle, risk analysis for variety choice is also possible without explicit climatic analysis, but this produces results that are difficult to interpret in terms of climatic causality and requires trials during a large number of  The results show how different criteria of variety selection can lead to different recommendations (best value according to each criterion is indicated in bold). Using the probability of winning as a criterion maximizes the average performance but ignores risk. Minimizing worst regret (the loss under the worst possible outcome) is a criterion that takes a conservative approach to risk. seasons to avoid sampling bias and to characterize probability distributions accurately (22). Fourth, climatic analysis of tricot trial data can generate variety recommendations for wider areas through geospatial extrapolation. To illustrate this, we generated maps of varieties recommendations based on \"average season\" model predictions (Fig. 3). In all three cases, geographical patterns of variety adaptation have no relationship to administrative boundaries or agroecological zones, which are commonly used to delineate recommendation domains. To assess what the tricot trial results mean in practice, we contrast our results with existing recommendations. For Nicaragua, we compare the results of the tricot trials with the recommendations of a recent national variety catalog (23). The catalog recommends INTA Rojo and INTA Matagalpa for the study area, but these varieties performed worse than the local varieties in the tricot trials (Fig. 3A). However, the tricot trials identified INTA Fuerte Sequía and INTA Centro Sur as top varieties (Table 3), but the variety catalog recommends them for warm areas outside our study area. In the tricot trials, INTA Fuerte Sequía and INTA Centro Sur outperformed other varieties, especially under heat stress, which apparently occurs with more frequency in our study area than assumed by current variety recommendations. In Nicaragua, then, the tricot trial results show that official variety recommendations fail to identify superior bean varieties that are sufficiently heat tolerant for the study area. For Ethiopia, the Wheat Atlas of the International Maize and Wheat Improvement Center (CIMMYT) recommends modern varieties Hitosa, Ude, and Assassa for all of the Ethiopian highlands, which it classifies as a single \"mega-environment\" (24). The tricot approach produced geographically more specific recommendations (Fig. 3B). With this, we confirm the results of a previous analysis based on multilocational trial data that showed the benefits of location-specific recommendation domains for durum wheat in Algeria, and we show that such an analysis can also be done with tricot data (25). The tricot results confirmed the superiority of farmer varieties 8208 and 208304 (Table 3), which were approved for official variety release in March 2017 (on the basis of other field trials) (26). Farmer variety 208279 also has a high probability of winning, but it has a high value of worst regret (Table 3). Our analysis suggests that 208279 could be considered for the coldest areas as shown in Fig. 3B. In Ethiopia, the tricot trial findings improve variety recommendations for durum wheat by uncovering the importance of cold adaptation. For India, we compare our findings with the front-line demonstrations of the Indian Institute for Wheat and Barley Research (IIWBR); the 1-ha plots demonstrate new varieties by comparing them with a check variety. IIWBR promoted the variety HD 2967 for the North-Eastern Plain Zone during 2016-2017 (27). HD 2967 was indeed the top variety in the tricot trial among the varieties considered by the IIWBR (Table 3). In the tricot trials, however, K 9107 (a variety released in 1996) outperformed HD 2967 (released in 2011), with a comparable level of worst regret (Table 3). The tricot trials also showed that another variety, HD 2733, outperformed HD 2967 in a large part of the study area (Table 3). In the IIWBR front-line demonstrations, HD 2733 was included as a check variety in four areas and was outyielded by HD 2967 in only one of four areas, while in the other three, the yield difference was not significant (27). Our analysis shows that HD 2733 generally does better than HD (Fig. 3C). In India, the analysis of the tricot trial data adds geographic specificity to the existing variety recommendations and suggests that a broader set of wheat varieties should be promoted to take into account the climatic differences across the study area. We quantified how much farmers can benefit from tricotbased variety recommendations by calculating variety reliability, the probability of outperforming a check variety (Eq. 2 in Materials and Methods). For each location, we compared the tricot-recommended variety (Fig. 3) with the bestperforming variety from the previous recommendations as the check. Reliabilities ranged from 0.59 to 0.65 in Ethiopia, from 0.58 to 0.60 in Nicaragua, and from 0.51 to 0.62 in India (SI Appendix, Fig. S4), indicating substantial benefits for large areas.",
          "The main question that we addressed is whether on-farm participatory crop trials, scaled through a farmer citizen science approach, can generate insights into climate adaptation of varieties. Citizen science data revealed generalizable relations between seasonal climate variables and crop variety performance that corresponded to known yield-determining factors. Climatic analyses of these data were shown to improve variety recommendations. Our study demonstrates that, in vulnerable, low-income areas, climatic analysis of variety performance is possible with trial data generated directly by farmer citizen scientists on farms. Arguably, similar results could be achieved by a combination of existing approaches (target environment characterization, multilocation trials, participatory variety selection, variety dissemination). The unique contribution of the tricot approach is that it integrates aspects of these approaches into a simple trial format that addresses the challenge of variety replacement for climate adaptation in a way that is, at the same time, scalable and demand led. Tricot trials can track climate trends as they manifest themselves on farms, adjust variety recommendations and recommendation domains, and contribute to understanding how climate affects on-farm varietal performance. Trial analysis combines insights in climatic adaptation mechanisms with a comprehensive evaluation of variety performance from the perspective of farmers, the end users of the seeds. Results can, therefore, be directly translated into actionable information for climate adaptation on the ground. The findings can serve to create variety portfolios that diminish climate risk (22), can feed into climate information services in combination with seasonal forecasts (28), and can become part of decentralized plant breeding strategies for climate adaptation (8). Combining the tricot trial data with other data could generate additional insights into variety performance and acceptability as influenced by environmental (11), socioeconomic (29), and genomic (30) factors. The tricot approach facilitates engaging large numbers of farmers in citizen science trials with large sets of varieties. Scaling does not only involve an expansion in terms of numbers and scope, however, but also, it implies new institutional arrangements. Carefully designed strategies should foster communication between providers and users of information (31). Wide-ranging collaborations are needed for climate adaptation in crop variety management, involving farmers, extension agents, seed retailers, seed producers, plant breeders, and climate information providers. The tricot approach can help to cut across these different domains, because it is able to link climatic and varietal information directly to farmer decision making. With appropriate institutional support and investment, citizen science can potentially make an important contribution to farmers' adaptive capacity and to the mobilization of crop genetic diversity for climate adaptation.",
          "Crop Trials. Trials were performed between 2012 and 2016 during three cropping seasons in Ethiopia, five cropping seasons in Nicaragua, and four cropping seasons in India (SI Appendix, Table S1). Trial design followed the tricot citizen science approach (14,15). Sets of varieties were allocated randomly to farms as incomplete blocks (7), maintaining spatial balance by assigning roughly equal frequencies of the varieties to each area. In Nicaragua and India, incomplete blocks contained three varieties. In Ethiopia, we used a modified approach that included four varieties per farm. Plots were small to facilitate farmer participation but in all cases, large enough to avoid strong edge effects. Farmers indicated the relative performance of varieties through ranking. Ranking is a robust data collection approach that avoids observer drift (32) and allows for aggregation across disparate datasets (33). The trials required three moments of contact with the farmers: (i) explaining the experiment and distributing the seeds, (ii) collecting evaluation data, and (iii) returning the results. Data were initially collected using paper forms and in subsequent seasons, through electronic formats linked to a purposebuilt digital platform, https://climmob.net. In the trials presented here, field agents collected the data through visits (phone calls are also feasible). Data Analysis. All analyses were done in R (34). For the analysis of the variety-ranking data generated by farmers, we used the Plackett-Luce model (35,36). The Plackett-Luce model estimates for each variety the probability that it wins, beating all other varieties in the set. The model determines the values of positive-valued parameters α i (worth) associated with each variety i. These parameters α are related to the probability that variety i wins against all other n varieties in the following way: The probability that variety i beats another variety j is calculated in a similar way. Eq. 2 also serves to calculate the reliability of a variety-its probability of beating a check variety (37). These equations follow from Luce's Choice Axiom, which states that the probability that one item beats another is independent from the presence or absence of any other items in the set (36). We report worth values that sum to one. This makes each worth value α i equal to the probability of variety i outperforming all other varieties: In the trials, we used rankings of three varieties (i j k), which have the following probability of occurring according to the Plackett-Luce model: P(i j k) = P(i {j, k}) • P(j k). [4] The log likelihood for a ranking i j k follows from Eqs. 1, 2, and 4 and takes the following form (38): (α) = ln(P(i {j, k})) + ln(P(j k)) = ln (α i )-ln α i + α j + α k + ln α j -ln α j + α k . [5] The log likelihood is then the sum of the log-likelihood (α) values across all rankings. Using an iterative algorithm, the log likelihood is maximized to identify the α values that make the observed rankings most probable. We also generated quasi-SEs for α (39). To take into account covariates, we created PLTs through recursive partitioning (40). Additional details are given in SI Appendix."
        ],
        "ground_truth_definitions": {
          "climatic sampling bias": {
            "definition": "a bias that occurs when trials are performed under unrepresentative seasonal climate conditions",
            "context": "We examined four ways in which climatic analysis afforded by tricot trials can improve variety recommendations. First, a potential improvement is that climatic analysis corrects the climatic sampling bias, a bias that occurs when trials are performed under unrepresentative seasonal climate conditions, thereby degrading variety recommendations. To assess the importance of climatic sampling bias, we followed the cross-validation procedure used to generate the generalizable models but did not use the seasonal climate data for predictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "1538c4777271ae6abb542801dac01423f4d566ad",
        "sections": [
          "Compared with many other disciplines in business and the social sciences, Information Systems (IS) is a relatively young field. In its initial development stage, IS was perceived as an applied discipline that almost exclusively drew on other, more fundamental, reference disciplines [Keen, 1980]; its progress and maturation became a preoccupation to some researchers [Banville and Landry, 1989]. Now, IS \"fully emerged as a discipline in its own right\" [Baskerville et al., 2002;p. 1]. The development of IS as a scientific field is evidenced by the building of solid research tradition. In an empirical study of the disciplines cited in IS journals, Vessey et al. [2002] found a substantial volume of IS research used IS itself as the reference discipline (27% in their analysis). A survey [Palvia et al., 2004] of the use of different research methodologies in IS showed that about 6.6% of studies published in seven IS journals during the years 1993-2003 used literature reviews as the primary research methodology. IS is also emerging as an important reference discipline for other fields [Baskerville et al., 2002]. A search of the Social Science Citation Index (SSCI) showed that it is common for many fields to conduct research based on IS theories or models. A search with the keyword of \"information systems or information technology\" resulted in a total of 719 articles published in 329 refereed journals in 2004. Of these journals, 26 are IS-focused, while the other 303 journals are from various fields covering almost all the research areas of business and social sciences. A good example of this use of IS theory in other areas is the technology acceptance model (TAM), which was tested and applied in psychology [e.g., Gentry et al., 2002], education [e.g., Liaw 2002;Selim, 2003], marketing [e.g., Keen et al., 2004], operations management [e.g., Olson et al., 2003], and many other management fields [King and He, 2005]. However, IS research inevitably displays one problem that is common in many mature fields: inconsistent empirical findings on essentially the same question. Knowledge accumulation increasingly relies on the integration of previous studies and findings. As Glass [1976] suggested, when the literature on a topic grows and knowledge lies untapped in completed research studies, \"this endeavor (of research synthesis) deserves higher priority .... than adding a new experiment or survey to the pile\" [Glass, 1976;p. 4]. A recent trend in research synthesis is to integrate, quantitatively, knowledge garnered from empirical studies on a topic by using meta-analysis. Meta-analysis is the most commonly used quantitative research synthesis method in the social and behavioral sciences [Hedges and Olkin, 1985]. It won recognition as a better way to conduct reviews of a body of completed research than the traditional narrative fashion [Wolf, 1986;Hunter and Schmidt, 1990;Rosenthal, 1991;Cooper and Hedges, 1994;Rosenthal and DiMatteo, 2001]. Some journals revised their review policy to encourage the use of this methodology [e.g., From the Editors of Academy of Management Journal, 2002]. In IS, however, meta-analysis is extremely underutilized [Hwang, 1996]. Moreover, some of the meta-analytic practices used in meta-analysis in IS are conceptually or methodologically flawed [King and He, 2005]. The objective of this paper is to provide guidelines for IS researchers who are interested in synthesizing a body of literature in a rigorous and quantitative fashion. After reviewing and comparing different literature synthesis methods (Section II), we turn our attention to metaanalysis. We describe the history, common methods, and recent developments of meta-analysis (Sections III and IV). We also provide a discussion of some major concerns with existing metaanalysis applications (Section V) and the great potential of applying meta-analysis in IS (Section VI).",
          "The refinement and accumulation of information and knowledge are an essential condition for a field to \"be scientific\" and to progress [Hunter et al., 1982;Pillemer and Light, 1980]. Researchers can use a number of techniques for making sense out of existing research literature, all with the purpose of casting current research findings into historical contexts or explaining contradictions that might exist among a set of primary research studies conducted on the same topic [Rumrill and Fitzgerald, 2001]. Many researchers dichotomize literature review methods as qualitative versus quantitative reviews [e.g., Wolf, 1986;Aldag and Stearns, 1988;Hunter and Schmidt, 1990;Rosenthal and DiMatteo, 2001;and Palvia et al., 2003]. This approach may be overly simplistic in that different review techniques vary in the extent of systematically synthesizing an existing literature body, ranging from purely qualitative (e.g., verbal description) to moderately quantitative (e.g., counting a number or calculating a percentage of certain research characteristic) to purely quantitative (e.g., meta-analysis). Following Guzzo et al.'s [1987] approach, we categorize the most commonly employed review techniques in IS along a continuum of quantification as narrative reviews, descriptive reviews, vote counting, and meta-analysis (Figure 1). ",
          "Narrative reviews present verbal descriptions of past studies focusing on theories and frameworks, elementary factors and their roles (predictor, moderator, or mediator), and/or research outcomes, (e.g., supported vs. unsupported) regarding a hypothesized relationship. Narrative reviews are of great heuristic value, and serve to postulate or advance new theories and models, to examine important and/or controversial topics, and to direct further development in a research domain. No commonly accepted or standardized procedure for conducting a narrative review exists. This lack is a key weakness of narrative reviews as a means of arriving at a firm understanding of a research tradition [Green and Hall, 1984;Hunter and Schmidt, 1990;Rosenthal and DiMatteo, 2001]. Researchers are relatively free to design their review strategy in terms of selecting relevant papers, categorizing research characteristics, and framing outcomes. When conducting a narrative review, researchers tend to consciously or subconsciously make judgments that support their own background, understanding, or established point-of-view. Often their goal is to come to some conclusions through classifications of the research methods and categorizations of results. One commonly-used strategy is to create a classification or a typology to organize the results. Researchers adopting this approach should be cautious that the creation of categories may result in less information since quantitative scales may be reduced to qualitative ones. Of course, this approach can result in greater understanding and lead to greater insight, but such a result is certainly not assured. Ives and Olson's [1984] study on user involvement may serve as a representative narrative review in IS. Ives and Olson proposed a model of user involvement prior to their review of the literature; and this model framed their discussions of the selected papers. Ives and Olson's [1984] work is considered influential in the field of user involvement.",
          "Descriptive reviews introduce some quantification, often a frequency analysis of a body of research. The purpose is to find out to what extent the existing literature supports a particular proposition or reveals an interpretable pattern [Guzzo et al., 1987]. To assure the generalizability of the results, a descriptive review often involves a systematic search of as many as relevant papers in an investigated area, and codes each selected paper on certain research characteristics, such as publication time, research methodology, main approach, grounded theory, and symbolic research outcomes (e.g., positive, negative, or non-significant). A frequency analysis (including its derivatives of trend analysis and cluster analysis) treats an individual study as one data record and identifies distinct patterns among the papers surveyed. In doing so, a descriptive review may claim its findings to represent the fact or state of a research domain. Palvia et al.'s [2003;2004] analyses of the use of research methodologies in IS are typical descriptive reviews. In the two studies, Palvia and colleagues surveyed articles in seven IS journals (communications of the ACM, Decision Sciences, Information & Management, Information Systems Research, Journal of Management Information Systems, and Management Science) in the years 1993 to 2003. They coded each article for up to two methodologies (primary methodology and secondary methodology), and calculated the frequency and analyzed the trend of each of thirteen research methodologies as used in these papers. The results \"provide the current state of research methodologies in use\" [Palvia et al., 2004;p. 306].",
          "Vote counting, also called \"combining probabilities\" [Rosenthal, 1991] and \"box score review\" [Guzzo et al., 1987], is commonly used for drawing qualitative inferences about a focal relationship (e.g., a correlation is significantly different from 0 or not) by combining individual research outcomes [Pickard et al., 1998]. Some researchers consider vote counting a meta-analytic technology [e.g., Rosenthal, 1978;1991]; and other researchers separate vote counting as an alternative quantitative review method mostly because this method does not analyze effect sizes. It uses the outcomes of tests of hypothesis reported in individual studies, such as probabilities, p-levels, or results falling into three categories: significantly positive effect, significantly negative effect, and non-significant effect. The philosophy is that repeated results in the same direction across multiple studies, even when some are non-significant, may be more powerful evidence than a single significant result [Rosenthal and DiMatteo, 2001]. Rosenthal [1991] provided a comprehensive review of nine different vote counting methods, and discussed the advantages and limitations of each. For illustrative purposes, in Appendix 1 we provide the computational formulas of two most popular vote counting methods: Fisher's procedure of combining p's and Stouffer's procedure of combining Z's. Vote counting does not require other statistics such as effect sizes and construct reliabilities. Thus, it is a conceptually simple and practically convenient method. Under certain circumstances (i.e., small number of sampled studies, when investigated effects are in the same direction) this method could produce statistically powerful results. In one extreme example, Cohen [1993] reported on two studies that dealt with the results of vaccinating monkeys. Because laboratory animals are difficult to obtain and expensive to maintain, the studies involved only six and eleven monkeys respectively (both experimental subjects and control animals). Neither study produced statistical significance. However, when the data were combined in a vote counting analysis, the plevel was considerably smaller and the effect was shown to be large. Vote counting contains some inherent limitations. In particular, it allows a weak test of a hypothesis (e.g., correlation is 0 or not 0) with little consideration of the magnitude of the effect, such as an estimated effect size and associated confidence intervals. In addition, vote-counting assumes homogeneity in the sample population investigated. In case of heterogeneity, which is most common in research, vote-counting cannot detect moderator effects, and the combined significance could be meaningless. Therefore, vote-counting is often suggested as a supplement to meta-analysis in the case of missing effect sizes [Bushman and Wang, 1995;Pickard et al., 1998]. In IS, vote counting is applied to produce a single quantitatively synthesized conclusion from a series experiments. In some fields within IS, such as software engineering, where much research involves modest experimental effects, small sample sizes, and hypothesis testing fails to conclude significant results due to low statistic power, vote counting is found to be particularly useful. For example, in a study of reading techniques for defect detection of software codes, Laitenberger et al. [2001] found that perspective-based reading (PBR) was statistically more effective than checklist-based reading (CBR) in one out of three experiments; the other two were in the same direction but not significant (p-value (one side) = 0.16 and 0.13). When applying Fisher's procedure, Laitenberger and colleagues concluded that PBR was a significantly more effective reading technique for defect detection than CBR, with a combined p-value = 0.000016 (p. 403). Similarly, Pfahl et al. [2004] applied vote counting to analyze results from a series of three experiments to assess the learning effectiveness of using a process simulation model for educating computer science students in software project management. The results supported that simulation involves more learning interests of students than that of without-simulation students (pvalues from the 3 experiments were 0.04, 0.21, and 0.28; the combined p-value were 0.06 (Fisher's procedure) and 0.03 (Stouffer's Z procedure) (p. 137-138).",
          "Meta-analysis is a statistical synthesis method that provides the opportunity to view the \"whole picture\" in a research context by combining and analyzing the quantitative results of many empirical studies [Glass, 1976]. It connotes a rigorous alternative to the narrative discussion of research studies which typify our attempts to make sense of the rapidly expanding research Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He literature [Wolf, 1986]. Since we believe that meta-analysis is of great potential significance and find that it is often inadequately applied in IS, we devote the remainder of this paper to it.",
          "The history of the use of statistical methods to synthesize research findings is as long as most commonly-used statistical procedures [Cooper, 1979]. Hedges [1992] credited Legendre, who invented the principle of least squares to solve regression problems, as an innovator in this area because his original purpose was to combine information across studies. Another often-cited example is Pearson's [1904] study of enteric fever inoculation, in which Pearson presented sets of data collected under different conditions, calculated individual effect sizes that were comparable across data sets, examined homogeneity, and computed an average of the effect sizes to determine the effectiveness of the treatment (inoculation). However, applications of this quantitative research synthesis method were rare before the 1970s [Cooper and Hedges, 1994]. It seems that the method grew in popularity since Glass coined the phrase \"meta-analysis\" in 1976. [Glass, 1976, p.3] Meta-analysis is now a legitimate research review tool accepted by, and dominant in, many fields in the social and behavioral sciences [Field, 2001]. We searched a comprehensive social science database -Social Science Citation Index (SSCI) -with the keyword \"meta-analysis\". The search resulted in 4407 articles published in refereed journals in all covered fields for the period 1992-2004, with an average annual increase of 20 articles (p-value < 0.0001).",
          "In IS, some researchers argued that meta-analysis is a rarely applied research methodology [e.g., Hwang, 1996;Palvia et al., 2003], mostly because of the small number of applications. For example, Hwang (1996) reviewed the use of meta-analysis in IS prior to 1996 and found 6 applications, including one conference paper. We searched SSCI with the restriction of IS as the research domain. The search resulted in 34 published meta-analyses in 1992-2004, with an average annual increase of 0.3 article (p-value = 0.013). The numbers of published metaanalyses in social science in general and in IS in particular are presented in Figure 2. As evidenced in Figure 2b, the use of meta-analysis increased steadily in IS, although the numbers of applications are still small. We believe this trend will be maintained, given the maturating of the field and the growing number of research topics that would benefit from conclusive answers.",
          "Meta-analysis is much less judgmental and subjective than other literature review methods, particularly narrative review, and therefore much more in tune with positivist tradition. The major difference between narrative reviews and quantitative meta-analyses may well be that narrative reviews primarily focus on the conclusions reached in various studies, whereas meta-analyses focus on data, as reflected by the operationalization of variables, the magnitude of effect sizes, and the sample sizes. Qualitative assessments involved in a narrative reviews usually do not take account of both the relative sizes of samples and the significance of the effects measured. For example, a smallsample study with significant results may be equally weighted with a similar large-sample study in such assessments, while an insignificant result may be ignored when it does, in fact, contribute to a body of research that overall, may show significant effects. To synthesize a research literature, statistical meta-analysis uses final results collected from a collection of similar studies [Glass, 1976]. The final results are effect sizes, or the magnitude of the effects. The focus on effect sizes rather than significances of these empirical findings is an advantage over traditional literature review methods. Meta-analysis enables the combining of various results, taking into account the relative sample and effect sizes, thereby permitting studies showing insignificant effects to be analyzed along with others that may show significant effects. The overall result may be either significant or insignificant, but it is undoubtedly more accurate and more credible because of the overarching span of such an analysis. Meta-analysis can focus attention on the cumulative impact of insignificant results that can be significant. For example, two studies showing sgnificance at the 0.06 level are much stronger evidence against the null hypothesis that is a single study at the 0.05 level, all else being equal. Yet, in some fields, the former two studies may not ever be published. Meta-analysis, by combining such results, enables us to see the big picture of the landscape of research results. In other fields, meta-analysis has provided answersto questions that were in great dispute because of conflicts in the results of various studies. For example, the viewing of violence on television was shown to be associated with a greater tendency toward aggressive and anti-social acts through a meta-analysis of more than 200 studies, many of which were individually inconclusive or had reached contrary conclusions [Comstock et al., 1991]. In business research, perhaps the area which made the greatest use of meta-analysis is organizational behavior. There, meta-analyses seem to have been conducted primarily in the source of job performance ratings involving combinations of ratings by supervisor, subordinates, peers and self-ratings [Conway and Huffcutt, 1997;Harris et al., 1988]. While meta-analysis is basically confirmatory in nature, it can also involve exploratory aspects. For example, when high variability exists in the effects that are reflected in various studies, metaanalysis promotes a search for moderator variables. While this use of meta-analysis introduces greater subjectivity on the part of the researchers, the subjectivity is certainly less than that involved in performing a non-quantitative literature review.",
          "Numerous researchers advocated the use of meta-analysis as a better way to conduct research reviews [e.g., Glass, 1976;Hedges and Olkin, 1985;Wolf, 1986;Hunter and Schmidt, 1990;Rosenthal, 1991;Rosenthal and DiMatteo, 2001]. However, as with other research methods, meta-analysis is not free from limitations. In this subsection, we discuss some major concerns with meta-analysis as a research review method.",
          "By its statistic analysis nature, meta-analysis contains an inherent sampling bias toward quantitative studies that report effect sizes. Using Palvia et al.'s [2003;2004] typology, the types of research that may provide data to a meta-analysis include survey, laboratory experiment, field study, and field experiment. Other types of research, such as frameworks and conceptual models, case studies, speculation/commentary, mathematical models, secondary data, interview, and qualitative research, are unlikely to be sampled in a meta-analysis. This limitation suggests that a significant number of studies (according to Palvia et al. [2004], these studies add up to 50.3% of all IS publications in seven leading IS journals) will be ignored when conducting a metaanalysis. In addition, some researchers may combine quantitative and qualitative research methods in their studies of the same phenomenon, called triangulation [Gable, 1994;Kaplan and Duchon, 1988;Lee, 1991;Mingers, 2001]. Triangulation may help deepen and widen our understanding on a certain phenomenon. One example is Markus' [1994] study of the use of e-mail in organizations. Markus conducted a field survey and data did not provide much support to the hypotheses developed from information richness theory; Markus also interviewed some respondents, whose answers and comments provided insight on factors affecting their use of email. If a meta-analysis on communication effectiveness were to be conducted, only the empirical part of Markus's [1994] study would be included and the qualitative part, which provides more value to media use research, would be ignored. The sampling bias toward empirical studies, a limitation of meta-analysis, is rarely addressed in discussions of this research method. We believe that it is important to call to the attention of meta analysts and researchers that the results from a meta-analysis are not necessarily more creditable or representative of a research population than those from a narrative review.",
          "Meta-analysis does not generally differentiate studies by their quality. This issue is often referred to as \"garbage in and garbage out\" [Hunt, 1997]. Research studies vary considerably in their research designs and approaches, sampling units, methods of measuring variables, data analysis methods, and presentations of research findings. The inclusion of the results from poorlyconducted studies with flawed designs into a meta-analysis could confuse the full understanding of the literature body investigated or even lead to unfounded conclusions. The judgment of quality is rather subjective, although some techniques have been suggested to correct this error [e.g., Rosenthal, 1991]. However, these techniques introduce other biases about the selection and weighting of quality criteria.",
          "Publication bias [Begg and Berlin, 1988], also known as the file-drawer problem [Rosenthal, 1979;Iyengar and Greenhouse, 1988], refers to the observation that significant results are more likely to be published while non-significant results tend to be relegated to file drawers. Thus, the meta-analysis result will focus on an unrepresentative proportion of a total research population. Of course, publication bias applies to all review methods. It is \"particularly problematic for a metaanalysis whose data come solely from the published scientific literature\" [Duval and Tweedie, 2000]. Using an unrepresentative set of data may result in conclusions biased toward significance or positivity. Although some correction techniques have been developed [e.g., Duval and Tweedie, 2000], the best solution to avoid this bias is to search multiple databases in a systematic way and sample studies from various sources [Rosenthal and DiMatteo, 2001]. For example, in their study of the effects of management support and task interdependence on IS implementation, Sharma and Yetton [2003] searched the literature in various ways, including bibliographic databases, manual searches, and the bibliographies of existing works. They located 22 empirical studies, of which 11 are from journal publications, 9 from dissertations, and 2 from book chapters. The comprehensive search resulted in a diverse sample that \"both increases the power of the meta-analysis by maximizing the number of studies and reduces (publication) source bias\" (p. 542).",
          "One criticism of meta-analysis is that it may compare \"apples and oranges,\" aggregating results derived from studies with incommensurable research goals, measures, and procedures. It is argued, therefore, that meta-analysis may sometimes be analogous to taking apples and oranges and averaging such measures as their weights, sizes, flavors, and shelf lives [Hunt, 1997]. This problem exists for all review methods, qualitative or quantitative, in that \"exact replications probably cannot occur\" [Hall et al., 1994;p. 20], \"(studies) ... are rarely precisely the same\" [Rosenthal and DiMatteo, 2001;p. 68]. This problem is not of dominant significance, especially when we want results that are generalizable to fruits, or to a broad research domain. On the other hand, synthesists must be sensitive to the problem of attempting aggregation of too diverse a sampling of studies.",
          "The statistical power of detecting a genuine effect size depends on both the number of studies and the total cumulated sample sizes included in a meta-analysis. The more studies that are included in the meta-analysis, the more creditable are the results at representing the investigated research domain. Using a Monte Carlo simulation, Field [2001] found that a meta-analysis should include at least 15 studies, otherwise the type I error (accepting a false null hypothesis) could be severely inflated. If the investigator cannot identify enough empirical studies on a common topic, it may indicate that the research domain is too immature for a conclusive study such as meta-analysis. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He \"when meta-analytical results have less evidential value because the number of individual studies in the meta-analysis is small … the alternative is neither reversion to reliance on the single study nor a return to the narrative review method of integrating study findings: both are vastly inferior to meta-analysis in the information yield\" [Hunter and Schmidt, 1990;p. 420]. Reviewers should base analyses on the largest number of studies available [Matt and Cook, 1994]. If the collection of studies is largely incomplete, then conclusions drawn from analysis are limited in scope. An exception will be meta analyzing a series of experimental studies. Here a researcher is interested in concluding generalizability of a proposition within, rather than beyond, a defined set of studies. For example, Laitenberger et al. [2001] and Phafl et al. [2004] meta analyzed a series of three experiments (an initial experiment and two replications). The combined results were stronger in statistical power than results calculated from any individual experiment.",
          "Comparatively, meta-analysis is a straightforward or \"formalized systematic review\" procedure that is more standardized than other review methods [Hunter and Schmidt, 1990]. Given the same set of data or sampled effect sizes from a literature body, different researchers should arrive at the same conclusion via meta-analysis. In other words, a typical meta-analysis is often based on the procedures and analytic methods that are commonly accepted. Thus, the results from a meta-analysis are often treated as reliable and objective [Rosenthal and DiMatteo, 2001]. However, a particular meta-analysis may contain methodological errors that lead to a false conclusion. We observed serious methodological errors in some IS meta-analyses. For example, in the two meta-analyses conducted by Mahmood andcolleagues [2000, 2001], some combined effect sizes were larger than any sampled individual effect size. In their meta-analysis of the correlations between perceived ease of use and Information technology usage (Table 1 of Mahmood et al. [2001], p. 116), Mahmood and colleagues concluded a combined effect size as large as 0.678, while the sampled correlation coefficients ranged from 0.059 to 0.375. If the result is used by a researcher who performs statistical power analysis at the 80% level to guide a research design (suggested by Cohen [1988Cohen [ , 1992]]), the researcher will conclude that a sample size of 28 may be adequate to detect a significant relationship (at α=0.05 level) between the two variables. In fact, the sampled studies in Mahmood et al. [2001] involved much larger sample sizes, ranging from 61 to 786, with a mean of 185. Statistics Used in a Meta-Analysis. To meta-analyze an issue, the researcher should extract the following statistics and information from the studies: Most IS studies report these statistics in their presentations. In the literature, effect sizes are of various forms and can be categorized into two main families, the r family and the d family. The r family effect sizes report correlations between variables; the specific type depends on whether the variables are in continuous (Pearson's r), dichotomous (phi), or ranked form (rho). In IS research, the most popularly reported effect size is the Pearson's r. The d family are used mostly in laboratory experiments and measure the standardized difference between an experimental group and a control group. (There are three main members in this family: Cohen's d, Hedges' g, and Glass's ∆. The three are calculated in a similar way: the difference between two means is divided by some variance. They differ in the denominator: the square root of the pooled variance of the two groups for Cohen's d, the square root of the pooled variance for Hedges' g, and the square root of the control group variance for Glass's ∆.) These effect sizes can be readily converted to one another. Effect sizes can also be calculated from various testing statistics, such as t, F, χ 2 , or Z, or their associated p levels. Detailed computational formulas to calculate and convert these statistics are beyond the purpose and scope of this study. Interested readers can refer to Wolf [1986], Hunter and Schmidt [1990], Rosenthal [1991], and Rosenthal and DiMatteo [2001]. Other statistics, including the descriptive statistics of investigated variables, cannot be directly used as effect sizes. As pointed out by Rosenthal and Dimatteo [2001], \"an r effect size cannot be computed from kappa, percent agreement, relative risk, risk difference, or the odds ratio unless all the raw data are available so the meta-analyst can compute the proper index\" (p.72). Fixed vs. Random Effects Models, Similar to other statistical methods, meta-analysis methods are developed based on assumptions about the population from which studies are taken. The two common assumptions lead to two different analysis methods: fixed-effects and random-effects models. The fixed-effects model assumes that studies in the meta-analysis are sampled from one population with a fixed \"true\" effect size. In other words, the true effect size is assumed to be the same for all studies included in the analysis, and the observed variance among effect sizes is dominated by sampling errors, which are unsystematic and can be estimated [Hunter and Schmidt, 1990]. The assumption of one population underlying a meta-analysis restricts the conclusions from being generalized to a study not included in the analysis unless the study shows independent evidence of belonging to the population; i.e., unless it is a close replication of the studies included in the meta-analysis. In contrast, the random effects model assumes that population effect sizes vary from study to study. As such, a study included in such a meta-analysis can be viewed as being sampled from a universe of possible effects in a research domain [Field, 2001]. As long as the meta-analysis covers the literature comprehensively, the conclusions are generalizable to the research domain as a whole and can be applied to studies not included in the analysis. In statistical terms the two models differ in the calculation of the weights used in the analysis, which in turn affects the standard errors associated with the combined effect size. Fixed-effects models use only within-study variability in their weights because all other \"unknowns\" in the model are assumed to be constant. In contrast, random-effects models account for the errors associated with sampling from populations that themselves were sampled from a superpopulation. The error term, therefore, contains variability arising from differences between studies in addition to within-study variability. Standard errors in the random-effects model are, therefore, larger than in the fixed case, which makes significance tests of combined effects more conservative [Field, 2003;p. 107]. Although random-effects models generally appear to be superior, fixed-effects models are in common use. If the fixed-effects model is employed for a meta-analysis, the assumption of one \"true\" effect size across studies should be tested before combining effect sizes [Hedges and Olkin, 1985]. The test of this assumption, labeled as a homogeneity test, is a chi-square test of the null hypothesis that all effect sizes are the same after controlling for sampling errors. The test result indicates whether the null hypothesis can be rejected at a certain level. Only when the test result is insignificant can the sampled studies be combined (e.g., calculating the combined effect sizes and their confidence intervals). In many cases, the test indicates a violation of the assumption and there is a need to switch to the random-effects model; however, in practice sometimes the fixed-effects model is chosen and the test is not performed. In contrast, the random-effects model assumes variation between the populations underlying the various studies that are incorporated into the meta-analysis. The homogeneity test in this case examines whether the interaction between sampling error and between-study variance is zero or Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He not. An insignificant test result (which occurs much of the time) justifies the techniques that are used under this model. As suggested by many meta-analysts [e.g., Field, 2001;Field, 2003;Hedges and Vevea, 1998;Rosenthal and DiMatteo, 2001], homogeneity is rare and the randomeffects model should be applied in most research domains.",
          "Over the past 20 years, three methods of meta-analysis emerged to be widely used [Field, 2001;Johnson et al., 1995]: the method devised • by Hedges and Olkin [1985]; • by Rosenthal and Rubin [Rosenthal, 1991], and • by Hunter and Schmidt [1990]. In IS, a fourth method ─ that devised by Glass and his associates [Glass et al., 1981] is also used. The four methods are briefly discussed subsection. Interested readers may examine the references for more detailed discussion on the methodologies. In addition, • Johnson et al. [1995] and Field [2001] compared the first three methods basing on Monte Carlo simulation tests; • Cooper and Hedges [1994] provided integrative reviews of the commonly-adopted meta-analytic approaches as well as computational formulas; • Lipsey and Wilson [2001] described well the bolts and nuts of the whole process. These studies will serve as good resources for potential meta-analysts.",
          "This approach to meta-analysis is based on a weighted least squares technique [Hedges, 1987]. In this approach, study outcomes (i.e., effect sizes r) are transformed into a standard normal metric (using Fisher's r-to-Z transformation). Then, the transformed effect sizes are weighted by the inverse variances of each study. For the fixed-effects model, the variance is within-study variance, which is determined by sample sizes only. For the random-effects model, the variance is composed of within-study variance and between-study variance, the latter of which is from a chi-square test (Q test under the fixed-effects model). The combined effect size is the average of the weighted effect sizes, and its variance is the reciprocal of the sum of the weights. A significance test (Z-test) and confidence intervals of the combined effect size are then calculated.",
          "Under a fixed-effects model, the Rosenthal-Rubin method employs essentially the same techniques as the Hedges-Olkin method, except for the significance test of combined effect size [Field, 2001]. Rosenthal and Rubin [Rosenthal, 1991] advocate the use of significance metrics (i.e., Zs associated with one-tailed probabilities) from sampled studies and examining the combined Z for the overall significance of the mean effect size. Rosenthal [1991] did not present a random-effects version of his meta-analysis procedures in his original work. In a later study, Rosenthal and DiMatteo [2001] suggested \"un-weighting\" the effect sizes when meta-analytically integrating them as an approach based on a random-effects model. The basic logic is to treat studies as the unit of analysis in observing of the between-study variance. Consistent with the random effects model, the combined effect size of the unweightedeffect approach is less statistically powerful and has larger confidence intervals as contrasted with that calculated from the weighted-effect approach, and can be generalized to studies not yet in the sample. \"If only one approach were to be used, it would be the one we prefer\" [Rosenthal and DiMatteo, 2001;p. 70]. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He",
          "This method is distinct for its efforts to correct effect size indices for potential sources of errors before meta-analytically integrating the effect sizes across studies [Johnson et al., 1995]. Besides the sampling error, other sources of errors include measurement error, range variation, construct validity, and variance due to extraneous factors. Errors other than sampling are labeled as \"Attenuation Factors\" in Hunter and Schmidt [1990], because they impact lower the observed correlations between investigated variables systematically, which can be corrected if assessed. Therefore, when information about these sources of error is available, this feature of Hunter-Schmidt method may recommend its use [Johnson et al., 1995]. In IS, at least one source of error, measurement error, is routinely assessed and reported by construct reliability (e.g., Cronbach α) [Chau 1999]. However, of the IS meta-analyses that we identified as using this method, none study explored this feature and corrected measurement error or other sources of error. To test moderator variables, Hunter and Schmidt [1990] suggested a partitioning approach; that is, to break the set of studies into subsets using the moderator variable and to do separate metaanalyses within each subset of studies. Then, the difference between subsets is tested to conclude the magnitude and significance of this moderator variable.",
          "Glass's approach to meta-analysis focuses on the detection of moderator variables. This method can be summarized as: 1. simulation of descriptive statistics across studies; 2. the coding of perhaps 50 to 100 study characteristics, such as date of study and number of threats to internal validity, and 3. regression of study outcome onto the coded study characteristics. The characteristics that show significant effects on the study outcome are considered to be moderators. Using a Monte Carlo test, Hunter and Schmidt [1990, p. 86-89] illustrated that this method has a severe problem of capitalization on (chance) sampling errors. Sampling errors are large because the sample size for looking at study characteristics is not the total number of subjects in the studies, but the number of studies. Correlating various study characteristics with study outcome leads to massive capitalization on chance when the correlations that are large enough to be statistically significant are identified ex post facto. A general suggestion for this approach is to derive few moderators basing on logical reasoning and existing theory before conducting the meta-analysis.",
          "After the term was coined by Glass in 1976, meta-analysis received much research attention. In early 1980s, many discussions of meta-analysis were centered on the legitimacy of this research method, i.e., comparing with other review methods, and identifying the advantages, limitations, and statistically soundness of meta-analysis [e.g., Glass, 1976;Hunter et al., 1982;Chow, 1987;Hedges, 1987]. The discussions of meta-analysis progressed to more advanced methodological topics, such as its application to structural equation modeling [Hom et al., 1992], and levels of analysis [Ostroff and Harrison, 1999]. This section reviews recent developments of meta-analysis in the analysis of moderator effects and mediator effects, the two issues that are most relevant to theory testing/building in IS. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He",
          "In the context of meta-analysis, a moderator variable is a systematic difference among studies under review that might explain differences in the magnitudes or signs of observed effect sizes. The study of moderator effects often involves two stages: the detection, or exploratory analysis, of possible moderators, and the assessment, or confirmatory analysis, of theoretically suggested moderators. The various moderator identification and assessment methods are shown in Graphing. As sample size increases, the effect size would theoretically approach the population effect size. Sample effects sizes have greater variability with smaller samples than with larger samples. Simple and intuitive, graphing is suggested for preliminarily detecting naturally occurring groupings and possible moderator effects in a meta-analytic data set [Rosenthal and DiMatteo, 2001]. Q Statistics or Chi-Square Test [Hedges and Olkin, 1985]. This test generates a decision rule specifying whether the variability in standardized effect sizes is statistically significant. The test is based on a chi-square assessment of the level of variance across study results relative to the sampling error variance across studies. This test is often referred to as homogeneity test, and can serve as a criterion for selecting a fixed effects model vs. a random effects model for a metaanalysis. Variance Analysis [Hunter et al., 1982]: The variance of sampled effect sizes is corrected for statistical artifacts, such as sampling error, differential reliability, and restriction of range. When artifacts fail to account for 75% of the variance, a search for moderator variables is indicated [Hunter and Schmidt, 1990]. Outlier Test. Studies that contain effect sizes more than two or three standard deviations from the mean may be examined. Differences and similarities between the studies in the tails may suggest possible moderator variables. Traditional outlier detection techniques include box-plot analysis for univariate data [Mosteller and Hoaglin, 1991] and studentized residuals for bivariate data [Freund and Littell, 1991]. An advance in this area is Huffcutt and Auther's [1995] Sample Adjusted Meta-Analytic Deviancy (SAMD) statistic, which takes into the account the sample sizes in a metaanalytic data set. It is based on the principle of sampling error that effect sizes based on a small sample size are more likely to be deviant than those from a large sample size [Hunter and Schmidt, 1990]. Beal et al. [2002] further discussed possible refinements of this technique by adjusting the inherent skewed distribution of effect sizes. Ordinary Least Squares (OLS) [Glass, 1977]. Regress the effect size on the potential moderator variable, using individual study effect size as the dependent variable and the moderator variable as the independent variable [Wolf, 1986;Glass et al., 1981]. If the coefficient is significant, then the effect of the moderator variable may be significant. This method is criticized for risk of capitalization on chance [Hunter and Schmidt, 1990], and is suggested for assessing moderator effects only on a confirmatory basis. Weighted Least Squares or WLS [Hedges and Olkin, 1985]. OLS assumes constant variance across observations or moderator variables retrieved from individual studies. This assumption is violated because sample error (main component of variance after controlling for moderator effects) is a function of effect size and sample size. Hedges and Olkin [1985] suggested using WLS and weight the multiple regression according the inverse of the sampling error variance. This method, although less popular than OLS, gives a more accurate assessment of moderator effects in most conditions [Steel and Kammeyer-Muller, 2002]. Partition Test [Hunter and Schmidt, 1990]. This test divides sampled effect sizes into subgroups by moderator factors, and compares subgroup means and variance, to assess if the means are significantly different. This method is particularly suggested for categorical moderator factors (e.g., gender, research methods, analysis levels, technology contexts). When applied to continuous moderator factors, such as by dividing data set into subgroups along a continuous moderator variable, this method performs poorly and underestimates moderator effects in most conditions [Steel and Kammeyer-Muller, 2002].",
          "Meta-analysis was initially developed to examine first-order effects, such as treatment effects (the \"d\" group effect sizes) or correlation effects (the \"r\" group effect sizes). Other effects, such as mediator effects or partial correlation coefficients, were not addressed. Such relations can often form the basis of a theory and help establish a mediator mechanism or explain a causal relationship. Assessing mediator effects in structural relationships as an important meta-analytic topic received much attention. Becker [1995] developed a technique to address structural relationships, analyzing whether a common population correlation matrix underlies a set of sampled empirical results. The analysis unit is correlation matrix instead of correlations. As few studies report the correlation matrix, application of the technique is limited in practice. One illustrative, but unsuccessful, example in IS is Legris et al.'s [2003] meta-analysis of the technology acceptance model (TAM). After an extensive search of empirical TAM studies, Legris and colleagues found usable matrices in only three out of 22 studies. Therefore, the small sample size resulted in \"a statistic shortfall\" and \"limit(ed) the presentation of the findings to the general conclusion\" (p. 202). Two alternative approaches to study mediator effects are: 1. combining and analyzing meta-analytically-developed correlations; and 2. directly studying coefficients of interest as the effect sizes [Rosenthal and DiMatteo, 2001]. Taking TAM as an example, the core model (figure 3) suggests that perceived ease of use (EU) and perceived usefulness (U) are two important predictors of an individual's behavioral intention to adopt a technology (BI); in addition, perceived usefulness partially mediates the effect of perceived ease of use on behavioral intention. The correlation coefficients (r's) and  The three equations hold for linear-regression-based analyses; they may differ a bit for structuralequation-modeling-based analyses (e.g., PLS, LISREL) because of different algorithms. But the differences are minor. In other words, we can infer the magnitude and the strength of path coefficients based on a set of meta-analytically-developed correlation coefficients. When applying the second approach -combining β's as the effect sizes, special caution should be taken that the sampled coefficients represent the relationship between the independent and the dependent variable controlling for other factors. Both approaches were applied in another TAM meta-analysis conducted by King and He [2005]. This study meta-analyzed 88 TAM empirical studies, and the results from the two approaches were almost identical.",
          "In this article, we first compare different review methods, then focus our discussion on metaanalysis, an important quantitative literature review method that is underutilized but which appears to offer great potential in IS literature synthesis research. The benefits from meta-analysis changed the course of research in several fields, and it is possible that these benefits can be achieved in IS as well. One distinct feature of IS is the rapid changes that occurred in both the technologies and the applications. The dynamic nature of IS requires accurate assessments of newly developed technologies and business practices in a timely fashion. By synthesizing existing empirical studies, meta-analysis can serve as an efficient tool to satisfy the needs for overall conclusions about phenomena, without the burden of conducting new research in a particular situation, or the dilemma of selecting from competing theories or different perspectives. Some researchers already work in this direction. For example, the development of various group support systems (GSS) in academia encouraged the use of teams, especially virtual teams, in organizational settings. However, the effectiveness of GSS was questioned by some inconsistent empirical findings in the literature [Fjermestad and Hiltz, 1999]. To address the concern, Dennis and Wixom [2003] meta-analyzed previous empirical studies. Their results not only validated GSS on improving group performance, but also explained conditions (i.e., the moderator effects of task, tool, the type of group, the size of the group, and facilitation) under which GSS would be most effective. Similar studies were done on computer-mediated-communication [Baltes et al., 2002], computer graphics [Hwang and Wu, 1990], and distance education [Allen et al., 2004]. Great potential exists for meta-analyzing other emerging or changing technologies and business practices, such as virtual teams and virtual organizations, knowledge acquisition technology (e.g., different interview techniques), system development practices (e.g., prototyping, user-centered system design, and rapid application development), IT governance, and knowledge management systems and practices, to name a few. A few empirical studies addressed these issues, and overall conclusive results will warrant the advance of the areas. We expect meta-analysis will help direct future research in other IS issues. For example, the study of IT productivity (or payoff of IT investment) generated discussions of pros and cons of IT investment by both academicians and practitioners, a debate commonly known as the \"productivity paradox\" [Roach, 1987;Stassmann, 1985]. Kohli and Devaraj [2003] meta-analyzed existing literature on this issue. Their study not only validated the relationship between IT investment and firm performance, but also identified various factors that influence the likelihood for an investigation to find such a relationship. A future study in this area may benefit from Kohli and Devaraj [2003] by following their design strategy. In addition, meta-analysis may help improve the publication practice in the IS field. Many studies that are performed with small sample sizes are never reported and many studies that produce non-significant results are rejected by journals. The development of a tradition of meta-analysis in IS would encourage the \"publication\" of such studies ─ perhaps on web sites and in electronic journals. Various techniques are used to conduct meta-analysis; However, while no single technique is universally-agreed-upon as a way to perform such a study [Hall et al., 1995], the basic procedures for conducting a meta-analysis are well-understood [Rosenthal and DiMatteo, 2001]. Based on a review of the commonly-used meta-analysis methods, we suggest the following procedures for conducting a meta-analysis in IS. The procedure is illustrative and is designed to ensure that readers share a common understanding of statistical meta-analysis. 1. Define the research domain and the relationships of interest. 2. Collect studies in a systematic way. Try multiple databases with the same selection criterion not only to enlarge the data pool, but also to avoid bias toward certain journals. 3. Extract effect sizes ─ the strength of a relationship or the magnitude of a difference 1 . If the researchers did not reported the desired effect sizes, scour the articles for the Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He information necessary to calculate these effects. It is also recommended contacting the authors for the needed information2 . 4. Select either the random effects (suggested for most cases) or the fixed-effect model, and a specific analysis method to combine the effect sizes. 5. Examine the variability among the obtained effect sizes. Perform a homogeneity test, or plot effect sizes graphically to judge the range of effect sizes and the existence of possible moderating effects. 6. Examine the signs and magnitudes of the combined effect sizes (the means). Although t-tests are commonly employed for the significance level of an effect size, it is more useful to calculate confidence intervals around the mean to indicate the range of the effect size [Rosenthal and DiMatteo, 2001]. This approach is especially appropriate for the random-effects model, because this model assumes variations among effect sizes. A combined effect size implies no more than an average of a set of possible population effect sizes. 7. If the objective of the research is to find or test moderating effects, studies should be coded for characteristics of their contexts. Code the characteristics of individual articles in an unbiased way. Coders (often Ph.D. students or colleagues) should be \"blind\" to the research, and the internal consistency of the coding results should be tested. 8. To test the moderating effects, two methods are commonly used: a. Subgroup comparison: group studies according to their coded research contexts, then compare the combined effect sizes that are calculated within each subgroup. b. Regression: regress the research study characteristics on the effect sizes, the significant factors indicate the significant moderators. 9. Summarize and discuss the findings. Editor's Note: This article was fully peer reviewed. It was received on July 7, 2005 and was published on October 17, 2005."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "Significant results are more likely to be published while non-significant results tend to be relegated to file drawers.",
            "context": "Publication Bias Publication bias [Begg and Berlin, 1988], also known as the file-drawer problem [Rosenthal, 1979; Iyengar and Greenhouse, 1988], refers to the observation that significant results are more likely to be published while non-significant results tend to be relegated to file drawers. Thus, the meta-analysis result will focus on an unrepresentative proportion of a total research population.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The passage contains a clear, explicit definition of the term “racial discrimination”: “The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity.” It also offers a definition of “structural racism.” Hence the section definitely includes an obvious definition of a term.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "Cross-culturally, women, relative to men, are a disadvantaged group, as indicated by, for example, differences in earnings and the low percentage of women in the most powerful roles in business and government (United Nations Development Programme, 1998). Nevertheless, Eagly and Mladinic (1994) found that women are actually stereotyped more positively than men are, with men as well as women attributing highly favorable traits (e.g., warmth and nurturance) to women. For an antipathy model of prejudice, the coexistence of women's general subordination and Eagly and Mladinic's \"women are wonderful\" effect is a paradox. One alternative is that women are viewed positively on some dimensions, such as warmth, and negatively on others, such as competence (Eagly & Mladinic~ 1994;Fiske, 1998;Fiske et al., 1999;Glick & Fiske, in press). Another related alternative is that subjectively favorable attitudes toward women can themselves be a form of prejudice in that they s~erve to justify and maintain women's subordination. Glick and Fiske (1996) hypothesized that hostile and benevolent attitudes toward women are complementary components of sexism common among past and present human societies. Apart from anecdotal evidence that polarized attitudes toward women (e.g., the virgin-whore dichotomy; Tavris & Wade, 1984) have existed since ancient times (see Pomeroy, 1975), sound theoretical reasons suggest that HS and BS are, and long have been, pervasive prejudices. Glick and Fiske (1996) argued that HS and BS stem from social and biological conditions common to human societies: patriarchy, gender differentiation, and sexual reproduction. Patriarchy (male dominance), varying considerably in degree, is widespread across cultures (Harris, 1991;Pratto, 1996), though not necessarily universal (see Salzman, 1999). Additionally, in most cultures, women and men are differentiated in that (to a greater or lesser extent) they often inhabit different social roles and occupations (Eagly & Wood, 1999) and are attributed different traits (Williams & Best, 1982). Finally, sexual reproduction is a biological constant that is related to social roles, as women's roles are largely defined by childbearing and child rearing (Eagly, 1987), and that promotes intimate relationships between men and women. Patriarchy, gender differentiation, and sexual reproduction combine to create HS and BS. Dominant groups, whether based on gender or other distinctions, inevitably propagate systemjustifying ideologies of their superiority, which are often accepted even by members of socially subordinate groups (Jost & Banaji, 1994; see also Jost & Burgess, 2000;Jost, Burgess, & Mosso, in press). Thus, men's dominance creates HS, hostile attitudes about women. This dominance is largely enacted in and reinforced by gender roles and stereotypes. Furthermore, men often exert their power over women within sexual relationships, and women can potentially counter men's power through sexuality (e.g., by using sexual attractiveness to control men). Therefore, concerns about power, gender differentiation, and sexuality are bound together as components of HS. In a modem context in which social move-ments and increasing gender equality threaten traditional male dominance, HS may be directed most strongly at women who challenge men's power (e.g., feminists) and status (e.g., career women), as well as toward women who are perceived as using their sexual allure to gain power over men (e.g., temptresses). However, sexual reproduction and men's dependence on women to fulfill domestic roles create a dependency and intimacy between the sexes that counterbalances sexist hostility with a subjectively benevolent view of women, BS. Although men dominate crossculturally, they rely on women to produce and to nurture offspring, for domestic labor, and to fulfill sexual and intimacy needs, lending women power in intimate relationships (Guttentag & Secord, 1983). This dependence, Glick and Fiske (1996) argued, precipitates subjectively benevolent but paternalistic attitudes toward women, as men \"can't live without them.\" BS is sexist in that it presumes women's inferiority (it recognizes and reinforces patriarchy by portraying women as needing men to protect and provide for them) hut is subjectively positive (from the perspective of the sexist perceiver) in that it characterizes (at least some) women as wonderful, pure creatures whose love is required to make a man whole. Like I-IS, BS encompasses attitudes related to power, gender differentiation, and heterosexuality. The 22-item Ambivalent Sexism Inventory (ASI; Glick & Fiske, 1996), initially developed and validated in six studies (involving both college students and older adults) in the United States (Glick & Fiske, 1996), is a self-report measure of sexist attitudes composed of separate 11-item I-IS and BS subscales (see Appendix). HS is evidenced by an adversarial view of gender relations in which women are perceived as seeking to control men, whether through sexuality or feminist ideology. Potential HS items that baldly asserted women's inferiority were strongly rejected by respondents in the United States and were therefore not included in the scale. Thus, the I-IS scale is a relatively subtle and contemporary measure of sexist hostility. Nevertheless, given that the predominant theme is a hostile reaction to women challenging men's authority and power, we believe that it is an extension of, and is consistent with, traditional forms of sexist hostility; this belief is supported by moderate to strong correlations (Glick & Fiske, 1996) between HS and measures of blatant sexism such as the Attitudes Toward Women Scale (AWS; Spence & I-Ielmreich, 1972) and the Old-Fashioned Sexism Scale (Swim, Aikin, Hall, & Hunter, 1995). In contrast, BS items suggest that women are pure creatures who ought to be adored and placed on a pedestal but are also weak and in need of protection. Although the BS scale, because it represents a subtle form of prejudice, is sometimes lumped together with measures of uniquely contemporary forms of sexism such as Modem Sexism (MS; Swim et al., 1995) or Neo-Sexism (NS; Tougas, Brown, Beaton, & Joly, 1995), BS was never hypothesized to be a recent development (Glick & Fiske, 1996). Indeed, BS items have a much less contemporary flavor (e.g., the notion that women are more pure than men) than I-IS items do, perhaps because BS, because of its positive tone, has not been challenged as vigorously in egalitarian societies as has sexist hostility. Unlike the BS scale, the MS and NS measures assume that sexism is an antipathy but that the antipathy is disguised as political and social egalitarianism within cultures that promote this value. The attitudes tapped by the BS scale are not disguised as egalitarian; rather, they seem closer to medieval ideologies of chivalry (Tavris & Wade, 1984) than they are to contemporary \"political correctness.\" BS is rooted in the structure of personal relationships between men and women, not in public politics. The BS scale is correlated moderately strongly with HS, but once this relationship is controlled, it has strong discriminant validity, correlating weakly or not at all with fi variety of other measures of sexism based on an antipathy model, such as the AWS and the MS Scale (Glick & Fiske, 1996;Massel~ & Abrams, 1999). Three subdimensions, related to the underlying structural factors of patriarchy, gender differentiation, and sexual reproduction, consistently emerge as BS subfactors in factor analyses of the ASI: protective paternalism (e.g., women ought to be rescued first in emergencies), complementary gender differentiation (e.g., women are more pure than men), and heterosexual intimacy (e.g., every man ought to have a woman he adores). Although HS scale items also address power relations, gender differentiation, and sexuality, this scale has proven to be unidimensional (see Glick & Fiske, 1996, for speculations as to why this is the case). That HS and BS are complementary ideologies is suggested by their positive correlation. When this correlation is controlled statistically, however, HS predicts negative and BS predicts positive attitudes toward and stereotypes about women (Glick, Diebold, Bailey-Werner, & Zhu, 1997;Glick & Fiske, 1996), supporting the contention that BS is a subjectively positive form of prejudice. Although people who score high on both scales can be characterized as ambivalent toward women, they seem to reconcile their hostile and benevolent attitudes by classifying women into good and bad subtypes, evincing, for example, hostility toward career women and affection for homemakers (Glick et al., 1997). BS may help to legitimate HS by allowing sexist men to conceive of themselves as benefactors of women and to excuse their hostility as being directed only at women who allegedly deserve it. Furthermore, because BS fosters behaviors that are normally deemed prosocial, such as helping or protecting women, women tend to accept rather than challenge BS (Glick & Fiske, 1996;Kilianski & Rudman, 1998). The claim that BS is a form of sexism relies on the putative relationship of BS to the subordination of women. Jackman (1994) illuminated the greater effectiveness of paternalistic prejudices such as BS, as compared with outfight antipathy, in gaining compliance (rather than resistance) from low-status groups. Whereas HS serves to punish women who fail to conform to (male-defined) acceptable roles, BS represents the rewards women reap when they do conform (Glick et al., 1997). Women who embrace conventional, sanctioned roles are protected and revered. As any psychologist knows, reward is far more effective than punishment is in eliciting conformity. Thus, the combination of HS and BS may be particularly effective as complementary justifications or \"legitimating myths\" (Sidanius, Pratto, & Bobo, 1994, p. 999) that serve to maintain gender inequality. Jost and Banaji's (1994) system-justification theory suggests that subordinate groups often accept legitimizing myths that justify the status quo but that acceptance of such ideologies is tempered because overtly hostile ideologies about one's group contradict individual and group interests (Jost et al., in press). In contrast, prejudiced ideologies that are ostensibly benevolent may be seductive to subordinate group members, because they do not appear to contradict self-and group interests. Women may be attracted to BS more than to HS because it not only justifies the system as a whole but also promises rewards from the more powerful group (protection, adoration, intimacy). What may be particularly insidious is that women are likely to value these rewards more highly in cultures in which they perceive many men to be hostile. In highly sexist societies, men provide both the threat (HS) and the solution to the threat (BS and the protection, provision, and affection it promises); women in these societies are presented with a stark choice--reject BS and face the wrath of HS, or accept BS and avoid HS. In more egalitarian societies, women may be freer to reject BS as well as HS, because they are less likely to be as dependent on men for resources and because rejecting BS is not as likely to elicit sexist hostility.",
        "reasoning": "The passage contains several explicit explanatory statements that serve as definitions of key terms. For example, it states \"Patriarchy (male dominance)\", providing a clear, parenthetical explanation of the term. It also uses phrases such as \"HS and BS are complementary components of sexism\" and explains HS and BS in the context of gender relations. These succinct descriptions match the cue phrases ('is defined as', 'refers to', 'means') that indicate a definition. Therefore, the section does contain an obvious definition of a term.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "Representation is a vexed term in other ways as well. Those who favor Aristotelian distinctions sometimes use the word presentation for stories that are acted and representation (re-presentation) for stories that are told or written. The difference highlights the idea that in theater we experience the story as immediately present while we do not when it is conveyed through a narrator. My own view is that both forms of narrative are mediated stories and therefore involved in re-presentation, conveying a story that at least seems to pre-exist the vehicle of conveyance. A good counter-argument to my position asks: Where is this story before it is realized in words or on stage? The answer, so the argument goes, is: Nowhere. If that is the case, then all renderings of stories, on the stage or on the page, are presentations not representations. The extent to which stories are at the mercy of the way they are rendered is an important issue, and I will return to it in this chapter and later in this book. But for my definition, I will stick to the term \"representation.\" I do this in part because the word is so commonly used in the way I am using it and in part because it describes at least the feeling that we often have that the story somehow pre-exists the narrative, even though this may be an illusion.",
        "reasoning": "The section contains a clear explanation of how the author (and others) use the term “representation.”  It states that “representation (re‑presentation)” is used for stories that are told or written, as opposed to “presentation” for stories that are acted.  This distinction and the author’s explanation that “representation” refers to the idea that stories seem to pre‑exist the vehicle of conveyance is an explicit definition of the term.  The wording “For my definition, I will stick to the term 'representation'” further confirms that the author is defining or clarifying the meaning of that term.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "Table 1 shows statistics about the documents of the PolyNarrative dataset, broken down per language, and aggregated over both domains. We can see that, except for Russian, all languages have approximately 400 annotated training documents; the test documents are approximately 35 for each language. We aimed at articles of relatively long text with a median length of 500 words and a minimum length of 250 words. Even after the rigorous data selection process, the final label distribution after annotation is highly imbalanced (see Table 2 and Appendix B for a breakdown per language). Additionally, the distribution of labels varies noticeably from language to language, due to differences in media interest across countries, as already discussed. For example, for URW, Russian is a notable outlier in terms of distribution, with \"Praise of Russia\" being one of the most common Narratives. One note to highlight, is that after the article selection process, the proportion of mainstream and alternative news varies widely across languages and could have an impact on label distribution. Moreover, 44% and 54% of the documents in CC and URW, respectively, are assigned more than one label; for paragraphs, the corresponding percentages are 12% and 15%, respectively. This observation justifies our choice to perform multi-label annotation.",
        "reasoning": "The excerpt describes dataset statistics and observations about language distributions, label imbalance, and multi-label annotation justification. It does not provide a formal or explicit definition of a specific term. No phrases such as “is defined as,” “refers to,” or “means” are present, and the content is purely descriptive and statistical. Therefore, this section does not contain an obvious definition.",
        "is_definition": false
      },
      {
        "paper_id": "dx.doi.org/https://doi.org/10.1016/j.ipm.2021.102505",
        "sections": [
          "Media has a substantial impact on public perception of events, and, accordingly, the way media presents events can potentially alter the beliefs and views of the public. One of the ways in which bias in news articles can be introduced is by altering word choice. Such a form of bias is very challenging to identify automatically due to the high context-dependence and the lack of a large-scale gold-standard data set. In this paper, we present a prototypical yet robust and diverse data set for media bias research. It consists of 1,700 statements representing various media bias instances and contains labels for media bias identification on the word and sentence level. In contrast to existing research, our data incorporate background information on the participants' demographics, political ideology, and their opinion about media in general. Based on our data, we also present a way to detect bias-inducing words in news articles automatically. Our approach is feature-oriented, which provides a strong descriptive and explanatory power compared to deep learning techniques. We identify and engineer various linguistic, lexical, and syntactic features that can potentially be media bias indicators. Our resource collection is the most complete within the media bias research area to the best of our knowledge. We evaluate all of our features in various combinations and retrieve their possible importance both for future research and for the task in general. We also evaluate various possible Machine Learning approaches with all of our features. XGBoost, a decision tree implementation, yields the best results. Our approach achieves an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79, which outperforms current media bias detection methods based on features. We propose future improvements, discuss the perspectives of the feature-based approach and a combination of neural networks and deep learning with our current system.",
          "News articles in online newspapers are considered a crucial information source that replace traditional media such as television or radio broadcasts and print media, and new sources of information such as social media (Dallmann, Lemmerich, Zoller, & Hotho, 2015). Many people consider such articles a reliable source of information about current events, even though it is also broadly believed and academically confirmed that news outlets are biased (Wolton, 2017). Given the trust readers put into news articles and the significant influence of media outlets on society and public opinion, media bias may potentially lead to the adoption of biased views by readers (Spinde, Hamborg, Donnay, Becerra, & Gipp, 2020a). However, ''unrestricted access to unbiased information is crucial for forming a well-balanced understanding of current events'' (Hamborg, Donnay, & Gipp, 2018). Highlighting media bias instances may have many positive implications and can mitigate the effects of such biases (Baumer, Elovic, Qin, Polletta, & Gay, 2015). While complete elimination of bias might be an unrealistic goal, drawing attention to its existence can not only warn readers that content is biased but also allow journalists and publishers to assess their work objectively (Dallmann et al., 2015). Furthermore, such insights could be very interesting for research projects, e.g., in social science. We want to point out that it is uncertain if and how actual news consumers would like to obtain such information. Only a few systems are already employed to help readers mitigate the consequences of media bias impact. Most of them focus on the aggregation of articles about the same event from various news sources to provide different perspectives (Lim, Jatowt, & Yoshikawa, 2018a). For example, the news aggregator Allsides 1 allows readers to compare articles on the same topic from media outlets known to have different political views. Various media bias charts, such as the Allsides media bias chart, 2 or the Ad Fontes media bias chart 3 provide up-to-date information on media outlets' political slants. The main objective of this paper is to present a prototypical system for the automated identification of bias-inducing words in news articles. In the following chapter, we will give an overview of research on the topic and show major currently existing drawbacks on the issue. They lead us to more fine-grained research contributions. Mainly, we will: 1. create a labeled data set for media bias analysis on different levels; 2. analyze and engineer features potentially indicating biased language; 3. train a classifier to detect bias-inducing words and 4. evaluate the performance. This study holds both theoretical and practical significance. We summarize all existing research to give a full overview of possible classification features for media bias. We also show the relevance of all these features. We provide a data set of media bias annotations. It is the first such data set in the field, reporting word and sentence level annotations and detailed information on annotator characteristics and background. Our current data set already significantly extends available data in this domain, providing a unique and more reliable insight into bias perception. It also offers grounds for future extension. Lastly, we train and present a classifier for biased words that outperforms existing feature-based classifiers for bias. The rest of the paper is organized as follows. Section 2 presents the literature review on media bias and its automated detection. Section 3 details the methodology, and Section 4 presents the results of this study. Finally, in Section 5 we present a discussion of the project and an outlook on future work.",
          "",
          "Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975). On the other hand, bias can also be unintentional and even unconscious (Baumer et al., 2015;Williams, 1975). Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently. In turn, an author can manipulate a reader's perception by implying a particular opinion or perspective or inducing positive or negative emotions. The following two examples present instances of media bias by word choice, respectively: 1. Practicing pro-life litigators know that Trump judges are saving lives by permitting restrictions on abortion to go into effect. 4 2. Tens of millions of children under 12 months are potentially at risk for diseases such as diphtheria and polio as the Chinese coronavirus pandemic interrupts routine vaccinations, according to data published by global public health experts on Friday. 5 In the first example, the author chooses the vaguer word ''pro-life'' to describe the very concrete ''anti-abortion'' position as highly positive. In the second example, labeling the coronavirus pandemic with the word ''Chinese'' implies China's fault in the pandemic.",
          "To the best of our knowledge, there are no tools or systems for automatic identification of media bias based on word choice. The task is a challenging one for several reasons. Firstly, while a vast amount of text data from the news is available, articles naturally are created without such sophisticated labels that could allow us to detect bias inducing words; therefore, existing data are unlabeled. Existing annotated data sets are very small. That means that currently, there is no large-scale gold standard data set for the identification of media bias by word choice (Hamborg et al., 2018). Furthermore, it has been proven that bias identification is a non-trivial task for non-experts (Lim et al., 2018a;Recasens, Danescu-Niculescu-Mizil, & Jurafsky, 2013), which can cause problems while creating such a data set. Secondly, bias words are highly dependent on the context. Therefore, simple presence of specific words is not a reliable indicator of bias (Hube & Fetahu, 2018). 1. An abortion is the murder of a human baby embryo or fetus from the uterus, resulting in or caused by its death (Hube & Fetahu, 2018). 2. In 2008, he was convicted of murder (Hube & Fetahu, 2018). In the first example, the word ''murder'' describes abortion as something highly negative. In the second example, ''murder'' is used to describe a pure fact of murder. In the first case, the word induces bias, whereas, in the second, it does not. Another good example of context dependence of bias is the bidirectionality of the epistemological bias, i.e., this bias can occur in two cases: when a truthful proposition is questioned or when a false or controversial proposition is presupposed or implicated. The word that will not cause bias in the first case, e.g., factive verb, will cause it in the second, and vice versa (Recasens et al., 2013). Finally, media bias and bias in reference works are subtle, implicit, and intricate since, in general, the news is expected to be factual and impartial (Baumer et al., 2015;Hube & Fetahu, 2018;Lim et al., 2018a;Recasens et al., 2013). As fairly noticed by Williams (1975), remarkable bias is not extreme but rather reasonable and plausible: extreme bias is obvious and does not threaten values or institutions. Despite the challenging nature of the task, several researchers attempted to annotate media bias by word choice automatically. We also derive valuable insights for this project from the research attempting to identify the biased language in the related fieldreference sources such as encyclopedias, where neutral language is also desired and required. Lim et al. (2018a) use crowdsourcing to construct a data set consisting of 1235 sentences from various news articles reporting on the same event, namely, the arrest of a black man. The data set provides labels on the articles and word level. The authors then train a Support Vector Machine on the Part of Speech (POS) tags and various handcrafted linguistic features to classify bias on the sentence level, achieving the accuracy of 70%. In related work, Lim, Jatowt, Färber, and Yoshikawa (2020) propose another media bias data set consisting of 966 sentences and containing labels on the sentence level. The data set covers various news about four different events: Donald Trump's statement about protesting athletes, Facebook data misuse, negotiations with North Korea, and a lawmaker's suicide. Baumer et al. (2015) focus on the automated identification of framing in political news and construct a data set of 74 news articles from various US news outlets covering diverse political issues and events. They then train Naïve Bayes on handcrafted features to identify whether a word is related to framing, and achieve 61% accuracy, 34% precision, 70% recall, 0.45-0.46 𝐹 1 -score. Hamborg, Zhukova and Gipp (2019) constructed a data set using content analysis. They created a codebook describing frame properties, coding rules, and examples. The data set consists of 50 news articles from various US news outlets and covers ten political events. The authors distinguish the target concept and phrases framing this concept, and define a number of framing properties, e.g., ''affection'', ''aggression'', ''other bias'', etc. The authors then automatically extract the candidates for target concepts and identify frames by looking for words semantically similar to the previously defined framing properties via exploiting word embeddings properties. Then, identified framing properties are assigned to the candidates via dependency parsing. The authors achieve an F1-score of 45.7%. Fan et al. (2019) create the data set BASIL, annotated by two experts, covering diverse events and containing lexical and informational bias. The data set allows analysis at the token level and relatively to the target, but only 448 sentences are available for lexical bias. Then, they employ BERT lexical sequence tagger to identify lexical and informational bias at the token level and achieve an 𝐹 1 -score of 25.98%. Chen, Al-Khatib, Wachsmuth, and Stein (2020) create a data set of 6964 articles covering various topics and news outlets containing political bias, unfairness, and non-objectivity labels at the article level. They then train the recurrent neural network to classify articles according to these labels. Finally, the authors conduct a reverse feature analysis and find that, at the word level, political bias correlates with such LIWC categories (Pennebaker, Boyd, Jordan, & Blackburn, 2015) as negative emotion, anger, and affect. Recasens et al. (2013) create static bias lexica based on Wikipedia bias-driven edits due to NPOV (Neutral Point of View) violations. 6 The bias lexicon and a set of various linguistic features are then fed into the logistic regression classifier to predict which words in the sentences are bias-inducing. The authors reached 34.35% to 58.70% accuracy for predicting 1 to 3 potential bias-inducing words in a sentence, respectively. Hube and Fetahu (2018) propose the semi-automated approach to extract domain-related bias words lexicon, based on the word embeddings properties. The authors then feed obtained bias words and other linguistic features into a random forest classifier to detect language bias in Wikipedia at the sentence level. The authors achieve 73% accuracy, 74% precision, 66% recall, and an 𝐹 1score 0.69 on the newly created ground truth based on Conservapedia,foot_1 and state that the approach is generalizable for Wikipedia with a precision of 66%. In their later work, Hube and Fetahu (2019) train a recurrent neural network on a combination of word embeddings and a few handcrafted features to classify bias in Wikipedia at the sentence level and achieve 81.9% accuracy, 91.7% precision, 66.8% recall, and 0.773 𝐹 1 -score. Spinde, Hamborg, andGipp (2020b, 2020c) analyze media bias in German news articles covering the refugee crisis. The three components: an IDF component, a combined dictionary-based component, and a component based on a semantically created bias dictionary, are analyzed to identify bias on the word level. The combination of the dictionary component and the topic-dependent bias word dictionary achieves an 𝐹 1 -score of 0.31, precision of 0.43, and recall of 0.26. The authors point out that considering adjectives separately increased the performance.",
          "",
          "The general workflow of a prototypical system for automated identification of bias-inducing words in news articles is presented in Fig. 1. In our work, we start from collecting the sentences and gathering annotations via a crowdsourcing process (1). We then obtain various features (3) described in more detail in Section 3.5. One of the features is a bias lexicon built semi-automatically by computing words similar to potential bias words using outlet-specific word embeddings (2). We then train a supervised classifier on our engineered features and annotated labels (4). After the best model is selected and optimized, we evaluate the performance of the feature-based approach for detection of media bias. Furthermore, we evaluate all features individually (5).",
          "One of the challenges in the automated detection of media bias is the lack of a gold standard large-scale data set with labeled media bias instances. The existing data sets described in Section 2.2 either do not allow for the analysis of media bias on the word level or can induce drawbacks due to the following limitations: (1) they only include a few topics (Lim et al., 2020(Lim et al., , 2018a)), (2) they mostly focus exclusively on framing (Baumer et al., 2015;Hamborg, Zhukova et al., 2019), (3) annotations are target-oriented (Fan et al., 2019;Hamborg, Zhukova et al., 2019), (4) annotations are not on the word level (Lim et al., 2018a), or (5) training data are too small (Fan et al., 2019). Therefore, we decided to create a diverse, robust, and extendable data set to identify media bias. We hand-picked 1.700 sentences from around 1.000 articles. According to our collection strategy, most of the sentences should contain media bias instances, while the smaller number of sentences should be neutral. However, the final annotations are made by the crowd-source annotators. The sentences equally represent the full political spectrum since we used the articles from the major left and right-wing outlets, classified by their political ideology within the media bias ratings of www.allsides.com. We covered 14 different topics (selected randomly out of a variety of possible topics), from very contentious (e.g., abortion, elections) to less contentious topics (e.g., student debt, sport). To gather annotations of the sentences and the words, we developed our own survey platform, combining classical survey functionality with text annotations, and hired participants via Amazon Mechanical Turk to complete microtasks. Annotation quality ensured by experts is often preferable, but we expressively wanted to collect a large number of annotations from non-experts. It has been shown that many complex problems can be resolved successfully through crowdsourcing if the existing crowdsourcing platforms are used in combination with appropriate management techniques and quality control (Mitrović, 2013;Mladenović, Mitrović, & Krstev, 2016)  8 . Seven hundred eighty-four annotators participated in the survey, all located in the United States. The vast majority (97.1%) of the annotators were native English speakers, 2.8% were near-native speakers. The annotators from diverse age groups participated in the survey; people from 20 to 40 years old (67.4%) prevail over other age groups. The annotators' gender is balanced between females (42.5%) and males (56.5%). The annotators have a diverse educational background; more than half have higher education. The political orientation is not well balanced: liberal annotators prevail (44.3%) over conservative annotators (26.7%) and annotators from the center (29.1%). The vast majority of the annotators read the news sometimes, more than half -one (46.4%) or more (23.1%) times per day. Each annotator received 20 randomly reshuffled sentences. We showed each sentence to ten annotators. Within our platform,foot_3 we first instruct participants about the general goals of the study. We explain the tasks in detail and ask them to leave aside their personal views. We also give them a few examples of bias and ask a control question to check whether participants understood media bias's general concept. If the control question was not answered correctly, participants had to reread the instructions. Within the annotation task itself, we provide detailed instructions on the workflow. We then ask each annotator to highlight words or phrases that induce bias according to the provided instructions. After that, we ask them to annotate the whole sentence as biased or impartial, and whether they would describe it as opinionated, factual, or mixed. To the best of our knowledge, our data set is the first in the research area to collect detailed background demographic information about the annotators, such as gender, age, education, English proficiency, but also information on political affiliation and news consumption. Overall, our data set allows for performing three different tasks: bias identification on the word level, sentence level, and a classification of the sentence as being opinionated, factual, or a mixture of both. We discuss the results of the annotation in Section 4.1.",
          "As one of our features, we present a lexicon of biased words, built explicitly for the news domain (Hube & Fetahu, 2018). Interestingly, although such a lexicon cannot serve as an exhaustive indicator of media bias due to high context-dependence (Hube & Fetahu, 2019), it can potentially serve as a useful feature of a more complex media bias detection system. To extract a biased word lexicon of high quality, we replicate the method proposed by Hube and Fetahu (2018). The authors proposed a semi-automated way to automatically extract biased words from corpora of interest using word embeddings. We present the whole pipeline of the approach in Fig. 2. We first manually create a list of words that describe contentious issues and concepts. Then, we use this list to manually select ''seed'' biased words in the two separate word embedding spaces trained on news articles potentially containing a high number of biased words. We select seed biased words among the words that have high cosine similarity to the words describing contentious issues. We publish our list of seed biased words at https://bit.ly/36guHdu. 10We assumed that news outlets with presumably stronger political ideology would use bias words when describing contentious issues with a higher likelihood than neutral mediums. To capture both liberal and conservative biases, we train word embeddings separately on the corpora of news articles from HuffPost and Breitbart, respectively. In the choice of the outlets, we relied on the information provided by Allsides: both outlets are presented at the media bias chart,foot_5 and for both outlets, the confidence level of the assigned ratings is high.foot_6 Noteworthy, these two sources are also ones of the most popular media sources that left-and right-leaning communities share respectively in Soliman, Hafer, and Lemmerich (2019). The articles from both sources, published from 2010 to 2020, are scraped from Common Crawlfoot_7 using NewsPlease (Hamborg, Meuschke, Breitinger and Gipp, 2017). We split the initial text into lower-cased tokens, remove punctuation marks and numbers, and train Word2Vec word embeddings (Mikolov, Chen, Corrado, & Dean, 2013). The chosen hyper-parameters are summarized in Table 1. Since evaluation of such an unsupervised task as word embeddings creation is quite challenging (Bakarov, 2018), we choose the hyper-parameters based on the existing research (Spinde, Rudnitckaia, Hamborg, & Gipp, 2021a). The number of dimensions is set to 300 and is not increased further due to the scarcity of the training data (Mikolov et al., 2013). The window size is set to 8 since  larger window size can capture broad topical content (Levy, Goldberg, & Dagan, 2015). We increase the number of iterations to 10 since the size of training data is small and cannot be increased. In the scope of this project, it is important to avoid unstable low-qualitative vectors, therefore words appearing less than 25 times are excluded. Finally, treating n-grams as single units may lead to better training of a given model (Camacho-Collados & Pilehvar, 2018). We use the default scoring for n-grams generation and run two passes over training data. The thresholds for n-grams inclusion are based on manual analysis of the generated n-grams. The rest of hyper-parameters are set to the default values. As a next step, we divide the set of seed biased words into random batches consisting of ten words and repeat this process ten times to create batches with various combinations of words. Then, for each batch, the average vector in the word embedding space trained on a 100 billion Google news data setfoot_8 is calculated. For each average vector, we extract the top 100 words close to this average vector. Hube and Fetahu (2018) do not reshuffle words in batches and extract the top 1000 words. However, the average cosine similarity of farthest words among the top 1000 is 0.47, whereas the average cosine similarity of farthest words among the top 100 is 0.52. Besides, extracting the top 1000 words introduces noise. Finally, we add extracted words to the resulting bias word lexicon, and remove duplicates.",
          "We define bias-inducing words detection as a binary classification problem where we have only two mutually exclusive classes: whether a word is biased (class 1) or not (class 0). With our binary classifier, and in the context of media bias by word choice, no exhaustive set of precise media bias characteristics exist. Therefore, we combine different linguistic features of biased language proposed by Recasens et al. (2013) and a variety of other syntactic and lexical features (Lim et al., 2020). As the context is highly important when distinguishing between unbiased and biased words, we attempt to capture useful information from context by including collective features adding two previous and two following words into a word's feature vector. We admit that such a way T. Spinde et al.",
          "The complete set of features used in our approach for detecting biased words.",
          "POS tags POS tag indicating the syntactic role of each word, e.g., noun, adverb, etc. Honnibal and Montani (2017).",
          "Dependencies revealing how words in the text relate to each other, e.g., whether a word is a root, object, or subject (Bird, Loper, & Klein, 2009;Honnibal & Montani, 2017).",
          "Named entities, e.g., persons, organizations, locations, etc. Bird et al. (2009) and Honnibal and Montani (2017).",
          "Norms of GloVe word embedding vectors pre-trained on the Common Crawl a Honnibal and Montani (2017). TF-IDF Frequency of the term in a document and in the whole article collection (Lim, Jatowt, & Yoshikawa, 2018b;Pedregosa et al., 2011).",
          "Word is a report/implicative/assertive/factive/positive/negative word, is strongly or weakly subjective, or a hedge (Recasens et al., 2013).",
          "Classifications as kill verb (Greene & Resnik, 2009), hyperbolic term (Chakraborty, Paranjape, Kakar, & Ganguly, 2016), boosters, and attitude markers (Hyland, 2019). LIWC features LIWC features based on psychological and psychometric analysis (Pennebaker et al., 2015). Semi-automated bias lexicon Previously described semi-automatically created bias word lexicon (Hube & Fetahu, 2018). a https://commoncrawl.org, accessed on 2020-10-31. to account for context is not optimal and requires elaboration in future. We compare different combinations of the features, and also train different machine learning classification algorithms, such as logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), complement Naïve bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), decision tree (DT), random forest (RF), XGBoost and a simple neural network -multilayer perceptron (MLP). To the best of our knowledge, we present the first detailed comparison of classifiers for word-level bias detection.",
          "We present our entire feature list in Table 2. In continuation, we describe the individual features and the intuition behind using them for our task. For POS tags, syntactic dependencies, named entity types, word vector norms, and linguistic features, we refer to the previous work on these topics as described by Hube and Fetahu (2018) and Recasens et al. (2013). TF-IDF. Lim et al. (2018b) propose detection of bias using inverse document frequency (IDF) as one of the features to detect media bias, under the assumption that more rarely occurring terms are more likely to be extreme in any direction and are hence more likely to induce bias. As our data set consists of sentences where the terms are probably rarely repeated within one sentence, we adjusted Lim et al.'s assumption (Lim et al., 2018b) slightly by calculating the TF-IDF statistic basing on the whole text of articles the sentences were collected from. LIWC Features. Linguistic Inquiry and Word Count (Pennebaker et al., 2015) is a common approach to analyzing various emotional, cognitive, and structural components in language. It identifies linguistic cues related to psychological processes such as anger, sadness, or social wording. We consider all feature categories from LIWC, as this has shown to be the most effective usage of the resource to identify bias. Additional lexical features. It is not well known which features are the most efficient indicators of media bias (Baumer et al., 2015). Therefore, we test additional features that have been used by researchers to study similar constructs but have not been applied for the detection of media bias yet. According to Greene and Resnik (2009), the so-called ''kill verbs'' together with the relevant grammatical relation (governor or dependent term) cause different sentiment perception. In the following example, the second one is perceived as more negative since it implies an intention (Yano, Resnik, & Smith, 2010): 1. Millions of people starved under Stalin. 2016) study click baits in online news and find that click bait headlines usually include hyperbolic wordswords with highly positive sentiment. We assume that hyperbolic words used in click bait titles to attract readers' attention can be used to emphasize some concepts and induce bias in news articles. Hyperbolic words are, for example, ''absolutely'', ''brilliant'', or ''impossibly''. Hyland (2019) introduces linguistic features that help authors to express their views on the discussed proposition. One such subcategory are boosters -words that express certainty about a particular position, e.g., ''believed'', ''always'', ''no doubt''. In some regard, boosters are opposite to hedges, which, on the contrary, reduce the confidence of a statement. Another subcategory are attitude markers -indicators of the author's expression of affective attitude to statements, e.g., ''fortunately'', ''shockingly'', ''disappointed'', etc. ",
          "",
          "",
          "In the final data set, classes are highly imbalanced (Section 4.1). Since accuracy does not capture the capability of a model to predict rare class correctly (Branco, Torgo, & Ribeiro, 2015), we focus on such evaluation metrics as confusion matrix, precision, recall, F1-score, and receiver operating characteristic area under curve (ROC AUC). We compare the performance of our system to several possible baselines: • Baseline 1 (B1) -a purely random classifier; • Baseline 2 (B2) -occurrence of a word in the negative sentiment lexicon; • Baseline 3 (B3) -occurrence of a word in the negative or positive lexicon; • Baseline 4 (B4) -occurrence of a word in the semi-automated bias lexicon. In our final data set, each observation corresponds to one word, its feature vector (including the collective context features), and the label. We perform 10-fold cross-validation when compare performance of different classifiers, 5-fold cross-validation when optimize hyper-parameters of the selected model, and finally, estimate the final performance on a test set of words that did not participate in training and manually investigate correctly and wrongly classified instances.",
          "",
          "To gain insights into the characteristics of our data set, we analyze it quantitatively and qualitatively. The results of sentences classification are presented in Figs. 3 and 4. The annotation results confirm our data sampling strategy: biased and non-biased statements are not balanced in the data set: biased statements prevail over non-biased statements. Besides, most media bias instances are taken from liberal and conservative news sources, whereas sources from the center were used mainly to retrieve non-biased statements. Note that this does not imply that liberal and conservative news outlets generally experience media bias by word choice and provide opinionated news more often than news outlets from the center. It is valid only due to our data collection scheme. We assigned a biased or impartial label to a sentence if more than half of respondents annotated a sentence as biased or impartial, respectively. 149 sentences could not be labeled due to a lack of agreement between annotators. Many measures for assessing interannotator agreement have been used in similar computational linguistics projects. Based on the way in which we have organized our crowdsourcing workflow, i.e. having 10 annotators per task, we have decided to use Fleiss (1971) to assess the inter-annotator agreement. It represents the task's general difficulty: for example, Hube and Fetahu (2018) reported 𝛼 = 0.124 on word-level bias, and Recasens et al. (2013) reported a 40.73% agreement when looking at only the most biased word in Wikipedia statements. The value of 0.21 that we achieved can be considered as a fair agreement. We assigned an opinionated, factual, or mixed label to a sentence if most respondents annotated a sentence as opinionated, factual, or mixed, respectively. We could not label 174 sentences due to the lack of agreement between annotators. According to our crowdsourced annotations, the data set contains an almost equal number of factual, opinionated, and mixed statements. The annotation scheme for biased words allowed respondents to highlight not only the words but also short phrases. A word was considered biased if at least four respondents highlighted it as biased. On average, a sentence that contains biased words contains two biased words. Out of 31,794 words for training, only 3018 are biased, which constitutes only 9.5% of our current data. The types of words annotated as biased are presented in Table 3.  We observe that annotators select not only extreme and emotional words that can be considered biased even without context, but also context-dependent bias words. For instance, while the word ''Chinese'' is generally not biased it can be in specific contexts, such as ''House Democrats' Chinese coronavirus relief package bails out coastal millionaires and billionaires while ensuring big businesses are able to freely hire illegal aliens and visa overstayers over unemployed Americans''. 15Albeit emphasizing in the instructions that words that are connected to very controversial topics or have very negative sentiment are not necessarily biased, some of such words were still annotated as biased. For example, the term ''neo-Nazis'' in the sentence ''For years now, Fox News has been mainstreaming arguments that used to be the province of fringe websites run by neo-Nazis and other groups who believe the U.S. is meant to be a country of white people and for white people''. 16Furthermore, we find that annotators sometimes fail to mark words as biased if a sentence contains clearly extreme and emotional words. For example, a majority of annotators marked ''cray-cray'' as biased but did not notice ''totally'' in the sentence ''Over the past few decades, RFK Jr.'s famous name has helped him get in the door to talk to important people, and it probably is not long before the person who is all jacked up to meet a Kennedy realizes the guy is totally cray-cray''. 17  As expected, we find a positive correlation between marking sentences as biased and opinionated, and factual and non-biased. Furthermore, more controversial topics are annotated as non-biased, on average, 7.4 p.p. less than less controversial topics. Interestingly, in 49.3% of the sentences labeled as non-biased, annotators still labeled some words as biased. Annotators who estimate themselves as conservative mark 3.76 p.p. more sentences as biased than others who describe themselves as being liberal -except if the sentence stems from a conservative news outlet (Yano et al., 2010). Furthermore, annotators who report that they check news at least sometimes, label sentences as biased 6.85 p.p. more than those who report to check news very rarely, and 19.95 p.p. more than those who report that they never check news.",
          "In this section, we first present the characteristics of the articles we used to train our word embedding models and the performance of the trained word embeddings. We also provide the characteristics of the pre-trained Google News embeddings. We measure semantic word similarity and word analogy (Bruni, Tran, & Baroni, 2014;Finkelstein et al., 2001;Mikolov et al., 2013). Table 4 depicts the results of our measures. Two data sets -WordSim-353 (Finkelstein et al., 2001) and MEN (Bruni et al., 2014) -allow to estimate the Pearson correlation for the semantic similarity between pairs of words in respective word embeddings and as estimated by human assessors. The Google analogy test set (Mikolov et al., 2013) allows to evaluate accuracy. Even though those evaluation data sets are not perfectly suited for our task, the comparison shows that our data sets are large enough to give comparable results to the full Google News data set. We also manually inspected the embeddings' results and confirmed that they capture bias to a reasonable extent. Second, we qualitatively investigate the lexicon of biased words resulting from the semi-automated expansion (Section 3.3). We manually inspected a random sample of 100 words and find that the vast majority (Around 69% in a random sample of 100 words) are negatively connotated, are emotional, and convey strong opinion (Fig. 5). Furthermore, the dictionary consists of disproportionally many rather uncommon words (e.g., ''teapartiers'', ''obamian'', ''eggheaded'', ''mobocracy''), that are only interpretable when knowing about the circumstances they developed in. We find only one word (''similarly'') that cannot directly be related to bias, while we ourselves would classify all 99 other words as being very likely to induce bias. Among 96 words for which POS tag can be identified unambiguously, 41.7% are nouns, 24.0% are verbs, 21.9% are adjectives, and 11.5% are adverbs. Finally, we compare the method of batches developed by Hube and Fetahu (2018) to the naive approach where close words are retrieved for a single seed bias word instead of an average of a batch. We find the employed batched extraction to be superior. Specifically, while both approaches yield a high proportion of biased words, naive approach also yields many words that are not biased but co-occur with biased words, such as ''abortion'', ''personhood'', ''immigrants'', ''mexicans'', etc. Table 5 contrasts extraction for two methods. So far, the lexicon seems to be valuable especially in finding negatively connotated neologisms and words that convey strong opinion even by themselves. Despite being more efficient than the naive approach, the method of batches, nevertheless, still cannot avoid some degree of noisy words and words falsely included as biased. Among such words are misspellings, abbreviations, and words that describe a contentious or a negative issue or concept, e.g., ''xenophobia'', ''criticize'', ''anti-Semite'', ''solipsist''. We will further evaluate the resource in future work.",
          "We first train ''quick and dirty'' models (Gron, 2017) on all available features with default parameters (as implemented in Scikit-Learn (Pedregosa et al., 2011) and XGBoost (Chen & Guestrin, 2016) libraries) and compare the performance based on scores T. Spinde et al. averaged from ten-fold cross-validation (Pedregosa et al., 2011). We compare 𝐹 1 -score, precision, recall, and ROC AUC. Since data are imbalanced (only ≈ 10% are biased), weighting of classes is employed for all methods (where possible). Table 6 shows that no model yields a high 𝐹 1 -score. Instead, best performing models yield either high precision or high recall. Since results of our method are, for now, intended to be verified by a user, we prefer recall over precision while still aiming for moderately high 𝐹 1 -score. We choose XGBoost for further optimization since it achieved both the highest ROC AUC score and the highest 𝐹 1 -score. It also has a relatively high recall: the model predicts more True Positives (biased words as biased) than False Negatives (biased words as unbiased). The model suffers from predicting many False Positives (unbiased words as biased) but to the smaller extent than other models with higher recall (Logistic regression, QDA, NB, SVM). XGBoost is ''a scalable end-to-end tree boosting system'' (Chen & Guestrin, 2016). The algorithm is based on gradient boosting -an ensemble method that adds predictors sequentially to an ensemble, each new one is fit to the residual errors made by the previous one (Gron, 2017). Thus, the final model -a combination of many weak learners -is itself a strong learner. In addition to the fact that XGBoost already achieved best results on our data set, it provides several advantages: it accounts for sparsity caused by one-hot encoding (Chen & Guestrin, 2016), allows for a fine parameter tuning using a computationally efficient algorithm (Bentéjac, Csörgo, & Martínez-Muñoz, 2019), and allows to estimate feature importance since we do not have reliable prior information about the importance of the features (Baumer et al., 2015). We fine-tune five hyper-parameters that help to control for overfitting. 18  For the fine-tuned model, we quantitatively evaluate the performance and feature importance. Table 7 shows that fine-tuning yields an insignificant performance improvement of 𝐹 1 = 1p.p. We find that the model suffers from underfitting since performance 18 We find the following values to be optimal. max-depth = 6, min-child-weight = 18, subsample = 1, colsample-bytree = 1, and eta = .2. Other hyper-parameters are set to the default values. The maximum number of boosting rounds is set to 999, and early stopping is applied if the 𝐹 1 -score for validation set does not increase within ten rounds. The model is weighting the imbalanced classes. The evaluation metric is the 𝐹 1 -score averaged on five-fold cross-validation. is also low on training (𝐹 1 = 0.51) and validation sets (𝐹 1 = 0.50). Comparing XGBoost performance to the defined baselines, we see that XGBoost significantly outperforms the random baseline (B1) but fails to significantly outperform the naive usage of the negative sentiment lexicon (B2). However, when analyzing results in a confusion matrix, we see that using just the negative dictionary, in fact, predicts 53% of biased words incorrectly as non-biased words whereas XGBoost predicts only 23% incorrectly. High 𝐹 1 -score and ROC AUC of baseline B2 is mostly due to the low number of False Positives, but this is a behavior close to simply predicting all words as non-biased. Since we do not have the prior information on which features are the most contributing into media bias detection, we first trained the classifier on the all the available features. When analyzing feature importance, we find that the most important features are the occurrence of a word in a negative lexicon (gain = 1195) and being a proper noun (470). The bias lexicon that we created semi-automatically is among the top 10 important features (gain = 106). Among linguistic features proposed by Recasens et al. (2013) as indicators of bias, only sentiment lexica, subjectivity lexica, and assertive verbs are among the top 30 important features. While report verbs and hedges still have minor importance, factive and implicative verbs have zero importance. We train several models feeding them with features that have different importance. Excluding features with low feature importance does not improve the performance (Table 7). Besides, we test how the model performs when different feature groups are not included. Thus, we train a model with all features except one particular feature or group of features. We notice that the performance drops significantly only when linguistic features are not used, most likely because of the negative sentiment lexicon's high importance. Lastly, we qualitatively investigate automatically detected bias candidates. Examples of correctly classified biased words (TPs) include mostly emotional words that can be considered biased even without context. Words that can be described as causing negative emotions occur more often than those causing positive emotions. 12.5% of TPs correctly indicate less obvious bias, which is most likely generally more rare. The following examples illustrate (1) obvious negative bias, (2) obvious positive bias, and (3) slightly more subtle bias among correctly classified words. 1. Large majorities of both parties seem to like the Green New Deal, despite efforts by Fox News to paint it as disastrous. 19  2. Right-wing media sprung into action to try to discredit her, of course, by implying that a woman who graduated summa cum laude with an economics degree is a bimbo and with Twitchy using a screenshot to make the usually genial Ocasio-Cortez somehow look like a ballbuster. 20 3. As leading 2020 Dems advocate spending big on the Green New Deal, it turns out most Americans are worried about other issues. 21   Examples of biased words incorrectly classified as non-biased (FNs) include words that are (1) parts of phrases, (2) ambivalent as to whether they are biased, (3) not generally biased but only in a particular context, (4) mistakes in the annotation, and random misclassifications: 1. By threatening the kids and their families with deportation, the administration's U.S. Citizenship and Immigration Services was effectively delivering death sentences. 22 19 https://www.alternet.org/2019/04/just-a-cover-for-sexism-and-white-nationalism-paul-krugman-explains-why-the-rights-attacks-on-new-democratic-  lawmakers-are-bogus/, accessed on 2020-10-31. 20 https://www.alternet.org/2019/01/alexandria-ocaio-cortez-is-absolutely-right-there-shouldnt-be-any-billionaires/, accessed on 2020-10-31. 21 https://fxn.ws/370GuwZ, accessed on 2020-10-31. 22 https://www.msnbc.com/rachel-maddow-show/trump-admin-backs-plan-deport-critically-ill-children-msna1280326, accessed on 2020-10-31. T. Spinde et al. 2. When the Muslim ban was first enacted, it triggered chaos at airports and prompted widespread protest and legal challenges, and it continues to impose devastating costs on families and people who wish to come to the U.S. 23 3. The specter of ''abortion regret'' has been used by lawmakers and judges alike to impose or uphold rules making it harder for people to get abortions. 24 4. Gun enthusiasts cannot admit that they like firearms because they fear black people. 25   Examples of non-biased words misclassified by the model as biased (FPs) include words that (1) are ambivalent as to whether they are biased, (2) describe negative or contentious issues, (3) are due to erroneous annotation, and (4) random misclassifications: 1. Justice Sonia Sotomayor, in her dissent, accused the majority of weaponizing the First Amendment -an unconscionable position for a person tasked with ''faithfully and impartially'' discharging the duty to protect the inherent rights of all Americans. 26 2. He also denounced the policy of Chancellor Angela Merkel and the attitude of the German media, which ''are constantly pushing'' for Europe to welcome more and more migrants, in opposition to the will of the Hungarian people. 27 3. Michelle Williams won a Golden Globe for her role in ''Fosse/Verdon'' on Sunday night, but perhaps her biggest moment came during her acceptance speech when she defended abortion rights and encouraged women to vote ''in your own self-interest''. 28 4. The case was sent back to lower courts to determine whether the gun owners may seek damages or press claims that the amended law still infringes their rights. 29",
          "One of the main contribution of our work is the creation and annotation of a robust and diverse media bias data set. Other than already existing studies on the topic, our data contain background information about the annotators, increasing our results' transparency and reliability. We perform visual analysis and observe the following findings. Topics that are less controversial are annotated as non-biased slightly more often than very controversial topics. Conservative annotators perceive statements as biased slightly more often than liberal annotators, but for both, it is only true unless the statement is from a conservative media outlet. We also find that annotators who read news never or very rarely are less likely to annotate statements as biased. Besides, our annotation results show the connection of bias and opinion. Even though our data set was developed on a small scale and cannot serve as a gold standard for the field of media bias research, it offers a complete framework for further data extension, especially in combination with our specifically developed survey platform. We plan to extend the data set in the future. While using crowdsourcing gave us an insight into the perception of bias by a broad audience, some related issues could not be resolved, e.g., the submission of random words. Furthermore, even honest workers made mistakes because the identification of media bias is, in general, not a trivial task, especially for non-experts (Lim et al., 2018a;Recasens et al., 2013). We will follow a dual strategy in future work: While extending the existing data set, we will develop an expert annotation guideline and evaluate the same sentences in cooperation with experts in the field. We will also test the difference between the annotation of single, isolated sentences, and sentences within an article's scope. Lastly, we will evaluate from a psychological perspective how different types of questioning affect the perception of bias, and have already collected around 500 varying questions on the issue. Regarding the quality of the annotations, our main strategy is to exclude noise from the final labels by setting up a threshold of the number of annotators required to label a word as biased or not. However, after manual analysis of final annotations, we find that setting up any strict threshold will introduce some noise and result in some words being omitted. The threshold of four is the most reasonable, but we admit that some words are omitted, and some non-biased words are included. We will experiment with more annotators per sentence to see whether we can reduce the percentage of errors. Our semi-automatically created bias lexicon is indeed able to find emotional words and words that convey a strong opinion. However, we conclude that while capturing emotional and negative opinionated words, the lexicon is unlikely to be exhaustive. So far, the approach lacks an additional method on how to expand the lexicon without adding non-biased words. Overall, our prototypical system achieves an 𝐹 1 -score of 0.43, precision of 0.29, recall of 0.77, and ROC AUC of 0.79. Our data set is the largest and most transparent in the area to date and our classifier is the first built on these data, making a direct comparison to other methods unfeasible. On their respective data sets, researchers who detected media bias on the word level achieved an T. Spinde et al. F1-score of 0.26 (Fan et al., 2019) and 0.31 (Spinde et al., 2020b); researchers, who detected framing on the word level, achieved an F1-score of 0.45-0.46 (Baumer et al., 2015;Hamborg, Zhukova et al., 2019). We present the most complete collection of features for classification to date, extending the work of Hube and Fetahu (2018) and Recasens et al. (2013). Especially Boosters were not used in previous research, but are among the most important features. We will continue our detailed analysis of feature importance for the overall task with our larger crowdsourced data set and the expert data. We will also improve the quality of our features. For example, for implicative verbs, Pavlick and Callison-Burch (2016) introduced a method to automatically predict implicativeness of a verb based on the known constraints on the tense of implicative verbs. We could also expand our sentiment and subjectivity lexicons by using WordNet, a de facto lexico-semantic network (Fellbaum, 1998;Miller, 1995), or SentiWordNet 3.0, a lexical resource that assigns sentiment scores to each synset of WordNet (Baccianella, Esuli, & Sebastiani, 2010). While recognizing around 77% of biased words correctly, our approach misclassifies around 20% of non-biased words. Due to the classes' imbalance, 20% of the misclassified majority class significantly decreases overall performance. In this section, we discuss the drawbacks of the implementation that lead to low performance. Especially words that are biased only in a particular context are rarely classified correctly, highlighting how media bias is usually very subtle and context-dependent. However, so far, we only accounted for context by using one collective feature for the window of four words surrounding the word. Overall, we believe that the feature-based approach is especially valuable because of its explanatory character, relating bias to specific features, which is impossible with automated feature extraction. It is also not as dependent on the amount of data as a neural network. However, we will integrate deep learning into our approach, as such an architecture especially helps to account for inter-dependencies between words. Both methods can also be combined, with our specific features giving meaning to the identification of words with automatically identified features. We will also investigate whether the classifier works better on the political left, right, or center sources. We will also determine whether we can distinguish bias for any particular ideology in our overall vocabulary of biased words. In this paper, we propose an approach to identifying media bias using an automatic feature-based classification. To evaluate our method, we also present a 1700-sentence data set of crowdsourced biased word annotations, where we show each sentence to ten survey participants. For the first time in the research area, we report each person's background and make our data more transparent and robust. We extend existing feature sets for the task and especially evaluate each feature in detail. We also experiment with different classifiers, with our final choice returning an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79. Our results slightly outperform existing methods in the area. To increase our results further, we show how we can improve our data and features in future work and how neural networks could be a suitable option to combine with our method, even though some drawbacks have to be overcome. We publish our code and current system at https://github.com/Media-Bias-Group/Automa-CRediT authorship contribution statement Timo Spinde: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Lada Rudnitckaia: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Jelena Mitrović: Helped to implement and completely financed the successful survey execution. Felix Hamborg: Wrote the discussion chapter. Michael Granitzer: Gave permanent expert feedback on our work. Bela Gipp: Gave permanent expert feedback on our work. Karsten Donnay: Helped to implement and completely financed the successful survey execution."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "slanted news coverage or internal bias, reflected in news articles.",
            "context": "2.1. Media bias Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975).",
            "type": "explicit"
          },
          "bias by word choice": {
            "definition": "when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept.",
            "context": "Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "26af8e81a7ca5b137912acb81d8c19f38fce0200",
        "sections": [
          "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
          "Howard's research on the \"chlorine sunset\" controversy is based on interviews and document analysis. He conducted twenty-seven semistructured interviews, lasting an hour on average, with staff members of federal regulatory agencies in the United States and Canada, staff members of the International Joint Commission (IJC), members of the Great Lakes Science Advisory Board, staff members or individuals otherwise associated with nongovernmental organizations (NGOs), academic or governmental members of the industrial ecology or green chemistry communities, and industrial chemists in industry and academia. A number of transcripts were supplemented with additional information from follow-up correspondence. Documents analyzed included (1) reports, press releases, Web documents, and other materials published by NGOs, the chemical industry, and federal agencies; (2) articles and commentaries in newspapers and popular and trade magazines; (3) research articles and commentaries in scholarly anthologies and peer-reviewed scholarly journals; (4) books written by key actors; and (5) transcripts of Congressional testimony. A little-studied controversy involving one of the major branches of industrial chemistry documents a striking example of undone science and illustrates the role it can play in structuring conflict between competing regulatory paradigms. Much of the controversy has centered on the Great Lakes region, where extensive chemical manufacturing and contamination has occurred; where scientists have documented threats to wildlife and humans from persistent, toxic, industrial chlorinated pollutants; where extensive citizen activism has emerged around this threat; and where a quasigovernmental advisory body has assumed a leadership role in addressing this concern (Botts et al. 2001). A number of environmental and health advocates have argued, based both on fundamental toxicology and on long historical experience with chlorinated synthetic chemicals (e.g., DDT and PCBs), that the entire class of thousands of such substances should be tentatively presumed dangerous and that the chemical industry accordingly should wean itself from most major uses of chlorine (Thornton 1991(Thornton , 2000;;International Joint Commission [IJC] 1992; see Howard 2004). The analysis offered here briefly considers the character and function of undone science in the debate provoked by proposals for a \"chlorine sunset.\" The chlorine sunset controversy revolves around conflict between two sharply contrasting regulatory paradigms: risk and precaution (Thornton 2000;Howard 2004). The powerful chemical industry has coevolved with, supports, and is supported by the dominant U.S. and Canadian environmental regulatory regime, which restricts chemical industry decision making only to the extent that detailed calculation of risk indicts individual chemical substances. Meanwhile, Greenpeace, a marginalized, reputedly radical environmental NGO, and the IJC, a prominent but marginalized binational advisory organization, argued for a regulatory regime based on the precautionary principle (see Tickner 2003), which in their view justified governmental action against an entire class of industrial chemicals. The dominant paradigm assumes the unit of analysis to be the individual substance and places the burden of proof on the public to prove harm; in contrast, the challenger paradigm allows, even requires, the primary unit of analysis to be the entire class of substances and places the burden of proof on corporate officials. Within this matrix of political and epistemological conflict, the political economy and political sociology of undone science can be seen to revolve around a series of three dyads, each paradigm implying parallel formulations of \"done science\" and undone science. The three dyads are summarized in Table 1. One dyad appears in the context of health impacts research. Industry and federal officials operating in the risk paradigm hold that the legitimate goal of health impacts research performed or mandated by government is ad hoc identification of individual chlorinated chemicals that cannot be safely manufactured and used. In this paradigm, chlorine chemistry itself is seen as immune to fundamental interrogation; the role of public science is limited to documenting the odd substance that can be definitively proven harmful and, on that basis, restricted. \"We've made the point over and over again that you have to look at each product's physical and chemical characteristics to draw conclusions about what it is going to do in the environment,\" argued Brad Lienhart, of the Chlorine Chemistry Council. To do otherwise would be to \"[make] non-science-or nonsense-into science\" (quoted in Sheridan 1994, 50). Beginning in the early 1990s, \"sunset\" proponents vigorously argued that such research is incapable of interrupting a long series of chlorinated \"Pandora's poisons\" from entering the environment and human tissues long before their deleterious effects are documented. Inevitably remaining undone, they argued, is science capable of systematically identifying unsafe chemicals from among tens, perhaps hundreds, of thousands of chlorinated industrial substances, by-products, and breakdown products, a scope of research that the risk paradigm is sometimes assumed to provide but, owing to the sheer enormity of the undertaking, cannot. The government's effort to identify unsafe chlorinated chemicals is ad hoc precisely because it cannot, in any meaningful sense, be systematic; not only are available resources insufficient, but the enterprise is technically infeasible. Viewed in this light, the science is undoable. The IJC argued: There is a growing body of evidence that [suggests that] these compounds are at best foreign to maintaining ecosystem integrity and quite probably persistent and toxic and harmful to health. They are produced in conjunction with proven persistent toxic substances. In practice, the mix and exact nature of these various compounds cannot be precisely predicted or controlled in production processes. Thus, it is prudent, sensible and indeed necessary to treat these substances as a class rather than as a series of isolated, individual chemicals. (IJC 1992, 29) A second dyad appears in the risk paradigm's stance on innovation. Industry has systematically pursued the development of chlorine chemistry, developing chlorinated chemicals and expanding markets for them; meanwhile, advocates of chlorine precaution have pointed to the need to systematically develop nonchlorine alternatives. This is in part science that the risk paradigm has long left undone-historical research and development trajectories that could have led to a wider range of nonchlorine chemicals and processes being available today. The implication of the historical analysis offered by a leading sunset proponent (Thornton 2000; see also Stringer and Johnston 2001) is that over the past century the technological, economic, and political momentum of chlorine chemistry has to some extent bent the overall industry research and development agenda toward chlorine and away from nonchlorine alternatives. Here undone science consists of a body of nonchlorine chemicals and processes that might now exist but for the long dominance of research and development predicated on chlorine. It is a point seemingly acknowledged by a confidential IJC informant who did not support the commission's sunset recommendation: \"There's no reason why we couldn't, as a global society, live a non-chlorine lifestyle. It's just, you know <laughs>, that ain't gonna happen, because that is not our history! We're kind of, in a way, captives of our past.\" In the risk paradigm, with its laissez-faire orientation, such research and development need not be undertaken by the industry but instead is tacitly left to whichever agency or organization might care to undertake it. Viewed from the vantage point of the industry, with its adamantine conception of chlorine chemistry as technologically and economically inevitable, the only conceivable motivation for conducting such research and development would be some kind of ideological fetish (see, e.g., Chlorine Chemistry Council n.d.). It would represent \"a veiled attempt to return to a pre-industrial Eden,\" one industry supporter suggested (Amato 1993). Crucially, although this agenda would have been and would now be technically feasible, such research would be hobbled by the absence of a sizable cadre of technoscientists devoted to the project and by a lack of financial resources to sustain the effort. A third dyad occurs within the challenger, precautionary paradigm and directly counters the values and priorities of the dominant paradigm's dyads. Paired with precaution advocates' assertion of the need for research to systematically develop nonchlorine alternatives-here seen as industry's responsibility rather than the public's-is an explicit assertion that industry should assume the burden of making the case for any specific chlorinated chemicals (or chemical processes) that can be demonstrated to be both essential (i.e., nonsubstitutable) and capable of being manufactured and used in ways that (to some as yet unstated standard) pose no significant environmental hazard. Industry's motivation for undertaking this latter effort would, of course, be profit. And owing to the presumably quite limited number of substances to be evaluated, it would be both technically feasible and, given the industry's substantial financial and technical resources, affordable. The chlorine sunset controversy is now effectively dormant. In the face of bitter industry resistance and U.S. and Canadian governmental intransigence, the IJC and Greenpeace ceased promoting their sunset recommendations in the mid-1990s (Howard 2004). Thornton's book, which appeared in 2000, reawakened (and in significant ways deepened) the debate, but it did so only briefly. The sunset proposals have not visibly shifted policy at any level in North America. A major international treaty on persistent organic pollutants signed in 2001 represented an important victory for activists, but it also underscored the lingering, unresolved character of the chlorine debate: all twelve of the \"dirty dozen\" substances it required to be phased out are chlorinated compounds, and each was targeted on the basis of its discreet, well-documented characteristics. Meanwhile, thousands of far less extensively studied chlorinated chemicals-and chlorine chemistry as a whole-remain unregulated. This analysis of the chlorine sunset controversy illustrates how regulatory regimes influence the construction and articulation of research priorities. In this case, advocates of the risk and precaution paradigms, on the basis of competing understandings of the appropriate unit of regulatory analysis and appropriate regulatory burden of proof, promote competing conceptualizations of science both done and undone. More specifically, the case suggests that done and undone science in such a controversy can be understood as occurring in dyadic pairs and that a major role for challenger discourses is making the implicit undone portion of dyads within the dominant paradigm visible and explicit. This analysis also highlights an important category of undone science in technoscience controversies-undoable sciencethat improves understanding of how regulatory regimes constrain the identification of undone science. Here, close examination of precautionary advocates' critique of the risk paradigm clarifies the process through which conventional regulatory structures veil undoable science in the form of systematic research for which insufficient resources and insufficient technical means are available.",
          "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (Löwy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] £1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center … there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life … and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't … appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
          "Kempner's research is based on an interview study that examines \"forbidden knowledge,\" a term used to capture scientists' decisions not to produce research because they believe it to be taboo, too contentious, or politically sensitive (a type of negative knowledge in the terminology introduced above). In 2002-2003, she and colleagues conducted ten pilot and forty-one in-depth, semistructured telephone interviews with a sample of researchers drawn from prestigious U.S. universities and representing a diverse range of disciplines, including neuroscience, microbiology, industrial/organizational psychology, sociology, and drug and alcohol research (Kempner, Perlis, and Merz 2005). Those fields were chosen to gauge the range, rather than the prevalence, of experiences with forbidden knowledge. Interviews lasted between thirty-five and forty-five minutes and were audiotaped, transcribed, coded, and analyzed according to the principles of grounded theory (Strauss and Corbin 1990). While many social movements organize around the identification and completion of undone science, others devote themselves to making sure that some kinds of knowledge are never produced. They are not alone. The idea that some knowledge ought to be forbidden is deeply embedded in Western cultures and appears in literature through the ages, from Adam and Eve's expulsion in Genesis to Dr. Frankenstein's struggle with a monster of his own creation (Shattuck 1996). Mertonian rhetoric aside, most people agree that some science poses unacceptable dangers to research subjects or to society at large. The widely accepted Nuremberg Code, for example, places strict limits on human experimentation, in an effort to ensure that some science-such as Nazi human experimentation in World War II-is never done again. Determining which knowledge ought to remain undone can often be contentious, as illustrated by current high-profile public debates surrounding the ethics and implications of stem cell research and cloning technologies. Nevertheless, as in research agenda-setting arenas (Hess 2007), debates and decisions about what knowledge should remain off limits to the scientific community typically occur among elites: legislators and federal agencies perennially issue guidelines and mandates regarding which research should not be conducted, setting limits on everything from reproductive and therapeutic cloning to studies of the psychological effects of Schedule I drugs, like heroin and marijuana. Scientists and the lay public both have limited opportunities to voice their opinion in these discussions. In dramatic cases, scientists have attempted to preempt mandates via self-regulation, as was the case in 1975 when scientists meeting at Asilomar called for a moratorium on certain kinds of recombinant DNA research (Holton and Morrison 1979). According to the forty-one elite researchers interviewed for this case study, these formal mechanisms account for only a portion of the limitations that can produce undone science (Kempner, Perlis, and Merz 2005). More often, researchers described how their research had been hamstrung by informal constraints-the noncodified, tacit rules of what could not be researched or written. Yet researchers were very clear about what constituted \"forbidden knowledge\" in their respective fields. The boundaries of what could not be done had been made known to them when either they or a colleague's work had been targeted for rebukein essence, their work had breached an unwritten rule. The management of forbidden knowledge, thus, worked much as Durkheim said it would: once someone's research had been identified as especially problematic by, for example, a group of activists, their work became a \"cautionary tale,\" warning others \"not to go there\" (Kempner, Bosk, and Merz 2008). In this way, social movement organizations and activists are able to play an important role in debates about what ought to remain undone, whether or not they are invited to the table. Besides their influence on shaping research agenda-setting arenas, social movements can and do influence individual researchers' decisions not to pursue particular types of studies. In recent decades, for example, animal rights organizations have had an enormous influence on the kinds of research that scientists choose not to produce. We found that the researchers in our sample who work with animal models took seriously the threat posed by those organizations. They spoke of \"terrorist-type attacks\" and told stories of colleagues who received \"razor blades in envelopes\" and \"threatening letters.\" Others faced activists who staked out at their houses. Researchers learned from these cautionary tales and, in many cases, said that they had self-censored as a result. One researcher, for example, explained that he would not work with primates-only \"lower order\" animals like mice and drosophilia because: I would like to lunatic-proof my life as much as possible … I, for one, do not want to do work that would attract the particular attention of terrorists … The paranoia was acute. One researcher refused to talk to the interviewer until she proved her institutional affiliation: \"For all I know, you are somebody from an animal rights organization, and you're trying to find out whatever you can before you come and storm the place.\" Over time, the overt interventions of animal rights organizations in the production of research have redefined the ethics of animal research, ushering in legislation like the Animal Welfare Act of 1985, which requires research institutions that receive federal funding to maintain \"Institutional Animal Care and Use Committees\" (Jasper and Nelkin 1992). However, lay groups do not need to use such directly confrontational tactics to influence researchers' decisions, especially if the groups are successful in their attempts to reframe a particular social problem. For example, substance abuse researchers argued that their research agendas were limited by the success of the Alcoholics Anonymous' campaign to define treatment for alcoholism as lifelong abstinence from drink. Although these researchers would like to conduct \"controlled drinking\" trials, in which alcoholics are taught to drink in moderation, they argued that \"There's a strong political segment of the population in the United States that without understanding the issues just considers the goal of controlled alcohol abuse to be totally taboo.\" The mere threat of interference from the grassroots was enough to keep many researchers from conducting certain studies. Several drug and alcohol researchers described great unwillingness to conduct studies on the health benefits of \"harm reduction\" programs, such as those that distribute free condoms in schools or clean needles in neighborhoods, because they might attract unwanted controversy from lay groups who oppose such public health interventions. Thus, in some contrast to the role that social movement organizations and lay experts/citizen scientists play in exposing undone science and encouraging knowledge creation in chemical, air monitoring, and breast cancer research, this study shows that the same actors can also play a powerful role in determining which knowledge is not produced. Moreover, conflict over the direction of funding streams, while critically important to the political of research agenda setting, do not solely determine what science is left undone. Rather, social movements are also effective beyond research agenda-setting processes that occur at the institutional level; this study provides evidence that they also shape the microlevel interactional cues and decision-making process of individual scientists. Although more research is needed to understand the circumstances under which researchers decide to selfcensor in response to pressure from outside groups, this case suggests that social movements may have much greater potential to thwart research than originally thought. The implications are intriguing and deserve greater attention. On one hand, disempowered groups may leverage these techniques to gain a voice in a system of knowledge from which they are typically excluded. On the other hand, it is troubling to learn that the subsequent \"chilling effect\" happens privately, often without public discussion and in response to intimidation and fear.",
          "The diverse cases provide an empirical basis for moving forward the theoretical conceptualization of undone science in relation to a new political sociology of science and that program's concern with how research agendas are established. Perhaps the most significant general observation is that the identification of undone science is part of a broader politics of knowledge, wherein multiple and competing groups-including academic scientists, government funders, industry, and civil society organizations-struggle over the construction and implementation of alternative research agendas. To a large extent, our case studies focus on attempts by civil society or quasigovernmental organizations to identify areas of research they feel should be targeted for more research. However, the identification of undone science can also involve claims about which lines of inquiry should warrant less attention than they currently receive, either because there are decreasing social returns on continued investments in heavily researched areas or because the knowledge is deemed not worth exploring and possibly dangerous or socially harmful-what Gross (2007) calls \"negative knowledge.\" Examples of the latter include the research programs and methods targeted by animal rights groups and research on chlorinated chemicals targeted by Greenpeace. There are many other cases that would fit this role for civil society organizations, including calls for research moratoria on weapons development, genetically modified food, nuclear energy, and nanotechnology. Five more specific insights follow from and add complexity to this general observation. First, while we see undone science as unfolding through conflict among actors positioned within a multiorganizational field, as Gibbons' case shows, definitions of undone science may also vary significantly within different organizational actors, coalitions, or social movements. Some portions of the movement may be captured by mainstream research, and consequently advocacy is channeled into support for the experts' prioritizations of research agendas. Thus, a research topic such as environmental etiologies of breast cancer may represent undone science to a marginalized segment of breast cancer advocates and their allies in the scientific community, but it may represent negative knowledge to the majority of breast cancer advocates and the dominant cancer research networks. To further complicate the picture, rapid developments and changes within the scientific field, such as the development of genomic research to better pinpoint environmental or epigenetic factors, may result in shifts in research priorities that can open up opportunities for research in areas of undone science. Here, one sees that internal changes and differences among both researchers and civil society advocates interact to define shifting coalitions of research priorities. Second, the dynamic nature of coalitions and alliances that emerge around undone science suggests that the articulation of research priorities is often a relatively fluid process; even when civil society groups target some areas of scientific research as deserving low or no priority, their views may in turn lead to the identification of other areas of research deserving higher priority. For example, the position of an animal rights group may begin with opposition to some types of animal research but lead to support for more \"humane\" forms of animal research that have been reviewed by animal research committees. Likewise, the position of an organization such as Greenpeace in opposition to chlorinated chemicals is linked to an articulation of the need for research on green chemistry alternatives. As these examples suggest, the identification of undone science can be viewed as multifaceted outcomes of coalitions and conflict among diverse groups representing various social categories, each promoting a mix of topics seen as deserving more, less, or no attention from the scientific community. Third, making sense of the complex processes that produce undone science involves attending to the distributions of power, resources, and opportunities that structure agenda setting within the scientific field. An important element of field structure is the role of regulatory regimes in shaping definitional conflicts over research priorities. Howard's work suggests that done and undone environmental science dyads can be a key expression of the regulatory paradigm in which they occur and intimately linked to the way expertise is conceptualized and deployed in the paradigm. Furthermore, he proposes that until mainstream science faces a challenger, important forms of undone science within the dominant paradigm can remain implicit and unarticulated. In other words, undone science may take the form of a latent scientific potential that is suppressed through \"mobilization of bias\" (Lukes 2005; see also Frickel and Vincent 2007). Ottinger (2005) also notes the important role of regulatory standards in defining opportunities for activists who attempt to get undone science done largely using their own resources. In the case of air monitoring devices, an alternative research protocol and data gathering device operated by laypeople provides a basis for challenging official assurances of air quality safety. Rather than advocate for shifts in a research agenda, they simply enact the shift. In Howard's terms, the lay research projects also dramatize the implicit and unarticulated bias in the dominant method of air quality monitoring. Ottinger's (2005) focus on the double role of standards as enabling and constraining factors in establishing both the conditions and limitations of undone science is intriguing, and it remains for future research to examine the efficacy of tactical dynamics in relation to structural constraints encountered across a range of regulatory and research contexts. Fourth, while access to financial resources is an implicit focus of efforts to identify undone science, Kempner's research demonstrates that the interaction of civil society and research priorities is not restricted to the broad issue of funding. Although civil society organizations can exert an effect on research funding allocations, as we have seen especially in environmental and health research priorities, Kempner notes that there are other mechanisms that can cause such shifts. Her work suggests that efforts to study the problem of undone science should also consider the role that a moral economy has in shaping scientists' decisions about what research programs they will and will not pursue (Thompson 1971; on moral economy in science, see Kohler 1994). Furthermore, even if scientists do not accept in principle the notion that certain knowledge should remain undone, they may simply decide not to invest in some areas of research because of intense direct pressures from civil society organizations such as animal rights groups. As a result of individual decisions not to engage in an area of research, changes in the research agendas of a field can occur even when funding is not shifting dramatically. Finally, sometimes structural constraints such as limited access to resources coincide with practical constraints to produce \"undoable science.\" In the case of the chlorine sunset provisions, precaution advocates see governmental programs for screening individual chemicals as obscuring a plain fact: the sheer number of chemicals and their complex interaction with ecological and biological systems make it impossible to predict whether a given concentration of a given chemical will in any meaningful sense be \"safe\" or whether it will be a risk. As a result of this \"wicked problem\" (Rittel and Weber 1973), the articulation of undone science as a goal for research prioritization and funding-in this case, the standard assumption of a need for ever more research on the environmental, health, and safety implications of new chemicals-turns against itself, because the call for research into specific chemicals tacitly supports a regulatory framework that systematically generates a policy failure (see Beck 1995).",
          "This study demonstrates some of the ways in which the analysis of undone science can enrich empirical understandings of research agenda-setting processes. The considerable variation we find in just four cases suggests that one promising avenue for future research lies in developing more systematic comparisons across academic, government, industry, and community settings. Doing so will further elaborate the ways in which the institutional contexts of research-including different sets of political and economic pressures, normative expectations, resource concentrations, and sizes and configurations of research networksshape the articulation of undone science and the successful or failed implementation of alternative research agendas. Our broader aim in seeking to give undone science higher visibility within STS is to broaden the foundations for a new political sociology of science. Much like feminist and antiracist science studies, the political sociology of science situates questions relating to the uneven distribution of power and resources in science at the center of the STS project while remaining attentive to how knowledge and its inverse-ignorance-is socially shaped, constructed, and contested. As we have argued here, one of the crucial sites where questions of power, knowledge, and ignorance come together is in the domain of research agenda setting, where intense coalitions and conflicts are forged to gain access to the limited resources that ultimately shape what science is done and what remains undone. ",
          ""
        ],
        "ground_truth_definitions": {
          "undone science": {
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.",
            "context": "What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s40537-025-01077-x",
        "sections": [
          "Automatic information extraction from texts has been an important research topic of NLP for decades. With the rapid development of big data, and social media, the amount of available texts on the Internet has increased exponentially. It becomes an important research field to analyze and extract the informative content from texts. The main research on automatic analysis of the texts includes keyphrase extraction (KPE), phrase mining, named entity recognition (NER), hot spot detection, new word detection, etc. Effective solutions to these problems have been instrumental for facilitating downstream tasks such as machine translation [1], mention detection [2], textual themes extraction [3], lexicon construction [4], ontology learning [5], and knowledge organization [6]. Terminology extraction, referred to as terminology recognition, is an important research topic continually. It is usually considered as a sub-problem of information extraction and aims to identify terminology in the specialized texts, where terminology can be regarded as the linguistic representation of domain-specific concepts [7]. It is labour-intensive to manually extract terminologies and prone to errors due to the shifting topics [8], so ATE has attracted more and more attention. Many ATE methods, frameworks and tools have been developed continually. Traditional terminology extraction methods can be categorized into three kinds: linguistics-based approaches, statistics-based approaches, and hybrid approaches. Linguistics-based approaches [9,10] rely on dictionaries and rules to extract terminologies by matching specific patterns or vocabulary. Statistics-based approaches [11] leverage frequency information from the text to identify terminologies. Hybrid approaches [12] combine linguistic and statistical techniques, effectively utilizing domain knowledge and data statistics to enhance accuracy. Each method has its advantages, and the choice depends on specific requirements. However, these supervised approaches are solely based on the linguistic and statistical characteristics, so that the scale and quality of the annotated corpus have a great impact on the performance of terminology extraction. Researchers have gradually applied external knowledge (Wikipedia, 1 WordNet [13]), semantic information [14], graph structure [15], and topic models [16,17] to terminology extraction. Building on the advancements in the field, terminology extraction has evolved into methods based on machine learning [18,19], deep learning [20,21], graph-based approaches [22,23], and LMs [24,25]. This paper provides a comprehensive review of terminology extraction from natural language texts. It's worth noting that our review, for the first time, emphasizes the related work of terminology extraction with LMs. By delving into significant research topics, the survey encompasses formal definitions, related topics, existing approaches' taxonomy, commonly-used datasets, application scenarios, and open research issues. Serving as a valuable resource, this paper offers researchers and practitioners in the terminology extraction field an insightful introduction to key methodologies, typical features, accessible demos, ongoing challenges, and emphasizes the significance of related software tools and datasets for research and development in this domain. The subsequent sections of this paper are structured as follows. Section 2 will expound upon related concepts and the research progress in the field of terminology extraction. In Sect. 3, a comprehensive depiction of a generic and widely applicable system architecture for terminology extraction will be provided. Traditional linguistic-based and statistical-based approaches are commonly used in earlier work and recent work on terminology extraction. Recent studies also apply graph-based approaches, machine learning, deep learning and LMs. Based on this taxonomy, approaches to terminology extraction are summarized in Sect. 4. We describe the annotated datasets, open-sourced tools, and evaluation metrics for terminology extraction in Sect. 5. Section 6 analyzes several key issues in the field of terminology extraction, while Sect. 7 explores future research directions in this domain. Finally, we conclude the paper in Sect. 8.",
          "The section describes background knowledge for the core concepts of terminology extraction. We first describe and define the two important concepts of \"terminology\" and \"domain\" and then describe the characteristic related to terminology.",
          "With the in-depth development of terminology extraction, researchers have proposed different definitions of terminology, but there is still no unified standard. For example, the authors in [26] propose to separate the concept of the terminology from the common word, marking the beginning of terminology research. Wüster defines terminology as \"a concept reference system within a specialized field\". Based on Wüster's terminology theory, Wright et al. [27] argue that terminology should be organized by subject and examined for its logical, holistic, part-whole, and associative relationships. Sager et al. [28] define terminology as items with special reference significance within a particular field, in contrast to common words, which are widely used in everyday language. With ongoing research into terminology extraction methods, the strict separation between terminology and common words is gradually diminishing. The authors in [29,30] indicate the concept of terminology must be combined with the context. Regarding terminology ambiguity in the context of text, an expression is considered a terminology if it has domain-specific semantics, whereas it is classified as a common word if it lacks domain relevance. However, common word and terminology themselves cannot be regarded as completely separated because general language and specificdomain language have overlapping parts. Concurrently, the definition of terminology provided by the International Organization for Standardization (ISO) [31] emphasizes that terminology serves as a verbal designation for a general concept within a specific domain. This underscores the fundamental association between terminology and the domain to which it pertains. Therefore, the general definition [32] for terminology can be summarized as terminology is a linguistic unit that characterizes a specific domain. Since the definition of terminology clearly indicates that terminology has tight connection to domain, this leads to what constitutes a domain. For example, Cole et al. [33] point out that terminology research should be adapted to the subject field. While there exists a prevailing inclination to confine terminology research within the realms of scientific and technical domains, it is imperative to recognize that virtually all professional fields possess the potential to be the focal point of terminology research. The authors in [29] argue that the knowledge domain is a semantically much wider terminology, somewhat regarded as a discipline in its own right. These descriptions provide some concepts about the domain but do not explicitly define the boundaries of the domain. ISO delineates a domain as a realm of professional knowledge, and the delineation of its boundaries is established with a focus on purpose-related considerations. ISO provides a definition of a domain as a specialized area of professional knowledge. The demarcation of a domain's boundaries is approached from a purpose-oriented perspective according to ISO standards. ConstanzeAnna et al. [32] implicitly define the content of a domain by conducting topic-relevant web crawls or utilizing existing specialized corpora.",
          "Terminology represents the related concepts of a domain, and terminology constructs the skeleton structure of the domain knowledge. The relationship with the domain is the main feature of distinguishing domain-specific terminology from a common word, but identifying terminology is not always simple. Whether it is manual annotation or automatic extraction via a model, there exist some expressions which cannot stably conform to the definition of terminology in the domain. For example, \"Graphics Processing Unit (GPU)\" is highly relevant to the field of computing and can be directly considered a computer terminology. However, common words like \"folder\" have some association with the computing domain, but it's uncertain if they can be classified as computer terminologies. As previously mentioned, terminologies cannot be considered entirely independent of general vocabulary. Kageura et al. [34] explored the relationship between terminologies and general vocabulary, as illustrated in Fig. 1. At the intersection of language and specialized domains, the phenomenon of overlap between terminologies and common words has been a notable issue. To better address the ambiguity of terminologies, researchers often categorize terminologies into different hierarchies, ensuring precise definitions. Hoffmann et al. [35] introduce a classification of terminology into three distinct categories: discipline-specific terminology, non-discipline-specific terminology, and common word. Common word is not considered as terminology, while nondiscipline-specific terminology is shared across multiple fields. Roelcke et al. [36] introduce a more detailed framework, establishing a nuanced model that discriminates among four tiers, as illustrated in Fig. 2. Intra-subject terminology is domain-specific, tailored to a particular field, while inter-subject terminology finds application across multiple domains. Extra-subject terminology, on the other hand, Fig. 1 The relation between terminology and general vocabulary [34] Fig. 2 Tiers of terminology [36] Translated by Constanze Anna [32] lacks domain affiliation but finds utility within specific fields. Lastly, non-subject terminology encompasses all terminological items employed universally across various specialized domains. Rigouts Terryn et al. [37] introduce a two-dimensional space with two distinct attributes (lexicon-specificity and domain-specificity) that categorizes terminologies into three classes: Specific Terms, Outof -Domain Terms , and Common Terms, as illustrated in Fig. 3. Here, lexicon-specificity measures the degree to which a terminology is a specialized vocabulary, and domain-specificity indicates its relevance to a specific domain.",
          "In this section, we present a generic system architecture that summarizes the common characteristics of terminology extraction methods. Terminology extraction approaches [38][39][40][41] initiate their discussions with a preprocessing phase, mirroring a prevalent trend in contemporary applied research across various NLP tasks [21,[42][43][44]. Nakagawa et al. [45] suggest that terminology recognition algorithms [32,46,47] typically employ two steps: candidate terminology extraction, candidate terminology scoring and ranking. Q. Zadeh et al. [48] describe the Fig. 3 Term model by [37] Fig. 4 General architecture of terminology extraction approaches [48] typical terminology extraction process. Building upon Q. Zadeh's work, we summarize a preprocessing step before terminology extraction, as depicted in Fig. 4. First, the pipeline performs preprocessing tasks, such as cleaning and tokenization on the raw texts. Subsequently, linguistic filters, such as the filter using Part-of-Speech (PoS) [49], are applied to extract candidate terminologies from the texts. Then unithood 2 and termhood 3 are evaluated on the candidate terminologies to generate a sorted list.",
          "The first step in terminology extraction is preprocessing the raw texts, as shown in Fig. 5. During the preprocessing stage, the most common tasks include: • Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems. • PoS tagging: Assigning a PoS tag to each word or token in the texts to indicate the word's grammatical role and lexical category within the sentence. This task is not mandatory in preprocessing. At present, there is a wide range of preprocessing tools available, catering to both English texts, such as TweetNLP, 4 Stanford CoreNLP, 5 and NLTK, 6 and Chinese texts, including HanLP, 7 jieba, 8 and THULAC. 9",
          "The generation of a candidate terminology set is a fundamental and crucial step in ATE methods because the quality of candidate terminologies directly influences the final performance of terminology extraction. After preprocessing the initial texts, various techniques can be employed to extract candidate terminologies and generate a set of candidate terminologies, including PoS tagging, specific markers, and statistical-based heuristic rules. Currently, PoS tagging is one of the most widely used methods to solve the problem about candidate terminology extraction.",
          "2 Unithood: the degree of a string to occur as a word or a phrase. 3 Termhood: the degree of a word or a phrase to occur as a domain specific terminologyrepresenting certain concepts in that domain. 4 http:// www. cs. cmu. edu/ ~ark/ Tweet NLP/. 5 https:// stanf ordnlp. github. io/ CoreN LP/. 6 https:// www. nltk. org/. 7 https:// www. hanlp. com/. 8 https:// github. com/ fxsjy/ jieba. 9 http:// thulac. thunlp. org/. By tagging words with their respective PoS and applying shallow parsing based on predefined syntactic patterns [50][51][52], such as noun phrases or verb phrases, multiword expressions. On the other hand, specific markers within the texts can also be relied upon for candidate terminology extraction. These markers (bold, italics), help determine boundaries of terminologies, and candidate terminologies are extracted based on boundary markers, excluding texts sequences that do not contain terminologies (phrases containing verbs and pronouns). Finally, statistical methods can also be employed for candidate terminology extraction using heuristic rules. These heuristic rules, such as n-gram filtering [53], are used to retain high-frequency n-word sequences (typically 1 n 6 ). This approach can generate a large number of candidate terminologies, necessitating additional filtering steps, such as stop-word filtering or checking for the presence of special symbols.",
          "After candidate terminology extraction, a scoring procedure is employed to measure the likelihood that a candidate terminology is one we can extract, this scoring procedure usually combines termhood and unithood scores. Different terminology extraction methods utilize different terminology ranking algorithms. Terminology candidate ranking algorithms constitute the most intricate step in ATE methods [54,55]. Therefore, section 4 provides a comprehensive summary of ATE methods, categorizing them into seven distinct categories. Section 4 also provides detailed explanations of candidate terminology scoring and ranking methods. In this paper, most of ATE methods follow the framework depicted in Fig. 4. However, there are exceptions among ATE methods. ATE approaches, such as sequence tagging-based approaches [56,57], directly generate the final terminology sets but do not perform scoring and ranking within the set. Methods, such as Segphrase [53], AutoPhrase [58] and SemRe-Rank [59], generate a ranked terminology set, and then they re-rank the previously sorted results using the extra information.",
          "In this section, we divide the existing work of terminology extraction into seven categories: Linguistic-based approaches, Statistical-based approaches, Hybrid approaches, Graph-based approaches, Machine learning approaches, Deep learning approaches, LM-based approaches, and Multilingual ATE Approaches (Table 1).",
          "Initially, we can only use Linguistic-based methods for terminology extraction limited by the capability of model, the scale of annotated corpus and the performance of computing resource at that time. These methods primarily leverage linguistic knowledge (lexical patterns, morphological features, semantic information). The core idea in these methods is that terminology often appear within superficial language structures and patterns. We can extract terminologies by formulating a well-designed set of linguistic rules which can summarize the commonly-used terminology patterns. Early linguisticsbased methods mainly relied on linguistic knowledge. They identify words or phrases with specific PoS tagging patterns as potential terminologies. These methods employ rule templates, which are crafted manually by the linguistic experts, to identify domainspecific terminologies from candidate terminologies. For example, systems such as LEXTER [60], FASTR [61], MeSH [62], DP [63], TF-RBM [64] and HRB-ATA [10] define hundreds of rules, including a large number of heuristic rules, to select candidate terminologies. Moreover, Chatterjee et al. [65] employ domain-specific patterns, expertly crafted and encoded as regular expressions, to identify initial terminologies. In a similar vein, Cram et al. [9] choose candidate terminologies through the application of superficial grammars expressed as a collection of regular expressions pertaining to PoS tagging. And Marciniak et al. [66] also use shallow grammars with the additional specification of morphological values dependencies to select correct terminologies. Venugopalan et al. [14] guide the input of Latent Dirichlet Allocation (LDA) [67] model for terminology extraction with regular expression-based linguistic rules. The linguistic-based methods mentioned above require manual rule construction, resulting their poor generalization. To address this issue, scholars have proposed a model for the automatic acquisition of domain-specific linguistic rules and the extraction of PoS rules for terminologies from corpora using this model. For example, Foo et al. [68] use the supervised machine learning algorithm, Repeated Incremental Pruning to Produce Error Reduction (RIPPER) Algorithm [69], to produce rules. Liu et al. [70] introduce an automated technique for the selection and learning of rule sets, with the objective of refining the syntactical rulebased method for extracting aspects in opinion mining. In addition, Shao et al. [71] introduce an unsupervised method that relies on sentence patterns and PoS sequences. This method does not necessitate learning from annotated data but merely initializes a few patterns to extract terminology extraction. In general, linguistics-based terminology extraction methods initially relied on manually crafted rules. Over time, scholars have Table 2 Statistical-based approaches Category Method name Calculation formula Unithood PMI PMI = log p(w1w2) p(w1)p(w2) ≈ log N•f (w1w2) f (w1)f (w2) , a represents a terminology consisting of |a| words, w 1 and w 2 are the words that make up terminology a, w 1 w 2 represents a bigram candidate term, p represents probability, and p(w 1 w 2 ) represents the probability of words w 1 and w 2 occurring together. 12 represent the frequencies of w 1 , w 2 , w 1 w 2 appearing, N represents the total number of words. , O i represents the observed TF of a certain category i in the candidate terminology a, and E i represents the expected TF calculated based on the corpus. Dice Termhood TF proposed methods for automatically acquiring rules, thus improving the generalization performance. These methods hold potential for terminology extraction in various domains. However, linguistics-based terminology extraction methods depend on expert knowledge and POS taggers, and manually written rules, which cannot cover all domainspecific features. As a result, there is limited research utilizing pure linguistic methods for ATE. Instead, these methods are often used as a preprocessing step for generating sets of terminology candidates in terminology extraction.",
          "Statistical-based terminology extraction approaches assume that candidate terminology with higher frequencies are more likely to be actual terminologies. These approaches use the target corpus's distribution frequency of words/phrases to extract terminologies. The advantage of statistical approaches is that it does not depend on the language of the dataset. Statistical approaches usually attribute the terminology features to two convenient principles [12], i.e., unithood and termhood. These Statistical approaches leverage a range of techniques, including term frequency (TF), term frequencyinverse document frequency (TF-IDF), mutual information, T-Score, cosine similarity, information gain, and so on. All these techniques are shown in Table 2. Unithood is used to measure the collocation strength and adhesion degree of multi-word candidate terminologies, commonly used unithood methods include Pointwise mutual information (PMI) [72], log-likelihood ratio (LLR) [73,74], z-score [75], T-score [72] and Chisquared ( χ 2 ) [76,77]. Other many studies [53,58,[78][79][80][81]] also use unithood to measure candidate terminology. Although unithood plays an indispensable role in terminology extraction, the research of [82] and [83] show that unithood is not enough to evaluate the validity of candidate terminology and it can only determine that the word sequence is a fixed collocation. Termhood measures the relevance between candidate terminologies and domains according to the statistical features of candidate terminologies from the target corpus, such as TF and TF-IDF. TF represents the frequency of a word appearing in a text, typically used to assess the importance of a word within the text. Inverse Document Frequency (IDF) [11] measures the reciprocal of the proportion of documents within the corpus in which a candidate terminology is present relative to the total number of documents. TF-IDF extends TF by combining the importance of a word within an individual text and its importance within the entire collection of texts. TF-IDF is one of the most effective methods in domain-specific measurements. There are also some other terminology extraction methods based on termhood are proposed, such as average term frequency (ATF) [84], domain consensus (DC) [85], and ResidualIDF (RIDF) [78]. In addition, the YAKE! [86] is not constrained by the source or collection of texts. It automatically identifies and extracts the most important Keyphrases from the text by considering factors such as TF, position, and relevance. This allows it to be effective across various text collections and domains. Statistical terminology extraction methods identify terminologies by analyzing probabilistic statistics, such as TF and document frequency, within text data. This approach is applicable to text data from various domains and does not require linguistic rules or annotated data. However, it heavily relies on the scale and quality of the text data and lacks a deep understanding of the underlying semantic information of terminologies.",
          "At the early stage of terminology extraction, the hybrid approaches mostly combine linguistic methods and statistical methods, among which C-value and NC-value are the early and representative ones. C-value [50] combines linguistic and statistical methods to generate a set of candidate terminologies using language rules and then filters them using statistical information. NC-value [50] builds upon C-value and fully leverages rich contextual information of phrases. The NC-value method begins by computing the C-value scores and ranking the terminology candidates in a corpus. It then generates a context information list for each candidate terminology, where each contextual word is assigned a weight. Finally, the NC-value value is calculated based on the C-value score of the candidate terminology and the context information list. Basic [87] modifies the C-value method by expanding nested terminologies. It considers that the number of nested terminologies in long word strings should also be a part of the termhood of candidate terminologies. The Basic model emphasizes extracting frequent long word sequences but overlooks the issue of domain-specific candidate terminologies. To address this concern, the ComboBasic model [88] introduces a customized functionality for terminology scope based on the Basic model. Yao et al. [89] propose a concept extraction approach that combines linguistic analysis, statistical analysis, and semantic analysis. This method retains the language and statistical analysis components of hybrid approaches while introducing semantic analysis to capture interrelated concepts. Furthermore, some works combine statistics features and rules to extract domain terminology [90,91]. Yan et al. [91] propose an algorithm based on rules and statistics to extract water environment terminology. They use n-gram to segment the preprocessed texts, use related rules to improve mutual information and adjacency entropy to filter candidate terminology, and then use TF-IDF to select terminology about the water environment. Hybrid methods from this group use terminology extraction methods as features and attempt to learn the importance of each feature in an unsupervised or supervised setting. Hammi et al. [92] introduce a hybrid approach for aspect terminology extraction that combines linguistic-based and deep learning methods. This approach leverages a set of rules to extract a highrecall list of aspect terminologies. Additionally, it employs deep learning techniques (word embeddings) to create another list of aspect terminologies enriched with semantic information. Finally, the extraction process combines these two lists of aspect terminologies. Hybrid terminology extraction methods combine linguistic analysis, statistical analysis, and other techniques, leveraging the strengths of each to improve the precision and recall of terminology extraction [93]. Their advantages lie in enhancing domain independence and language neutrality through feature combinations, making them adaptable to various application scenarios, and enabling the extraction of domain-specific terminologies to overcome the limitations of traditional methods. However, the drawbacks of hybrid methods are also apparent. Many methods rely on linear combinations of voting or heuristic algorithms, which fail to fully account for the nonlinear relationships between features, leading to insufficient depth in feature combinations and affecting the extraction results [51]. Moreover, hybrid methods face limitations in adapting to different domains and languages, and may struggle with complex tasks.",
          "Graph-based approaches are inspired by PageRank [94]. These approaches convert documents into graphs, where nodes represent words, and connections between nodes signify the co-occurrence of those words. Graph-based approaches can fuse more vertex features. Table 3 summarizes graph-based terminology extraction methods. In 2004, Mihalcea et al. [22] first propose the KPE algorithm Textrank based on PageRank.",
          "",
          ",where σ represents the damping factor, typically set to 0.85, In(v i ) is the set of vertices that point to it, Out(v i ) is the set of vertices that vertex v i point to. w jk , w ij is either 1 or 0, with 1 indicating that v i and v j co-occur, and 0 otherwise. SingleRank, ExpandRank Using PageRank algorithm, but is the co-occurrence count of words v i and v j , sim doc (d 0 , d p ) is the similarity between documents d 0 and d p . PositionRank Improving PageRank using a non-uniform distribution prior, represents the position weight of vertex v i with respect to other vertices.",
          "Improving PageRank using a non-uniform distribution prior, but p(v i ) is a topic distribution inferred by LDA.",
          "Using TextRank Algorithm, but , where P(v i ) is the set of the word offset positions of candidate v i .",
          "Using TextRank Algorithm, but is the candidates set belonging to the same topic as v j and α is a hyperparameter.",
          "Using TextRank Algorithm, but the weight is a embedding weight w(i, j) vi , e vj are the term embeddings for v i , v j .",
          "Using personalised PageRank algorithm Pr = σ MPr + (1 -σ )v, M is a transition matrix, Pr is a vector where each element represents the score assigned to the corresponding node.",
          "Using TextRank Algorithm, but the weight is determined using either pre-trained vector similarity or WordNet-based concept similarity.",
          "Improving PageRank using a non-uniform distribution prior, but",
          "Improving PageRank using a non-uniform distribution prior, but where w vi is the weight of v i as we defined before, and deg(v i ) is the degree of v i . TextRank constructs a semantic text graph based on the co-occurrence frequency of words and calculates the importance score of each word after iteration. SingleRank [95] extends TextRank by introducing weights to the edges between nodes. On top of SingleRank, ExpandRank [95] incorporates TF-IDF information from neighboring documents. Ushio et al. [96] introduce TFIDFRank and LexRank, each of which extends SingleRank by computing word distributions using TF-IDF and lexical specificity, respectively. PositionRank [97] integrate the position information of words into a biased PageRank to score and rank keyphrases. TopicRank [98] employs hierarchical agglomerative clustering to group candidate phrases into distinct topics. As an extension of TopicRank, MultipartiteRank [23] improves the selection of candidate terminologies within a cluster by representing candidate terminologies and topics in a multipartite graph, thereby enhancing the ranking of candidate terminologies. For the first time, TopicalPageRank [16,99] integrates topic information into the formula for PageRank computation. TPR utilizes the inferred latent topic distribution from a LDA model to perform PageRank on noun phrases extracted from the document, resulting in the final set of Keyphrases. To use the semantic relevance and contextual information between words, Khan et al. [100] propose the Term Ranker based on TextRank, which learns the semantic representation of candidate terminologies by embedding. This semantic representation is used to capture the similarity and relationship strength between terminologies. Then edges with weight are added between nodes to construct an undirected weighted graph. In addition, the Term Ranker method also integrates the synonym terminologies glossary, which combines the nodes representing synonymous terminologies. It increases the number of central nodes in the semantic graph, thus improving the score of low-frequency terminologies. Pan et al. [101] propose a new graph propagation algorithm to address the issue of low-frequency terminologies in the MOOC domain. This algorithm ranks candidate terminologies based on the learned semantic similarity between vocabulary, the quantity, and quality of voting terminologies. The authors in [59,102] propose a general SemRe-Rank method to enhance the existing terminology extraction methods. First, SemRe-Rank extracts a set of candidate terminologies and scores them using existing ATE methods. Then, it constructs a graph for each document, embedding words into a personalized PageRank and iterates through the graph until convergence. This process aims to calculate revised importance scores for each candidate terminology, achieving the goal of re-ranking. Zhang et al. [103] introduce TextRankembedding and TextRank-wpath, which combine semantic similarity with TextRank for terminology ranking. However, TextRank-embedding uses cosine similarity based on word embedding, while TextRank-wpath utilizes WordNet-based concept similarity. WikiRank [104] is an unsupervised extraction method based on Wikipedia background knowledge. It constructs a semantic graph based on semantics and obtains the optimal set of key phrases by solving optimization problems on this graph. Graph-based terminology extraction methods construct a graph from the text to model semantic relationships between terminologies. This approach can incorporate various features, such as TF, position, topics, to calculate the weights of words. In comparison to statistical methods primarily based on TF, graph-based methods can handle low-frequency but significant terminologies well, while avoiding the expensive cost of manual annotation. However, the performance of this method is influenced by the size of the graph and the density of edges, so achieving fast and efficient propagation of the graph remains a challenge to be addressed.",
          "Machine learning-based terminology extraction methods are powerful techniques for automatically identifying and extracting domain-specific or topic-specific terminologies from texts. These machine learning-based extraction methods typically start with annotating a dataset and then converting training instances into a feature space. The feature space integrates various natural language features to improve the accuracy of terminology extraction. Traditional machine learning algorithms, such as Conditional Random Field (CRF) [105], CRF++ [106], Support Vector Machine (SVM) [107], Decision Tree [108], and Naive Bayes, fundamentally focus on enriching the features of input data for terminology extraction. These features can be linguistic-based features (POS patterns, the presence of special characters), statistical-based features, or a combination of both. In 1999, Frank et al. [109] propose the KPE method, which utilizes a Naive Bayes model to classify candidate terminologies. The features used in this method include the TF-IDF of words and their positional information. In 2000, Turney et al. [110] compare the performance of genetic algorithms and decision trees in the task of KPE. Ercan et al. [111] employ a Bagged decision tree to extract keyphrases by combining scores derived from WordNet with the positional information of words. Zheng et al. [112] employ a CRF model for domain-specific terminology extraction. The model used only six features (POS tags, TF-IDF, semantic information, mutual information, left entropy, and right entropy). Doan et al. [113] employ an SVM model to build a binary classifier on annotated training samples for recognizing drug entities. Haddoud et al. [114] employ the logistic regression algorithm to tackle the KPE task, utilizing features such as word length and TF. Shirakawa et al. [115] introduce an extended naive Bayes model for extracting key terminologies from texts, enabling the classification of noisy short texts. Liu et al. [53] address the limitation that the original TF cannot accurately assess the true quality of terminologies. They introduce the SegPhrase method, which combines the concept of phrase segmentation with machine learningbased terminology extraction methods, resulting in promising extraction outcomes. Bay et al. [19] propose approach of machine learning using word embeddings with shallow linguistic information. They combine the established statistical extraction method TF-IDF with Bay's new approach. They incrementally add candidate terminologies using the Word2Vec model [116] and exploit semantic features from a corpus based on domain-specific seed vocabulary. In addition to traditional machine learning methods, hybrid algorithms combine statistical machine learning and deep learning techniques, such as LSTM-CRF [117], BiLSTM-CRF [118], BiLSTM-CNNs-CRF [119], and BERT-BILSTM-CRF [120]. These methods will be discussed in detail in Sect. 4.6. Machine learning-based terminology extraction methods offer advantages in accuracy and flexibility. They effectively integrate various features [112,114], including linguistic features (POS patterns and special characters) and statistical features (TF-IDF and mutual information), improving extraction precision. These methods are widely applicable and stable across many tasks, especially in domain-specific data, where task optimization can lead to better results. However, they have limitations. These methods depend heavily on feature engineering, requiring significant manual effort and domain knowledge, making them time-consuming and costly. They also rely on high-quality annotated datasets, limiting their use in unsupervised or weakly supervised scenarios. Additionally, simply combining statistical and linguistic features often fails to capture the deep semantic information of terminologies [121], resulting in suboptimal performance in complex tasks.",
          "Deep learning methods offer a variety of solutions for NLP tasks and have garnered significant attention in the field of terminology extraction. These methods typically achieve accuracy levels close to expert performance and are widely employed for terminology extraction. Within the realm of deep learning methods, we categorize them into traditional deep learning approaches and embedding-based methods to comprehensively explore their applications and effectiveness. The early deep learning approaches regard terminology extraction as a classification problem and need to extract and select candidate terminology firstly. Caragea et al. [122] propose CeKE, a binary classification model designed to categorize candidate phrases into keyphrases and non-keyphrases. In 2016, Wang et al. [123] propose a weakly-supervised joint model with Long Short-Term Memory (LSTM) [124] classifier and CNN classifier to learn different representations of candidate terminology without manually selecting features. Inspired by Wang's research, Khosla et al. [125] employ a joint training model, which introduces character-level n-gram embeddings with CNN classifier, to extract terminology. Amjadian et al. [126] initially employ local-global embeddings and a classifier to filter terminologies. Subsequently, the classifier is used in reverse to determine whether the candidate terminologies are genuine terminologies, thereby significantly improving the results of terminologies extraction. Gao et al. [20] propose an end-to-end deep learning model, which extracts nested terminology by learning the vector representation of candidate terminology. Moreover, the majority of research based on deep learning transforms terminology extraction into a sequence labeling problem [57]. In this approach, character vectors, word vectors, PoS features, entity features, and others are assigned corresponding tags. The general steps are illustrated in Fig. 6. In [127], an LSTM-CRF model, which combines word embedding features with character-level information to generate the final word representation, is used. In 2018, Zhao et al. [56] propose the BiLSTM-CRF model, which regards terminology extraction as a sequence tagging task. This model extracts the word vector features, PoS features and entity features of each word as input and then uses CRF to map the word to one of B, I, O, E, S tags after bidirectional multilayer hiding layer. In [119], the BiLSTM-CNNs-CRF model is introduced. Building upon the BiLSTM-CRF model, it incorporates a convolutional neural network (CNN) [128,129] layer to extract local features of the current word. Kucza et al. [57] use the B, I, O, L, U tagging scheme to identify terminology by sequence tagging. At the same time, various Recurrent Neural Networks (RNN) and word embedding methods have been used for terminology extraction. Zheng et al. [120] integrate the BERT [130] pretrained model with the BiLSTM-CRF architecture for terminology extraction, called BERT-BILSTM-CRF. BERT is used to capture context-sensitive word embedding as the input of the next component, and then BiLSTM-CRF is employed to obtain sentence semantic features for identifying and extracting domain terminology. Tran et al. [131] investigate the performance of the XLMRoBERTa multilingual Transformer language model [132] in monolingual and cross-domain sequence tagging terminology extraction tasks. Fusco et al. [133] employ pseudo labels generated by a fully unsupervised annotator to fine-tune transformer-based models, thus creating an efficient transformerbased sequence tagging model for terminology extraction. With the advancement of embedding techniques, distributed vector representations of words trained by neural networks have become a major trend, and they have demonstrated significant effectiveness in the field of terminology extraction [134] (Table 4). Wang et al. [135] have confirmed the significant impact of word embedding in the field of KPE. Key2Vec [136] utilizes FastText [137] for the training of phrase embeddings and subsequently ranks candidate phrases employing a personalized PageRank algorithm with weighting. Drawing on PoS tagging, Bennani-Smires et al. [138] extract candidate phrases within the document. Subsequently, it computes the cosine similarity between the embeddings of these candidate phrases and document embeddings for ranking candidate phrases based on their similarity, and ultimately obtains the keyphrases. SIFrank utilizes PoS tags to extract noun phrases (NPs) as candidate phrases. It combines the",
          "",
          "[136] Key2Vec Generate phrase and document embeddings utilizing FastText, followed by the application of the PageRank algorithm to identify keyphrase candidates from the pool of candidate keyphrases. [138] EmbedRank Calculate the cosine similarity between phrases embeddings and document embeddings for ranking. [144] SIFRank Enhancing the static embeddings within EmbedRank involves the utilization of a pretrained language models (ELMo), and a sentence embedding model (SIF). [142] AttentionRank Calculate self-attention and cross-attention using pre-trained language models (PLMs). [143] MDERank By employing a masking strategy, the ranking of candidate documents is determined by assessing the similarity between the embeddings of the source document and the masked document. sentence embedding model SIF [139] and the autoregressive pre-trained language model ELMo [140] to obtain semantic vectors for both sentences and candidate phrases. Finally, candidate phrases are ranked by computing the similarity between the sentences and candidate phrase vectors. KeyBERT [141] introduces a method for extracting keyphrases using BERT. Initially, it employs BERT to extract document embeddings for obtaining document-level vector representations. Next, it extracts word/phrase vectors for N-grams, and finally identifies words/phrases that match the documents with cosine similarity. AttentionRank [142] also utilizes BERT for keyphrases extraction. They employ BERT to calculate the self-attention and cross-attention of candidate phrases and subsequently rank them based on their importance. MDERank [143] is the latest embedded-based keyword extraction algorithm. This research underscores the significance of keyphrases in the semantic content of documents, highlighting that a lack of keyphrases in a document often lead to a significant alteration in its semantics. To address the issue of mismatch between phrases and document length in SIFRank, this algorithm evaluates the importance of candidate keyphrases by comparing the similarity between the original document and a document with masked candidate keyphrases. A lower similarity indicates a higher importance of the candidate keyphrase. In the field of terminology extraction, embedding models play a crucial role in transforming textual information into continuous vector representations. Traditional embedding models typically include word embedding, phrase embedding, and sentence embedding. Word Embedding models like Word2Vec and GloVe [145] aim to map words into vector spaces, capturing semantic relationships between words. Phrase Embedding models attempt to represent phrases as vectors, as seen in Skip-Thought Vectors [146]. Sentence Embedding models focus on representations at the entire sentence level, examples of which include InferSent [147] and Universal Sentence Encoder [148]. However, these traditional embedding models are often unsupervised and struggle to adapt automatically to different tasks and domains. Additionally, they perform less effectively when dealing with complex syntactic and semantic structures due to their limited context awareness and shallow language understanding. In recent years, pre-trained embedding models such as ELMo, BERT, RoBERTa [149], and XLNET [150] have emerged and achieved tremendous success. ELMo employs bidirectional language models to produce context-sensitive embeddings for individual words. BERT has achieved state-of-the-art (SOTA) performance across multiple NLP tasks through pre-training, taking full advantage of contextual information. RoBERTa further enhances performance through large-scale data and improved training strategies. XLNET introduces auto-regressive properties, better capturing global dependencies in text. To address the issues of high memory usage and prolonged training time with BERT, Kumar et al. [151] introduce the Hierarchical Self-Attention Network (HSAN) for aspect terminology extraction. HSAN combines the importance of words within sentences with the dependencies between words in a sentence to effectively predict aspect terminologies within the sentence. Jerdhaf et al. [152] employ BERT to extract specialized terminology associated with the semantic domain indicating or suggesting the presence of implants in electronic medical records. Furthermore, Jia et al. [153] propose a bilingual terminology extraction model which is initialized with an e-commerce cross-lingual pretrained vectorized semantic representation. Given the terminology in the source language and the sentences in the target language, this model can distinguish whether the target sentence contains the translation of the source terminology and predict its position in the target sentence. Fan et al. [154] design a neural network architecture aimed at enhancing clinical and biomedical concept extraction while improving the efficacy of word embeddings. This framework utilizes CNN with multi-size filters to extract character-level information and employs cross-attention mechanism to incorporate both global and local information into word embeddings. The authors in [25] introduce a graph-enhanced sequence tagging framework that integrates graph embeddings [155] and PLM for the extraction of keyphrases from lengthy documents. The advantages of these pre-trained embedding models lie in their adaptability to multiple tasks without the need for complex feature engineering and their ability to provide deeper language understanding. Deep learning-based terminology extraction methods are generally accurate, as they automatically learn complex features from text [56,119], reducing the need for manual feature engineering. They perform well with diverse and complex terminologies and are sensitive to contextual information [156], especially when using PLMs like BERT and XLNet. These methods can capture semantic and syntactic relationships, helping to understand the meanings and usage of terminologies. However, they have some limitations. Deep learning models require significant computational resources, which can be challenging in resource-limited environments [157]. They also rely heavily on high-quality annotated data [158], including accurate context and term boundaries. Poor data can reduce model performance. Additionally, while these models are adaptable across domains, their ability to generalize may be limited, especially when textual structures vary significantly between domains.",
          "In recent years, the field of NLP has witnessed a revolutionary transformation, centered around LMs (including PLMs and LLMs). LMs such as GPT-4 [159], GPT-2 [160], LLaMA [161], Vicuna [162], ChatGLM3 [163], RoBERTa [149], have achieved remarkable success in various NLP tasks, providing researchers and practitioners with unprecedented tools and resources. As the advent of LMs is serving as a catalyst for a new phase of innovation and progress, the future prospects of terminology extraction can also be largely influenced. The introduction of LMs has not only shown great theoretical potential but has also demonstrated significant practical impact. Numerous studies [24,25,164] have confirmed the outstanding performance of LMs in terminology extraction tasks. Lange et al. [24] introduce a novel PLM, CLIN-X, for terminology extraction in the clinical domain. This model is designed for both English and Spanish. Initially, they perform pre-training of the CLIN-X language model on clinical documents using mask language modeling (MLM) [165]. Subsequently, they employ four techniques, including subwordbased concept extraction [166] with cross-sentence context [167], BIOSE labels, CRF and model transfer [168], to fine-tune the concept extraction corpus. Tran et al. [169] conduct an evaluation of various monolingual and multilingual transformer models (RoBERTa, XLNet, XLM-RoBERTa) in the domain of cross-domain terminology extraction. This evaluation is carried out using the ACTER dataset 10 (English, French and Dutch) and the RSDo5 dataset 11 (Slovenian). The experimental results demonstrate that, in terminology extraction tasks excluding NER, monolingual models outperform multilingual models in all languages except Dutch. To address the issue of resource scarcity in the e-commerce domain, Jia et al. [153] introduce a new task: discovering bilingual terminologies from comparable data. In order to tackle this task, they propose a bilingual terminology extraction framework. This framework first involves fine-tuning a cross-lingual PLM with a large-scale bilingual e-commerce corpus. Subsequently, using the initial settings of this model, they extract terminologies from target sentences based on deep semantic relationships between the source terminologies and the target sentences. The authors in [25] combine Graph Neural Network (GNN) representations with LLMs to enhance KPE in long texts. Fang et al. [170] introduce a concept model (conceptor) for concept extraction. The concept model employs a prompt-tuning [171] method to fine-tune PLMs (BERT, RoB-ERTa) to encode statements. Subsequently, MLM is utilized to rank candidate concepts and select the concept with the highest scores. Lu et al. [172] introduce a three-stage framework DS-MOCE, for distantly supervised concept extraction. In this framework, it initially employs prompt-based learning to categorize concepts in MOOCs. Then, it utilizes high-precision annotation to eliminate noise. Finally, it employs PLMs with concept distribution and positive unlabeled learning (PUL) to handle noise and incomplete mentions, resulting in the final course concepts. Yuan et al. [173] introduce KPCE, a concept extraction framework with knowledge-guided prompt. KPCE consists of a prompt constructor and a concept extractor. The prompt constructor employs topics obtained from a knowledge graph as a knowledge-guided prompt. These prompts are then used to train a BERT-based extractor, responsible for extracting concepts of various granularities from input texts. With the rapid advancement of LLMs, ChatGPT has emerged. Upon its introduction, ChatGPT demonstrates excellent performance in the field of NLP. Researchers have proposed a series of methods utilizing ChatGPT to assist in terminology extraction. In [174], the NER task is defined as a zero-shot or few-shot text generation task, and a comparison is made between ChatGPT method with prompt engineering and traditional fine-tuning methods in the context of rare disease NER. Overall, traditional fine-tuning models outperform ChatGPT in terminologies of performance, but in a one-shot setting, ChatGPT achieves higher accuracy. Romano et al. [164] introduce a data collection and curation framework for theme-driven KPE. This framework extracts clinically relevant keyphrases from user-generated health texts, and generates the MOUD-Keyphrase dataset. The study also quantitatively analyzes unsupervised KPE models and ChatGPT on this dataset. The findings indicate that ChatGPT demonstrates superior performance compared to unsupervised KPE models. To address the issue of data scarcity in terminology extraction, Veyseh et al. [160] propose a multi-step terminology extraction training method. Initially, they obtain optimal representations and sentences  for training data to augment the input data. Subsequently, fine-tuning is performed on the generative language model GPT-2 [175] to generate additional sentences. Song et al. [176] assess ChatGPT's ability in keyphrase generation on six benchmark datasets. The research results indicate that ChatGPT has significant potential in keyphrase generation tasks but still faces challenges in generating missing keyphrases. Building upon Song's research, the authors in [177] further assess ChatGPT's performance. In Martínez-Cruz's research, the keyphrase generation performance of ChatGPT is compared with SOTA models on six publicly available datasets from scientific articles and news domains. The results indicate that ChatGPT achieved SOTA performance across all test datasets and settings without requiring additional training or fine-tuning, solely through promptbased learning. Recently, the success of ChatGPT has spurred more researchers to explore domain-adaptive LLMs. Current research indicates that with better prompts, LLMs demonstrate exceptional terminology extraction capabilities. However, these LLMs require a substantial amount of data for pre-training and fine-tuning. The quality and quantity of the data directly impact the effectiveness of terminology extraction. Additionally, they demand significant computational resources and time. Some researchers have also investigated ChatGPT's performance in terminology extraction tasks, demonstrating its significant potential in this domain without the need for additional training and fine-tuning. Nevertheless, there is still some gap when compared to traditional PLMs.",
          "In recent years, significant progress has been made in multilingual terminology extraction methods, particularly in improving the accuracy and efficiency of cross-domain ATE. [178] proposed the TExSIS system, which generates term candidates using chunkbased alignment and combines multiple statistical filters to determine term specificity, adopting a multilingual perspective for terminology extraction, and outperforming traditional monolingual re-alignment methods. [41] introduced a unified framework that independently aligns bilingual terminologies for single-word and multi-word terminologies by improving context-based and neural network approaches, with the system adaptable to domain-specific terminology extraction. [179] introduced the TermEnsembler system, which integrates seven bilingual alignment methods using ensemble learning to automatically extract and align terminologies from English and Slovenian texts, achieving an accuracy of over 96%. [180] proposed a bilingual terminology recognition system, LUIZ, based on an \"equivalence bag\" method, which combines morphological and syntactic patterns with statistical ranking to identify cross-lingual multi-word terminologies in domains such as tourism, accounting, and military, using translation equivalent pairs. [181] introduced three Transformer-based multilingual terminology extraction methods, which enhance the performance of cross-domain ATE for English, French, and Dutch by utilizing sentence-level token classification, sequence classification, and neural machine translation models. [182] proposed using cross-lingual and cross-domain transfer learning with fine-tuned BERT models for ATE, showing its effectiveness in extracting single and multi-word terminologies across multiple languages and specialized domains. [183] presented a tool based on existing lexicons and bilingual parallel corpora, utilizing varying match functions to extract and align English-Serbian power engineering domain terminologies, generating bilingual terminology pairs. [131] treated terminology extraction as a sequence labeling task, using the Transformer-based XLM-RoBERTa model to evaluate the performance of multilingual PLMs in cross-domain tasks, achieving a significant improvement in F1 score on the Slovenian dataset. [184] proposed an unsupervised multilingual keyword extraction algorithm that selected keywords based on high TF-IDF ranks in both the target and other languages, improving accuracy by leveraging multilingual information. [185] proposed a multilingual terminology extraction method that improves cross-domain performance through cross-lingual transfer learning and a nested terminology labeling mechanism (NOBI), significantly boosting recall and F1 scores, especially in the extraction of short nested terminologies. Overall, these advancements demonstrate the growing potential of multilingual models in extracting and aligning terminologies across diverse languages and domains. The continuous improvement in both accuracy and efficiency underscores the increasing applicability of these methods in real-world, cross-lingual contexts.",
          "In the field of terminology extraction, the selection of datasets, tools, and evaluation metrics is of paramount importance. These elements form the foundation of research methodology and outcomes, providing a means to assess the efficacy of terminology extraction algorithms and driving continuous progress and innovation in the field. In this section, we delve into the datasets, tools and evaluation metrics related to terminology extraction, aiming to assist researchers in better understanding, selecting and utilizing them to advance the research in terminology extraction.",
          "This section introduces some commonly used terminology extraction datasets, which provide researchers with rich experimental materials, promoting the development and innovation of terminology extraction techniques. The selection and design of these datasets aim to cover the diversity of different domains and languages to meet various research requirements. In the following section, we present several representative terminology extraction datasets, summarizing their domains, languages, data sources, and data characteristics, as shown in Table 5.",
          "This subsection briefly introduces and compare the software and tools related to terminology extraction and Table 6 summarizes these terminology extraction tools. • TermoUDfoot_1  [211] is a language-independent terminology extraction tool that utilizes language-dependent shallow syntactic patterns to select candidate terminologies. This tool is applicable to languages with a Universal Dependencies (UD) parser. • TermoPL 12 [212,213] is a terminology extraction tool from domain corpora in Polish. It applies C-value to rank candidate terminologies as the longest identified nominal phrases or their nested sub-phrases. It can also compare two candidate terminology lists using three different coefficients showing asymmetry of terminology occurrences in this data. • D -Terminer 13 [214] is an open-access online demonstration for ATE from parallel corpora for both monolingual and multilingual purposes. Monolingual terminology extraction is based on recursive neural networks and employs a supervised approach with dependency on pre-trained embeddings. Table 5 Datasets Datasets Studies Language Domains Docs Words(K) Terms GENIA [186] English Biomedicine 193 435 93,293 FAO [187] English Agriculture 780 26,672 1554 Krapivin [188] English Informology 2304 21,189 8766 ACL [48] English Computer Science 10,922 41,202 21,543 ACL2.0 [189] English Computer Science 300 33 6818 TTC [190] Multilingual Wind Energy 103 801 287 Mobile Technology 37 305 254 Europarl [191] Multilingual Eurovoc Thesaurus 9672 63,279 15,094 SemEval [192] English Computer Science 244 2033 4002 Theses100 -English Miscellaneous 100 472 767 Wiki20 [193] English Miscellaneous 20 122 730 ACTER [194] Multilingual Corruption 44 469 6385 Dressage 89 103 10,889 Heart failure 190 46 14,011 Wind Energy 38 315 9478 CRAFT [195] English Biomedicine 67 560 10,000 Hindi Wiki [196] Hindi Education 71 12 953 Irish Wiki [197] Irish Education 11 5 864 Mathematical [198] Chinese Math 3000 31 14,500 WellXplain [199] English Math 3092 72 14,500 Hospital-EMRs [200] Swedish Medicine 48,000 71,220 48,088 JSynCC [19] German Medicine 1006 368 2583 RSDO5 [201] Slovene Miscellaneous 12 257 37,985 LDKP3K [202] English Scientific 100,000 602,710 76,110 KP20k [203] English Scientific 568,000 106,863 2,780,316 NUS [204] English Scientific 211 1605 2834 DUC-2001 [95] English News 308 227 6160 Inspec [25] English Scientific 2000 27 19,275 OpenKP [205] English Scientific 147,200 132,538 264,906 COBEC [206] English DM 20 310 477 DB 27 296 395 OS 1 4 42 IIR [207] English computer science 16 -3175 ScienceIE [208] English Computer Science, Physics, Material Science 500 -5730 BitterCorpus [209] Multilingual Information Technology 56 -874 KAS-biterm [210] Multilingual Academic writing 816 618 15,929 13 https:// lt3. ugent. be/ dterm iner/. • TEST [215] is used to automatically detect the existence of new technologies and tools in texts, and extract terminologies used to describe these new technologies. • YAKE! 14 [86] is a lightweight, multilingual, unsupervised automatic KPE method. It selects the most important keyphrases in a text based on textual statistical features extracted from a single document. • KeyBERT 15 [141] presents a toolkit designed for KPE with BERT, aligning with the paradigm of Phrase-Document (PD) based methods. • ATR 4S 16  [54], implemented in Scala, is an open-source terminology recognition toolkit. It incorporates fifteen terminology extraction algorithms, including CValue, Basic, Com-boBasic, and more. • JATE2.0 17 [216] is a Java ATE toolkit developed within the framework of Apache Solr, and an upgraded version of JATE [84]. At the same time, JATE2.0 is an open-source, highly modular, adaptable and extensible ATE library, containing dozens of implemented ATE algorithms. • TermSuite 18 [9] is an ATE tool based on Java and UIMA framework, which is extensible and capable of managing variations in terminology. This tool mainly uses the Weirdness method to sort candidate terminologies, focusing on syntax and morphological pattern to identify terminology variants.  [220] is a freely available terminology extraction tool that combines linguistic and statistical methods. It takes into account the structure of potential terminologies and their relative frequency of occurrence. • TerMine 24 [50] is an ATE tool based on the C-value, which uses linguistic and statistical information. Its primary disadvantage is that it can only extract multi-word terminology, but many critical single-word terminologies are in the domain. ",
          "Require annotated data. and R(i) respectively represent the Precision(p) and Recall(R) for the extracted i-th terminology. 21 http:// www. cs. cf. ac. uk/ flexi term. 22 https:// gate. ac. uk/ proje cts/ neon. 23 http:// termo stat. ling. umont real. ca. 24 http:// www. nactem. ac. uk/ softw are/ termi ne/. 19 https:// sourc eforge. net/ proje cts/ tbxto ols/. 20 http:// tubo. lirmm. fr/ biotex/. • TOPIA 25 is an ATE package based on python. It offers a terminology extraction approach combining PoS tagging with simple statistical metrics, such as frequency.",
          "Terminology extraction is a fundamental task in NLP and information retrieval, aiming to automatically identify domain-specific terminologies from text. This chapter focuses on three aspects of terminology extraction evaluation: evaluation methods, evaluation metrics, and other key factors. The evaluation methods are further categorized into manual evaluation and automatic evaluation. Evaluation metrics are divided into traditional metrics, and derived metrics from information retrieval. Other key factors include frameworks for evaluating software quality, considering characteristics like functionality, usability, reliability, and efficiency, providing a comprehensive perspective on terminology extraction evaluation. Manual evaluation is a traditional approach in terminology extraction, relying on domain experts to compare extracted terminologies with predefined references and assess system performance. It also allows for analysis of terminology importance, diversity, and relevance. However, due to high labor costs and potential bias, it is less suitable for large-scale assessments. In terminology extraction evaluation, automatic evaluation methods based on standard terminology tables compare the extraction results with predefined terminology sets (such as terminology dictionaries or gold standards) [221] to quickly calculate metrics like precision (P) [180] and recall (R) [222]. This approach relies on automated tools, making it suitable for large-scale task evaluation, particularly in domains where terminology sets are relatively stable. The dictionary-based method directly matches the extracted terminologies with the terminology list in the standard dictionary [178]. While simple to implement, it has limited adaptability to terminology variations. In contrast, the gold standard-based method [223,224] uses diversified terminology lists constructed by domain experts as references, enabling the capture of different expressions of terminologies and providing more fine-grained evaluation results. Although automatic evaluation based on standard terminology tables offers significant advantages in efficiency, its accuracy depends on the quality and coverage of the dictionary or gold standard, which may pose challenges in dynamically evolving domains. To measure the performance of terminology extraction systems quantitatively, various evaluation metrics are employed. Traditional metrics such as precision (P) and recall (R) are foundational in assessing the correctness of extracted terminologies. Precision (P) measures the proportion of correctly identified terminologies among all extracted terminologies, while recall (R) evaluates the proportion of correct terminologies successfully extracted from the total relevant set. The F1 score (F1), as the harmonic mean of precision and recall, provides a balanced measure of system performance. Additionally, derived metrics from information retrieval, such as Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Average Precision (AvP), offer a more nuanced assessment of ranking and retrieval quality. Mean Average Precision (MAP) [225,226] 25 https:// pypi. python. org/ pypi/ topia. terme xtract. evaluates the overall ranking quality of the extraction results by calculating the average precision over multiple queries. This metric is valuable for assessing how well the system ranks relevant terminologies among all extracted terminologies. Mean Reciprocal Rank (MRR) [96,97,227] measures the rank of the first relevant terminology in the extraction results, providing a clear indicator of how quickly the model retrieves relevant information. Average Precision (AvP) [51,82,228] computes the precision for each relevant terminology in the extraction and averages the values, offering an overall measure of the system's effectiveness in ranking and retrieving relevant terminologies from the set of extracted terminologies. Descriptions and formulas for these metrics are shown in Table 7. Existing research has proposed several key factors beyond simple performance metrics, constructing a more comprehensive framework for evaluating terminology extraction software. [229] introduced a standardized method for evaluating terminology extraction software, focusing on four characteristics: functionality, usability, reliability, and efficiency, which are further subdivided into seven sub-characteristics, such as accuracy and learnability, providing users with quantifiable software quality references. [230] designed a framework focusing on external quality, selecting three characteristicsfunctionality, usability, and efficiency-and subdividing them into four sub-characteristics, such as precision and operability. Through empirical research, they compared the quality characteristics of multiple tools, offering new insights into the evaluation framework's practical application and result interpretation.",
          "",
          "One of the major challenges currently faced in the field of terminology extraction is how to handle polysemy, poor domain adaptability, and low computational efficiency [231]. Traditional terminology extraction methods often struggle with polysemy, failing to distinguish between different meanings of the same term in various contexts. In addition, existing models typically perform well only within specific domains, with clear limitations when it comes to extracting terminology across domains, lacking sufficient generalization capability. On the other hand, while deep learning methods have improved the accuracy of terminology recognition to some extent, they are computationally inefficient due to the large amount of computational resources required for training and inference, especially during pre-training and fine-tuning [25,130,131,[232][233][234][235]. Therefore, how to improve the computational efficiency of terminology extraction methods while addressing polysemy and enhancing domain adaptability is an urgent challenge that remains to be solved.",
          "Current terminology extraction methods often rely heavily on large amounts of labeled data for training, especially in deep learning applications where data quality and quantity directly impact performance [236,237]. This reliance on labeled data creates challenges in domains where obtaining large datasets is difficult due to privacy concerns, complexity, or high labeling costs. In many cases, labeled data is scarce, limiting the effectiveness and applicability of these methods. Additionally, existing models lack flexibility to adapt to new domains or tasks without substantial retraining, leading to poor performance when labeled datasets are limited. Furthermore, these models often require extensive manual labeling and domain expert input, which is time-consuming and resourceintensive. Reducing the dependence on labeled data and improving the adaptability and generalization of terminology extraction methods are essential for their advancement, especially in low-resource and cross-domain applications.",
          "Despite advancements in terminology extraction technology, the lack of unified and effective validation standards remains a critical issue. Current evaluation methods are often focused on specific datasets or metrics tailored to particular domains, limiting their generalizability across different contexts. This lack of cross-domain and crosstask evaluation hinders the comparability of research results and the development of universally applicable methods. Deep learning-based models, which are increasingly used for terminology extraction, are complex, and traditional performance metrics like precision, recall, or F1 score fail to fully capture their capabilities in realworld applications [198,238]. These metrics often overlook factors like contextual understanding, domain-specific nuances, and the model's ability to handle ambiguity [239]. Moreover, existing validation methods don't account for the adaptability and robustness of models in dynamic, real-world environments where data quality and domain shifts are constant challenges. As a result, validation processes often don't reflect the true performance of models in diverse, practical scenarios. Developing comprehensive and effective validation methods to evaluate terminology extraction models across various application scenarios, while considering their adaptability and real-world effectiveness, remains a key challenge.",
          "",
          "Large language models (LLMs) are rapidly advancing in the field of NLP, and many studies have explored their application in terminology extraction tasks [240,241]. These models undergo extensive pre-training on large-scale text data, endowing them with a strong capability to capture rich linguistic and contextual information. This enhancement significantly improves their potential for understanding context and offers promise in addressing the challenges of polysemy, ambiguity, and contextual understanding that traditional terminology extraction methods face. Terminology extraction methods based on LLMs also have the potential to adapt to diverse domain-specific and cross-domain requirements [169,185]. These models have been trained across multiple domains and languages, creating new opportunities for cross-domain terminology extraction and a better response to domain-specific challenges. However, it is essential to note that, despite the promising prospects of terminology extraction based on LLMs, the pre-training and fine-tuning processes require substantial computational resources [242]. Researchers and practitioners should give careful consideration to this aspect to ensure effective resource management. This matter is anticipated to be among the challenges requiring attention in future research endeavors.",
          "The rise of LLMs has sparked widespread interest in the field of prompt engineering [243]. Prompt engineering is a method that leverages natural language prompts to guide models in performing specific tasks. In the domain of terminology extraction, promptbased techniques have become increasingly prominent [244]. This approach involves cleverly designed natural language prompts that enable models to more accurately extract domain-specific terminologies and phrases without requiring significant updates to the model parameters. It fully leverages the potential of LLMs (such as GPT-4, ChatGLM3, etc.) in understanding context. By embedding prompts into the model, it allows the model to gain a deeper understanding of the contextual background of terminologies, leading to more precise terminology recognition and extraction. Furthermore, prompt-based methods offer models the opportunity for pre-training on large-scale unlabeled text. By defining new prompt functions, this approach enables models to engage in few-shot or even zero-shot learning [245], allowing adaptation to new scenarios, particularly in instances where labeled data is limited or unavailable. What is particularly fascinating is that prompt-based methods exhibit exceptional flexibility, catering to various domain-specific and crossdomain needs. Researchers and practitioners can easily customize models to meet specific task requirements by simply adjusting or modifying prompts, making them more versatile and practical. This trend demonstrates the synergy between LLMs and natural language prompts, unlocking significant potential in the terminology extraction field, thereby enhancing the accuracy and applicability of information extraction.",
          "A key direction for future terminology extraction research is improving the evaluation system. Current methods primarily focus on standard metrics like precision and recall, which often don't fully reflect the practical effectiveness of these methods. Future evaluation systems should prioritize multi-dimensional assessments, including scalability, adaptability, domain independence, and language neutrality [246]. Given the performance differences across domains, languages, and datasets, a major challenge will be designing a more universal and easily applicable evaluation framework. Additionally, combining quantitative and qualitative evaluations is crucial. This involves not only automated assessments but also manual review and expert feedback to validate extraction quality. A comprehensive evaluation system will provide more accurate standards for assessing terminology extraction methods, driving further advancements in the field.",
          "Our review provides a detailed exposition of various aspects of the field, including relevant concepts, the interplay of issues, generic system architectures, terminology extraction methods, related resources and metrics, and prospective research topics. We delve into the evolution of different approaches to terminology extraction, encompassing traditional linguistics-based and statistically-based methods, as well as more recent developments such as graph-based algorithms, machine learning, deep learning, and LMs. These methods represent different research directions within the terminology extraction domain. Traditional linguistics-based and statistically-based methods, such as PoS tagging, KPE, and TF analysis, have laid a robust foundation for terminology extraction. However, they exhibit certain limitations in handling polysemy, domain adaption, and semantic relatedness. Machine learning-based approaches rely on feature engineering and classical supervised learning algorithms. They have demonstrated favorable results in terminology extraction tasks, albeit typically requiring substantial manual effort and domain knowledge. Deep learning methods, on the other hand, leverage end-to-end training of neural network models to automatically learn higher-level feature representations but often consume large-scale annotated data. Embedding techniques map text into lower-dimensional vector spaces, encoding terminology semantically and facilitating the capture of semantic relationships among terminologies. Additionally, graphbased methods construct term-concept networks, offering enhanced representations of intricate relationships among terminologies. However, one of the most remarkable developments is the rise of LMs, particularly massive LMs. Models like BERT and GPT-4 have achieved tremendous success in the field of NLP and have profoundly impacted terminology extraction. These models undergo self-training on vast amounts of textual data, endowing them with formidable language comprehension capabilities, enabling them to better understand the context and meanings of terminologies. Furthermore, their powerful representation enhances the accuracy of terminology extraction, especially when dealing with polysemy and domain-specific terminologies, as they excel in capturing linguistic nuances. In the future, as LLMs evolving and adapting to different domains, we can anticipate further improvements in terminology extraction methods. Additionally, researchers can explore how to better integrate various approaches to address the challenges in terminology extraction. Recently, many articles [247][248][249] have proposed methods that combine LLMs with knowledge graphs for knowledge extraction. The integration of LLMs with knowledge graphs can become a new trend in addressing terminology extraction tasks. LLMs can play a pivotal role in future terminology extraction research, providing us with more advanced solutions for terminology extraction, and advancing the fields of information retrieval and knowledge management. NER Named entity recognition NLP Natural language processing NPs Noun phrases PD Phrase-document PMI Pointwise mutual information PLMs Pre-trained language models PoS Part-of-Speech P Precision R Recall RIDF ResidualIDF RIPPER Repeated incremental pruning to produce error reduction RNN Recurrent neural networks SOTA State-of-the-art SVM Support vector machine TF Term frequency TF-IDF Term frequency-inverse document frequency"
        ],
        "ground_truth_definitions": {
          "Lemmatization": {
            "definition": "Transforming various forms of vocabulary into their base forms, which are lemmas or stems",
            "context": "Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems. • PoS tagging: Assigning a PoS tag to each word or token in the texts to indicate the word’s grammatical role and lexical category within the sentence. This task is not mandatory in preprocessing.",
            "type": "implicit"
          },
          "Tokenization": {
            "definition": "Segmenting into the smallest meaningful units, such as words or phrases",
            "context": "• Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases. • Lemmatization: Transforming various forms of vocabulary into their base forms, which are lemmas or stems.",
            "type": "implicit"
          },
          "terminology": {
            "definition": "a concept reference system within a specialized field",
            "context": "For example, the authors in propose to separate the concept of the terminology from the common word, marking the beginning of terminology research. Wüster defines terminology as “a concept reference system within a specialized field”. Based on Wüster’s terminology theory, Wright et al. argue that terminology should be organized by subject and examined for its logical, holistic, part-whole, and associative relationships.",
            "type": "explicit"
          },
          "Cleaning": {
            "definition": "Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks",
            "context": "During the preprocessing stage, the most common tasks include: • Cleaning: Removing irrelevant content from the texts, such as stopwords, special characters, HTML tags, numbers, and punctuation marks. • Tokenization: Segmenting into the smallest meaningful units, such as words or phrases.",
            "type": "implicit"
          },
          "domain": {
            "definition": "a realm of professional knowledge",
            "context": "These descriptions provide some concepts about the domain but do not explicitly define the boundaries of the domain. ISO delineates a domain as a realm of professional knowledge, and the delineation of its boundaries is established with a focus on purpose-related considerations. ISO provides a definition of a domain as a specialized area of professional knowledge.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "c84a169e6df175c4662012d3ba7dbf8fa1b5abc9",
        "sections": [
          "In the past couple of years, the debate concerning so-called 'fake news' has prominently surfaced in news and politicians' speeches around the world. This catchy but fuzzy tag (European Commission, 2018) has mainly been used to describe misleading content disseminating across social media, but it has also been invoked by political actors to discredit some news organizations' critical reporting (Caplan et al., 2018;Tandoc et al., 2018). Alarmed by the simplification of this buzzword, researchers have started to unpack the concept, defining diverse types of 'fake news' (Allcott and Gentzkow, 2017;Jack, 2017;Marwick and Lewis, 2017;Warde, 2017) in order to understand the implications and solutions. In common with classic studies on misleading information dealing with rumours, propaganda and conspiracy theories (Allport and Postman, 1946;Lasswell, 1927;Sunstein and Vermeule, 2009), these most recent attempts to define 'fake news' seek to differentiate between specific forms of the phenomenon with reference to the source's intent to deceive (disinformation) versus the honest mistakes, negligence, or unconscious biases (misinformation) (Fallis, 2015;Floridi, 2011). In other words, the act of creating and injecting 'fake news' in the system is a defining moment for most classic and contemporary studies. In this article, we suggest a radical change of perspective. Such change is driven by deep transformations characterizing contemporary news systems, wherein older and newer media actors -with different degrees of potential reach, epistemological authority and skills of media manipulation -operate on the basis of overlapping and competing media logics. In this 'hybrid news system' (Chadwick, 2013), judgements with regard to the falsehood and motivations of propagators (the actors who share the fake news) can easily be different from the motivations of the original creator. Such patterns mean that what happens after the 'generative act' of a piece of false news is crucial to the study of real-world cases. The radical change of perspective required by the hybrid news system thus consists of a shift from exclusive attention to producers of 'fake news' to a broader approach that also focuses on propagators and, as a result, on the dynamic and diverse processes that characterize the dissemination of problematic information through multiple chains of propagation. On a theoretical level, our proposal is informed by the conceptual framework of sociocybernetics with specific reference to 'second-order cybernetics'. According to this approach -originally developed by Heinz von Foerster, whose aphorism 'Truth: The invention of a liar' has inspired the title of this article -information is not something that can be transmitted, stored, or retrieved in itself; it only exists 'when looked upon by a human mind' (von Foerster, 2003: 103). Following what von Foerster said about truth, the title of this article retains and twists the double meaning of its original inspiration insofar as it not only indicates simply and obviously that a piece of false information is invented by a liar. It also intrinsically turns its pronouncer into a liar, i.e. someone who indirectly claims to speak the truth by asserting that something is false. In this sense, the term 'fake news' is inherently divisive and detrimental to healthy debates. Information is an eminently social process: it is 'a difference which makes a difference' for an observer (Bateson, 1972: 321). What is informative for one observer can be uninformative for another observer. Interests, backgrounds, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. We contend that, in the hybrid media system, each 'fake news' cycle can only be studied as a unique process that emerges from multiple combinations of judgements on the falsehood of the news. Such judgements are made by all the diverse actors who decide to inject and share it, following logics that are related to their perception of the source, of the story itself and of the context. In order to describe our analytical model of problematic information, in the next section we trace back the notion of 'fake news' to classic and contemporary studies on disinformation and misinformation, highlighting what we consider the weaknesses of these analytical definitions: that is, their sole focus on the initial step of the process -when someone introduces false information into the system. We then briefly describe the literature that has informed our definition of the contemporary media context in terms of the hybrid news system and the radical change in perspective that it requires: that is, analytical attention to both the generative acts of 'fake news' and to what happens in the processes of circulation. We thereafter consider this innovative perspective's roots and its focus on the observer's judgements regarding informativeness. Next, we discuss the profound consequences of this theoretical gaze on a micro, meso and macro level analysis of real-world 'fake news' news stories.",
          "Even if the notion of 'fake news' has been popularized only recently, the dangerous implications of unverified, inaccurate, defective and false information have been extensively studied by a diverse set of academic literature since the early 20th century. Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989(Habermas, [1962]]; Jack, 2017). There is, however, a lack of agreement regarding the diverse types of misleading information that fall under these two concepts of disinformation and misinformation, for example, satire, parody, and even bullshit. In the cases of satire and parody, in particular, the source intentionally produces misleading content, yet the intent is not to mislead, and the source does not benefit from being misleading (Fallis, 2015). Nevertheless recent studies have included (news) satire (content that exaggerates facts to convey a critique) and (news) parody (non-factual information produced to inject humour) among the forms of problematic information that can fool both large publics and influence intermediaries of information (Jack, 2017;Warde, 2017). Another type of misleading information that evades the classic distinction between disinformation and misinformation is what Frankfurt and Bischoff (2005) define as 'bullshit', wherein the source does not care if what he/she says is true or false but only aims to persuade or to provoke a reaction. Among the various types of problematic information, rumour has received the most systematic academic attention (Allport and Postman, 1946;Rojecki and Meraz, 2016). Rumours are forms of information that are characterized by uncertain veracity and that can be later proven false. The credibility of rumours is unrelated to direct evidence but to the fact that other people seem to believe them. Rumours are often at the origin of conspiracy theories, which are explanations for events through the causal agency of a group of individuals acting in secret (Keeley, 1999). If we had to describe conspiracy theories using the distinction between disinformation and misinformation, one could say that conspiracy theories are a form of disinformation since these theories can often easily be proven false, and it is no accident that the information is misleading (Fallis, 2015). At the same time, conspiracy theories could be identified as misinformation since some supporters of these theories are entirely sincere and believe them to be true (Sunstein and Vermeule, 2009). Another type of problematic information that has received considerable attention from researchers is propaganda. Studies on propaganda can be traced back to Lasswell (1927), who explained how manipulated significant symbols can manage collective attitudes, promoting a particular political side or perspective. The study of propaganda is obtaining renewed interest due to the increasing skills of media manipulation that have enriched online propaganda tactics, with the development of bots (Albright, 2016), which give the impression that many people are liking a piece of misleading news. Another form of disinformation that is garnering renewed academic interest is deceptive advertising, particularly advertising materials that simulate the formats of legacy news media to confer more legitimacy to a one-sided article or website, with the aim of obtaining financial gain or a positive public perception (Allcott and Gentzkow, 2017;Tandoc et al., 2018;Warde, 2017). Other types of problematic information related to new digital techniques are doctored and manipulated photos/videos (Farid, 2009), which may have no factual basis or may be factual but misappropriated. Contemporary media cultures also challenge the definition of the motivations that drive problematic information in contemporary media environments: internet trolls (Marwick and Lewis, 2017;Phillips, 2015), for example, may deliberately create misleading content for fun, with the aim of provoking outrage or fooling their audience. While helpful, most of these concepts and typologies of misleading information focus on the act of creation as the defining moment, looking at the author's judgement with regard to the falsehood and distinguishing between his/her intentions (to deceive, convey a critique, inject humour, persuade, reduce uncertainty, give an impression of events, make money, promote a particular political perspective/product, have fun). We argue that this focus represents a limit for the study of problematic information in the contemporary media environment, in which the diffusion of news depends on the multiple actors involved in the process of newsmaking and news sharing.",
          "Information cycles of the 21st century are characterized by a multitude of highly interdependent media technologies, in which numerous media actors follow newer and older, overlapping and competing media logics, producing news and other products with a news format (Chadwick, 2009(Chadwick, , 2013)). According to this vision, multiple and integrated platforms have rendered the process of newsmaking more inclusive, blurring the last century's boundaries between professional journalism and user-generated content as well as affecting the norms and values that guide journalists' practices in the (mixed) genres of journalistic information and the organization of newsrooms (Hermida and Thurman, 2008;Tandoc, 2014). In the hybrid media system, a plethora of actors -such as bots (Albright, 2016)participate in the production and dissemination of both journalists' ratified news and a wide range of other products that take on a news format. As boyd (2017) wrote, contemporary publics are increasing able to 'hack the attention economy'. These multiple actors, all involved in the hybrid processes of newsmaking and news sharing, require a radical shift in the study of misleading information, from exclusive focus on the first step of the process (the creation of news) to also considering what happens after this generative act. Judgements regarding the falsehood of news and motivations to share it can easily be different from those of the authors. We trace this shift of perspective back to second-order cybernetics.",
          "As anticipated by the review of the literature, most of the existing works emphasize that judgements on the level of facticity of online content are strongly interlinked with assessments regarding the intentions of the content creator. Understanding the intent of the creator of 'fake news' is thus considered a crucial indicator for differentiating among different types of problematic information. Nevertheless, several recent works recognize the difficulties in clearly assessing such intent (Jack, 2017). Even employing the state-of-the-art techniques of digital investigative journalism, the cues left by these actors are often inadequate for clearly differentiating between honest mistakes, deliberate deception and satire. This phenomenon is often referred to as 'Poe's Law' (Aikin, 2013). Furthermore, certain actors (e.g. trolls) deliberately sow confusion about their real intentions by framing as satire their false and/or outrageous content (Marwick and Lewis, 2017). No matter how difficult this assessment, billions of actors of the hybrid media news system are called upon every day, often multiple times a day, to quickly make such judgements on the online content to which they are exposed. Many experts both inside and outside academia are working to make this judgement process easier and less prone to error (Caplan et al., 2018). While recognizing the importance of these efforts, we take a different perspective, a perspective that includes at its core the possibility that misjudgements occur: a second-order perspective. By second-order perspective, we specifically refer to the work carried out by an interdisciplinary group of scientists (Norbert Wiener, Claude Shannon and Warren McCulloch, to name just a few) in the United States following the Second World War and developed under the programme title of 'second-order cybernetics' by a team led by Heinz von Foerster, an Austrian-American scientist combining physics and epistemology, at the Biological Computer Laboratory of the University of Illinois at Urbana-Champaign (Heims, 1991). Second-order cybernetics postulates that a system comes 'into being' when an observer acts to 'draw its boundaries ' (von Foerster, 2003). Once applied to media and communication, this approach suggests shifting attention from the source (the content creator) to the observer of the source (those who are exposed to this content). According to von Foerster (2003), a source in itself is rarely important unless someone pays attention to it. In a certain sense, sources only 'come into being' when an observer recognizes them as sources. According to Gregory Bateson (1972), an anthropologist who met von Foerster and Shannon multiple times during the Macy conferences (Heims, 1991), information is in fact 'a difference that makes a difference'. Bateson's definition of information underlines the crucial role played by the expectations of the receiver/observer. From this perspective, information is not something that can be transmitted, stored, or retrieved; it exists only 'when looked upon by a human mind' (von Foerster, 2003). What is informative for one observer may be uninformative for another observer. The observer's interests, background, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. While the intent of the content creator (first-order perspective) is often difficult to assess, the observers exposed to certain content (second-order perspective) formulate their best guess regarding both facticity and creator's intent. These judgements affecton different scales, depending on the observer's position in the system and number of connections to the actors -the entire process. On this basis, the observer defines the subsequent course of action (e.g. decides to further share a content). By extension, a researcher analysing the way an information cycle unravels from an external perspective is a third-order observer attempts to assess the intentions and goals of both first-order and second-order observers: they try to guess the intention and goals of the initiators of an information cycle, as well as the motivations of those who take part in it. At the same time, researchers also define a case of online spread of information as 'fake news', which thus represents the actual starting point of the analytical process. Given the multiple levels of observations at stake, our perspective is constructivist but not relativistic. All observers, including a researcher potentially studying the process, tend to judge the truthfulness of content from their unavoidably limited perspectives. Different actors are differently equipped to support this judgement process. Nevertheless, even professional subjects and organizations sometimes misjudge. These perspectives are different but not equal. For this reason, we introduce the distinction between 'true'/'false' (with lower case initial letters) according to the perspective of the actors involved in the process and 'True'/'False' (with capitalized initial letters) according to an external perspective, such as that of the researcher using the model. By the same token, a propagator is an actor of the hybrid media system (e.g. a journalist, news organization, citizen, politician, troll, fake account, bot) who is exposed to False information (created/shared by a creator or previous propagator) and, for numerous possible reasons, decides to share it further (thereby creating propagation). It may be the case that the propagator is exercising bad judgement (believing to be true what is in fact False), or it may be the case that she/he correctly judged the information as false and deliberately decides to share it to deceive others. In other terms, echoing Bateson's extension of Shannon's information theory (Shannon and Weaver, 1949), we suggest adopting a second-order perspective to shift attention from the source -as an entity with its own static properties (motivations, biases, drivers) -to the observer who consumes content originally published by the source and makes a judgement about its properties. We argue that, however right or wrong this judgement may be, the descriptions made by these observers shape the process of misleading information as a complex phenomenon emerging from the interplay of a multitude of actors in the hybrid news system. From this perspective, a False information cycle is rarely reducible to a single typology. A news article clearly fabricated with the aim of exploiting the logic of online advertising (and thus produced as disinformation) may become a must-read article within a community of like-minded believers of certain conspiracy theories and shared as a legitimate piece of news (misinformation). The implications of this radical change of perspective are presented and discussed in the next sections dedicated to the micro, meso and macro levels of our analytical model. By employing such terminology and analytical approach we address the call to precision that Turner (2006) has recommended to sociologists by inviting them to develop theories aimed at explaining social phenomena by paying attention to relationships between forces operating at different levels of the social universe.",
          "Multiple cycles of information constantly populate hybrid news systems. They result from the activity of various actors who produce and share information with small and large audiences that might or might not decide to actively contribute to their further circulation. Each of these cycles is grounded upon cascades of judgements (whether or not to believe a piece of information) and decisions (whether or not to share a piece of content). These actions are taken, with very different levels of sophistication, by diverse individuals, groups and organizations. Despite being guided by different logics, ethical concerns and skills, all the different actors populating these systems, we argue, ground their judgements and their decisions upon similar principles. In the following sections, we discuss the dissemination of False information within the hybrid media system by focusing on three different levels: a micro level consisting of the bases on which subjects judge the truthfulness of the information with which they engage; a meso level consisting of the matrix of expectations between actors and content at each propagation of False information; and a macro level consisting of the broad and heterogeneous process emerging from a chain of False news propagation. While these three levels of the analysis are presented, for sake of clarity, as a bottom-up structure, it is worth underlining that both the meso and the macro level are emergent phenomena.",
          "First, we focus on how single actors process information. False information is potentially an extremely 'informative' (and thereby effective) form of information (Karlova and Fisher, 2013), as unexpected novelty is a defining characteristic of information (Shannon and Weaver, 1949). Focusing on the 'informativeness' of False information allows us to employ journalism studies and literature on information sharing within digital environments to also discuss how the multiple actors of the hybrid media system make judgements and take decisions when exposed to False information. Starting from the classification proposed by Wathen and Burkell (2002) for the internet 1.0, we argue that all individuals and institutions inhabiting hybrid news systems evaluate the truthfulness of the information they manage on the basis of specific patterns regarding: (a) the source, (b) the story and (c) the context. According to the literature on digital information ecologies, two elements influence individuals in their assessments of a piece of information's truthfulness on the basis of its sources: authority (Clark and Slotta, 2000;Rieh, 2002) and proximity (Meyer, 1994). Given that the credibility of established institutions has plummeted over the past decades (Peters and Broersma, 2013), multiple scales of authority have emerged by which the status of 'influencers' is granted on the basis of the attention received from a very specific community (Marwick and boyd, 2011;Tufekci, 2013). When authority becomes strictly contextual, it is much more complicated for individuals to adopt such parameters to assess the trustworthiness of the original source of a piece of news. In this context, the (social or ideological) proximity characterizing our relationship with the node in our digital network endorsing a piece of information to which we are exposed becomes much more important than its original source (Messing and Westwood, 2014). Authority and proximity thus overlap almost completely, with the result that the source's expertise -a key element of authority -loses its centrality in how individuals evaluate the trustworthiness of a piece of information. Classic literature on newsmaking has shown that a source's capital of authority influences the credit granted to that information by journalists as well (Gans, 1979;Tuchman, 1978). Such patterns privilege institutional sources over other types of sources since institutional authority is considered a byproduct of the position of power the institution occupies within society. Digital and especially social media have offered institutional and political actors the opportunity for an uninterrupted flow of communication, within which False information can be strategically disseminated, exploiting some of the affordances of these platforms. Just as with everyday citizens, journalists' proximity is likewise indicative of a source's credibility (Gans, 1979): a frequently 'encountered' source becomes familiar and is thus more likely to be trusted. Within contemporary media ecologies, it becomes much easier for journalists to constantly engage with multiple potential sources of news through digital media, allowing them to become proximate with an increasingly diverse range of sources, including nonelite and even anonymous subjects. A second crucial element orienting individuals and media institutions in their evaluations of the truthfulness of a piece of information is its content. Social psychology literature has stressed that people tend to believe true stories that are coherent with their vision of the world and are relevant to them, terming such processes 'confirmation bias' (Nickerson, 1998). Two opposing patterns characterizing news consumption on social media -i.e. high selectivity (Prior, 2013) and incidental exposure (Fletcher and Nielsen, 2017) -both interact with confirmation bias in affecting judgements of truthfulness. Since users craft their information networks on the basis of commonalities related to specific interests and visions, tribalism can emerge (Sunstein, 2017), with in-group members judging content truthfulness on the basis of its capacity to reinforce community bonds. At the same time, since social media are 'news-finds-me' environments (Gil de Zúñiga et al., 2017), they can expose users to highly conflicting information, with confirmation bias becoming the most important principle orienting the way in which information is processed (Zollo et al., 2017). Despite professional standards, congruence (with their own or their audience's vision of the world as well as with the editorial line of their company) and relevance (for the public debate at a given moment) can also be regarded as key to the process through which journalists and news organizations evaluate a story's newsworthiness (Golding and Elliott, 1979). Finally, contextual factors influence people and institutions in their assessments of the truthfulness of a piece of information. Here we refer to the situation within which information is processed. The informational exuberance (Chadwick, 2009) characterizing contemporary information ecosystems can result in what has been described as 'information overload' (Austin et al., 2012), a condition in which content is very difficult to process, negatively impacting the benefits individuals derive from it. The result is a limited attention (boyd, 2010) to processing information, potentially influencing how individuals assess truthfulness. Contextual factors also impact the ways in which professional actors process information and assess its truthfulness. When an extremely relevant story unexpectedly breaks out, professional actors can experience an information overload due to the highly augmented density of information flowing online. In this manner, as it has been described both in the literature and in journalistic accounts (Schifferes et al., 2014), inaccurate or even fabricated information produced and shared by known and unknown sources passes professional newsmaker gatekeepers. The resulting practice has been described by Bruno (2011) as 'tweet first, verify later'.",
          "Multiple judgements (true/false) on the information circulating within the system and sharing decisions place actors in relationships with one another and with information content, generating multiple and multifaceted chains of propagation. We will discuss the possible structures of these chains as a whole in the next section; we focus here on the links in the chain, i.e. the individual propagations or the meso level of our analytical framework. At this level, the observer-dependent perspective introduced above becomes relevant. Since we aim to consider multiple, and frequently conflicting, judgements regarding the truthfulness of information produced and/or shared, recognizing the unavoidable subjectivity characterizing the whole process -including the process of the researcher studying the cycle -is very important. Indeed, it is the observer's perspective that draws the boundary of the system by determining that we are dealing with a cascade of propagations of False content (here, as we have already explained above, the capitalized initial letter distinguishes the outcome of the researcher's judgement from the outcomes of the propagators' judgement). Starting from these premises, the possible combinations of the judgement of the original author of the False information (injector) and the judgements of further propagators and between various propagators when the circulation cascade is activated generate a matrix of four possible scenarios (Table 1). In the first case (1), both actors are aware of the false nature of the information but nevertheless decide to share it. While this pattern can be observed in the propagation of humoristic and satirical content, it can also be the case of two subjects who, for strategic reasons (e.g. propaganda), deliberately produce and share false information. We define this as 'pure disinformation' propagation. Conversely, when a piece of information originally injected as true is shared by a propagator who thinks it is false (2), we witness a case in which misinformation is exploited to become disinformation. In most of the cases in which False information is produced as true, the creator unintentionally injects misleading information into the system. A third option occurs when a piece of information is devised as false by the injector but perceived as true (3) by the propagator: we are observing a case of disinformation propagated through misinformation. In this case, an actor produces information knowing it is False, but other users perceive it as true and share it as such. Gross examples of this often result in embarrassing moments for the propagator (especially when the subject is a news organization) who mistakenly understood as true fabricated false information. Finally, when False information is perceived as true by the injector and by the propagator (4), we are witnessing what we could assume to be a classic process of propagation of misinformation. It is the case with the most common conspiracy theories, which flourish thanks to the content created and shared by members of polarized online communities that firmly believe in what they communicate. Other times, False information inadvertently injected into the system by an authoritative source (e.g. a respected news media organization) keeps propagating as a legitimate piece of content until someone proves it False. Given the context described above, it should be clear that trying to understand the nature of contemporary disinformation by focusing solely on the initial step of the process -when someone introduces information into the system -is at the very least an oversimplification. Over 60 years ago, Katz and Lazarsfeld (1955) argued that mass information flows depended on the personal influence that opinion leaders exercised upon other citizens, showing how, in a process, the 'first step' is not necessarily the most important one. Testing the two-step flow theory in the context of social media, Hilbert et al. (2017: 456) empirically demonstrated how, by considering the multiple perspectives of all actors involved in information cascades, the model best describing information flows can vary from a simple single step to a 'some kind of intricate network-step flow'. Similarly, Messing andWestwood (2014: 1058) contend that, within social media ecologies, the agenda-setting power that was originally concentrated in newsrooms is now diffused across social networks. The multiple actions of propagation, guided by single judgements and decisions, have thus become the backbone of news flows within the contemporary hybrid information system. Information cycles are thus better represented by a dynamic growing tree-like structure of propagation cascades, conceptually similar to those studied in the field of complex propagation (Centola et al., 2007). Conceptualizing False information cycles in terms of propagation cascades has two major consequences. First, a cycle/cascade is very unlikely to exclusively include propagations of a single type. Every real-world cycle is most likely a combination of multiple semi-independent types of propagation (Table 1) in which individual assessments and decisions are constantly influenced but not determined by the local dynamics discussed above. This process is illustrated in Figure 1. A central consequence of this way of understanding information cycles is that, despite each information cycle being a set of diverse propagations, it is still possible to theoretically describe 'global cascades' as an emergent and thus autonomous phenomenon. Within this perspective, the emergence of spreading dynamics on a systemic level (e.g. news and misinformation propagating through viral and apparently unstoppable dynamics) is coherently built upon several individual processes, thus the study of specific cases of propagation can be used to understand underlying dynamics and how these micro and meso dynamics merge into the macro evolution of a single information cycle. Keeping this is mind, it is important to stress that while every information cycle is most probably composed of several coexisting types of propagation, it is likely that every cycle will be dominated by a single type or by a combination of types.",
          "On 15 November 2015, the Spanish newspaper La Razon published on its front page the face of one of the terrorists responsible for the attacks that had hit Paris a few days earlier. Unfortunately, the face did not belong to one of the terrorists involved in the terrible attack on the French capital but instead to Veerender Jubbal, a young Canadian man. Before the attacks, Veerender Jubbal -who is Sikh and wears a turban -had posted a selfie taken with his iPad, asking his Twitter followers to wish him 'good luck'. After the attack, someone easily edited the photograph by replacing the iPad with a Quran and Photoshopping a suicide vest onto his shirt. The fake picture went viral online and was shared by various news organizations, even making it to the front page of the print edition of La Razon, before it was finally debunked and people began listening to Veerender's complaints. The origin of the Photoshopped image is in some sense largely irrelevant to the study of the cycle of False information. The picture was created and shared with the knowledge that it was a fake, but it was quickly picked up by several sources who judged it true. The diffuse proximity allowed by social media, confirmation bias dynamics (terrorists must wear turbans, and it does not matter if it is a dastar, a Sikh turban) and the 'tweet first, verify later' logic can perhaps explain how a photograph of a smiling man from the internet can be judged as the true picture of a suicide bomber, taken just before an attack. Nevertheless, once the picture was online, it was quickly shared as a legitimate picture of a terrorist, and it took over 24 hours to stop the dissemination of the False information. On 18 November 2016, Chris Lamb wrote in the US edition of The Huffington Post a satirical piece in which he imagined the US president Donald Trump suggesting the removal of the Statue of Liberty because it encouraged immigration. Despite the obvious satirical content of the article, several Italian newspapers (including the leading Il Corriere della Sera and La Repubblica) and TV channels (including La7 and RAI News) reported the news that 'Trump attacked the Statue of Liberty'. The news got great attention in Italy and was widely shared as a story confirming the unpredictability and unorthodox behaviour of the US president, before being retracted by the same newspapers that initially shared it. On 9 November 2016, the day after Republican presidential nominee Donald Trump claimed an unexpected victory in the US presidential race, a grainy picture began circulating on social media, along with the claim that the Ku Klux Klan were openly marching in Mebane, North Carolina to celebrate the win. According to investigations by the website Snopes.com (Palma, 2016), the people marching were indeed supporters of the conservative candidate celebrating the victory, but what the original creator of the picture mistook as a robe was in fact a flag. Nevertheless, the photo circulated widely on social media and partisan websites as an evidence of the connection between the conservative president-elect and the Ku Klux Klan. These are just three minor stories that illustrate key elements of the system we have described: a. The intention of the injector does not determine the future evolution of the False information cycle. Given the contemporary hybrid media system and the complex nature of information cycles, False and misleading information need not be born as such. The real goal behind the production of a piece of False information may be largely unknown (as in the case of Veerender Jubbal), can be satirical (as in the case of Lamb's article) or may be born from an honest mistake (as in the case of the picture taken in North Carolina). b. False news can only be understood as a process. Information will be constantly assessed by a number of different actors, which will act in accordance with their individual criteria to establish its truthfulness. If and when these individuals share the information, this will generate a nearly unique chain of types of propagations, defined by the model presented in Table 1. c. Information equilibrium does not imply consensus. One consequence of the model and the first two points above is that societal consensus regarding the nature of any information is somewhat unlikely. When information is constantly assessed by a number of independent actors acting in accordance with the aforementioned principles, every operation aimed at establishing the real nature of a piece of content (e.g. debunking or fact checking a piece of news) will itself be judged as true or false and ultimately accepted only by a limited group of actors.",
          "In this article, we have proposed a new approach to studying False information. Building upon an interdisciplinary theoretical background, we have suggested moving beyond a simple focus on the initial step of the news cycle (when news is actually produced and regarding the producer's intentions) to instead observe the cycle as an emergent whole influenced but not determined by individual assessments (concerning the nature of the information) and decisions (concerning whether to propagate the information). We have suggested that this is necessary due to the hybrid nature of contemporary media systems, and we theoretically grounded our approach in a second-order perspective. We have suggested that the propagation of False information should be investigated especially at the micro level (actors' judgements) and meso level (combinations of actors' decisions), while we contend that the processes characterizing the macro level should be seen as the result of the multiple combinations characterizing the previous two levels. We have argued that, at the micro levels, all actors in the hybrid media system operate, at their very core, in the very same way (despite the different levels of perceived responsibility, ethical concern, and ultimate goals characterizing them). These key processes, when connected within a relational model, produce a set of four typologies of propagation that can explain all the various types of actual propagation, including those that have always been highly problematic for pre-existing models (e.g. satire, bullshit). We therefore suggest that 'theorizing about the propagator' is beneficial to understand, mutatis mutandis, how both False and True news circulate. Certainly, we are not the first to suggest placing more focus on propagation processes. Academic research on rumours and conspiracy theories has gone beyond the generative moment of information, looking at the process of disseminating misleading information. And yet the scenarios that emerge from this wide body of research have focused only on a type of propagation in which -at times clashing with the initial goals of the authorspeople share False information while believing it to be true. A more coherent focus on the propagation process would allow us to address a number of cases that would otherwise be difficult to explain with an appropriate theoretical model, such as the case of propagators of False information who recognize its falsehood but nevertheless decide to diffuse it. This also applies to the cases of satire and parody, which may be generated with positive intentions but may nevertheless be judged true and diffused as such as the result of an honest yet dangerous mistake. Adopting this approach allows us to observe diffusion of False information as a much more nuanced phenomenon that, as shown in our examples, does not require a single understanding of the news or assume a single possible reaction. When information is constantly assessed by a number of independent actors, who then act in accordance with their assessments, a situation of coexisting opposing beliefs is the normal status of the system rather than the exception. Finally -much like Niklas Luhmann's idea of communication as an ephemeral event -a propagation is, by definition, an event in a chain of propagations and a specific cascade of propagations with its emergent properties becomes observable as an autonomous phenomenon. As such, societal communication can reference this whole case in terms of codes other than true/false. Different systems in a functional differentiated society (Luhmann, 2012) can thus address a cascade as a whole using their own perspective. While true/false is the code used by science, the media system would deal with this case in terms of its informativeness (information/non-information) and the law system using its own binary code (legal/illegal). The term 'fake news' as it is now commonly used covers completely such reality, which is why we provocatively argue that only a liar can breezily use it."
        ],
        "ground_truth_definitions": {
          "disinformation": {
            "definition": "Misleading information deliberately aimed at deceiving others.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          },
          "misinformation": {
            "definition": "False or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0f7eda998bbce003745ff2fdbcaa1d9a8119368b",
        "sections": [
          "as corrupt and untrustworthy. Many of us have started to wonder: are we trapped in echo chambers of our own making? 1The recent conversation, however, has blurred two distinct, but interrelated, social epistemic phenomena, which I will call epistemic bubbles and echo chambers. Both are problematic social structures that lead their members astray. Both reinforce ideological separation. But they are different in their origins, mechanisms for operation, and avenues for treatment. Both are structures of exclusion -but epistemic bubbles exclude through omission, while echo chambers exclude by manipulating trust and credence. However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation. We seek to stay in touch with our friends, who also tend to have similar political views. But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation. According to Jamieson and Capella, an echo chamber's members share beliefs which include reasons to distrust those outside the echo chamber. Echo chambers work by systematically isolating their members from all outside epistemic sources (Jamieson and Cappella 2008, 163-236). This mechanism bears a striking resemblance to some accounts of cult indoctrination. By discrediting outsiders, echo chambers leave their members overly dependent on approved inside sources for information. In epistemic bubbles, other voices are merely not heard; in echo chambers, other voices are actively undermined. (This is a conceptual distinction; a community can practice both forms of exclusion to varying degrees.) The contemporary discussion has been mostly focused on the phenomenon of epistemic bubbles. Cass Sunstein's famous discussions of group polarization, extremism, and the Internet largely focus on matters of constricted information flow and omitted viewpoints (Sunstein 2001(Sunstein , 2009b(Sunstein , 2009a)). Eli Pariser's The Filter Bubble focuses entirely on filtration effects from personalization technology, as in Google searches, and self-selected informational networks, as in Facebook (Pariser 2011). Popular conversation has tended to follow Pariser's focus on technologically-mediated filtration. The term \"echo chamber\" has, in recent usage, been reduced to a synonym for such bubbles and their constricted information flow. When the specifically trust-oriented manipulations of echo chambers are discussed, they are usually lumped in with epistemic bubbles as part of one unified phenomenon. This is a mistake; it is vital to distinguish between these two phenomena. An epistemic bubble is an epistemic structure emerging from the informational architecture of communities, social networks, media, and other sources of information and argument. It is an impaired informational topology -a structure with poor connectivity. An echo chamber, on the other hand, is an epistemic structure created through the manipulation of trust; it can exist within a healthy informational topology by adding a superstructure of discredit and authority. I hope to show, contra the recent focus on epistemic bubbles, that echo chambers pose a significant and distinctive threat -perhaps even a more dangerous one -that requires a very different mode of repair. Furthermore, echo chambers can explain what epistemic bubbles cannot: the apparent resistance to clear evidence found in some groups, for example, climate change deniers and anti-vaccination groups. It may be tempting to treat members of echo chambers as mere sheep, and accuse them of problematic acquiescence to epistemic authority. But that accusation relies on an unreasonable expectation for radical epistemic autonomy. Contemporary epistemology, especially social epistemology, has taught us that trust in others is necessary and ineradicable. We are, as John Hardwig says, irredeemably epistemically dependent on each other (Hardwig 1985(Hardwig , 1991)). Echo chambers prey on our epistemic interdependence. Thus, in some circumstances, echo chamber members do not have full epistemic responsibility for their beliefs. Once one is trapped in an echo chamber, one might follow good epistemic practices and still be led further astray. And some people can be trapped in echo chambers because of circumstances beyond their control -for example, they can be raised in them. Which leads to the most important questions: how can one tell if one is in an echo chamber? And how can one escape? I will argue that, for those trapped within an echo chamber, prospects for detection are poor and the escape path daunting. Detecting and escaping from echo chambers will require a radical restructuring of a member's relationship to their epistemic past, which may be more than we can reasonably expect of one another.",
          "Let's start with a simple picture of how many of us conduct our epistemic lives right now. I get much of my news via Facebook. I have selected most of my Facebook friends for social reasons; they are my friends and colleagues. A significant conduit for my learning about events in the world is by people re-posting articles that they have found newsworthy or interesting. When I go outside of Facebook, I usually turn to sources which, by and large, are affiliated with my own political beliefs and intellectual culture. This process imposes a filter on my uptake of information. Selection and exclusion are not bad in and of themselves -the world is overstuffed with supposed sources of information, many of them terrible. The better my filter, the more it will focus my attention on relevant, useful, and reliable information. But the bare fact that each individual member of the system is themselves reliable will not guarantee any broadness or completeness of coverage. Suppose, for example, that my social network was composed entirely of intelligent, reliable professors of aesthetics whose interests were largely focused on new developments in opera, ballet, and avant-garde contemporary art. Through this system, I might learn about all the exciting new developments in the New York Art scene, but entirely miss, say, relevant developments in rap, or the fact that my country was slowly sliding into fascism. The system lacks what Goldberg calls coverage-reliability -the completeness of relevant testimony from across one's whole epistemic community (Goldberg 2011, 93-4). Bad coverage can not only leave out relevant facts and evidence; it can also fail to bring relevant arguments to our attention. Thus, bad coverage can starve us of adequate exposure to relevant arguments. Notice that bad coverage is an epistemic flaw of epistemic systems and networks, not of individuals. I can now specify my use of \"epistemic bubble\" with greater precision. An epistemic bubble is a social epistemic structure which has inadequate coverage through a process of exclusion by omission. Epistemic bubbles form by leaving out relevant epistemic sources, rather than actively discrediting them. There are at two primary forces encouraging this omission. First, there is an epistemic agent's own tendency to seek like-minded sources. This phenomenon is sometimes called \"selective exposure\" by social scientists (Nelson and Webster 2017). In many contemporary cases, such as Facebook, the process of omission can occur inadvertently. I typically put people in my Facebook feed for social reasons -because I like them or I find them funny. But social selection does not guarantee good coverage reliability; in fact, the typical bases of social selection are inimical to good coverage reliability. 2 We usually like people who are similar to us, and such similarity makes coverage gaps more likely. Friends make for good parties, but poor information networks. We now have a straightforward account of one way in which epistemic bubbles can form. We can build a structure for one set of purposes -maintaining social relations, finding -and then proceed to use it for an entirely different purpose, for which it functions badly: information gathering. Second, there are the processes by which an epistemic agent's informational landscape is modified by other agents. This might include, say, systematic censorship or media control by the state or other actors. The most worrisome of these external forces, at the moment, seems to be the algorithmic personal filtering of online experiences (Pariser 2011;Watson 2015). Internet search engines, for example, will track personal information for each particular user, and adapt their search results to suit each user's interest. Certainly, newspapers and other traditional media technologies do place external filters on their readers, but the modern instantiation is particularly powerful and troubling. As Boaz Miller and Isaac Record argue, Internet technologies create hyper-individualized, secret filters. The secrecy is particularly threatening. Many users do not know about the existence of algorithmic personal filtering. Even amongst those that do, most do not have access to the particularities of the algorithms 2 For an overview of empirical research on personal similarity and polarization, see (Sunstein 2009a, 83-5). Curiously, Sunstein notes the group polarization literature has thought relatively little about the impact of personal similarity. doing the filtering; thus, the very opacity of the process makes it harder for a user to successfully evaluate and epistemically compensate for such filtering (Miller and Record 2013). Thus, most users significantly underestimate the degree to which their exposure to information, via search results, has already been tailored to present search results to which the user will already be amenable. Both the agent-driven process of selective exposure, and the externalities of algorithmic filtering, contribute to the creation of epistemic bubbles. I introduce the term \"epistemic bubble\" here to indicate a broader set of phenomena. Pariser introduced the term \"filter bubble\" to refer specifically to technologically mediated filtering, especially via algorithmic matching. Epistemic bubbles are those structures which omit relevant voices by any means, technological or otherwise. Epistemic bubbles include filter bubbles, but also nontechnological selection processes, such as physically sorting oneself into neighborhoods of like-minded people (Bishop 2009). The account I've given of epistemic bubbles focuses on the way they omit relevant information, but that omission can also threaten us with bootstrapped corroboration. Users of social networks and personalized search technologies will encounter agreement more frequently and so be tempted to over-inflate their epistemic self-confidence. This danger threatens because, in general, corroboration is often a very good reason to increase one's confidence in the relevant beliefs (Nguyen 2010(Nguyen , 2018a)). But corroboration ought to only have weight if it adds something epistemically, rather than being a mere copy. To borrow an example from Wittgenstein: imagine looking through a stack of identical newspapers and treating each next newspaper headline saying p as a reason to increase your belief that p (Wittgenstein 2010, 100). This is clearly a mistake; the fact that a newspaper claims that p has some epistemic weight, but the number of copies of that newspaper one encounters ought not add any extra weight. Similarly, imagine speaking to a bunch of acolytes of Guru Jane, who repeat anything that Guru Jane says without any further thought. The fact that all these acolytes repeat Guru Jane's testimony should add no extra weight. So long as the disciplines repeat anything Guru Jane says -so long as they are mere conduits for information, rather than sources of information -they are simply another sort of copy. But copying isn't the only route to a problematic form of non-independence. Suppose I believe that the Paleo diet is the best diet. I proceed to assemble a body of peers who I trust precisely because they also believe that Paleo is the best diet. In that case, the existence of perfect agreement on Paleo's amazingness throughout that group ought to count for far less than it might for other groups that I had not assembled on that basis. Even if all the group members arrived at their beliefs independently, their agreement is already guaranteed by my selection principle. To the degree that I have pre-selected the members in my epistemic network from agreement with some set of beliefs of mine, then their agreement with that set of beliefs and any other beliefs that it entails ought to be epistemically discounted. 3 If we fail to so discount, we are ignoring a pernicious hidden circularity in our corroborative process. It is easy to forget to discount because the bootstrap here is obscured by time and interface. But we must actively adjust for the increased likelihood of agreement inside our bubbles, or we will unwarrantedly bootstrap our confidence levels. 4To summarize: an epistemic bubble is an epistemic network that has inadequate coverage through a process of exclusion by omission. That omission need not be malicious or even intentional, but members of that community will not receive all the relevant evidence, nor be exposed to a balanced set of arguments.",
          "Luckily for us, epistemic bubbles are relatively fragile. Relevant sources have simply been left out; they have not been discredited. It is possible to pop an epistemic bubble by exposing a member to relevant information or arguments that they have missed. Echo chambers, on the other hand, are significantly more robust. My analysis here combines empirical work and analysis from Jamieson and Cappella on the nature of right-wing echo-chambers with recent insights from social epistemology. Jamieson and Cappella studied echo chambers built around particular charismatic personalities -Rush Limbaugh and the news team of Fox News, and certain other members of conservative talk radio. Their data and analysis suggest that Limbaugh uses methods to actively isolate his community of followers from other epistemic sources. Limbaugh's consistent attacks on the \"mainstream media\" serve to discredit all potential sources of knowledge or testimony besides Limbaugh and a select inner-cadre of other approved sources (Jamieson and Cappella 2008, 140-76). Limbaugh also develops what they call a private language, full of alternate meanings for familiar terms and new jargon (for example, \"SJWs\"), in order to exaggerate the insularity and separateness of the in-group. Finally, Limbaugh provides counter-explanations of all contrary views, intended not only to attack each particular view, but also to undermine the general trustworthiness and integrity of anybody expressing a contrary view. The resulting world-view is one of highly opposed forces; once one has subscribed to Limbaugh's view, one has reason to think that anybody who does not also subscribe is actively opposed to the side of right, and thereby morally unsound and so generally untrustworthy (177-90). Jamieson and Cappella suggest that this makes a follower dependent on a single source or group of sources, and makes them highly resistant to any outside sources. They offer the following definition of an echo chamber: an echo chamber is a bounded and enclosed group that magnifies the internal voices and insulates them from rebuttal (76). I will use the term \"echo chamber\" here following their analysis, but I adapt the definition slightly for philosophical use. I use \"echo chamber\" to mean an epistemic community which creates a significant disparity in trust between members and non-members. This disparity is created by excluding non-members through epistemic discrediting, while simultaneously amplifying insider members' epistemic credential. Finally, echo chambers are such that in which general agreement with some core set of beliefs is a pre-requisite for membership, where those core beliefs include beliefs that support that disparity in trust. By \"epistemic discrediting\", I mean that non-members are not simply omitted or not heard, but are actively assigned some epistemic demerit, such as unreliability, epistemic maliciousness, or dishonesty. By \"amplifying epistemic credentials\", I mean that members are assigned very high levels of trust. Of course, these two processes can feedback into one another. So long as an echo chamber's trusted insiders continue to claim that outsiders are untrustworthy, then the inner trust will reinforce the outward distrust. And so long as outsiders are largely distrusted, then the insiders will be insulated from various forms of counter-evidence and rebuff, thus increasing their relative credence. Once a sufficient disparity in credence between insiders and outsiders has been established, so long as trusted insiders continue to hold and espouse epistemically dismissive beliefs towards outsiders, then the echo chambers' beliefs system will be extremely difficult to dislodge. Compare this process of credence manipulation to the process of omission found in epistemic bubbles. In one standard scenario, I add others as trusted members of my epistemic network based on agreement. I am then less likely to encounter an outside voicebut when I do actually have such an encounter with an outsider, I have no background reason to dismiss them. Bubbles restrict access to outsiders, but don't necessarily change their credibility. Echo chambers, on the other hand, work by offering a pre-emptive discredit towards any outside sources. 5The result is a rather striking parallel to the techniques of isolation typically practiced in cult indoctrination. The standard techniques of cult indoctrination, by a traditional account, are the aggressive emotional isolation of cult members from all non-cult members, which amplifies indoctrinated member's dependency on the cult (Singer 1979;Langone 1994;Lifton 1991). 6 New cult members are brought to distrust all non-cult members, which provides an epistemic buffer against any attempts to extract the indoctrinated person from the cult. This is nothing like how epistemic bubbles work. Epistemic bubbles merely leave their members ignorant, but ignorance can be fixed with simple exposure. The function of an echo chamber, on the other hand, is to credentially isolate its members by a manipulation of trust. By this, I mean that members are not just cut off, but are actively alienated from any of the usual sources of contrary argument, consideration, or evidence. Members have been prepared to discredit and distrust any outside sources; thus, mere exposure to relevant outside information will have no effect. In fact, echo chambers can avail themselves of another epistemic protective mechanism: they can contain what I'll call a disagreement-reinforcement mechanism. Members can be brought to hold a set of beliefs such that the existence and expression of contrary beliefs reinforces the original set of beliefs and the discrediting story. A toy example: suppose I am a cult leader, and I have taught my followers to believe that every human except the members of our group has been infested and mind-controlled by alien ghosts from Mars. I also teach my followers that these alien ghosts from Mars hate our group for knowing the truth, and so will constantly seek to undermine our knowledge of their existence through mechanisms like calling us a 'cult' and calling us lunatics. Endre Begby has offered a careful analysis of this particular sort of disagreement-reinforcement mechanism, which he calls \"evidential preemption.\" Suppose that I tell my followers to expect outsiders to falsely claim that there are no ghosts from Mars. When my followers do confront such contrary claims from outsiders, those contrary claims are exactly what they expected to hear. Thus, new contrary testimony is neutralized, because it was predicted by past beliefs. This, says Begby, functions as a kind of epistemic inoculation. There is also a secondary effect. When my followers hear exactly what I predicted, then my claims have been verified, and so my followers will have some reason to increase their trust in me. Thus, the echo chamber's belief system not only neutralizes the epistemic impact of exposure to outsiders with contrary beliefs; the existence of those contrary beliefs will actively corroborate the pre-emptor and so increase the credence level of the entire echo chamber (Begby 2017). This creates a feedback mechanism within the echo chamber -in making undermining predictions about contrary testimony, inside authorities not only discredit that contrary testimony, but increase their trustworthiness for future predictions. Once such a system of beliefs is set up, it can be very difficult to dislodge -it is selfreinforcing, bounded, and built to discount any contrary input. In fact, though my definition of echo chambers is conceptually separable from such a disagreement-reinforcement mechanism, every plausible real-world candidate for an echo chamber I've ever encountered includes some version of a disagreement-reinforcement mechanism. For a depressing realworld example, consider Pizzagate. Pizzagate is a conspiracy theory that boiled out of a rightwing online forum on Reddit, which included beliefs that Comet Ping Pong, a pizza restaurant, was the site of a child sex trafficking ring owned by a liberal conspiracy involving Hillary Clinton and Barack Obama. Eventually, Edgar Welch, a member of that forum, forcibly entered the pizza parlor armed with an assault rifle to investigate; when he satisfied himself that the restaurant contained no child slaves, he gave himself up to the police. The online forum, however, did not take this as contrary evidence. Instead, they leaned on their belief that the liberal conspiracy had total control of the mainstream media, and was willing to stage fake events to discredit the right-wing. The forum took Welch's claims that there was no sex trafficking ring as evidence that Welch was a paid actor, and thus as further confirmation of the existence of a powerful cabal of liberal child sex traffickers (Mengus 2016;Vogt and Goldman 2016). Conspiracy theories function here in a fascinating inversion to corroborative bootstrapping. In corroborative bootstrapping, the mistake is to treat problematically dependently selected insiders as if they were independent, and thus overweight their testimony. When an echo chamber uses a conspiracy theory in this manner, they are attributing a problematic form of non-independence to outsiders who are actually independent, and thereby underweighting outside testimony. An echo chamber here works by discrediting the apparent independence of, say, different climate change scientists by claiming that all their various testimonies are problematically derived from a single source. Incidentally, I am not claiming here that conspiracy theories are always or necessarily incorrect or the product of epistemic vice. As others have argued, believing in conspiracy theories isn't bad per se, because some conspiracy theories are true (Coady 2012, 110-137;Dentith 2017). But the fact that conspiracy theories can function to reinforce the boundaries of echo chambers -though they do not necessarily do so -might explain part of conspiracy theories' bad rap. Because of their effectiveness in setting up disagreement-reinforcement mechanisms, conspiracy theories are often conscripted as a powerful tool in the bad epistemic behavior of certain groups. It is important to note that the epistemic mechanisms by which echo chambers work, though problematic, are not sui generis. They are perversions of natural, useful, and necessary attitudes of individual and institutional trust. The problem isn't that we trust and distrust groups and institutions. In fact, we must do so. Eljiah Millgram calls it the problem of hyperspecialization. Human knowledge has splintered into a vast set of specialized fields that depend on each other. No one human can manage that information; we are forced to trust each other (Millgram 2015, 27-44). 7 None of us is in a position to reliably identify an expert in 7 Though this paper relies on insights from modern work in the epistemology of testimony, I have tried to rely only on uncontroversial claims from that field, and not on the technical details of any particular view. In particular, I have attempted to construct my analysis so as to be independent of the debate on whether or not testimony is a basic source of knowledge. I have also attempted to make the paper compatible with the major accounts of trust. most specialist fields outside of our own. I am, on my own, helpless to evaluate the virtues of antibiotics or the expertise of a particular doctor or surgeon. I am, instead, reliant on a vast network of institutional licensing practices in order to choose my health care sourcesincluding journal peer review, medical board exams, university hiring practices, and the like (Nguyen 2017a). Often, I trust via what Philip Kitcher calls indirect calibration -I trust mechanical engineers because they make things that work, but I know that mechanical engineers trust applied physicists, and I know that applied physicists trust theoretical physicists, so I acquire trust through a long chain of field-wide links (Kitcher 1993, 320-3). I even use litmus tests: the fact that any person or group is in favor of, say, sexual orientation conversion therapy is enough for me to discredit them on any social or moral topics. We must resort to such tactics in order to navigate the hyper-specialized world (Nguyen forthcoming). Echo chambers function parasitically on these practices by applying discredits without regard for the actual epistemic worth of the discredited institutions or individuals. The discredit is instead applied strategically and defensively, towards all outsiders solely on the basis of their being outsiders. Once the discrediting beliefs are in place, the ensuing beliefs and action the echo chambers' members are surprisingly close to rational. In fact, we can easily imagine alternative scenarios in which a very similar set of beliefs were appropriate and veristic. If I was an anti-Nazi in Germany during the rise of the Nazi party, I would do well to maintain the beliefs that the most people were corrupt, untrustworthy, and out to maliciously undermine my own true beliefs. But if such beliefs become implanted in an inappropriate context, they can lead their believers entirely astray. Epistemic bubbles can easily form accidentally. But the most plausible explanation for the particular features of echo chambers is something more malicious. Echo chambers are excellent tools to maintain, reinforce and expand power through epistemic control. Thus, it is likely (though not necessary) that echo chambers are set up intentionally, or at least maintained for this functionality. My account thus bears some resemblance to some work on testimonial injustice and the epistemology of ignorance, but it is importantly distinct. Miranda Fricker has argued for a kind of testimonial injustice, based on a gap between actual reliability and perceived credibility. For example, says Fricker, being white and being male are both bonuses to credibility. Since credibility is a source of power, anybody with credibility will seek to increase it, using that very credibility. Thus, says Fricker, credibility gaps can be turned into epistemic tools of social oppression (Fricker 2011). Similarly, Charles Mills argues that there is an active practice of ignorance among members of oppressive groups, such as white Americans. It is to the benefit of those in power to actively ignore many aspects of the existence of oppressed groups (Mills 2007;Alcoff 2007, 47-57). My account is compatible with, but independent from, Fricker's and Mills' accounts. Echo chambers can and surely are used to maintain social oppression through enhancing credibility gaps and supporting practices of active ignorance. The systematic mistrust of an echo chambers is a powerful tool for perpetuating epistemic injustice and active ignorance. However, the concept of an echo chamber does not require that they be deployed only in political contexts, nor does it require that they only be deployed only in the service of oppression. Echo chambers could potentially exist among the oppressed, and surely exist in apolitical contexts. I believe I have witnessed echo chambers forming around topics such as anti-vaccination, multi-level marketing programs, particular diets, exercise programs, liberal activism, therapeutic methodologies, philosophies of child-rearing, particular academic subdisciplines, and Crossfit (Weathers 2014).",
          "It has often been claimed, during and after the American political season of 2016, that we have entered a 'post-truth era'. Not only do some political figures seem to speak with a blatant disregard for the facts, their supporters seem unswayed by reason or contrary evidence. To many, it seems as if a vast swath of the electorate has become entirely unmoored from any interest in facts or evidence. Call this the \"total irrationality\" explanation of the post-truth phenomenon. But echo chambers offer an alternative explanation for the apparent post-truth mood. It seems likely that there is at least one vast partisan echo-chamber present in the political landscape. Jamieson and Cappella's study is a decade old, but sources like Breitbart and Alex Jones' Infowars seem like clear extensions of the same right-wing echo chamber. (Other echo chambers surely exist elsewhere on the political spectrum, though, to my mind, the left-wing echo chambers have been unable to exert a similar level of political force.) In that case, the account of echo chambers I've offered has significant explanatory force. The apparent \"posttruth\" attitude can be explained, at least in part, as the result of credence manipulations wrought by echo chambers. In healthy epistemic communities, there is something like an upper ceiling on the credence level accorded to any individual. A healthy epistemic network will supply a steady stream of contrary evidence and counterarguments; thus, no single individual or group will ever go unchallenged. Epistemic bubbles make the discovery of mistakes significantly less likely, and so tend to exaggerate the credence levels of epistemic sources inside the bubble. But when an echo chamber is in place and all outside sources have been effectively discredited, that ceiling disappears categorically. Echo chambers can create runaway credence levels for approved individuals. By removing disconfirmations and discorroboration from the system through the systematic discrediting of outsiders, echo chambers can create exceptionally high -one is tempted to say unnaturally high -levels of trust. That potential for runaway credence is built right into the foundations of any echo chamber, and arises from an interaction between the two main components of any echo chamber. First, an echo chamber involves a significant disparity of trust between the insiders and the outsiders. Second, an echo chamber involves beliefs, espoused by the insiders, reinforcing that disparity. The essential features of echo chambers seem designed to selfreinforce their peculiar arrangement of trust. Notice that epistemic bubbles alone cannot explain the post-truth phenomenon. Since epistemic bubbles work only via coverage gaps, they offer little in the way of explanation for why an individual would reject clear evidence when they actually do encounter it. Coverage gaps cannot explain how somebody could, say, continue to deny the existence of climate change when actually confronted with the overwhelming evidence. One would be tempted, then, to accuse climate change deniers of some kind of brute error. But echo chambers offer an explanation of the phenomenon without resorting to attributions of brute irrationality. Climate change deniers have entered an epistemic structure whereby all outside sources of evidence have been thoroughly discredited. Entering that epistemic structure might itself involve various epistemic mistakes and vices -but here the story can be one of the slow accumulation of minor mistakes, which gradually embed the believer in a self-reinforcing, internally coherent, but ultimately misleading epistemic structure. Similarly, some have suggested that, in the post-truth era, many people's interest in the truth has evaporated. Once again, this account of echo chambers suggests a less damning and more modest explanation. An echo chamber doesn't erode a member's interest in the truth; it merely manipulates their credence levels such that radically different sources and institutions will be considered proper sources of evidence. This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak -deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth. But by my account, we should expect discourse within echo chambers to be entirely differentwe should expect such discourse to be crisp and clear, and to present unambiguous claims about what is the case, what secret conspiracies are in place, and which sources are to be entirely distrusted. And this is precisely what we find (Jamieson and Cappella 2008, 3-41,140-176). Consider, for example, Breitbart's attacks on other media sources. One article begins: \"Mainstream media outlets continue to print false and defamatory descriptions of Breitbart News in a nakedly political effort to marginalize a growing competitor\" (Pollak 2017). This is not the double-speak of administrators and bureaucrats-this is a clear, strident, and unambiguously worded discredit. One might be tempted to say: but just give them the real evidence! You can't discredit neutral evidence! But this response radically underestimates the degree of trust and social processing involved in most presentations of evidence. Except for empirical evidence I myself have gathered, all other presentations of evidence rely on trust. My belief in the reality of climate change depends on enormous amounts of institutional trust. I have not gathered the climate change evidence myself; I mostly just trust science journalists who, in turn, trust institutional credentialing systems. Even if I had been on, say, a core sampling expedition to the Arctic, I would be unable to process that information for myself, or even vet whether somebody else has properly processed it. Even the climatologist who actually processes that information must also depend on trusting a vast array of other experts, including statisticians, chemists, and the programmers of their data analysis software. Most so-called \"neutral evidence\" depends on long and robust chains of trust (Millgram 2015, 27-44). Members of an echo chamber have acquired beliefs which break the usual arrangements of trust. But despite their evident explanatory force, echo chambers have been largely neglected by recent empirical research. Much of the recent research on causes of belief polarization focuses on the causal role of individual psychology, such as the tendency towards laziness in the scrutiny of one's own beliefs (Trouche et al. 2016). Similarly, recent studies on climate change denial focus on studying the relationship between an individual's stated political beliefs and their reactions to climate change information, without inquiring into the social epistemic structures in which the individuals are embedded (Corner, Whitmarsh and Xenias 2012). Famously, Dan Kahan and Donald Braman argue for the cultural cognition thesisthat is, that cultural commitments are prior to factual beliefs, and that non-evidentially formed cultural values inform which future presentations of evidence will be admitted as weighty (Kahan and Braman 2006). Though the values may originally come from an individual's culture, Kahan and Braman focus their analysis on how those acquired values function in individual reasoning to create polarization. They pay little attention to the continuing role of the contingent social structures in which the individual is embedded. The direct literature on echo chambers and epistemic bubbles is new and relatively small, compared to the sizable literature on individual belief polarization. Unfortunately, even in that literature, echo chambers and epistemic bubbles have often been confused. They are usually addressed in the popular media together, and the terms 'epistemic bubble' and 'echo chamber' are typically used interchangeably (El-Bermawy 2016). The same blurring has occurred in the treatment of the phenomena in academic epistemology in the surprisingly small literature on echo chambers. For example, Bert Baumgaertner, in his analysis of echo chambers via computer modeling, lumps together under the heading 'echo chamber' both Though he professes to cover both filter bubbles and echo chambers, his work focuses almost entirely on epistemic bubble effects: constricted information flow, lack of exposure to alternate arguments, and bootstrapped corroboration (Sunstein 2009b(Sunstein , xi,19-06, 2009a, 1-98), 1-98). The point here is about more just than his choice of words: his subjects of analysis include, among other things, Facebook friend groups, hate groups, extremist online political forums, conspiracy theorists, and terrorist groups (99-125, 2009b, 46-96). Clearly, this list includes prime candidates for both epistemic bubbles and echo chambers. But his analysis focuses almost entirely on the effects of bootstrapped corroboration and lack of exposure. For Sunstein, the primary mechanism driving polarization and extremism is the loss of truly public forums, because technology has over-empowered people's tendency to self-select sources offering familiar views. Thus, his solution is to re-create, in the new media environment, the kind of general public forums where people might be more likely to serendipitously encounter contrary views and arguments. His solutions include governmentfunded public news websites with diverse coverage and voluntary work by corporations and individuals to burst their bubbles. His recommendations for repair largely have to do with increasing exposure (Sunstein 2009a(Sunstein , 135-48, 2009b, 19-45,190-211), 19-45,190-211). But, again, if what's going on is actually an echo chamber effect, exposure is useless or worse. The blurring of the two concepts has also lead to some problematic dismissals of the whole cluster of phenomena. A number of recent articles in social science, communications, and media studies have argued that the whole set of worries about bubbles and echo chambers is wildly overstated. These articles share the same argumentative pattern. First, they use the terms \"filter bubble\" and \"echo chamber\" interchangeably, and address themselves to the same cluster of phenomena as Sunstein, treating them as singular. In fact, James Nelson and James Webster conflate Jamieson and Cappella's analysis of echo chambers and Pariser's analysis of filter bubbles, and erroneously attribute to Jamieson and Cappella the view that political partisans only seek out and encounter media from sources with matching political alignments -that is, Nelson and Webster attribute to an epistemic bubbles account to Jamieson and Cappella, where Jamieson and Cappella's actual text is clearly an echo chambers account (Nelson and Webster 2017, 2). More importantly, these recent articles proceed to argue against the existence of filter bubbles and echo chambers by demonstrating that, through the analysis of empirical data about media consumption, most people in fact expose themselves to media from across the political spectrum. Nelson and Webster, for example, argue against Jamieson and Capella, claiming that filter bubbles and echo chambers don't exist. Nelson and Webster support their claim with data showing that both liberals and conservatives visit the same media sites and spend comparable amounts of time at those sites (6-7). Again, this misses the mark -this is evidence only against the existence of epistemic bubbles, and not against the existence of echo chambers. Similarly, Seth Flaxman et al seeks to problematize the existence of filter bubbles and echo chambers with data that social media platforms seem to actually increase people's exposure to media from across the political divide (Flaxman, Goel and Rao 2016). Again, these data only concern the exposure and omission, and only weigh against the existence of epistemic bubbles. They say nothing about whether echo chambers exist. Echo chambers, recall, are structures of strategic discrediting, rather than bad informational connectivity. Echo chambers can exist even when information flows well. In fact, echo chambers should hope that their members are exposed to media from the outside; if the right disagreement reinforcement mechanisms are in place, that exposure will only reinforce the echo chambers' members' allegiance. We ought not conclude then, from data that epistemic bubbles do not exist, that echo chambers also do not exist. We can see now crucial it is to keep these two categories distinct. Epistemic bubbles are rather ramshackle -they go up easily, but they are easy to take down. Since there is no systematic discrediting of outsiders, simple exposure to excluded voices can relieve the problem. Echo chambers, on the other hand, are much harder to escape. Echo chambers can start to seem almost like living things -the belief systems provide structural integrity and resilience. Mere exposure will be ineffective. Jamieson and Cappella offer evidence of this effect: once listeners are caught in Rush Limbaugh's language, framing, and discredentialing of the mainstream media, their beliefs can survive frequent contact with contrary viewpoints. Limbaugh's technique, say Jamieson and Cappella, serves to insulate and inoculate his audience from being affected by exposure to contrary viewpoints (Jamieson and Cappella 2008, 163-190). In fact, if the appropriate disagreement-reinforcement mechanisms are in place, exposure will simply strengthen the attacked belief systems. Thus, an outsider's attempt to break an echo chamber as if it were a mere bubble is likely to backfire and reinforce the echo chamber's grip. 9",
          "So what, then, are the epistemic responsibilities of an agent to discover whether they are in one of these social epistemic traps, and what are their prospects for actually discovering their predicament and successfully escaping? To answer this, we must consider two distinct questions: The escape route question: Is there any way out of an echo chamber or epistemic bubble? The escape responsibility question: Could one behave epistemically virtuously, and yet still remain caught within an echo chamber or epistemic bubble? In other words, to what degree is an epistemic agent embedded within such a structure blameworthy, or blameless, for the faultiness of their beliefs? The first question asks about the possible existence of an escape route. The second asks whether there is an escape route that we might reasonably expect an epistemically virtuous agent to discover and enact. These are distinct questions, because an escape route might turn 9 Sunstein does briefly note the empirical data for the disagreement-reinforcement effect in passing, but then seems to ignore it in proposing his solutions (Sunstein 2009a, 54-5) out to be possible, but so difficult to discover or use that it was beyond what we might reasonably expect of an agent of moderate epistemic virtue. For epistemic bubbles, the answers are straightforward. As I've argued, epistemic bubbles are quite easy to shatter. One just needs exposure to excluded information. Insofar as that information is available, but simply not part of one's standard network, then members of epistemic bubbles are failing to live up to their epistemic duties, which include proactively gathering relevant data. To translate into contemporary terms: if you're subject to an epistemic bubble because you get all your news from Facebook and don't bother to look at other sources, you are, indeed, blameworthy for that failure. If one finds the language of epistemic virtues and vices appealing, then we can say that members of epistemic bubbles are committing the vice of epistemic laziness. Answering these two questions is much more difficult for echo chambers. Recall: where encountering excluded voices and evidence will shatter an epistemic bubble, such encounters are likely to reinforce an echo chamber. Let's grant that intentionally constructing an echo chamber, as Jamieson and Cappella claim that Rush Limbaugh did, is epistemically (and morally) blameworthy. Furthermore, actively entering an echo chamber seems epistemically blameworthy in many circumstances. For agent in full possession of a wide range of informational sources, to abandon most of them and place their trust in an echo chamber for, say, an increased sense of comfort and security, is surely some form of epistemic vice. There is some evidence that this may be the case; Jamieson and Cappella suggest that people enter echo chambers for the sake of the community bonds and the sense of belonging to an ingroup (Jamieson and Cappella 2008, 180). But there are many cases in which the agent seems plausibly blameless. Imagine a person raised in an echo chamber. Their earliest epistemic contacts -let's say their parents, relatives, and close family friends -are all firmly committed members of the echo chamber. Suppose that the child is either home-schooled by those echo chamber members or sent to a school that reinforces the beliefs of that particular echo chamber. I take it that it is reasonable for a child to trust their parents and those of seeming epistemic authority, and that a child is epistemically blameless for having done so (Goldberg 2013). Thus, when that child eventually comes into contact with the larger epistemic world -say, as a teenager -the echo chamber's beliefs are fully in place, such that the teenager discredits all sources outside of their echo chamber. It seems, at first glance, that our teenager could be acting very much like a reasonable epistemic agent. They could, in fact, be epistemically voracious: seeking out new sources, investigating them, and evaluating them using their background beliefs. They investigate the reliability of purported experts and discredit experts when they have apparently good reason to do so, using their background beliefs. Our teenager seems, in fact, to be behaving with many epistemic virtues. They are not at all lazy; they are proactive in seeking out new sources. They are not blindly trusting; they investigate claims of epistemic authority and decide for themselves, using all the evidence and beliefs that they presently accept, whether to accept or deny the purported expertise of others. They have theories, which they have acquired by reasonable methods, predicting the maliciousness of outsiders; they increase their trust in those theories when their predictions are confirmed. 10The worry here is that agents raised within an echo chamber are, through no fault of their own, epistemically trapped -their earnest attempts at good epistemic practices are transformed into something epistemically harmful by the social structure into which they have been embedded and which they have ingested. Paul Smart has argued for the possibility of a transformative social epistemic phenomenon which he dubs \"mandevillian intelligence,\" in honor of Bernard Mandeville. Mandeville argued that, in the right social context, individual vices could lead to collective economic prosperity. For a certain kind of economic theorist, capitalism is such a transformative structure -individuals act selfishly, but the structure of the market transforms that selfishness into virtuous collective action. According to Smart, there is an epistemic analog: the mandevillian intelligence, which transforms the individual epistemic vices of its members into a collective epistemic virtue by virtue of the social structure into which they are embedded (Smart 2017). Intellectual stubbornness, for example, might be an intellectual vice for individuals. But set those stubborn individuals in a properly arranged social structure (like, perhaps, academia) and you might get a collective system that properly explores every relevant nook and cranny with optimal thoroughness. But echo chambers are the very opposite; they are reverse-mandevillian intelligences. Echo chambers are social epistemic structures which convert individually epistemically virtuous activity into collective epistemic vice. In fact, the reverse-mandevillian nature contributes to the stickiness of the echo chamber trap. If our teenager self-reflects on their epistemic practices, what they will see might be rather gratifying. Their epistemic behavior might very well be earnest, vigorous, and engaged. It is their external context -the social epistemic system into which they have been unluckily raised -which makes such behavior problematic. Contrast this account with Quassim Cassam's treatment of Oliver, his fictional 9/11 conspiracy theorist. Oliver believes that the collapse of the twin towers was an inside job, and he is happy to provide reasons and point to supporting evidence from a great many conspiracy theorist websites. Says Cassam: Oliver is obviously mistaken -Oliver relies on outrageous, baseless claims from clearly discredited sources. The best explanation for Oliver's beliefs is in terms of epistemic vice -that is, in terms of Oliver's bad intellectual character traits. Oliver is \"gullible, cynical, and prejudiced,\" says Cassam. Oliver is gullible with regard to his conspiracy theorist sites, cynical with regard to the mainstream media, and his prejudice consists of, among other things, intellectual pride, wishful thinking, closedmindedness, and a lack of thoroughness (Cassam 2016, 162-4). And I certainly grant that such epistemic vices can lead to these sorts of beliefs. But the story of our hapless teenager offers an alternate epistemic path to such beliefs and such narrowcasted trust -one in which epistemically virtuous character traits have been wrong-footed by the social epistemic structure in which the agent has been embedded. The crucial difference between the reversemandevillian account and Cassam's account is where the brunt of the responsibility lies. In Cassam's account, the responsibility lies with the individual, and their own intellectual habits and practices. 11 In a reverse-mandevillian account, a significant part of the responsibility lies with the social structure in which the actors are embedded. The epistemic vice is a feature of the collective intelligence, rather than of the individual. Or, if one is averse to thinking in terms of collective intelligences, here's a conceptually minimal way to put the claim: echo chambers are local background conditions that turn generally good epistemic practices into locally unreliable ones. But the possibility of a truly faultless epistemic agent, wholly misled by an echo chamber, also depends on the lack of an accessible escape route. So: are there escape routes from an 11 Note, however, that Cassam distinguishes between epistemic responsibility and epistemic blameworthiness, and does not take blameworthiness to necessarily follow from responsibility (168-9). Cassam leaves room for the view that the individual's intellectual vices were epistemically responsible for their bad beliefs, but that the individual wasn't blameworthy for those vices, because the vices were inculcated in them at an early age. However, my complaint still stands, for I contest Cassam's claim that the responsibility is in the individual. echo chamber, and how reasonable is it to expect echo chamber members to discover and make use of them? Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass. Thus, their critical reflection is likely to reinforce previously held beliefs. Kelly notes that the belief polarization violates the Commutativity of Evidence Principle: The Commutativity of Evidence Principle: to the extent that what it is reasonable for one to believe depends on one's total evidence, historical facts about the order in which that evidence is acquired make no difference to what it is reasonable for one to believe. (Kelly 2008, 616) In short, belief polarization makes it matter what order they received the evidence, but the historical ordering of evidence ought not matter. Note that our epistemically hapless teenager has also violated the Commutativity of Evidence Principle. For them, it matters very much what order that they received the evidence. If they had been raised outside the echo chamber and fed a broader diet of epistemic sources before encountering the echo chamber, then they would likely have found the echo chamber's world-view to be problematic. But since our teenager encountered the echo chamber and assimilated its beliefs first, their use of background beliefs to vet new sources leads them to continually increase their trust in the echo chamber and their distrust of outsiders. Even if our echo chambered teenager eventually came to encounter all the same evidence as their epistemically free-range counterpart, their early education within the echo chamber would be decisive. So long as each new piece of evidence is assessed using the currently held set of beliefs, then early education in an echo chamber becomes domineeringly powerful. However, the Commutativity of Evidence Principle suggests a way out. In order to free themselves of the echo chamber's grip, our teenager needs to undo the influence of the historical ordering of their encounters with the evidence. How could they possibly do this? Our teenager would have to suspend belief in all their particular background knowledge and restart the knowledge-gathering process, treating all testimony as equally viable. They would need to, in a sense, throw away all their beliefs and start over again. This suggests a process that, in its outlines, might sound awfully familiar. Our escape route turns out to be a something like a modified version of Descartes' infamous method. What proceeds from this point is admittedly something of a fantasy, but perhaps it is a fantasy from which we can eventually draw some sort of moral. The story of the history of Western epistemology might be cartoonishly summarized thusly: Descartes had a dream of radical intellectual autonomy. By his accounting, he came to realize that many of the beliefs he had acquired in his early life were false, and that those early false beliefs might have infected any number of other beliefs. His response was that famed method: to get rid of his beliefs and start over again, trusting no-one and nothing and only permitting those beliefs of which he was entirely certain. Call this the Cartesian epistemic reboot. But if recent epistemology has taught us anything, it's that this total reboot is nothing but a pipe dream. Any sort of reasonable epistemic life is essentially impossible without trusting the testimony of others (Burge 1993;Faulkner 2000;Goldberg 2010;Zagzebski 2012;Hardwig 1985Hardwig , 1991)). But recall that the reason Descartes wanted to discard everything and start over from scratch -the motivation for his project, and not the method -was explained in the very first line of \"Meditation 1\": He was worried by the falsehoods he had learned in childhood and the shakiness of the edifice that had been built from those falsehoods (Descartes 1984, 24). Our teenager faces a problem quite similar in structure. The credentialing structure of their upbringing is flawed; that credentialing structure has influenced any number of their other beliefs, and the degree of that influence is impossible to track. Furthermore, these later beliefs, approved by the echo chambers' credentialed sources, will often serve to reinforce that credentialing structure. The pernicious effect of an echo chamber cannot be attacked one belief at a time. Any single belief that our teenager re-considered would come under the influence of the network of the flawed background beliefs that sustains an echo chamber. What they need is some way to start over. In order to undo the influence of historical ordering, an epistemic agent will have to temporarily suspend belief in all their beliefs, in particular their credentialing beliefs, and start from scratch. But when they start from scratch, they need not disregard the testimony of others, nor need they hold to Descartes' stringent demand for certainty. Let's call this procedure the social epistemic reboot. In the social epistemic reboot, the agent is permitted, during the belief re-acquisition process, to trust that things are as they seem and to trust in the testimony of others. But they must begin afresh socially, by re-considering all testimonial sources with presumptive equanimity, without deploying their previous credentialing beliefs. Furthermore, they must discard all their other background beliefs, because those potentially arose from the flawed credential structure of the echo chamber, and very likely have been designed to support and reinforce that very credential structure. Our rebooter must take on the social epistemic posture that we might expect of a cognitive newborn: one of tentative, but defeasible, trust in all apparent testimonial sources (Burge 1993) (Nguyen 2011). This method will, if successfully applied, undo the historical dependence of our epistemic agent and remove the undue influence of the echo chamber. The social epistemic reboot is, theoretically at least, the escape route we've been searching for. 12This reboot, described in such clinical terms, might seem rather fantastical. But it is not, I think, utterly unrealistic. Consider the stories of actual escapees from echo chambers. Take, for example, the story of Derek Black, who was raised by a neo-Nazi father, groomed from childhood to be a neo-Nazi leader, and who became a teenaged breakout star of white nationalist talk radio. When Black left the movement, he went through years-long process of self-transformation. He had to completely abandon his belief system, and he spent years rebuilding a world-view of his own, immersing himself broadly and open-mindedly in everything he'd missed -pop culture, Arabic literature, the pronouncements of the mainstream media and the US government, rap -all with an overall attitude of trust (Saslow 2016). Of course, all we have shown so far is that the social epistemic reboot would, if pulled off, undo the effects of an echo chambered upbringing. Whether or not an epistemic agent might reasonably be expected to reboot, or blameworthy for failing to reboot, is a separate and significantly more difficult question. First, a social epistemic reboot might be psychologically impossible, or at least beyond what we could reasonably expect of normal epistemic agents. Second, what reason would an epistemic agent have to undertake a social epistemic reboot? Such an undertaking would be justified only if the agent had a significant reason to think that their belief system was systematically flawed. But echo chamber members don't seem likely to have access to any such apparent reason. After all, they have clear and coherent explanations for all the evidence and testimony they encounter. If this is all right then we arrive at a worrying conclusion: that echo chambers may, theoretically, be escapable, but we have little reason to expect members of echo chambers realize that they are members of something that needs escaping. What could hope do we have, then, of motivating a reboot? Derek Black's own story gives us a hint. Black went to college and was shunned by almost everyone in his college community. But then Matthew Stevenson, a Jewish fellow undergraduate, began to invite Black to his Shabbat dinners. Stevenson was unfailingly kind, open, and generous, and he slowly earned Black's trust. This eventually lead to a massive upheaval for Black -a slow dawning realization of the depths to which he had been systematically misled. Black went through a profound transformation, and is now an anti-Nazi spokesperson. The turning point seems to be precisely that Stevenson, an outsider, gained Black's trust. And this is exactly where we should expect the turning point to be. Since echo chambers work by building distrust towards outside members, then the route to unmaking them should involve cultivating trust between echo chamber members and outsiders. In order to motivate the social epistemic reboot, an echo chamber member needs to become aware of how in the echo chamber's grip they are, and forming a trust relationship with an outsider might could mediate that awareness. But how that trust could be reliably cultivated is a very difficult matter, and a topic for future investigation. We have, however, arrived at a tentative moral of the story. Echo chambers work by a manipulation of trust. Thus, the route to undoing their influence is not through direct exposure to supposedly neutral facts and information; those sources have been preemptively undermined. It is to address the structures of discredit --to"
        ],
        "ground_truth_definitions": {
          "epistemic bubble": {
            "definition": "a social epistemic structure in which some relevant voices have been excluded through omission.",
            "context": "However, the modern conversation often fails to distinguish between them. Loosely, an epistemic bubble is a social epistemic structure in which some relevant voices have been excluded through omission. Epistemic bubbles can form with no ill intent, through ordinary processes of social selection and community formation.",
            "type": "explicit"
          },
          "echo chamber": {
            "definition": "a social epistemic structure in which other relevant voices have been actively discredited.",
            "context": "But when we also use those same social networks as sources of news, then we impose on ourselves a narrowed and selfreinforcing epistemic filter, which leaves out contrary views and illegitimately inflates our epistemic self-confidence. An echo chamber, on the other hand, is a social epistemic structure in which other relevant voices have been actively discredited. My analysis builds on Kathleen Hall Jamieson and Frank Capella's work, with some philosophical augmentation.",
            "type": "explicit"
          },
          "belief polarization": {
            "definition": "the tendency of individuals, once they believe that p, to increase their belief that p.",
            "context": "Here is one possible escape route. Consider Thomas Kelly's discussion of belief polarization. Belief polarization is the tendency of individuals, once they believe that p, to increase their belief that p. Kelly argues that belief polarization works by the mechanism that, once an agent has acquired a belief, they tend to subject counter-arguments to greater scrutiny and give supporting arguments a relative pass.",
            "type": "explicit"
          },
          "orwellian double speak": {
            "definition": "deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker.",
            "context": "This phenomenon stands in stark contrast to accounts of obfuscatory speech. Take, for instance, Orwellian double speak - deliberately ambiguous, euphemism-filled language, designed to hide the intent of the speaker (Orwell 1968). Double speak is a practice that evinces no interest in coherence, clarity, or truth.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "1acbc58041a408e33ba33e2f0af5808f2bc56ff1",
        "sections": [
          "C rop improvement is important to increase agricultural pro- ductivity and to contribute to food and nutrition security. The need for new crop varieties is exacerbated by climate change. Farmers need to replace crop varieties with better-adapted ones to match rapidly evolving climate conditions (1)(2)(3)(4). Where suitable modern varieties do not exist, suitable farmer varieties are needed instead (\"variety\" is applied to all cultivated materials here) (4). The variety replacement challenge has yet to be effectively addressed. One proposed solution is to increase variety supply by accelerating crop breeding, removing older varieties from the seed supply chain, and assiduously promoting new varieties for farmers (2). Supply-driven variety replacement requires that new varieties are locally adapted and acceptable, but varieties are often recommended without prior geographic analysis to determine recommendation domains (5) on the basis of trials that do not adequately represent local production conditions (6)(7)(8). Therefore, a supply-driven approach may introduce varieties that perform worse than locally grown varieties. Demandoriented approaches address this issue but also fall short of a solution. They involve farmers directly in the selection of crop varieties in on-farm experiments (6). Farmer-participatory selection stimulates local interest in new varieties and produces information on variety performance that is immediately relevant to local climate adaptation. This local focus is a strength as well as a limitation. Scaling is constrained by the resource-intensive nature of current participatory experimental methods and the incompatibility of datasets across different efforts (9). The resulting paucity of data is a problem, because variety trials need to capture spatiotemporal environmental variation to characterize climatic responses. A solution could come from a more scalable type of participatory research: citizen science using digital \"crowdsourcing\" approaches (10)(11)(12). This has already shown its potential to engage large numbers of volunteering citizen scientists who jointly generate sizable datasets that allow for geospatial analysis of climate change impact (for example, on cross-continental Significance Climate adaptation requires farmers to adjust their crop varieties over time and use the right varieties to minimize climate risk. Generating variety recommendations for farmers working in marginal, heterogeneous environments requires variety evaluation under farm conditions. On-farm evaluation is difficult to scale with conventional methods. We used a scalable approach to on-farm participatory variety evaluation using crowdsourced citizen science, assigning small experimental tasks to many volunteering farmers. We generated a unique dataset from 12,409 trial plots in Nicaragua, Ethiopia, and India, a participatory variety evaluation dataset of large size and scope. We show the potential of crowdsourced citizen science to generate insights into variety adaptation, recommend adapted varieties, and help smallholder farmers respond to climate change. bird migration) (13). In a similar way, farmer citizen scientists could provide information about crop variety performance, which would feed into a demand-driven, scalable solution to varietal climate adaptation. To test this idea, we applied a recently developed citizen science approach tricot-triadic comparisons of technologies (14,15). In tricot variety evaluation, each farmer plants seeds from a personal test package of three varieties, which are randomly assigned from a larger pool of tested varieties. Farmers' independent on-farm observations are compiled and analyzed centrally. A simple ranking-based feedback format allows even farmers with low literacy skills to contribute their evaluation data through various channels, including mobile telephones (15). Pilots with the tricot approach have established its potential to produce accurate data (16) and to engage motivated farmers as citizen scientists (17). The question that we address is if tricot trials can provide robust, actionable information on varietal climate adaptation. We organized tricot trials to obtain a dataset covering 842 plots of common bean in Nicaragua, 1,090 plots of durum wheat in Ethiopia, and 10,477 plots of bread wheat in India (Fig. 1). The trials captured environmental variation through broad sampling both spatially (many fields distributed across the landscape) and temporally (different seasons and planting dates). We linked farmers' observations via their geographic coordinates and planting dates to agroclimatic and soil variables. We modeled the influence of the environmental variables on the probability that varieties outperform the other varieties in the trials. We evaluated whether seasonal climate adequately predicts variety performance in the tricot trials. Then, we explored if climatic analysis of tricot trial data improves variety recommendations.",
          "Cross-validation showed that the tricot trials uncovered statistically robust differences in variety performance (Table 1). From a previous pilot study, we expected consistently positive, but low to moderate, pseudo-R 2 values (16). In this study, model fit was comparatively low for bread wheat in India (0.04-0.09), moderate for common bean in Nicaragua (0.15-0.20), and high for durum wheat in Ethiopia (0.39-0.48). The three case studies each provide independent confirmation of the predictive value of the tricot trials. Various factors influenced model fit, includ-  ing farmers' observation skills and environmental variation. The largest differences were between countries, which were probably due to the different levels of diversity within the sets of varieties. Indian and Nicaraguan farmers evaluated a small, carefully selected group of modern varieties with relatively homogeneous performance. In Ethiopia, farmers tested a diverse set of modern and farmer varieties drawn from a wide area and evidently found easily observable differences in performance between varieties. For each country, we modeled the environmental influence on variety performance. We were specifically interested in models with covariates derived from seasonal climatic conditions (climate in Table 1), because these covariates can potentially enhance extrapolation of variety performance predictions across time and space. In all cases, these models had indeed a better fit than the respective model without environmental covariates (no covariates in Table 1). The next question that we addressed was if the models with climatic variables captured the main environmental factors or missed important aspects. Therefore, we compared these models with two other types of models. One type of model includes covariates that represent the experimental design and are known in advance: geolocation, season, planting dates, and soil categories (design in Table 1). These models reflect how multilocation trials are often analyzed and capture variation in terms of the trial structure but not in terms of the underlying climatic causal factors, hence limiting the potential of extrapolation beyond the trial. In all cases, the models with climatic covariates slightly outperformed the models with trial design covariates. This means that the climatic covariates contain unique and substantial information explaining varietal performance. A second comparison was with models that include the climatic covariates together with additional covariates that represent geographic structure (climate + geolocation in Table 1). This comparison tested if important local factors are being overlooked that are not covered by the climatic covariates. Adding these geolocational variables did not improve the models, however, and even slightly degraded them. This implies that no large-scale geographical structure remained after accounting for seasonal climate. From this analysis, it is clear that the models with climatic covariates captured a large part of the environmental variation in variety performance. Therefore, in subsequent analyses, we focused on models with climatic covariates only. We generated generalizable models that afford extrapolation across seasons of variety performance predictions by selecting those climatic variables that contribute to predictivity across seasons. The variable selection procedure retained one climatic variable in each case (Fig. 2 and SI Appendix, Fig. S1). We discuss the results for each case study. For Nicaragua, Fig. 2 shows the Plackett-Luce tree (PLT) with the retained variable of the generalizable model for common bean. We found that bean variety performance changed when the maximum night temperature exceeded 18.7 • C. This finding corresponds to the threshold temperature for heat stress reported in the literature of 20 • C at night (18). Our estimate is slightly lower than the reported threshold but refers to land surface temperature rather than air temperature. Three For durum wheat in Ethiopia, varietal differences in performance were related to the lowest night temperature during the vegetative period (SI Appendix, Fig. S2). Performance patterns changed when at least one 8-day period had average night temperatures under 8.4 • C. This temperature corresponds to the threshold temperatures for vernalization and cold acclimation induction (19). Under warm conditions, vernalizationrequiring varieties will delay flowering. Under cold conditions, cold-sensitive varieties will reduce their yield due to chilling or frost damage. Most of the varieties tested in Ethiopia were farmer varieties and likely adapted to their original environments, which may have led to differences in adaptiveness between varieties. To test the effect of local adaptation, we compared cold-adapted varieties with cold-sensitive farmer varieties as detected by the tricot trials (Materials and Methods). Coldadapted varieties came from higher elevations (2,483 ± 113 meters above sea level) than cold-sensitive ones (2,101 ± 485 meters), a significant difference [t(594) = 16.1, P < 2.2 • 10 -16 ]. Our results indicate that cold tolerance is a main geographic adaptation factor for durum wheat in the Ethiopian highlands. For bread wheat in India, varietal performance patterns changed with the diurnal temperature range (DTR) during the vegetative period, which is the difference between minimum and maximum daily temperatures (SI Appendix, Fig. S3). Splits occurred at DTR values of 14.5 • C and 15.7 • C. Between these two values, the varieties showed very similar performance. Many varieties that performed above average under high DTR performed below average under low DTR and vice versa. Some varieties performed well under both high and low DTR, especially HD 2967. Our interpretation is that low and high ranges of DTR are related to different sets of stresses, while the middle range has relatively low stress. DTR has an impact on crop yield through several mechanisms: high DTR is associated with increased heat or cold stress, and low DTR is associated with high cloud coverage, low solar radiation, and high rainfall. Consistent with our results, a study has shown that DTR explains a substantial share of wheat yield variation in India (20). This same study found that DTR has a negative correlation with wheat yields in some areas and a positive correlation in other areas, in line with high and low DTRs having an association with different types of crop stress.",
          "We examined four ways in which climatic analysis afforded by tricot trials can improve variety recommendations. First, a potential improvement is that climatic analysis corrects the climatic sampling bias, a bias that occurs when trials are performed under unrepresentative seasonal climate conditions, thereby degrading variety recommendations. To assess the importance of climatic sampling bias, we followed the cross-validation procedure used to generate the generalizable models but did not use the seasonal climate data for predictions. Instead, we predicted variety performance for a representative 15-y base period of seasonal climate data and averaged the results (average season in Table 2). The averaged prediction had slightly higher pseudo-R 2 values than the \"no covariates\" model in all cases. This analysis shows that, even when climatic sampling bias is low, correction can help to further improve predictions. Second, climatic analysis can improve variety recommendations by incorporating seasonal forecasts. Perfect forecast in Table 2 shows that the pseudo-R 2 values increase further when observed climate information is available for prediction. The improvement gained from a perfect forecast was substantially larger than the improvement from sampling bias correction. It requires additional work to quantify the improvement of variety recommendations with a realistic climate forecast skill. It is clear, however, that variety recommendations derived from tricot trials can benefit from seasonal forecasts. Third, climatic analysis can support risk analysis. Table 3 shows the expected probability of outperforming all other varieties, which is a metric of average performance, and a risk metric, worst regret (21)-the largest underperformance of the recommended variety relative to the best variety. These two metrics produced divergent variety recommendations in all three cases (indicated in bold in Table 3). In principle, risk analysis for variety choice is also possible without explicit climatic analysis, but this produces results that are difficult to interpret in terms of climatic causality and requires trials during a large number of  The results show how different criteria of variety selection can lead to different recommendations (best value according to each criterion is indicated in bold). Using the probability of winning as a criterion maximizes the average performance but ignores risk. Minimizing worst regret (the loss under the worst possible outcome) is a criterion that takes a conservative approach to risk. seasons to avoid sampling bias and to characterize probability distributions accurately (22). Fourth, climatic analysis of tricot trial data can generate variety recommendations for wider areas through geospatial extrapolation. To illustrate this, we generated maps of varieties recommendations based on \"average season\" model predictions (Fig. 3). In all three cases, geographical patterns of variety adaptation have no relationship to administrative boundaries or agroecological zones, which are commonly used to delineate recommendation domains. To assess what the tricot trial results mean in practice, we contrast our results with existing recommendations. For Nicaragua, we compare the results of the tricot trials with the recommendations of a recent national variety catalog (23). The catalog recommends INTA Rojo and INTA Matagalpa for the study area, but these varieties performed worse than the local varieties in the tricot trials (Fig. 3A). However, the tricot trials identified INTA Fuerte Sequía and INTA Centro Sur as top varieties (Table 3), but the variety catalog recommends them for warm areas outside our study area. In the tricot trials, INTA Fuerte Sequía and INTA Centro Sur outperformed other varieties, especially under heat stress, which apparently occurs with more frequency in our study area than assumed by current variety recommendations. In Nicaragua, then, the tricot trial results show that official variety recommendations fail to identify superior bean varieties that are sufficiently heat tolerant for the study area. For Ethiopia, the Wheat Atlas of the International Maize and Wheat Improvement Center (CIMMYT) recommends modern varieties Hitosa, Ude, and Assassa for all of the Ethiopian highlands, which it classifies as a single \"mega-environment\" (24). The tricot approach produced geographically more specific recommendations (Fig. 3B). With this, we confirm the results of a previous analysis based on multilocational trial data that showed the benefits of location-specific recommendation domains for durum wheat in Algeria, and we show that such an analysis can also be done with tricot data (25). The tricot results confirmed the superiority of farmer varieties 8208 and 208304 (Table 3), which were approved for official variety release in March 2017 (on the basis of other field trials) (26). Farmer variety 208279 also has a high probability of winning, but it has a high value of worst regret (Table 3). Our analysis suggests that 208279 could be considered for the coldest areas as shown in Fig. 3B. In Ethiopia, the tricot trial findings improve variety recommendations for durum wheat by uncovering the importance of cold adaptation. For India, we compare our findings with the front-line demonstrations of the Indian Institute for Wheat and Barley Research (IIWBR); the 1-ha plots demonstrate new varieties by comparing them with a check variety. IIWBR promoted the variety HD 2967 for the North-Eastern Plain Zone during 2016-2017 (27). HD 2967 was indeed the top variety in the tricot trial among the varieties considered by the IIWBR (Table 3). In the tricot trials, however, K 9107 (a variety released in 1996) outperformed HD 2967 (released in 2011), with a comparable level of worst regret (Table 3). The tricot trials also showed that another variety, HD 2733, outperformed HD 2967 in a large part of the study area (Table 3). In the IIWBR front-line demonstrations, HD 2733 was included as a check variety in four areas and was outyielded by HD 2967 in only one of four areas, while in the other three, the yield difference was not significant (27). Our analysis shows that HD 2733 generally does better than HD (Fig. 3C). In India, the analysis of the tricot trial data adds geographic specificity to the existing variety recommendations and suggests that a broader set of wheat varieties should be promoted to take into account the climatic differences across the study area. We quantified how much farmers can benefit from tricotbased variety recommendations by calculating variety reliability, the probability of outperforming a check variety (Eq. 2 in Materials and Methods). For each location, we compared the tricot-recommended variety (Fig. 3) with the bestperforming variety from the previous recommendations as the check. Reliabilities ranged from 0.59 to 0.65 in Ethiopia, from 0.58 to 0.60 in Nicaragua, and from 0.51 to 0.62 in India (SI Appendix, Fig. S4), indicating substantial benefits for large areas.",
          "The main question that we addressed is whether on-farm participatory crop trials, scaled through a farmer citizen science approach, can generate insights into climate adaptation of varieties. Citizen science data revealed generalizable relations between seasonal climate variables and crop variety performance that corresponded to known yield-determining factors. Climatic analyses of these data were shown to improve variety recommendations. Our study demonstrates that, in vulnerable, low-income areas, climatic analysis of variety performance is possible with trial data generated directly by farmer citizen scientists on farms. Arguably, similar results could be achieved by a combination of existing approaches (target environment characterization, multilocation trials, participatory variety selection, variety dissemination). The unique contribution of the tricot approach is that it integrates aspects of these approaches into a simple trial format that addresses the challenge of variety replacement for climate adaptation in a way that is, at the same time, scalable and demand led. Tricot trials can track climate trends as they manifest themselves on farms, adjust variety recommendations and recommendation domains, and contribute to understanding how climate affects on-farm varietal performance. Trial analysis combines insights in climatic adaptation mechanisms with a comprehensive evaluation of variety performance from the perspective of farmers, the end users of the seeds. Results can, therefore, be directly translated into actionable information for climate adaptation on the ground. The findings can serve to create variety portfolios that diminish climate risk (22), can feed into climate information services in combination with seasonal forecasts (28), and can become part of decentralized plant breeding strategies for climate adaptation (8). Combining the tricot trial data with other data could generate additional insights into variety performance and acceptability as influenced by environmental (11), socioeconomic (29), and genomic (30) factors. The tricot approach facilitates engaging large numbers of farmers in citizen science trials with large sets of varieties. Scaling does not only involve an expansion in terms of numbers and scope, however, but also, it implies new institutional arrangements. Carefully designed strategies should foster communication between providers and users of information (31). Wide-ranging collaborations are needed for climate adaptation in crop variety management, involving farmers, extension agents, seed retailers, seed producers, plant breeders, and climate information providers. The tricot approach can help to cut across these different domains, because it is able to link climatic and varietal information directly to farmer decision making. With appropriate institutional support and investment, citizen science can potentially make an important contribution to farmers' adaptive capacity and to the mobilization of crop genetic diversity for climate adaptation.",
          "Crop Trials. Trials were performed between 2012 and 2016 during three cropping seasons in Ethiopia, five cropping seasons in Nicaragua, and four cropping seasons in India (SI Appendix, Table S1). Trial design followed the tricot citizen science approach (14,15). Sets of varieties were allocated randomly to farms as incomplete blocks (7), maintaining spatial balance by assigning roughly equal frequencies of the varieties to each area. In Nicaragua and India, incomplete blocks contained three varieties. In Ethiopia, we used a modified approach that included four varieties per farm. Plots were small to facilitate farmer participation but in all cases, large enough to avoid strong edge effects. Farmers indicated the relative performance of varieties through ranking. Ranking is a robust data collection approach that avoids observer drift (32) and allows for aggregation across disparate datasets (33). The trials required three moments of contact with the farmers: (i) explaining the experiment and distributing the seeds, (ii) collecting evaluation data, and (iii) returning the results. Data were initially collected using paper forms and in subsequent seasons, through electronic formats linked to a purposebuilt digital platform, https://climmob.net. In the trials presented here, field agents collected the data through visits (phone calls are also feasible). Data Analysis. All analyses were done in R (34). For the analysis of the variety-ranking data generated by farmers, we used the Plackett-Luce model (35,36). The Plackett-Luce model estimates for each variety the probability that it wins, beating all other varieties in the set. The model determines the values of positive-valued parameters α i (worth) associated with each variety i. These parameters α are related to the probability that variety i wins against all other n varieties in the following way: The probability that variety i beats another variety j is calculated in a similar way. Eq. 2 also serves to calculate the reliability of a variety-its probability of beating a check variety (37). These equations follow from Luce's Choice Axiom, which states that the probability that one item beats another is independent from the presence or absence of any other items in the set (36). We report worth values that sum to one. This makes each worth value α i equal to the probability of variety i outperforming all other varieties: In the trials, we used rankings of three varieties (i j k), which have the following probability of occurring according to the Plackett-Luce model: P(i j k) = P(i {j, k}) • P(j k). [4] The log likelihood for a ranking i j k follows from Eqs. 1, 2, and 4 and takes the following form (38): (α) = ln(P(i {j, k})) + ln(P(j k)) = ln (α i )-ln α i + α j + α k + ln α j -ln α j + α k . [5] The log likelihood is then the sum of the log-likelihood (α) values across all rankings. Using an iterative algorithm, the log likelihood is maximized to identify the α values that make the observed rankings most probable. We also generated quasi-SEs for α (39). To take into account covariates, we created PLTs through recursive partitioning (40). Additional details are given in SI Appendix."
        ],
        "ground_truth_definitions": {
          "climatic sampling bias": {
            "definition": "a bias that occurs when trials are performed under unrepresentative seasonal climate conditions",
            "context": "We examined four ways in which climatic analysis afforded by tricot trials can improve variety recommendations. First, a potential improvement is that climatic analysis corrects the climatic sampling bias, a bias that occurs when trials are performed under unrepresentative seasonal climate conditions, thereby degrading variety recommendations. To assess the importance of climatic sampling bias, we followed the cross-validation procedure used to generate the generalizable models but did not use the seasonal climate data for predictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 π r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP α for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return α (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "1538c4777271ae6abb542801dac01423f4d566ad",
        "sections": [
          "Compared with many other disciplines in business and the social sciences, Information Systems (IS) is a relatively young field. In its initial development stage, IS was perceived as an applied discipline that almost exclusively drew on other, more fundamental, reference disciplines [Keen, 1980]; its progress and maturation became a preoccupation to some researchers [Banville and Landry, 1989]. Now, IS \"fully emerged as a discipline in its own right\" [Baskerville et al., 2002;p. 1]. The development of IS as a scientific field is evidenced by the building of solid research tradition. In an empirical study of the disciplines cited in IS journals, Vessey et al. [2002] found a substantial volume of IS research used IS itself as the reference discipline (27% in their analysis). A survey [Palvia et al., 2004] of the use of different research methodologies in IS showed that about 6.6% of studies published in seven IS journals during the years 1993-2003 used literature reviews as the primary research methodology. IS is also emerging as an important reference discipline for other fields [Baskerville et al., 2002]. A search of the Social Science Citation Index (SSCI) showed that it is common for many fields to conduct research based on IS theories or models. A search with the keyword of \"information systems or information technology\" resulted in a total of 719 articles published in 329 refereed journals in 2004. Of these journals, 26 are IS-focused, while the other 303 journals are from various fields covering almost all the research areas of business and social sciences. A good example of this use of IS theory in other areas is the technology acceptance model (TAM), which was tested and applied in psychology [e.g., Gentry et al., 2002], education [e.g., Liaw 2002;Selim, 2003], marketing [e.g., Keen et al., 2004], operations management [e.g., Olson et al., 2003], and many other management fields [King and He, 2005]. However, IS research inevitably displays one problem that is common in many mature fields: inconsistent empirical findings on essentially the same question. Knowledge accumulation increasingly relies on the integration of previous studies and findings. As Glass [1976] suggested, when the literature on a topic grows and knowledge lies untapped in completed research studies, \"this endeavor (of research synthesis) deserves higher priority .... than adding a new experiment or survey to the pile\" [Glass, 1976;p. 4]. A recent trend in research synthesis is to integrate, quantitatively, knowledge garnered from empirical studies on a topic by using meta-analysis. Meta-analysis is the most commonly used quantitative research synthesis method in the social and behavioral sciences [Hedges and Olkin, 1985]. It won recognition as a better way to conduct reviews of a body of completed research than the traditional narrative fashion [Wolf, 1986;Hunter and Schmidt, 1990;Rosenthal, 1991;Cooper and Hedges, 1994;Rosenthal and DiMatteo, 2001]. Some journals revised their review policy to encourage the use of this methodology [e.g., From the Editors of Academy of Management Journal, 2002]. In IS, however, meta-analysis is extremely underutilized [Hwang, 1996]. Moreover, some of the meta-analytic practices used in meta-analysis in IS are conceptually or methodologically flawed [King and He, 2005]. The objective of this paper is to provide guidelines for IS researchers who are interested in synthesizing a body of literature in a rigorous and quantitative fashion. After reviewing and comparing different literature synthesis methods (Section II), we turn our attention to metaanalysis. We describe the history, common methods, and recent developments of meta-analysis (Sections III and IV). We also provide a discussion of some major concerns with existing metaanalysis applications (Section V) and the great potential of applying meta-analysis in IS (Section VI).",
          "The refinement and accumulation of information and knowledge are an essential condition for a field to \"be scientific\" and to progress [Hunter et al., 1982;Pillemer and Light, 1980]. Researchers can use a number of techniques for making sense out of existing research literature, all with the purpose of casting current research findings into historical contexts or explaining contradictions that might exist among a set of primary research studies conducted on the same topic [Rumrill and Fitzgerald, 2001]. Many researchers dichotomize literature review methods as qualitative versus quantitative reviews [e.g., Wolf, 1986;Aldag and Stearns, 1988;Hunter and Schmidt, 1990;Rosenthal and DiMatteo, 2001;and Palvia et al., 2003]. This approach may be overly simplistic in that different review techniques vary in the extent of systematically synthesizing an existing literature body, ranging from purely qualitative (e.g., verbal description) to moderately quantitative (e.g., counting a number or calculating a percentage of certain research characteristic) to purely quantitative (e.g., meta-analysis). Following Guzzo et al.'s [1987] approach, we categorize the most commonly employed review techniques in IS along a continuum of quantification as narrative reviews, descriptive reviews, vote counting, and meta-analysis (Figure 1). ",
          "Narrative reviews present verbal descriptions of past studies focusing on theories and frameworks, elementary factors and their roles (predictor, moderator, or mediator), and/or research outcomes, (e.g., supported vs. unsupported) regarding a hypothesized relationship. Narrative reviews are of great heuristic value, and serve to postulate or advance new theories and models, to examine important and/or controversial topics, and to direct further development in a research domain. No commonly accepted or standardized procedure for conducting a narrative review exists. This lack is a key weakness of narrative reviews as a means of arriving at a firm understanding of a research tradition [Green and Hall, 1984;Hunter and Schmidt, 1990;Rosenthal and DiMatteo, 2001]. Researchers are relatively free to design their review strategy in terms of selecting relevant papers, categorizing research characteristics, and framing outcomes. When conducting a narrative review, researchers tend to consciously or subconsciously make judgments that support their own background, understanding, or established point-of-view. Often their goal is to come to some conclusions through classifications of the research methods and categorizations of results. One commonly-used strategy is to create a classification or a typology to organize the results. Researchers adopting this approach should be cautious that the creation of categories may result in less information since quantitative scales may be reduced to qualitative ones. Of course, this approach can result in greater understanding and lead to greater insight, but such a result is certainly not assured. Ives and Olson's [1984] study on user involvement may serve as a representative narrative review in IS. Ives and Olson proposed a model of user involvement prior to their review of the literature; and this model framed their discussions of the selected papers. Ives and Olson's [1984] work is considered influential in the field of user involvement.",
          "Descriptive reviews introduce some quantification, often a frequency analysis of a body of research. The purpose is to find out to what extent the existing literature supports a particular proposition or reveals an interpretable pattern [Guzzo et al., 1987]. To assure the generalizability of the results, a descriptive review often involves a systematic search of as many as relevant papers in an investigated area, and codes each selected paper on certain research characteristics, such as publication time, research methodology, main approach, grounded theory, and symbolic research outcomes (e.g., positive, negative, or non-significant). A frequency analysis (including its derivatives of trend analysis and cluster analysis) treats an individual study as one data record and identifies distinct patterns among the papers surveyed. In doing so, a descriptive review may claim its findings to represent the fact or state of a research domain. Palvia et al.'s [2003;2004] analyses of the use of research methodologies in IS are typical descriptive reviews. In the two studies, Palvia and colleagues surveyed articles in seven IS journals (communications of the ACM, Decision Sciences, Information & Management, Information Systems Research, Journal of Management Information Systems, and Management Science) in the years 1993 to 2003. They coded each article for up to two methodologies (primary methodology and secondary methodology), and calculated the frequency and analyzed the trend of each of thirteen research methodologies as used in these papers. The results \"provide the current state of research methodologies in use\" [Palvia et al., 2004;p. 306].",
          "Vote counting, also called \"combining probabilities\" [Rosenthal, 1991] and \"box score review\" [Guzzo et al., 1987], is commonly used for drawing qualitative inferences about a focal relationship (e.g., a correlation is significantly different from 0 or not) by combining individual research outcomes [Pickard et al., 1998]. Some researchers consider vote counting a meta-analytic technology [e.g., Rosenthal, 1978;1991]; and other researchers separate vote counting as an alternative quantitative review method mostly because this method does not analyze effect sizes. It uses the outcomes of tests of hypothesis reported in individual studies, such as probabilities, p-levels, or results falling into three categories: significantly positive effect, significantly negative effect, and non-significant effect. The philosophy is that repeated results in the same direction across multiple studies, even when some are non-significant, may be more powerful evidence than a single significant result [Rosenthal and DiMatteo, 2001]. Rosenthal [1991] provided a comprehensive review of nine different vote counting methods, and discussed the advantages and limitations of each. For illustrative purposes, in Appendix 1 we provide the computational formulas of two most popular vote counting methods: Fisher's procedure of combining p's and Stouffer's procedure of combining Z's. Vote counting does not require other statistics such as effect sizes and construct reliabilities. Thus, it is a conceptually simple and practically convenient method. Under certain circumstances (i.e., small number of sampled studies, when investigated effects are in the same direction) this method could produce statistically powerful results. In one extreme example, Cohen [1993] reported on two studies that dealt with the results of vaccinating monkeys. Because laboratory animals are difficult to obtain and expensive to maintain, the studies involved only six and eleven monkeys respectively (both experimental subjects and control animals). Neither study produced statistical significance. However, when the data were combined in a vote counting analysis, the plevel was considerably smaller and the effect was shown to be large. Vote counting contains some inherent limitations. In particular, it allows a weak test of a hypothesis (e.g., correlation is 0 or not 0) with little consideration of the magnitude of the effect, such as an estimated effect size and associated confidence intervals. In addition, vote-counting assumes homogeneity in the sample population investigated. In case of heterogeneity, which is most common in research, vote-counting cannot detect moderator effects, and the combined significance could be meaningless. Therefore, vote-counting is often suggested as a supplement to meta-analysis in the case of missing effect sizes [Bushman and Wang, 1995;Pickard et al., 1998]. In IS, vote counting is applied to produce a single quantitatively synthesized conclusion from a series experiments. In some fields within IS, such as software engineering, where much research involves modest experimental effects, small sample sizes, and hypothesis testing fails to conclude significant results due to low statistic power, vote counting is found to be particularly useful. For example, in a study of reading techniques for defect detection of software codes, Laitenberger et al. [2001] found that perspective-based reading (PBR) was statistically more effective than checklist-based reading (CBR) in one out of three experiments; the other two were in the same direction but not significant (p-value (one side) = 0.16 and 0.13). When applying Fisher's procedure, Laitenberger and colleagues concluded that PBR was a significantly more effective reading technique for defect detection than CBR, with a combined p-value = 0.000016 (p. 403). Similarly, Pfahl et al. [2004] applied vote counting to analyze results from a series of three experiments to assess the learning effectiveness of using a process simulation model for educating computer science students in software project management. The results supported that simulation involves more learning interests of students than that of without-simulation students (pvalues from the 3 experiments were 0.04, 0.21, and 0.28; the combined p-value were 0.06 (Fisher's procedure) and 0.03 (Stouffer's Z procedure) (p. 137-138).",
          "Meta-analysis is a statistical synthesis method that provides the opportunity to view the \"whole picture\" in a research context by combining and analyzing the quantitative results of many empirical studies [Glass, 1976]. It connotes a rigorous alternative to the narrative discussion of research studies which typify our attempts to make sense of the rapidly expanding research Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He literature [Wolf, 1986]. Since we believe that meta-analysis is of great potential significance and find that it is often inadequately applied in IS, we devote the remainder of this paper to it.",
          "The history of the use of statistical methods to synthesize research findings is as long as most commonly-used statistical procedures [Cooper, 1979]. Hedges [1992] credited Legendre, who invented the principle of least squares to solve regression problems, as an innovator in this area because his original purpose was to combine information across studies. Another often-cited example is Pearson's [1904] study of enteric fever inoculation, in which Pearson presented sets of data collected under different conditions, calculated individual effect sizes that were comparable across data sets, examined homogeneity, and computed an average of the effect sizes to determine the effectiveness of the treatment (inoculation). However, applications of this quantitative research synthesis method were rare before the 1970s [Cooper and Hedges, 1994]. It seems that the method grew in popularity since Glass coined the phrase \"meta-analysis\" in 1976. [Glass, 1976, p.3] Meta-analysis is now a legitimate research review tool accepted by, and dominant in, many fields in the social and behavioral sciences [Field, 2001]. We searched a comprehensive social science database -Social Science Citation Index (SSCI) -with the keyword \"meta-analysis\". The search resulted in 4407 articles published in refereed journals in all covered fields for the period 1992-2004, with an average annual increase of 20 articles (p-value < 0.0001).",
          "In IS, some researchers argued that meta-analysis is a rarely applied research methodology [e.g., Hwang, 1996;Palvia et al., 2003], mostly because of the small number of applications. For example, Hwang (1996) reviewed the use of meta-analysis in IS prior to 1996 and found 6 applications, including one conference paper. We searched SSCI with the restriction of IS as the research domain. The search resulted in 34 published meta-analyses in 1992-2004, with an average annual increase of 0.3 article (p-value = 0.013). The numbers of published metaanalyses in social science in general and in IS in particular are presented in Figure 2. As evidenced in Figure 2b, the use of meta-analysis increased steadily in IS, although the numbers of applications are still small. We believe this trend will be maintained, given the maturating of the field and the growing number of research topics that would benefit from conclusive answers.",
          "Meta-analysis is much less judgmental and subjective than other literature review methods, particularly narrative review, and therefore much more in tune with positivist tradition. The major difference between narrative reviews and quantitative meta-analyses may well be that narrative reviews primarily focus on the conclusions reached in various studies, whereas meta-analyses focus on data, as reflected by the operationalization of variables, the magnitude of effect sizes, and the sample sizes. Qualitative assessments involved in a narrative reviews usually do not take account of both the relative sizes of samples and the significance of the effects measured. For example, a smallsample study with significant results may be equally weighted with a similar large-sample study in such assessments, while an insignificant result may be ignored when it does, in fact, contribute to a body of research that overall, may show significant effects. To synthesize a research literature, statistical meta-analysis uses final results collected from a collection of similar studies [Glass, 1976]. The final results are effect sizes, or the magnitude of the effects. The focus on effect sizes rather than significances of these empirical findings is an advantage over traditional literature review methods. Meta-analysis enables the combining of various results, taking into account the relative sample and effect sizes, thereby permitting studies showing insignificant effects to be analyzed along with others that may show significant effects. The overall result may be either significant or insignificant, but it is undoubtedly more accurate and more credible because of the overarching span of such an analysis. Meta-analysis can focus attention on the cumulative impact of insignificant results that can be significant. For example, two studies showing sgnificance at the 0.06 level are much stronger evidence against the null hypothesis that is a single study at the 0.05 level, all else being equal. Yet, in some fields, the former two studies may not ever be published. Meta-analysis, by combining such results, enables us to see the big picture of the landscape of research results. In other fields, meta-analysis has provided answersto questions that were in great dispute because of conflicts in the results of various studies. For example, the viewing of violence on television was shown to be associated with a greater tendency toward aggressive and anti-social acts through a meta-analysis of more than 200 studies, many of which were individually inconclusive or had reached contrary conclusions [Comstock et al., 1991]. In business research, perhaps the area which made the greatest use of meta-analysis is organizational behavior. There, meta-analyses seem to have been conducted primarily in the source of job performance ratings involving combinations of ratings by supervisor, subordinates, peers and self-ratings [Conway and Huffcutt, 1997;Harris et al., 1988]. While meta-analysis is basically confirmatory in nature, it can also involve exploratory aspects. For example, when high variability exists in the effects that are reflected in various studies, metaanalysis promotes a search for moderator variables. While this use of meta-analysis introduces greater subjectivity on the part of the researchers, the subjectivity is certainly less than that involved in performing a non-quantitative literature review.",
          "Numerous researchers advocated the use of meta-analysis as a better way to conduct research reviews [e.g., Glass, 1976;Hedges and Olkin, 1985;Wolf, 1986;Hunter and Schmidt, 1990;Rosenthal, 1991;Rosenthal and DiMatteo, 2001]. However, as with other research methods, meta-analysis is not free from limitations. In this subsection, we discuss some major concerns with meta-analysis as a research review method.",
          "By its statistic analysis nature, meta-analysis contains an inherent sampling bias toward quantitative studies that report effect sizes. Using Palvia et al.'s [2003;2004] typology, the types of research that may provide data to a meta-analysis include survey, laboratory experiment, field study, and field experiment. Other types of research, such as frameworks and conceptual models, case studies, speculation/commentary, mathematical models, secondary data, interview, and qualitative research, are unlikely to be sampled in a meta-analysis. This limitation suggests that a significant number of studies (according to Palvia et al. [2004], these studies add up to 50.3% of all IS publications in seven leading IS journals) will be ignored when conducting a metaanalysis. In addition, some researchers may combine quantitative and qualitative research methods in their studies of the same phenomenon, called triangulation [Gable, 1994;Kaplan and Duchon, 1988;Lee, 1991;Mingers, 2001]. Triangulation may help deepen and widen our understanding on a certain phenomenon. One example is Markus' [1994] study of the use of e-mail in organizations. Markus conducted a field survey and data did not provide much support to the hypotheses developed from information richness theory; Markus also interviewed some respondents, whose answers and comments provided insight on factors affecting their use of email. If a meta-analysis on communication effectiveness were to be conducted, only the empirical part of Markus's [1994] study would be included and the qualitative part, which provides more value to media use research, would be ignored. The sampling bias toward empirical studies, a limitation of meta-analysis, is rarely addressed in discussions of this research method. We believe that it is important to call to the attention of meta analysts and researchers that the results from a meta-analysis are not necessarily more creditable or representative of a research population than those from a narrative review.",
          "Meta-analysis does not generally differentiate studies by their quality. This issue is often referred to as \"garbage in and garbage out\" [Hunt, 1997]. Research studies vary considerably in their research designs and approaches, sampling units, methods of measuring variables, data analysis methods, and presentations of research findings. The inclusion of the results from poorlyconducted studies with flawed designs into a meta-analysis could confuse the full understanding of the literature body investigated or even lead to unfounded conclusions. The judgment of quality is rather subjective, although some techniques have been suggested to correct this error [e.g., Rosenthal, 1991]. However, these techniques introduce other biases about the selection and weighting of quality criteria.",
          "Publication bias [Begg and Berlin, 1988], also known as the file-drawer problem [Rosenthal, 1979;Iyengar and Greenhouse, 1988], refers to the observation that significant results are more likely to be published while non-significant results tend to be relegated to file drawers. Thus, the meta-analysis result will focus on an unrepresentative proportion of a total research population. Of course, publication bias applies to all review methods. It is \"particularly problematic for a metaanalysis whose data come solely from the published scientific literature\" [Duval and Tweedie, 2000]. Using an unrepresentative set of data may result in conclusions biased toward significance or positivity. Although some correction techniques have been developed [e.g., Duval and Tweedie, 2000], the best solution to avoid this bias is to search multiple databases in a systematic way and sample studies from various sources [Rosenthal and DiMatteo, 2001]. For example, in their study of the effects of management support and task interdependence on IS implementation, Sharma and Yetton [2003] searched the literature in various ways, including bibliographic databases, manual searches, and the bibliographies of existing works. They located 22 empirical studies, of which 11 are from journal publications, 9 from dissertations, and 2 from book chapters. The comprehensive search resulted in a diverse sample that \"both increases the power of the meta-analysis by maximizing the number of studies and reduces (publication) source bias\" (p. 542).",
          "One criticism of meta-analysis is that it may compare \"apples and oranges,\" aggregating results derived from studies with incommensurable research goals, measures, and procedures. It is argued, therefore, that meta-analysis may sometimes be analogous to taking apples and oranges and averaging such measures as their weights, sizes, flavors, and shelf lives [Hunt, 1997]. This problem exists for all review methods, qualitative or quantitative, in that \"exact replications probably cannot occur\" [Hall et al., 1994;p. 20], \"(studies) ... are rarely precisely the same\" [Rosenthal and DiMatteo, 2001;p. 68]. This problem is not of dominant significance, especially when we want results that are generalizable to fruits, or to a broad research domain. On the other hand, synthesists must be sensitive to the problem of attempting aggregation of too diverse a sampling of studies.",
          "The statistical power of detecting a genuine effect size depends on both the number of studies and the total cumulated sample sizes included in a meta-analysis. The more studies that are included in the meta-analysis, the more creditable are the results at representing the investigated research domain. Using a Monte Carlo simulation, Field [2001] found that a meta-analysis should include at least 15 studies, otherwise the type I error (accepting a false null hypothesis) could be severely inflated. If the investigator cannot identify enough empirical studies on a common topic, it may indicate that the research domain is too immature for a conclusive study such as meta-analysis. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He \"when meta-analytical results have less evidential value because the number of individual studies in the meta-analysis is small … the alternative is neither reversion to reliance on the single study nor a return to the narrative review method of integrating study findings: both are vastly inferior to meta-analysis in the information yield\" [Hunter and Schmidt, 1990;p. 420]. Reviewers should base analyses on the largest number of studies available [Matt and Cook, 1994]. If the collection of studies is largely incomplete, then conclusions drawn from analysis are limited in scope. An exception will be meta analyzing a series of experimental studies. Here a researcher is interested in concluding generalizability of a proposition within, rather than beyond, a defined set of studies. For example, Laitenberger et al. [2001] and Phafl et al. [2004] meta analyzed a series of three experiments (an initial experiment and two replications). The combined results were stronger in statistical power than results calculated from any individual experiment.",
          "Comparatively, meta-analysis is a straightforward or \"formalized systematic review\" procedure that is more standardized than other review methods [Hunter and Schmidt, 1990]. Given the same set of data or sampled effect sizes from a literature body, different researchers should arrive at the same conclusion via meta-analysis. In other words, a typical meta-analysis is often based on the procedures and analytic methods that are commonly accepted. Thus, the results from a meta-analysis are often treated as reliable and objective [Rosenthal and DiMatteo, 2001]. However, a particular meta-analysis may contain methodological errors that lead to a false conclusion. We observed serious methodological errors in some IS meta-analyses. For example, in the two meta-analyses conducted by Mahmood andcolleagues [2000, 2001], some combined effect sizes were larger than any sampled individual effect size. In their meta-analysis of the correlations between perceived ease of use and Information technology usage (Table 1 of Mahmood et al. [2001], p. 116), Mahmood and colleagues concluded a combined effect size as large as 0.678, while the sampled correlation coefficients ranged from 0.059 to 0.375. If the result is used by a researcher who performs statistical power analysis at the 80% level to guide a research design (suggested by Cohen [1988Cohen [ , 1992]]), the researcher will conclude that a sample size of 28 may be adequate to detect a significant relationship (at α=0.05 level) between the two variables. In fact, the sampled studies in Mahmood et al. [2001] involved much larger sample sizes, ranging from 61 to 786, with a mean of 185. Statistics Used in a Meta-Analysis. To meta-analyze an issue, the researcher should extract the following statistics and information from the studies: Most IS studies report these statistics in their presentations. In the literature, effect sizes are of various forms and can be categorized into two main families, the r family and the d family. The r family effect sizes report correlations between variables; the specific type depends on whether the variables are in continuous (Pearson's r), dichotomous (phi), or ranked form (rho). In IS research, the most popularly reported effect size is the Pearson's r. The d family are used mostly in laboratory experiments and measure the standardized difference between an experimental group and a control group. (There are three main members in this family: Cohen's d, Hedges' g, and Glass's ∆. The three are calculated in a similar way: the difference between two means is divided by some variance. They differ in the denominator: the square root of the pooled variance of the two groups for Cohen's d, the square root of the pooled variance for Hedges' g, and the square root of the control group variance for Glass's ∆.) These effect sizes can be readily converted to one another. Effect sizes can also be calculated from various testing statistics, such as t, F, χ 2 , or Z, or their associated p levels. Detailed computational formulas to calculate and convert these statistics are beyond the purpose and scope of this study. Interested readers can refer to Wolf [1986], Hunter and Schmidt [1990], Rosenthal [1991], and Rosenthal and DiMatteo [2001]. Other statistics, including the descriptive statistics of investigated variables, cannot be directly used as effect sizes. As pointed out by Rosenthal and Dimatteo [2001], \"an r effect size cannot be computed from kappa, percent agreement, relative risk, risk difference, or the odds ratio unless all the raw data are available so the meta-analyst can compute the proper index\" (p.72). Fixed vs. Random Effects Models, Similar to other statistical methods, meta-analysis methods are developed based on assumptions about the population from which studies are taken. The two common assumptions lead to two different analysis methods: fixed-effects and random-effects models. The fixed-effects model assumes that studies in the meta-analysis are sampled from one population with a fixed \"true\" effect size. In other words, the true effect size is assumed to be the same for all studies included in the analysis, and the observed variance among effect sizes is dominated by sampling errors, which are unsystematic and can be estimated [Hunter and Schmidt, 1990]. The assumption of one population underlying a meta-analysis restricts the conclusions from being generalized to a study not included in the analysis unless the study shows independent evidence of belonging to the population; i.e., unless it is a close replication of the studies included in the meta-analysis. In contrast, the random effects model assumes that population effect sizes vary from study to study. As such, a study included in such a meta-analysis can be viewed as being sampled from a universe of possible effects in a research domain [Field, 2001]. As long as the meta-analysis covers the literature comprehensively, the conclusions are generalizable to the research domain as a whole and can be applied to studies not included in the analysis. In statistical terms the two models differ in the calculation of the weights used in the analysis, which in turn affects the standard errors associated with the combined effect size. Fixed-effects models use only within-study variability in their weights because all other \"unknowns\" in the model are assumed to be constant. In contrast, random-effects models account for the errors associated with sampling from populations that themselves were sampled from a superpopulation. The error term, therefore, contains variability arising from differences between studies in addition to within-study variability. Standard errors in the random-effects model are, therefore, larger than in the fixed case, which makes significance tests of combined effects more conservative [Field, 2003;p. 107]. Although random-effects models generally appear to be superior, fixed-effects models are in common use. If the fixed-effects model is employed for a meta-analysis, the assumption of one \"true\" effect size across studies should be tested before combining effect sizes [Hedges and Olkin, 1985]. The test of this assumption, labeled as a homogeneity test, is a chi-square test of the null hypothesis that all effect sizes are the same after controlling for sampling errors. The test result indicates whether the null hypothesis can be rejected at a certain level. Only when the test result is insignificant can the sampled studies be combined (e.g., calculating the combined effect sizes and their confidence intervals). In many cases, the test indicates a violation of the assumption and there is a need to switch to the random-effects model; however, in practice sometimes the fixed-effects model is chosen and the test is not performed. In contrast, the random-effects model assumes variation between the populations underlying the various studies that are incorporated into the meta-analysis. The homogeneity test in this case examines whether the interaction between sampling error and between-study variance is zero or Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He not. An insignificant test result (which occurs much of the time) justifies the techniques that are used under this model. As suggested by many meta-analysts [e.g., Field, 2001;Field, 2003;Hedges and Vevea, 1998;Rosenthal and DiMatteo, 2001], homogeneity is rare and the randomeffects model should be applied in most research domains.",
          "Over the past 20 years, three methods of meta-analysis emerged to be widely used [Field, 2001;Johnson et al., 1995]: the method devised • by Hedges and Olkin [1985]; • by Rosenthal and Rubin [Rosenthal, 1991], and • by Hunter and Schmidt [1990]. In IS, a fourth method ─ that devised by Glass and his associates [Glass et al., 1981] is also used. The four methods are briefly discussed subsection. Interested readers may examine the references for more detailed discussion on the methodologies. In addition, • Johnson et al. [1995] and Field [2001] compared the first three methods basing on Monte Carlo simulation tests; • Cooper and Hedges [1994] provided integrative reviews of the commonly-adopted meta-analytic approaches as well as computational formulas; • Lipsey and Wilson [2001] described well the bolts and nuts of the whole process. These studies will serve as good resources for potential meta-analysts.",
          "This approach to meta-analysis is based on a weighted least squares technique [Hedges, 1987]. In this approach, study outcomes (i.e., effect sizes r) are transformed into a standard normal metric (using Fisher's r-to-Z transformation). Then, the transformed effect sizes are weighted by the inverse variances of each study. For the fixed-effects model, the variance is within-study variance, which is determined by sample sizes only. For the random-effects model, the variance is composed of within-study variance and between-study variance, the latter of which is from a chi-square test (Q test under the fixed-effects model). The combined effect size is the average of the weighted effect sizes, and its variance is the reciprocal of the sum of the weights. A significance test (Z-test) and confidence intervals of the combined effect size are then calculated.",
          "Under a fixed-effects model, the Rosenthal-Rubin method employs essentially the same techniques as the Hedges-Olkin method, except for the significance test of combined effect size [Field, 2001]. Rosenthal and Rubin [Rosenthal, 1991] advocate the use of significance metrics (i.e., Zs associated with one-tailed probabilities) from sampled studies and examining the combined Z for the overall significance of the mean effect size. Rosenthal [1991] did not present a random-effects version of his meta-analysis procedures in his original work. In a later study, Rosenthal and DiMatteo [2001] suggested \"un-weighting\" the effect sizes when meta-analytically integrating them as an approach based on a random-effects model. The basic logic is to treat studies as the unit of analysis in observing of the between-study variance. Consistent with the random effects model, the combined effect size of the unweightedeffect approach is less statistically powerful and has larger confidence intervals as contrasted with that calculated from the weighted-effect approach, and can be generalized to studies not yet in the sample. \"If only one approach were to be used, it would be the one we prefer\" [Rosenthal and DiMatteo, 2001;p. 70]. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He",
          "This method is distinct for its efforts to correct effect size indices for potential sources of errors before meta-analytically integrating the effect sizes across studies [Johnson et al., 1995]. Besides the sampling error, other sources of errors include measurement error, range variation, construct validity, and variance due to extraneous factors. Errors other than sampling are labeled as \"Attenuation Factors\" in Hunter and Schmidt [1990], because they impact lower the observed correlations between investigated variables systematically, which can be corrected if assessed. Therefore, when information about these sources of error is available, this feature of Hunter-Schmidt method may recommend its use [Johnson et al., 1995]. In IS, at least one source of error, measurement error, is routinely assessed and reported by construct reliability (e.g., Cronbach α) [Chau 1999]. However, of the IS meta-analyses that we identified as using this method, none study explored this feature and corrected measurement error or other sources of error. To test moderator variables, Hunter and Schmidt [1990] suggested a partitioning approach; that is, to break the set of studies into subsets using the moderator variable and to do separate metaanalyses within each subset of studies. Then, the difference between subsets is tested to conclude the magnitude and significance of this moderator variable.",
          "Glass's approach to meta-analysis focuses on the detection of moderator variables. This method can be summarized as: 1. simulation of descriptive statistics across studies; 2. the coding of perhaps 50 to 100 study characteristics, such as date of study and number of threats to internal validity, and 3. regression of study outcome onto the coded study characteristics. The characteristics that show significant effects on the study outcome are considered to be moderators. Using a Monte Carlo test, Hunter and Schmidt [1990, p. 86-89] illustrated that this method has a severe problem of capitalization on (chance) sampling errors. Sampling errors are large because the sample size for looking at study characteristics is not the total number of subjects in the studies, but the number of studies. Correlating various study characteristics with study outcome leads to massive capitalization on chance when the correlations that are large enough to be statistically significant are identified ex post facto. A general suggestion for this approach is to derive few moderators basing on logical reasoning and existing theory before conducting the meta-analysis.",
          "After the term was coined by Glass in 1976, meta-analysis received much research attention. In early 1980s, many discussions of meta-analysis were centered on the legitimacy of this research method, i.e., comparing with other review methods, and identifying the advantages, limitations, and statistically soundness of meta-analysis [e.g., Glass, 1976;Hunter et al., 1982;Chow, 1987;Hedges, 1987]. The discussions of meta-analysis progressed to more advanced methodological topics, such as its application to structural equation modeling [Hom et al., 1992], and levels of analysis [Ostroff and Harrison, 1999]. This section reviews recent developments of meta-analysis in the analysis of moderator effects and mediator effects, the two issues that are most relevant to theory testing/building in IS. Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He",
          "In the context of meta-analysis, a moderator variable is a systematic difference among studies under review that might explain differences in the magnitudes or signs of observed effect sizes. The study of moderator effects often involves two stages: the detection, or exploratory analysis, of possible moderators, and the assessment, or confirmatory analysis, of theoretically suggested moderators. The various moderator identification and assessment methods are shown in Graphing. As sample size increases, the effect size would theoretically approach the population effect size. Sample effects sizes have greater variability with smaller samples than with larger samples. Simple and intuitive, graphing is suggested for preliminarily detecting naturally occurring groupings and possible moderator effects in a meta-analytic data set [Rosenthal and DiMatteo, 2001]. Q Statistics or Chi-Square Test [Hedges and Olkin, 1985]. This test generates a decision rule specifying whether the variability in standardized effect sizes is statistically significant. The test is based on a chi-square assessment of the level of variance across study results relative to the sampling error variance across studies. This test is often referred to as homogeneity test, and can serve as a criterion for selecting a fixed effects model vs. a random effects model for a metaanalysis. Variance Analysis [Hunter et al., 1982]: The variance of sampled effect sizes is corrected for statistical artifacts, such as sampling error, differential reliability, and restriction of range. When artifacts fail to account for 75% of the variance, a search for moderator variables is indicated [Hunter and Schmidt, 1990]. Outlier Test. Studies that contain effect sizes more than two or three standard deviations from the mean may be examined. Differences and similarities between the studies in the tails may suggest possible moderator variables. Traditional outlier detection techniques include box-plot analysis for univariate data [Mosteller and Hoaglin, 1991] and studentized residuals for bivariate data [Freund and Littell, 1991]. An advance in this area is Huffcutt and Auther's [1995] Sample Adjusted Meta-Analytic Deviancy (SAMD) statistic, which takes into the account the sample sizes in a metaanalytic data set. It is based on the principle of sampling error that effect sizes based on a small sample size are more likely to be deviant than those from a large sample size [Hunter and Schmidt, 1990]. Beal et al. [2002] further discussed possible refinements of this technique by adjusting the inherent skewed distribution of effect sizes. Ordinary Least Squares (OLS) [Glass, 1977]. Regress the effect size on the potential moderator variable, using individual study effect size as the dependent variable and the moderator variable as the independent variable [Wolf, 1986;Glass et al., 1981]. If the coefficient is significant, then the effect of the moderator variable may be significant. This method is criticized for risk of capitalization on chance [Hunter and Schmidt, 1990], and is suggested for assessing moderator effects only on a confirmatory basis. Weighted Least Squares or WLS [Hedges and Olkin, 1985]. OLS assumes constant variance across observations or moderator variables retrieved from individual studies. This assumption is violated because sample error (main component of variance after controlling for moderator effects) is a function of effect size and sample size. Hedges and Olkin [1985] suggested using WLS and weight the multiple regression according the inverse of the sampling error variance. This method, although less popular than OLS, gives a more accurate assessment of moderator effects in most conditions [Steel and Kammeyer-Muller, 2002]. Partition Test [Hunter and Schmidt, 1990]. This test divides sampled effect sizes into subgroups by moderator factors, and compares subgroup means and variance, to assess if the means are significantly different. This method is particularly suggested for categorical moderator factors (e.g., gender, research methods, analysis levels, technology contexts). When applied to continuous moderator factors, such as by dividing data set into subgroups along a continuous moderator variable, this method performs poorly and underestimates moderator effects in most conditions [Steel and Kammeyer-Muller, 2002].",
          "Meta-analysis was initially developed to examine first-order effects, such as treatment effects (the \"d\" group effect sizes) or correlation effects (the \"r\" group effect sizes). Other effects, such as mediator effects or partial correlation coefficients, were not addressed. Such relations can often form the basis of a theory and help establish a mediator mechanism or explain a causal relationship. Assessing mediator effects in structural relationships as an important meta-analytic topic received much attention. Becker [1995] developed a technique to address structural relationships, analyzing whether a common population correlation matrix underlies a set of sampled empirical results. The analysis unit is correlation matrix instead of correlations. As few studies report the correlation matrix, application of the technique is limited in practice. One illustrative, but unsuccessful, example in IS is Legris et al.'s [2003] meta-analysis of the technology acceptance model (TAM). After an extensive search of empirical TAM studies, Legris and colleagues found usable matrices in only three out of 22 studies. Therefore, the small sample size resulted in \"a statistic shortfall\" and \"limit(ed) the presentation of the findings to the general conclusion\" (p. 202). Two alternative approaches to study mediator effects are: 1. combining and analyzing meta-analytically-developed correlations; and 2. directly studying coefficients of interest as the effect sizes [Rosenthal and DiMatteo, 2001]. Taking TAM as an example, the core model (figure 3) suggests that perceived ease of use (EU) and perceived usefulness (U) are two important predictors of an individual's behavioral intention to adopt a technology (BI); in addition, perceived usefulness partially mediates the effect of perceived ease of use on behavioral intention. The correlation coefficients (r's) and  The three equations hold for linear-regression-based analyses; they may differ a bit for structuralequation-modeling-based analyses (e.g., PLS, LISREL) because of different algorithms. But the differences are minor. In other words, we can infer the magnitude and the strength of path coefficients based on a set of meta-analytically-developed correlation coefficients. When applying the second approach -combining β's as the effect sizes, special caution should be taken that the sampled coefficients represent the relationship between the independent and the dependent variable controlling for other factors. Both approaches were applied in another TAM meta-analysis conducted by King and He [2005]. This study meta-analyzed 88 TAM empirical studies, and the results from the two approaches were almost identical.",
          "In this article, we first compare different review methods, then focus our discussion on metaanalysis, an important quantitative literature review method that is underutilized but which appears to offer great potential in IS literature synthesis research. The benefits from meta-analysis changed the course of research in several fields, and it is possible that these benefits can be achieved in IS as well. One distinct feature of IS is the rapid changes that occurred in both the technologies and the applications. The dynamic nature of IS requires accurate assessments of newly developed technologies and business practices in a timely fashion. By synthesizing existing empirical studies, meta-analysis can serve as an efficient tool to satisfy the needs for overall conclusions about phenomena, without the burden of conducting new research in a particular situation, or the dilemma of selecting from competing theories or different perspectives. Some researchers already work in this direction. For example, the development of various group support systems (GSS) in academia encouraged the use of teams, especially virtual teams, in organizational settings. However, the effectiveness of GSS was questioned by some inconsistent empirical findings in the literature [Fjermestad and Hiltz, 1999]. To address the concern, Dennis and Wixom [2003] meta-analyzed previous empirical studies. Their results not only validated GSS on improving group performance, but also explained conditions (i.e., the moderator effects of task, tool, the type of group, the size of the group, and facilitation) under which GSS would be most effective. Similar studies were done on computer-mediated-communication [Baltes et al., 2002], computer graphics [Hwang and Wu, 1990], and distance education [Allen et al., 2004]. Great potential exists for meta-analyzing other emerging or changing technologies and business practices, such as virtual teams and virtual organizations, knowledge acquisition technology (e.g., different interview techniques), system development practices (e.g., prototyping, user-centered system design, and rapid application development), IT governance, and knowledge management systems and practices, to name a few. A few empirical studies addressed these issues, and overall conclusive results will warrant the advance of the areas. We expect meta-analysis will help direct future research in other IS issues. For example, the study of IT productivity (or payoff of IT investment) generated discussions of pros and cons of IT investment by both academicians and practitioners, a debate commonly known as the \"productivity paradox\" [Roach, 1987;Stassmann, 1985]. Kohli and Devaraj [2003] meta-analyzed existing literature on this issue. Their study not only validated the relationship between IT investment and firm performance, but also identified various factors that influence the likelihood for an investigation to find such a relationship. A future study in this area may benefit from Kohli and Devaraj [2003] by following their design strategy. In addition, meta-analysis may help improve the publication practice in the IS field. Many studies that are performed with small sample sizes are never reported and many studies that produce non-significant results are rejected by journals. The development of a tradition of meta-analysis in IS would encourage the \"publication\" of such studies ─ perhaps on web sites and in electronic journals. Various techniques are used to conduct meta-analysis; However, while no single technique is universally-agreed-upon as a way to perform such a study [Hall et al., 1995], the basic procedures for conducting a meta-analysis are well-understood [Rosenthal and DiMatteo, 2001]. Based on a review of the commonly-used meta-analysis methods, we suggest the following procedures for conducting a meta-analysis in IS. The procedure is illustrative and is designed to ensure that readers share a common understanding of statistical meta-analysis. 1. Define the research domain and the relationships of interest. 2. Collect studies in a systematic way. Try multiple databases with the same selection criterion not only to enlarge the data pool, but also to avoid bias toward certain journals. 3. Extract effect sizes ─ the strength of a relationship or the magnitude of a difference 1 . If the researchers did not reported the desired effect sizes, scour the articles for the Understanding the Role and Methods of Meta-Analysis in IS Research by W. R. King and J. He information necessary to calculate these effects. It is also recommended contacting the authors for the needed information2 . 4. Select either the random effects (suggested for most cases) or the fixed-effect model, and a specific analysis method to combine the effect sizes. 5. Examine the variability among the obtained effect sizes. Perform a homogeneity test, or plot effect sizes graphically to judge the range of effect sizes and the existence of possible moderating effects. 6. Examine the signs and magnitudes of the combined effect sizes (the means). Although t-tests are commonly employed for the significance level of an effect size, it is more useful to calculate confidence intervals around the mean to indicate the range of the effect size [Rosenthal and DiMatteo, 2001]. This approach is especially appropriate for the random-effects model, because this model assumes variations among effect sizes. A combined effect size implies no more than an average of a set of possible population effect sizes. 7. If the objective of the research is to find or test moderating effects, studies should be coded for characteristics of their contexts. Code the characteristics of individual articles in an unbiased way. Coders (often Ph.D. students or colleagues) should be \"blind\" to the research, and the internal consistency of the coding results should be tested. 8. To test the moderating effects, two methods are commonly used: a. Subgroup comparison: group studies according to their coded research contexts, then compare the combined effect sizes that are calculated within each subgroup. b. Regression: regress the research study characteristics on the effect sizes, the significant factors indicate the significant moderators. 9. Summarize and discuss the findings. Editor's Note: This article was fully peer reviewed. It was received on July 7, 2005 and was published on October 17, 2005."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "Significant results are more likely to be published while non-significant results tend to be relegated to file drawers.",
            "context": "Publication Bias Publication bias [Begg and Berlin, 1988], also known as the file-drawer problem [Rosenthal, 1979; Iyengar and Greenhouse, 1988], refers to the observation that significant results are more likely to be published while non-significant results tend to be relegated to file drawers. Thus, the meta-analysis result will focus on an unrepresentative proportion of a total research population.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4eb5a9745dfc892cb42ed427a5f08e6abf8df7e3",
        "sections": [
          "T he rise of affective polarization-most notably, the tendency for partisans to dislike and distrust those from the other party 1 -is one of the most striking developments of twenty-first-century US politics 2,3 . Affective polarization has wide-ranging implications for our social and economic lives. It plays a role in how much time we spend with our families, where we want to work and shop and whom we want to date and marry 4 . But what does it mean for our politics? The answer is surprisingly unclear, as Iyengar and colleagues note (p. 139): \"little has been written on this topic (that is, the political effects), as most studies have focused on the more surprising apolitical ramifications\" 4 . Here, we take up a crucial dimension of that question: how are individuals' issue positions related to their level of affective polarization? We argue that the two are strongly connected in ways not addressed in previous research. Partisans with high levels of animus toward the other party are more motivated to distinguish themselves from their political opponents. They do so by taking positions on new issues that differ from the other (disliked) party and match those of their own preferred party. While this argument-that previous levels of partisan animus play a role in subsequent issue positions-is straightforward, testing it is difficult given the inherent endogeneity between policy beliefs, affective polarization and elite issue positions: if scholars find that those who harbour the most animus toward the other party also hold more extreme beliefs, is that due to animus driving those particular beliefs, to policy beliefs driving animus 5 or due to elite issue polarization simultaneously driving both the public's out-party animus 6 and policy beliefs 7 ? The emergence of the novel COVID-19 pandemic in the winter of 2020 presents us with the conditions needed to overcome some of the endogeneity that limits existing work. We collected data on respondents' levels of affective polarization in 2019, before the emergence of the coronavirus. We therefore have a measure of affective polarization that is exogenous to the pandemic: we can examine how pre-existing levels of partisan animus correlate with subsequent responses to COVID-19 without concern that the responses to the pandemic are, in fact, shaping affective polarization (and, more directly, out-party animus). Put another way, this design allows us to rule out the aforementioned possibilities that individuals' or elites' policy beliefs drive affective polarization (and hence any relationships between polarization and beliefs). Although our approach cannot isolate causal effects-given that we use observational data without a clear causal identification strategy-it does allow us to overcome the endogeneity identified above, which has been the key limitation encountered in previous work. We find a strong association between out-party animus and subsequent responses to the pandemic, offering evidence that policy beliefs reflect affective feelings toward the other party rather than just the issues at hand. That said, however, our findings also highlight how local context matters, as this relationship is muted among those who live in areas with particularly severe outbreaks of COVID-19. In these locations, even those with high levels of partisan animus have good reason to be concerned about the virus-it is personally salient to them. This highlights how real-world conditions affect citizens' issue positions, and suggests a potential limit to the types of partisan-motivated reasoning that probably underlie our results. The implications of our work go beyond political ramifications; we demonstrate that partisan hostility combined with conflicting elite cues can intersect with national efforts and can, quite literally, mean the difference between life and death 8 . To explicate our argument, we start conceptually by connecting affective polarization with partisanship 1 . Partisanship is a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party) 9 . This process gives rise to two of the underlying components of affective polarization: in-group favouritism and out-group animosity 4 . Over-time shifts in affinity for one's own party and animosity toward the other party have not been symmetric 2,4,10,11 . Indeed, out-party animus has increased dramatically in recent years 2,4 while in-party warmth has, if anything, slightly declined over the same time period 10 . Consistent with evidence of increasing out-party animosity, individuals report that they are less likely to date those from the",
          "NATUre HUmAN BeHAVIOUr other party 12 , they would pay out-partisans less for the same work 13 and they would prefer not to have out-partisans as roommates 14 . Furthermore, those with higher levels of out-party animosity report engaging in more discriminatory behaviour against those from the other party (for example, they do not want to work with those from the other party) 15 . Out-party animus, rather than in-party favouritism, is key to these associations in the literature 11 . Partisan identity alone, however, is not enough to explain out-group animus 2,16 -one must also account for other changes in the political and media environment 2,4 . The partisan-ideological sorting of liberals to the Democratic Party and conservatives to the Republican Party 7 , as well as the social sorting that has led to more demographically homogenous parties 17 , have both contributed to partisan animosity. Also at work are other changes in elite behaviours 18 and increasing elite polarization 6,19 . Moreover, changes in the information environment, such as the rise of partisan media 20,21 , increasingly negative campaigns 22 and new social media outlets, contribute to out-party animosity 23 . Given that out-party animus has elevated the partisan cue in social contexts, it may also have affected people's responses to elite political cues. As Pierce and Lau argue, for example (p. 9), \"strong affective reactions to a politician may themselves engender awareness of and like or dislike for certain policies. For instance, a visceral aversion to a candidate may lead a voter to reject positions associated with that politician\" 24 . To this end, people are motivated to do the opposite of what the other, disliked, party endorses [25][26][27] . They do this because the out-party animus is so strong that they want to differentiate themselves from that disliked party. And, importantly, it follows that those with greater out-party animus (that is, stronger affective reactions) will be most motived to hold distinctive views 28,29 , taking positions opposite to those put forth by out-party elites (for example, elected officials) and in line with those of their own party's elites. This response to cues may be especially apparent when the difference between in-party and out-party cues is stark 30 , as it is in the case of COVID-19 (refs. 31,32 ). Demonstrating that affective polarization (and its key underlying component, out-party animus) relates to policy beliefs, however, is surprisingly complicated 33 . There is an empirical relationship between alignment in issue positions and partisan animus but it is difficult to identify the original source of this relationship 34 . Indeed, theoretically, the relationship between issue beliefs and out-party animus could stem from three possible scenarios: (1) animus driving cue taking on issues (as just explained), (2) issue position extremity causing greater partisan animus 5,34 or (3) elite issue polarization leading separately to both public issue divides 7 and out-party animus among the public 6,35 . As a result, it is difficult to determine how animus connects to political views-that is, whether policy positions are undergirded by affective dislike beyond substantive considerations. Although it is difficult to address this issue fully without manipulating affective polarization, one approach that would allow us to address a part of this problem is a measure of out-party animus taken before the emergence of an issue. This allows us to record levels of animus (at time t -1) before the existence of those issue positions (at time t). This means that elite polarization on the issue at time t cannot have affected earlier measures of partisan animus taken at time t -1, or that attitudes measured at time t are the cause of this time t -1 animus. Nevertheless, the persistence of existing issues on the policy agenda, and the unpredictability of new issues emerging onto the agenda, make it extremely difficult to use an ex ante measure (and, to our knowledge, this has not been done). The COVID-19 pandemic, however, allows us to consider an issue as it emerges. To do this, we need measures of partisan animus taken before COVID-19 began to spread in the United States. If we instead used a measure taken after COVID-19 entered the agenda, the issue itselfand politicians' reactions to it-could shape those recorded levels ",
          "NATUre HUmAN BeHAVIOUr of partisan animus. For example, Democrats' levels of partisan animus might reflect not only their underlying hostility toward President Trump, but also how he specifically responded to COVID-19 (that is, downplaying its severity, refusing to acknowledge its existence in the United States for several weeks, and so on). If we see that this measure of animus is related to attitudes about the pandemic, then, it could simply reflect politicians' reactions to it. We therefore use pre-pandemic measures of partisan animus (from August 2019)-paired with attitudes toward the pandemic measured once it emerged (from April 2020)-to study the relationship between the two. The partisan difference in elite responses to the pandemic suggests why affective polarization, and specifically out-party animus, may play a key role in driving issue positions here. From the beginning of the outbreak, Democratic politicians, relative to Republican ones, expressed greater concern about the virus, implored the public to take more precautions and supported more restrictive policies 36 . President Trump-with his dismissal of the virus, demands to reopen the economy and refusal to wear a mask-is the apotheosis of this trend, but is far from the only example of it, as Democratic governors typically took swifter and more public actions to combat the virus than did most Republican governors 37 . Moreover, these partisan debates and polarization on the issue were reflected in the media coverage 38 . The fact that the two parties behaved as mirror opposites in response to the pandemic is especially notable here, as it means that citizens simultaneously received distinct information about how members of both partisan groups should behave, making the elite cues especially clear 7 . This makes for clear cues, but it also means we cannot empirically differentiate the relative impact of in-party versus out-party cues. Future work would benefit from looking at situations with cues from only one party 32 , although it is a situation that is increasingly rare 39 . We would expect that, in such situations, animus would drive reactions from the out-party cue alone and (possibly) the in-party cue alone since partisans want to distinguish themselves. In line with the aforementioned theoretic logic that affective polarization-and especially its key ingredient, out-party partisan animus-may increase the motivation to follow cues, we expect the following pattern: as out-party animus increases, Democrats will express more concern about the virus, be more willing to take actions to prevent its spread (for example, wash their hands more, avoid large crowds, cancel travel and so on) and be more supportive of policies to stop the virus (for example, stay-at-home orders) (hypothesis 1a). Conversely, among Republicans we expect that as out-party animus increases, worries about COVID-19 will decrease, there will be a lower likelihood of taking actions to prevent its spread and less support for policies to stop the spread of the virus (hypothesis 1b). Our argument is not simply that partisan gaps have emerged: that point has been thoroughly documented elsewhere 8,40,41 . Instead, our argument is that it is the animus component of affective polarization, at least partially, that drives these gaps. Our argument implicitly invokes partisan-motivated reasoning since we posit that partisans have a directional motivation in forming opinions 42 . Partisan-motivated reasoning means partisans process information and form attitudes with the goal of confirming their partisan identities and differentiating themselves from the other party (this contrasts with issue-based motivated reasoning where the goal is to confirm a standing issue belief) 43 . While directional partisan reasoning predominates in highly political situations 44 , it can shift when particular issues rise in salience 45 . Of ",
          "NATUre HUmAN BeHAVIOUr particular relevance are conditions that prompt partisans to shift from having a directional motivation to an accuracy motivation. In this latter case, individuals assess information based on the 'best' available evidence rather than to affirm an identity [46][47][48] . In the case of COVID-19, this will occur as the direct threat of the virus increases and is captured by the number of cases in one's local area. An increase in cases can alter personal experiences (for example, increasing the likelihood that someone you know personally has been infected) which, in turn, vitiates partisan reasoning 47 . We thus predict that, as the number of COVID-19 cases in one's area increases, the impact of out-party animus will decrease and the partisan gap will similarly decrease (hypothesis 2). In short, partisan animus matters, but so too does the geography of the COVID-19 outbreak in the United States. Broadly, then, our study suggests the possibility that partisan-motivated reasoning is conditional and may be shaped by context. Following a similar logic, we also might expect the partisan animus effect to decline among those who have had, or are vulnerable to, COVID-19, but at the time of our data collection the number of such individuals in our sample was too small to test that possibility. We use a multi-wave, nationally representative survey. In the summer of 2019, 3,345 respondents answered a set of questions (for an unrelated survey) that provide our pre-COVID-19 measure of partisan animosity. These participants were re-interviewed in April 2020 as the coronavirus spread throughout the nation; a total of 2,484 respondents who answered our 2019 questionnaire completed  ",
          "NATUre HUmAN BeHAVIOUr our re-interview, for a re-contact rate of 74% (more details on the sample are given in Supplementary Methods). In this re-interview, we measured participant reactions to the COVID-19 outbreak focusing on three relevant dimensions: (1) how worried they are about the virus, both for themselves and for the nation as a whole, measured by a range of items assembled into an index (α = 0.89); (2) which behaviours (from a list of 14) they are taking to avoid becoming infected with COVID-19 (that is, washing their hands more, cancelling travel and so on); and (3) their support for various policies to limit the spread of COVID-19 (that is, stay-at-home orders, business closures and so on), again analysed as an index (α = 0.73). All analyses treat these three measures as dependent variables, and the pre-pandemic measure of animosity is our key explanatory variable. More information on the survey is provided in Methods and Supplementary Methods.",
          "Figure 1 shows kernel density plots (separately for Democrats and Republicans) for each of the three dependent variables: (1) worry about COVID-19, (2) behaviours being taken to avoid becoming infected with COVID-19 and (3) support for various policies to limit the spread of COVID-19; see Methods for details on the coding of these and all other variables. The plots show that the average Democrat is more worried, is more likely to have changed behaviours and is more supportive of polices to stop the spread of infections than the average Republican, consistent with other analyses showing partisan gaps in these areas 40,41 . There is, however, substantial overlap in the attitudes of Republicans and Democrats, which suggests the possibility that something moderates the relationship between partisanship and COVID-19 attitudes. Figure 2 contains scatter plots for each of the dependent variables (on the y axes) along with the number of cases in the respondent's county (on the x axes), as well as a locally weighted scatterplot smoothing (LOWESS) smoother to show the nonparametric bivariate relationship between the two variables. It is clear that, as case numbers increase, values on all dependent variables also increase. Because the relationship is nonlinear, especially for low-infection areas, and there is a long right-tail of cases (that is, a small number of areas, primarily in New York City, with extremely high rates of infection), we use the natural log of cases in all of our models. We next turn to our quantities of interest-the relationship between partisanship, partisan animosity and responses to the COVID-19 pandemic-and estimates of uncertainty around those effects. To ensure the robustness of our results, we estimate a series of models with additional controls added in each model (Methods and Supplementary Methods). The results we present are robust to changes in estimation approach, and to the inclusion of a variety of controls including a measure of partisan affect and strength of identity. We present the results of our main models in Figs. 34567. These are represented as plots since our models rely on interactions, and the coefficient estimates on interactions and their constitutive terms do not easily translate to our quantities of interest; as a result, the significance levels of these coefficients may not be informative in terms of testing our hypotheses [49][50][51] . Relevant here is the slope of the outcome variable at various levels of other covariates, a quantity termed the 'marginal effect'; this term is not intended to signal a causal relationship 52 . By definition, all tests of the statistical significance of this effect are two-tailed. We begin with plots from a model that includes an interaction between partisanship and partisan animosity, while controlling for the number of cases. The top panels of Fig. 3 present the marginal effect of out-party animus for Democrats and Republicans for each dependent variable, while the bottom panels show the marginal effect of Republican partisanship for various levels of animus. Hence, the top parts directly test hypotheses 1a and 1b for each dependent variable, while the bottom parts plot the partisan gap as out-party animus increases. Beginning ",
          "NATUre HUmAN BeHAVIOUr see a similar relationship between partisan animus and how worried Democrats are about COVID-19. When we move to the middle panels of Fig. 3 examining behaviours, here we see that Democrats with high levels of animus report engaging in more behaviours to combat COVID-19 than do Democrats who do not hold as much animus. With this dependent variable, however, there is no similar, statistically significant result among Republicans. Finally, with regards to policy, we see that out-party animus is associated with greater support for policies to combat COVID-19 among Democrats while Republicans with high levels of animus are less supportive of the same policies than Republicans with less animus. Hence we see support for the partisan animus hypothesis for at least one party across all three variables. As the bottom panels of Fig. 3 show, there is a partisan gap for each dependent variable and the size of that gap grows as out-party animus increases. Since the model controls for the number of cases in the respondent's county, the partisan gaps are not the result of areas with many Democrats having more severe outbreaks than areas with many Republicans. Recall, however, we argue that severe outbreaks might mitigate the role of partisan animosity. The figures now described will look at the models with triple interaction, including cases. In Figs. 456, we present the marginal effect of the Republican dummy variable at different levels of out-party animus-that is, the difference in the expected value in the dependent variable for Republicans minus the expected value in the dependent variable for Democrats. For each dependent variable, we include separate plots for a low number of cases (the 25th percentile) and a high number of cases (the 75th percentile). If our argument is correct, then we should find that as out-party animus increases, the gap between the parties increases as well (that is, the marginal effect of partisanship increases; this is the test of hypothesis 1). However, we should also find that this relationship is muted in areas with large numbers of cases, as all citizens are more concerned about the virus. Simply put, we should see a steeper slope (larger marginal effect) in areas with low cases relative to high cases if hypothesis 2 is correct. In Fig. 4 we present the marginal effect of being a Republican (as opposed to a Democrat) on worry, as partisan animus increases. In the left-hand panel, which presents the relationship between partisanship and out-party animus in areas with few cases, we see that as partisan animus increases, the partisan gap emerges: when animus is low, partisans are indistinguishable from one another but, when animus is high, partisans significantly diverge. In contrast, in the right-hand panel, depicting the pattern in areas with high levels of cases, there no significant partisan gap among those with high levels of animus (that is, the confidence interval overlaps zero). We do see small partisan differences for moderate levels of out-party animus (probably because the majority of our respondents have moderate levels of animus), but these gaps are distinctly smaller than the partisan gaps among those who live in areas with few cases. Figure 5 presents the same analysis for the behaviour dependent variable. Partisan animus again has a clear correlation with political outcomes, as we observe partisan divides on COVID-19 behaviours. The difference, however, is that we see the same increasing partisan gap regardless of the number of cases in the county. Higher numbers of cases correlate with more preventative behaviours overall, but higher partisan gaps in behaviour emerge alongside animus regardless of the number of cases. The reason for this is that, while individuals with low or moderate levels of animus are responsive to the number of cases, Democrats and Republicans with high levels of animus are not. Why do those with such animus not change their behaviour as the number of cases increases? The answer is probably different for Republicans and Democrats. Republicans with high animus took low-cost actions (for example, hand washing) in low-case areas and were forced to avoid certain behaviours (like going to restaurants) due to local restrictions. As cases increased, they may have believed they were already doing enough. Democrats with high levels of animus are probably less responsive because they are engaging in more behaviours even in counties with few cases. Nevertheless, because the group is already changing behaviours in low-case areas, they are ",
          "NATUre HUmAN BeHAVIOUr unlikely (or perhaps, even unable) to take on more behaviours in areas with more severe outbreaks. Turning next to Fig. 6, we see that when it comes to policy support there is once again a relationship between partisan animus and opinions. Here, much as with worry, the number of cases moderates the relationship. Although there is a significant partisan difference among partisans with high animus when cases are low, there is no statistically significant difference between Republicans and Democrats in those counties with high numbers of cases, regardless of the level of animus. We note that support for policies to prevent the spread of infections is high; among both Democrats and Republicans, a majority supported these policies at the time of the re-interview. As a result, it probably should not be a surprise that, in areas with a significant outbreak of COVID-19 infections, partisan gaps disappear. Otherwise, like worry but unlike behaviours, expressing policy support is not a costly behaviour per se. It is worth noting that Republicans with high animus and in high-case areas appear to be more supportive of government policy intervention than of engaging in relevant behaviours, and this is an area for future work to explore more carefully. In Fig. 7, we use the same models to present a different perspective on the results, now focusing on the marginal effect of out-party animus for Democrats and Republicans in low-and high-case counties. The goal here is to see which party is the root of the partisan gaps at higher levels of partisan animus, by considering the relationship between a unit increase in animus and the likelihood of worrying, changing behaviour and supporting policy. For worry about COVID-19 and support for COVID-19 policies, the marginal effect of animus is significant and negative for Republicans in counties with few cases; the confidence intervals for the other marginal effects overlap with zero. Increases in animus are statistically significant only for Republicans in counties with low cases, suggesting that, for worry and support, partisan gaps are largely a function of Republicans with considerable animus towards Democrats. When it comes to behaviours, however, the only statistically significant marginal effect for partisan animus is among Democrats in counties with few cases-these individuals are engaging in preventative behaviours despite the low community spread of COVID-19. At the same time, however, in counties with large outbreaks, the absolute size of the marginal effects for both Democrats and Republicans is still equivalent to a full behaviour in both cases-even if the confidence intervals overlap zero. That might explain why the partisan gap remains in those counties: the difference within parties between those with high animus is smaller but, since the parties move in opposite directions, the partisan gap remains the same. Overall, the results make clear that the partisan gaps observed in the data are at least partially a function of partisan animus, suggesting that it is cue taking that divides the parties on this issue. This type of partisan reasoning, however, is blunted when real-world threats become salient: in areas with substantial numbers of infections the role of animus is more muted, as all citizens respond to the outbreak. Furthermore, we see that with regards to worry and policy support, the correlation with partisan animus in counties with low cases is most pronounced among Republicans. Given the messaging from the President, this pattern makes sense. It is understandable why Democrats, regardless of animus, and (to a lesser extent) Republicans who do not have high animus worried about the virus's impact on public health and the economy, and supported shutdowns and stay-at-home orders even when the local severity was low. On the other hand, Republicans with high animus attuned to a Republican President saying, counter to the clear Democratic message, that it will all just disappear 53 , needed an active, local outbreak to increase their concerns to the level that Democrats were feeling.",
          "While scholars, pundits and citizens alike invoke affective polarization as a factor in driving issue positions, there has, to date, been no direct evidence that it actually does. We leveraged a unique ",
          "NATUre HUmAN BeHAVIOUr data opportunity to track the association between affective polarization and, more directly, out-party animus and responses to the COVID-19 pandemic. Rhetorical differences between party elites help to produce our results: not only were Democratic elites much more likely to emphasize the threat of the virus to public health and the importance of taking appropriate precautions 54 , but President Trump downplayed the danger and advocated for treatment approaches shown to be ineffective 55 . Other Republican elected officials were similarly dismissive of the virus at the time of our study. These rhetorical divisions are associated with mass partisan divisions: as animus increases, Republicans become less concerned about COVID-19 and less willing to support policies to mitigate the threat of the virus. Real-world threat-here, a high level of infections in one's county-tempers that relationship, since, as theories of reasoning suggest, it pushes individuals away from directional partisan motivations towards an accuracy motivation-that is, a desire to rely on the best available evidence to which they have access. Still, we note that even in counties with high numbers of cases, Democrats and Republicans with high levels of animus differ in how likely they are to report engaging in actual, costly behaviours. This is because Democrats with high levels of out-party animus are already engaging in a high number of mitigating behaviours while Republicans with high levels of out-party animus remain resistant to costly behaviours as case levels increase. These findings have implications for understanding how best to combat COVID-19. Since affective polarization (particularly partisan animus) underlies partisan gaps, policymakers will need to devise different strategies to bring the parties together on these issues. Simply highlighting areas of commonality, scientific directives or economic forecasts is not enough; instead, they will need to ameliorate partisan animus to shrink the gaps. This would require, for example, correcting misperceptions about the parties 56,57 , priming superordinate identities 58 and/or fostering inter-party contact and dialogue 59 . The results also offer insights for theories of partisan reasoning, insofar as they show how such thinking can drive opinions but also how real-world threats can alter motivations. More broadly, our findings suggest that policy differences between the parties are not simply a function of different information 60,61 , or different values 62 , but possibly of partisan animus as well. This is a substantial finding insofar as a large literature documents correlations between partisan animosity and social and economic behaviours (for example, on friendships, romantic relationships, business transactions and so forth) 4 , but there is much less work examining the association between animus and political attitudes, due to the data difficulties we highlighted earlier in the paper. We show clear political consequences with respect to perhaps the most important of policies: government directives for preventing a public health and economic crisis.      ",
          "For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.",
          "The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly The statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.",
          "A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals) For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted Give P values as exact values whenever suitable. For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above.",
          "Policy information about availability of computer code Data collection no software was used.",
          "For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.",
          "All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: -Accession codes, unique identifiers, or web links for publicly available datasets -A list of figures that have associated raw data -A description of any restrictions on data availability Data availability. The data that support the findings of this study are available via Dataverse at: https://doi.org/10.7910/DVN/H7AT3N"
        ],
        "ground_truth_definitions": {
          "partisanship": {
            "definition": "a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party).",
            "context": "To explicate our argument, we start conceptually by connecting affective polarization with partisanship partisanship. Partisanship is a type of social identity and, by identifying with one party, individuals divide the world into two groups: their liked in-group (our own party) and a disliked out-group (the other party). This process gives rise to two of the underlying components of affective polarization: in-group favouritism and out-group animosity.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
