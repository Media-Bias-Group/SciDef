{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "The general workflow of a prototypical system for automated identification of bias-inducing words in news articles is presented in Fig. 1. In our work, we start from collecting the sentences and gathering annotations via a crowdsourcing process (1). We then obtain various features (3) described in more detail in Section 3.5. One of the features is a bias lexicon built semi-automatically by computing words similar to potential bias words using outlet-specific word embeddings (2). We then train a supervised classifier on our engineered features and annotated labels (4). After the best model is selected and optimized, we evaluate the performance of the feature-based approach for detection of media bias. Furthermore, we evaluate all features individually (5).",
        "reasoning": "The section discusses the workflow of a system for identifying bias-inducing words but does not contain any explicit term definitions. No term is followed by a definition sentence or a clear ‘is defined as’ structure. Therefore, no term-definition pairs can be extracted from this section.",
        "extracted_terms": []
      },
      {
        "augmented": true,
        "section": "Interpretability \"aims at developing tools to understand and investigate the behavior of an AI system\" (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
        "reasoning": "The section contains a clear definition of the term \"Interpretability\" in quotation marks, followed by an elaboration that reinforces the definition. The definition is explicit and provided in a single sentence, so it qualifies for extraction as an explicit definition. The context for this definition should include the exact sentence containing the definition and the following sentence that expands upon it. No other terms are explicitly defined in this section.",
        "extracted_terms": [
          {
            "term": "Interpretability",
            "definition": "aims at developing tools to understand and investigate the behavior of an AI system",
            "context": "\"Interpretability\" aims at developing tools to understand and investigate the behavior of an AI system (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "26af8e81a7ca5b137912acb81d8c19f38fce0200",
        "sections": [
          "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
          "Howard's research on the \"chlorine sunset\" controversy is based on interviews and document analysis. He conducted twenty-seven semistructured interviews, lasting an hour on average, with staff members of federal regulatory agencies in the United States and Canada, staff members of the International Joint Commission (IJC), members of the Great Lakes Science Advisory Board, staff members or individuals otherwise associated with nongovernmental organizations (NGOs), academic or governmental members of the industrial ecology or green chemistry communities, and industrial chemists in industry and academia. A number of transcripts were supplemented with additional information from follow-up correspondence. Documents analyzed included (1) reports, press releases, Web documents, and other materials published by NGOs, the chemical industry, and federal agencies; (2) articles and commentaries in newspapers and popular and trade magazines; (3) research articles and commentaries in scholarly anthologies and peer-reviewed scholarly journals; (4) books written by key actors; and (5) transcripts of Congressional testimony. A little-studied controversy involving one of the major branches of industrial chemistry documents a striking example of undone science and illustrates the role it can play in structuring conflict between competing regulatory paradigms. Much of the controversy has centered on the Great Lakes region, where extensive chemical manufacturing and contamination has occurred; where scientists have documented threats to wildlife and humans from persistent, toxic, industrial chlorinated pollutants; where extensive citizen activism has emerged around this threat; and where a quasigovernmental advisory body has assumed a leadership role in addressing this concern (Botts et al. 2001). A number of environmental and health advocates have argued, based both on fundamental toxicology and on long historical experience with chlorinated synthetic chemicals (e.g., DDT and PCBs), that the entire class of thousands of such substances should be tentatively presumed dangerous and that the chemical industry accordingly should wean itself from most major uses of chlorine (Thornton 1991(Thornton , 2000;;International Joint Commission [IJC] 1992; see Howard 2004). The analysis offered here briefly considers the character and function of undone science in the debate provoked by proposals for a \"chlorine sunset.\" The chlorine sunset controversy revolves around conflict between two sharply contrasting regulatory paradigms: risk and precaution (Thornton 2000;Howard 2004). The powerful chemical industry has coevolved with, supports, and is supported by the dominant U.S. and Canadian environmental regulatory regime, which restricts chemical industry decision making only to the extent that detailed calculation of risk indicts individual chemical substances. Meanwhile, Greenpeace, a marginalized, reputedly radical environmental NGO, and the IJC, a prominent but marginalized binational advisory organization, argued for a regulatory regime based on the precautionary principle (see Tickner 2003), which in their view justified governmental action against an entire class of industrial chemicals. The dominant paradigm assumes the unit of analysis to be the individual substance and places the burden of proof on the public to prove harm; in contrast, the challenger paradigm allows, even requires, the primary unit of analysis to be the entire class of substances and places the burden of proof on corporate officials. Within this matrix of political and epistemological conflict, the political economy and political sociology of undone science can be seen to revolve around a series of three dyads, each paradigm implying parallel formulations of \"done science\" and undone science. The three dyads are summarized in Table 1. One dyad appears in the context of health impacts research. Industry and federal officials operating in the risk paradigm hold that the legitimate goal of health impacts research performed or mandated by government is ad hoc identification of individual chlorinated chemicals that cannot be safely manufactured and used. In this paradigm, chlorine chemistry itself is seen as immune to fundamental interrogation; the role of public science is limited to documenting the odd substance that can be definitively proven harmful and, on that basis, restricted. \"We've made the point over and over again that you have to look at each product's physical and chemical characteristics to draw conclusions about what it is going to do in the environment,\" argued Brad Lienhart, of the Chlorine Chemistry Council. To do otherwise would be to \"[make] non-science-or nonsense-into science\" (quoted in Sheridan 1994, 50). Beginning in the early 1990s, \"sunset\" proponents vigorously argued that such research is incapable of interrupting a long series of chlorinated \"Pandora's poisons\" from entering the environment and human tissues long before their deleterious effects are documented. Inevitably remaining undone, they argued, is science capable of systematically identifying unsafe chemicals from among tens, perhaps hundreds, of thousands of chlorinated industrial substances, by-products, and breakdown products, a scope of research that the risk paradigm is sometimes assumed to provide but, owing to the sheer enormity of the undertaking, cannot. The government's effort to identify unsafe chlorinated chemicals is ad hoc precisely because it cannot, in any meaningful sense, be systematic; not only are available resources insufficient, but the enterprise is technically infeasible. Viewed in this light, the science is undoable. The IJC argued: There is a growing body of evidence that [suggests that] these compounds are at best foreign to maintaining ecosystem integrity and quite probably persistent and toxic and harmful to health. They are produced in conjunction with proven persistent toxic substances. In practice, the mix and exact nature of these various compounds cannot be precisely predicted or controlled in production processes. Thus, it is prudent, sensible and indeed necessary to treat these substances as a class rather than as a series of isolated, individual chemicals. (IJC 1992, 29) A second dyad appears in the risk paradigm's stance on innovation. Industry has systematically pursued the development of chlorine chemistry, developing chlorinated chemicals and expanding markets for them; meanwhile, advocates of chlorine precaution have pointed to the need to systematically develop nonchlorine alternatives. This is in part science that the risk paradigm has long left undone-historical research and development trajectories that could have led to a wider range of nonchlorine chemicals and processes being available today. The implication of the historical analysis offered by a leading sunset proponent (Thornton 2000; see also Stringer and Johnston 2001) is that over the past century the technological, economic, and political momentum of chlorine chemistry has to some extent bent the overall industry research and development agenda toward chlorine and away from nonchlorine alternatives. Here undone science consists of a body of nonchlorine chemicals and processes that might now exist but for the long dominance of research and development predicated on chlorine. It is a point seemingly acknowledged by a confidential IJC informant who did not support the commission's sunset recommendation: \"There's no reason why we couldn't, as a global society, live a non-chlorine lifestyle. It's just, you know <laughs>, that ain't gonna happen, because that is not our history! We're kind of, in a way, captives of our past.\" In the risk paradigm, with its laissez-faire orientation, such research and development need not be undertaken by the industry but instead is tacitly left to whichever agency or organization might care to undertake it. Viewed from the vantage point of the industry, with its adamantine conception of chlorine chemistry as technologically and economically inevitable, the only conceivable motivation for conducting such research and development would be some kind of ideological fetish (see, e.g., Chlorine Chemistry Council n.d.). It would represent \"a veiled attempt to return to a pre-industrial Eden,\" one industry supporter suggested (Amato 1993). Crucially, although this agenda would have been and would now be technically feasible, such research would be hobbled by the absence of a sizable cadre of technoscientists devoted to the project and by a lack of financial resources to sustain the effort. A third dyad occurs within the challenger, precautionary paradigm and directly counters the values and priorities of the dominant paradigm's dyads. Paired with precaution advocates' assertion of the need for research to systematically develop nonchlorine alternatives-here seen as industry's responsibility rather than the public's-is an explicit assertion that industry should assume the burden of making the case for any specific chlorinated chemicals (or chemical processes) that can be demonstrated to be both essential (i.e., nonsubstitutable) and capable of being manufactured and used in ways that (to some as yet unstated standard) pose no significant environmental hazard. Industry's motivation for undertaking this latter effort would, of course, be profit. And owing to the presumably quite limited number of substances to be evaluated, it would be both technically feasible and, given the industry's substantial financial and technical resources, affordable. The chlorine sunset controversy is now effectively dormant. In the face of bitter industry resistance and U.S. and Canadian governmental intransigence, the IJC and Greenpeace ceased promoting their sunset recommendations in the mid-1990s (Howard 2004). Thornton's book, which appeared in 2000, reawakened (and in significant ways deepened) the debate, but it did so only briefly. The sunset proposals have not visibly shifted policy at any level in North America. A major international treaty on persistent organic pollutants signed in 2001 represented an important victory for activists, but it also underscored the lingering, unresolved character of the chlorine debate: all twelve of the \"dirty dozen\" substances it required to be phased out are chlorinated compounds, and each was targeted on the basis of its discreet, well-documented characteristics. Meanwhile, thousands of far less extensively studied chlorinated chemicals-and chlorine chemistry as a whole-remain unregulated. This analysis of the chlorine sunset controversy illustrates how regulatory regimes influence the construction and articulation of research priorities. In this case, advocates of the risk and precaution paradigms, on the basis of competing understandings of the appropriate unit of regulatory analysis and appropriate regulatory burden of proof, promote competing conceptualizations of science both done and undone. More specifically, the case suggests that done and undone science in such a controversy can be understood as occurring in dyadic pairs and that a major role for challenger discourses is making the implicit undone portion of dyads within the dominant paradigm visible and explicit. This analysis also highlights an important category of undone science in technoscience controversies-undoable sciencethat improves understanding of how regulatory regimes constrain the identification of undone science. Here, close examination of precautionary advocates' critique of the risk paradigm clarifies the process through which conventional regulatory structures veil undoable science in the form of systematic research for which insufficient resources and insufficient technical means are available.",
          "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (Löwy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] £1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center … there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life … and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't … appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
          "Kempner's research is based on an interview study that examines \"forbidden knowledge,\" a term used to capture scientists' decisions not to produce research because they believe it to be taboo, too contentious, or politically sensitive (a type of negative knowledge in the terminology introduced above). In 2002-2003, she and colleagues conducted ten pilot and forty-one in-depth, semistructured telephone interviews with a sample of researchers drawn from prestigious U.S. universities and representing a diverse range of disciplines, including neuroscience, microbiology, industrial/organizational psychology, sociology, and drug and alcohol research (Kempner, Perlis, and Merz 2005). Those fields were chosen to gauge the range, rather than the prevalence, of experiences with forbidden knowledge. Interviews lasted between thirty-five and forty-five minutes and were audiotaped, transcribed, coded, and analyzed according to the principles of grounded theory (Strauss and Corbin 1990). While many social movements organize around the identification and completion of undone science, others devote themselves to making sure that some kinds of knowledge are never produced. They are not alone. The idea that some knowledge ought to be forbidden is deeply embedded in Western cultures and appears in literature through the ages, from Adam and Eve's expulsion in Genesis to Dr. Frankenstein's struggle with a monster of his own creation (Shattuck 1996). Mertonian rhetoric aside, most people agree that some science poses unacceptable dangers to research subjects or to society at large. The widely accepted Nuremberg Code, for example, places strict limits on human experimentation, in an effort to ensure that some science-such as Nazi human experimentation in World War II-is never done again. Determining which knowledge ought to remain undone can often be contentious, as illustrated by current high-profile public debates surrounding the ethics and implications of stem cell research and cloning technologies. Nevertheless, as in research agenda-setting arenas (Hess 2007), debates and decisions about what knowledge should remain off limits to the scientific community typically occur among elites: legislators and federal agencies perennially issue guidelines and mandates regarding which research should not be conducted, setting limits on everything from reproductive and therapeutic cloning to studies of the psychological effects of Schedule I drugs, like heroin and marijuana. Scientists and the lay public both have limited opportunities to voice their opinion in these discussions. In dramatic cases, scientists have attempted to preempt mandates via self-regulation, as was the case in 1975 when scientists meeting at Asilomar called for a moratorium on certain kinds of recombinant DNA research (Holton and Morrison 1979). According to the forty-one elite researchers interviewed for this case study, these formal mechanisms account for only a portion of the limitations that can produce undone science (Kempner, Perlis, and Merz 2005). More often, researchers described how their research had been hamstrung by informal constraints-the noncodified, tacit rules of what could not be researched or written. Yet researchers were very clear about what constituted \"forbidden knowledge\" in their respective fields. The boundaries of what could not be done had been made known to them when either they or a colleague's work had been targeted for rebukein essence, their work had breached an unwritten rule. The management of forbidden knowledge, thus, worked much as Durkheim said it would: once someone's research had been identified as especially problematic by, for example, a group of activists, their work became a \"cautionary tale,\" warning others \"not to go there\" (Kempner, Bosk, and Merz 2008). In this way, social movement organizations and activists are able to play an important role in debates about what ought to remain undone, whether or not they are invited to the table. Besides their influence on shaping research agenda-setting arenas, social movements can and do influence individual researchers' decisions not to pursue particular types of studies. In recent decades, for example, animal rights organizations have had an enormous influence on the kinds of research that scientists choose not to produce. We found that the researchers in our sample who work with animal models took seriously the threat posed by those organizations. They spoke of \"terrorist-type attacks\" and told stories of colleagues who received \"razor blades in envelopes\" and \"threatening letters.\" Others faced activists who staked out at their houses. Researchers learned from these cautionary tales and, in many cases, said that they had self-censored as a result. One researcher, for example, explained that he would not work with primates-only \"lower order\" animals like mice and drosophilia because: I would like to lunatic-proof my life as much as possible … I, for one, do not want to do work that would attract the particular attention of terrorists … The paranoia was acute. One researcher refused to talk to the interviewer until she proved her institutional affiliation: \"For all I know, you are somebody from an animal rights organization, and you're trying to find out whatever you can before you come and storm the place.\" Over time, the overt interventions of animal rights organizations in the production of research have redefined the ethics of animal research, ushering in legislation like the Animal Welfare Act of 1985, which requires research institutions that receive federal funding to maintain \"Institutional Animal Care and Use Committees\" (Jasper and Nelkin 1992). However, lay groups do not need to use such directly confrontational tactics to influence researchers' decisions, especially if the groups are successful in their attempts to reframe a particular social problem. For example, substance abuse researchers argued that their research agendas were limited by the success of the Alcoholics Anonymous' campaign to define treatment for alcoholism as lifelong abstinence from drink. Although these researchers would like to conduct \"controlled drinking\" trials, in which alcoholics are taught to drink in moderation, they argued that \"There's a strong political segment of the population in the United States that without understanding the issues just considers the goal of controlled alcohol abuse to be totally taboo.\" The mere threat of interference from the grassroots was enough to keep many researchers from conducting certain studies. Several drug and alcohol researchers described great unwillingness to conduct studies on the health benefits of \"harm reduction\" programs, such as those that distribute free condoms in schools or clean needles in neighborhoods, because they might attract unwanted controversy from lay groups who oppose such public health interventions. Thus, in some contrast to the role that social movement organizations and lay experts/citizen scientists play in exposing undone science and encouraging knowledge creation in chemical, air monitoring, and breast cancer research, this study shows that the same actors can also play a powerful role in determining which knowledge is not produced. Moreover, conflict over the direction of funding streams, while critically important to the political of research agenda setting, do not solely determine what science is left undone. Rather, social movements are also effective beyond research agenda-setting processes that occur at the institutional level; this study provides evidence that they also shape the microlevel interactional cues and decision-making process of individual scientists. Although more research is needed to understand the circumstances under which researchers decide to selfcensor in response to pressure from outside groups, this case suggests that social movements may have much greater potential to thwart research than originally thought. The implications are intriguing and deserve greater attention. On one hand, disempowered groups may leverage these techniques to gain a voice in a system of knowledge from which they are typically excluded. On the other hand, it is troubling to learn that the subsequent \"chilling effect\" happens privately, often without public discussion and in response to intimidation and fear.",
          "The diverse cases provide an empirical basis for moving forward the theoretical conceptualization of undone science in relation to a new political sociology of science and that program's concern with how research agendas are established. Perhaps the most significant general observation is that the identification of undone science is part of a broader politics of knowledge, wherein multiple and competing groups-including academic scientists, government funders, industry, and civil society organizations-struggle over the construction and implementation of alternative research agendas. To a large extent, our case studies focus on attempts by civil society or quasigovernmental organizations to identify areas of research they feel should be targeted for more research. However, the identification of undone science can also involve claims about which lines of inquiry should warrant less attention than they currently receive, either because there are decreasing social returns on continued investments in heavily researched areas or because the knowledge is deemed not worth exploring and possibly dangerous or socially harmful-what Gross (2007) calls \"negative knowledge.\" Examples of the latter include the research programs and methods targeted by animal rights groups and research on chlorinated chemicals targeted by Greenpeace. There are many other cases that would fit this role for civil society organizations, including calls for research moratoria on weapons development, genetically modified food, nuclear energy, and nanotechnology. Five more specific insights follow from and add complexity to this general observation. First, while we see undone science as unfolding through conflict among actors positioned within a multiorganizational field, as Gibbons' case shows, definitions of undone science may also vary significantly within different organizational actors, coalitions, or social movements. Some portions of the movement may be captured by mainstream research, and consequently advocacy is channeled into support for the experts' prioritizations of research agendas. Thus, a research topic such as environmental etiologies of breast cancer may represent undone science to a marginalized segment of breast cancer advocates and their allies in the scientific community, but it may represent negative knowledge to the majority of breast cancer advocates and the dominant cancer research networks. To further complicate the picture, rapid developments and changes within the scientific field, such as the development of genomic research to better pinpoint environmental or epigenetic factors, may result in shifts in research priorities that can open up opportunities for research in areas of undone science. Here, one sees that internal changes and differences among both researchers and civil society advocates interact to define shifting coalitions of research priorities. Second, the dynamic nature of coalitions and alliances that emerge around undone science suggests that the articulation of research priorities is often a relatively fluid process; even when civil society groups target some areas of scientific research as deserving low or no priority, their views may in turn lead to the identification of other areas of research deserving higher priority. For example, the position of an animal rights group may begin with opposition to some types of animal research but lead to support for more \"humane\" forms of animal research that have been reviewed by animal research committees. Likewise, the position of an organization such as Greenpeace in opposition to chlorinated chemicals is linked to an articulation of the need for research on green chemistry alternatives. As these examples suggest, the identification of undone science can be viewed as multifaceted outcomes of coalitions and conflict among diverse groups representing various social categories, each promoting a mix of topics seen as deserving more, less, or no attention from the scientific community. Third, making sense of the complex processes that produce undone science involves attending to the distributions of power, resources, and opportunities that structure agenda setting within the scientific field. An important element of field structure is the role of regulatory regimes in shaping definitional conflicts over research priorities. Howard's work suggests that done and undone environmental science dyads can be a key expression of the regulatory paradigm in which they occur and intimately linked to the way expertise is conceptualized and deployed in the paradigm. Furthermore, he proposes that until mainstream science faces a challenger, important forms of undone science within the dominant paradigm can remain implicit and unarticulated. In other words, undone science may take the form of a latent scientific potential that is suppressed through \"mobilization of bias\" (Lukes 2005; see also Frickel and Vincent 2007). Ottinger (2005) also notes the important role of regulatory standards in defining opportunities for activists who attempt to get undone science done largely using their own resources. In the case of air monitoring devices, an alternative research protocol and data gathering device operated by laypeople provides a basis for challenging official assurances of air quality safety. Rather than advocate for shifts in a research agenda, they simply enact the shift. In Howard's terms, the lay research projects also dramatize the implicit and unarticulated bias in the dominant method of air quality monitoring. Ottinger's (2005) focus on the double role of standards as enabling and constraining factors in establishing both the conditions and limitations of undone science is intriguing, and it remains for future research to examine the efficacy of tactical dynamics in relation to structural constraints encountered across a range of regulatory and research contexts. Fourth, while access to financial resources is an implicit focus of efforts to identify undone science, Kempner's research demonstrates that the interaction of civil society and research priorities is not restricted to the broad issue of funding. Although civil society organizations can exert an effect on research funding allocations, as we have seen especially in environmental and health research priorities, Kempner notes that there are other mechanisms that can cause such shifts. Her work suggests that efforts to study the problem of undone science should also consider the role that a moral economy has in shaping scientists' decisions about what research programs they will and will not pursue (Thompson 1971; on moral economy in science, see Kohler 1994). Furthermore, even if scientists do not accept in principle the notion that certain knowledge should remain undone, they may simply decide not to invest in some areas of research because of intense direct pressures from civil society organizations such as animal rights groups. As a result of individual decisions not to engage in an area of research, changes in the research agendas of a field can occur even when funding is not shifting dramatically. Finally, sometimes structural constraints such as limited access to resources coincide with practical constraints to produce \"undoable science.\" In the case of the chlorine sunset provisions, precaution advocates see governmental programs for screening individual chemicals as obscuring a plain fact: the sheer number of chemicals and their complex interaction with ecological and biological systems make it impossible to predict whether a given concentration of a given chemical will in any meaningful sense be \"safe\" or whether it will be a risk. As a result of this \"wicked problem\" (Rittel and Weber 1973), the articulation of undone science as a goal for research prioritization and funding-in this case, the standard assumption of a need for ever more research on the environmental, health, and safety implications of new chemicals-turns against itself, because the call for research into specific chemicals tacitly supports a regulatory framework that systematically generates a policy failure (see Beck 1995).",
          "This study demonstrates some of the ways in which the analysis of undone science can enrich empirical understandings of research agenda-setting processes. The considerable variation we find in just four cases suggests that one promising avenue for future research lies in developing more systematic comparisons across academic, government, industry, and community settings. Doing so will further elaborate the ways in which the institutional contexts of research-including different sets of political and economic pressures, normative expectations, resource concentrations, and sizes and configurations of research networksshape the articulation of undone science and the successful or failed implementation of alternative research agendas. Our broader aim in seeking to give undone science higher visibility within STS is to broaden the foundations for a new political sociology of science. Much like feminist and antiracist science studies, the political sociology of science situates questions relating to the uneven distribution of power and resources in science at the center of the STS project while remaining attentive to how knowledge and its inverse-ignorance-is socially shaped, constructed, and contested. As we have argued here, one of the crucial sites where questions of power, knowledge, and ignorance come together is in the domain of research agenda setting, where intense coalitions and conflicts are forged to gain access to the limited resources that ultimately shape what science is done and what remains undone. ",
          ""
        ],
        "ground_truth_definitions": {
          "undone science": {
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.",
            "context": "What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "c84a169e6df175c4662012d3ba7dbf8fa1b5abc9",
        "sections": [
          "In the past couple of years, the debate concerning so-called 'fake news' has prominently surfaced in news and politicians' speeches around the world. This catchy but fuzzy tag (European Commission, 2018) has mainly been used to describe misleading content disseminating across social media, but it has also been invoked by political actors to discredit some news organizations' critical reporting (Caplan et al., 2018;Tandoc et al., 2018). Alarmed by the simplification of this buzzword, researchers have started to unpack the concept, defining diverse types of 'fake news' (Allcott and Gentzkow, 2017;Jack, 2017;Marwick and Lewis, 2017;Warde, 2017) in order to understand the implications and solutions. In common with classic studies on misleading information dealing with rumours, propaganda and conspiracy theories (Allport and Postman, 1946;Lasswell, 1927;Sunstein and Vermeule, 2009), these most recent attempts to define 'fake news' seek to differentiate between specific forms of the phenomenon with reference to the source's intent to deceive (disinformation) versus the honest mistakes, negligence, or unconscious biases (misinformation) (Fallis, 2015;Floridi, 2011). In other words, the act of creating and injecting 'fake news' in the system is a defining moment for most classic and contemporary studies. In this article, we suggest a radical change of perspective. Such change is driven by deep transformations characterizing contemporary news systems, wherein older and newer media actors -with different degrees of potential reach, epistemological authority and skills of media manipulation -operate on the basis of overlapping and competing media logics. In this 'hybrid news system' (Chadwick, 2013), judgements with regard to the falsehood and motivations of propagators (the actors who share the fake news) can easily be different from the motivations of the original creator. Such patterns mean that what happens after the 'generative act' of a piece of false news is crucial to the study of real-world cases. The radical change of perspective required by the hybrid news system thus consists of a shift from exclusive attention to producers of 'fake news' to a broader approach that also focuses on propagators and, as a result, on the dynamic and diverse processes that characterize the dissemination of problematic information through multiple chains of propagation. On a theoretical level, our proposal is informed by the conceptual framework of sociocybernetics with specific reference to 'second-order cybernetics'. According to this approach -originally developed by Heinz von Foerster, whose aphorism 'Truth: The invention of a liar' has inspired the title of this article -information is not something that can be transmitted, stored, or retrieved in itself; it only exists 'when looked upon by a human mind' (von Foerster, 2003: 103). Following what von Foerster said about truth, the title of this article retains and twists the double meaning of its original inspiration insofar as it not only indicates simply and obviously that a piece of false information is invented by a liar. It also intrinsically turns its pronouncer into a liar, i.e. someone who indirectly claims to speak the truth by asserting that something is false. In this sense, the term 'fake news' is inherently divisive and detrimental to healthy debates. Information is an eminently social process: it is 'a difference which makes a difference' for an observer (Bateson, 1972: 321). What is informative for one observer can be uninformative for another observer. Interests, backgrounds, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. We contend that, in the hybrid media system, each 'fake news' cycle can only be studied as a unique process that emerges from multiple combinations of judgements on the falsehood of the news. Such judgements are made by all the diverse actors who decide to inject and share it, following logics that are related to their perception of the source, of the story itself and of the context. In order to describe our analytical model of problematic information, in the next section we trace back the notion of 'fake news' to classic and contemporary studies on disinformation and misinformation, highlighting what we consider the weaknesses of these analytical definitions: that is, their sole focus on the initial step of the process -when someone introduces false information into the system. We then briefly describe the literature that has informed our definition of the contemporary media context in terms of the hybrid news system and the radical change in perspective that it requires: that is, analytical attention to both the generative acts of 'fake news' and to what happens in the processes of circulation. We thereafter consider this innovative perspective's roots and its focus on the observer's judgements regarding informativeness. Next, we discuss the profound consequences of this theoretical gaze on a micro, meso and macro level analysis of real-world 'fake news' news stories.",
          "Even if the notion of 'fake news' has been popularized only recently, the dangerous implications of unverified, inaccurate, defective and false information have been extensively studied by a diverse set of academic literature since the early 20th century. Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989(Habermas, [1962]]; Jack, 2017). There is, however, a lack of agreement regarding the diverse types of misleading information that fall under these two concepts of disinformation and misinformation, for example, satire, parody, and even bullshit. In the cases of satire and parody, in particular, the source intentionally produces misleading content, yet the intent is not to mislead, and the source does not benefit from being misleading (Fallis, 2015). Nevertheless recent studies have included (news) satire (content that exaggerates facts to convey a critique) and (news) parody (non-factual information produced to inject humour) among the forms of problematic information that can fool both large publics and influence intermediaries of information (Jack, 2017;Warde, 2017). Another type of misleading information that evades the classic distinction between disinformation and misinformation is what Frankfurt and Bischoff (2005) define as 'bullshit', wherein the source does not care if what he/she says is true or false but only aims to persuade or to provoke a reaction. Among the various types of problematic information, rumour has received the most systematic academic attention (Allport and Postman, 1946;Rojecki and Meraz, 2016). Rumours are forms of information that are characterized by uncertain veracity and that can be later proven false. The credibility of rumours is unrelated to direct evidence but to the fact that other people seem to believe them. Rumours are often at the origin of conspiracy theories, which are explanations for events through the causal agency of a group of individuals acting in secret (Keeley, 1999). If we had to describe conspiracy theories using the distinction between disinformation and misinformation, one could say that conspiracy theories are a form of disinformation since these theories can often easily be proven false, and it is no accident that the information is misleading (Fallis, 2015). At the same time, conspiracy theories could be identified as misinformation since some supporters of these theories are entirely sincere and believe them to be true (Sunstein and Vermeule, 2009). Another type of problematic information that has received considerable attention from researchers is propaganda. Studies on propaganda can be traced back to Lasswell (1927), who explained how manipulated significant symbols can manage collective attitudes, promoting a particular political side or perspective. The study of propaganda is obtaining renewed interest due to the increasing skills of media manipulation that have enriched online propaganda tactics, with the development of bots (Albright, 2016), which give the impression that many people are liking a piece of misleading news. Another form of disinformation that is garnering renewed academic interest is deceptive advertising, particularly advertising materials that simulate the formats of legacy news media to confer more legitimacy to a one-sided article or website, with the aim of obtaining financial gain or a positive public perception (Allcott and Gentzkow, 2017;Tandoc et al., 2018;Warde, 2017). Other types of problematic information related to new digital techniques are doctored and manipulated photos/videos (Farid, 2009), which may have no factual basis or may be factual but misappropriated. Contemporary media cultures also challenge the definition of the motivations that drive problematic information in contemporary media environments: internet trolls (Marwick and Lewis, 2017;Phillips, 2015), for example, may deliberately create misleading content for fun, with the aim of provoking outrage or fooling their audience. While helpful, most of these concepts and typologies of misleading information focus on the act of creation as the defining moment, looking at the author's judgement with regard to the falsehood and distinguishing between his/her intentions (to deceive, convey a critique, inject humour, persuade, reduce uncertainty, give an impression of events, make money, promote a particular political perspective/product, have fun). We argue that this focus represents a limit for the study of problematic information in the contemporary media environment, in which the diffusion of news depends on the multiple actors involved in the process of newsmaking and news sharing.",
          "Information cycles of the 21st century are characterized by a multitude of highly interdependent media technologies, in which numerous media actors follow newer and older, overlapping and competing media logics, producing news and other products with a news format (Chadwick, 2009(Chadwick, , 2013)). According to this vision, multiple and integrated platforms have rendered the process of newsmaking more inclusive, blurring the last century's boundaries between professional journalism and user-generated content as well as affecting the norms and values that guide journalists' practices in the (mixed) genres of journalistic information and the organization of newsrooms (Hermida and Thurman, 2008;Tandoc, 2014). In the hybrid media system, a plethora of actors -such as bots (Albright, 2016)participate in the production and dissemination of both journalists' ratified news and a wide range of other products that take on a news format. As boyd (2017) wrote, contemporary publics are increasing able to 'hack the attention economy'. These multiple actors, all involved in the hybrid processes of newsmaking and news sharing, require a radical shift in the study of misleading information, from exclusive focus on the first step of the process (the creation of news) to also considering what happens after this generative act. Judgements regarding the falsehood of news and motivations to share it can easily be different from those of the authors. We trace this shift of perspective back to second-order cybernetics.",
          "As anticipated by the review of the literature, most of the existing works emphasize that judgements on the level of facticity of online content are strongly interlinked with assessments regarding the intentions of the content creator. Understanding the intent of the creator of 'fake news' is thus considered a crucial indicator for differentiating among different types of problematic information. Nevertheless, several recent works recognize the difficulties in clearly assessing such intent (Jack, 2017). Even employing the state-of-the-art techniques of digital investigative journalism, the cues left by these actors are often inadequate for clearly differentiating between honest mistakes, deliberate deception and satire. This phenomenon is often referred to as 'Poe's Law' (Aikin, 2013). Furthermore, certain actors (e.g. trolls) deliberately sow confusion about their real intentions by framing as satire their false and/or outrageous content (Marwick and Lewis, 2017). No matter how difficult this assessment, billions of actors of the hybrid media news system are called upon every day, often multiple times a day, to quickly make such judgements on the online content to which they are exposed. Many experts both inside and outside academia are working to make this judgement process easier and less prone to error (Caplan et al., 2018). While recognizing the importance of these efforts, we take a different perspective, a perspective that includes at its core the possibility that misjudgements occur: a second-order perspective. By second-order perspective, we specifically refer to the work carried out by an interdisciplinary group of scientists (Norbert Wiener, Claude Shannon and Warren McCulloch, to name just a few) in the United States following the Second World War and developed under the programme title of 'second-order cybernetics' by a team led by Heinz von Foerster, an Austrian-American scientist combining physics and epistemology, at the Biological Computer Laboratory of the University of Illinois at Urbana-Champaign (Heims, 1991). Second-order cybernetics postulates that a system comes 'into being' when an observer acts to 'draw its boundaries ' (von Foerster, 2003). Once applied to media and communication, this approach suggests shifting attention from the source (the content creator) to the observer of the source (those who are exposed to this content). According to von Foerster (2003), a source in itself is rarely important unless someone pays attention to it. In a certain sense, sources only 'come into being' when an observer recognizes them as sources. According to Gregory Bateson (1972), an anthropologist who met von Foerster and Shannon multiple times during the Macy conferences (Heims, 1991), information is in fact 'a difference that makes a difference'. Bateson's definition of information underlines the crucial role played by the expectations of the receiver/observer. From this perspective, information is not something that can be transmitted, stored, or retrieved; it exists only 'when looked upon by a human mind' (von Foerster, 2003). What is informative for one observer may be uninformative for another observer. The observer's interests, background, previous knowledge and biases matter both at the level of 'recognizing' (paying attention to a source) and 'making' (inducing some sort of change in the receiver, e.g. propagating a certain content) the difference. While the intent of the content creator (first-order perspective) is often difficult to assess, the observers exposed to certain content (second-order perspective) formulate their best guess regarding both facticity and creator's intent. These judgements affecton different scales, depending on the observer's position in the system and number of connections to the actors -the entire process. On this basis, the observer defines the subsequent course of action (e.g. decides to further share a content). By extension, a researcher analysing the way an information cycle unravels from an external perspective is a third-order observer attempts to assess the intentions and goals of both first-order and second-order observers: they try to guess the intention and goals of the initiators of an information cycle, as well as the motivations of those who take part in it. At the same time, researchers also define a case of online spread of information as 'fake news', which thus represents the actual starting point of the analytical process. Given the multiple levels of observations at stake, our perspective is constructivist but not relativistic. All observers, including a researcher potentially studying the process, tend to judge the truthfulness of content from their unavoidably limited perspectives. Different actors are differently equipped to support this judgement process. Nevertheless, even professional subjects and organizations sometimes misjudge. These perspectives are different but not equal. For this reason, we introduce the distinction between 'true'/'false' (with lower case initial letters) according to the perspective of the actors involved in the process and 'True'/'False' (with capitalized initial letters) according to an external perspective, such as that of the researcher using the model. By the same token, a propagator is an actor of the hybrid media system (e.g. a journalist, news organization, citizen, politician, troll, fake account, bot) who is exposed to False information (created/shared by a creator or previous propagator) and, for numerous possible reasons, decides to share it further (thereby creating propagation). It may be the case that the propagator is exercising bad judgement (believing to be true what is in fact False), or it may be the case that she/he correctly judged the information as false and deliberately decides to share it to deceive others. In other terms, echoing Bateson's extension of Shannon's information theory (Shannon and Weaver, 1949), we suggest adopting a second-order perspective to shift attention from the source -as an entity with its own static properties (motivations, biases, drivers) -to the observer who consumes content originally published by the source and makes a judgement about its properties. We argue that, however right or wrong this judgement may be, the descriptions made by these observers shape the process of misleading information as a complex phenomenon emerging from the interplay of a multitude of actors in the hybrid news system. From this perspective, a False information cycle is rarely reducible to a single typology. A news article clearly fabricated with the aim of exploiting the logic of online advertising (and thus produced as disinformation) may become a must-read article within a community of like-minded believers of certain conspiracy theories and shared as a legitimate piece of news (misinformation). The implications of this radical change of perspective are presented and discussed in the next sections dedicated to the micro, meso and macro levels of our analytical model. By employing such terminology and analytical approach we address the call to precision that Turner (2006) has recommended to sociologists by inviting them to develop theories aimed at explaining social phenomena by paying attention to relationships between forces operating at different levels of the social universe.",
          "Multiple cycles of information constantly populate hybrid news systems. They result from the activity of various actors who produce and share information with small and large audiences that might or might not decide to actively contribute to their further circulation. Each of these cycles is grounded upon cascades of judgements (whether or not to believe a piece of information) and decisions (whether or not to share a piece of content). These actions are taken, with very different levels of sophistication, by diverse individuals, groups and organizations. Despite being guided by different logics, ethical concerns and skills, all the different actors populating these systems, we argue, ground their judgements and their decisions upon similar principles. In the following sections, we discuss the dissemination of False information within the hybrid media system by focusing on three different levels: a micro level consisting of the bases on which subjects judge the truthfulness of the information with which they engage; a meso level consisting of the matrix of expectations between actors and content at each propagation of False information; and a macro level consisting of the broad and heterogeneous process emerging from a chain of False news propagation. While these three levels of the analysis are presented, for sake of clarity, as a bottom-up structure, it is worth underlining that both the meso and the macro level are emergent phenomena.",
          "First, we focus on how single actors process information. False information is potentially an extremely 'informative' (and thereby effective) form of information (Karlova and Fisher, 2013), as unexpected novelty is a defining characteristic of information (Shannon and Weaver, 1949). Focusing on the 'informativeness' of False information allows us to employ journalism studies and literature on information sharing within digital environments to also discuss how the multiple actors of the hybrid media system make judgements and take decisions when exposed to False information. Starting from the classification proposed by Wathen and Burkell (2002) for the internet 1.0, we argue that all individuals and institutions inhabiting hybrid news systems evaluate the truthfulness of the information they manage on the basis of specific patterns regarding: (a) the source, (b) the story and (c) the context. According to the literature on digital information ecologies, two elements influence individuals in their assessments of a piece of information's truthfulness on the basis of its sources: authority (Clark and Slotta, 2000;Rieh, 2002) and proximity (Meyer, 1994). Given that the credibility of established institutions has plummeted over the past decades (Peters and Broersma, 2013), multiple scales of authority have emerged by which the status of 'influencers' is granted on the basis of the attention received from a very specific community (Marwick and boyd, 2011;Tufekci, 2013). When authority becomes strictly contextual, it is much more complicated for individuals to adopt such parameters to assess the trustworthiness of the original source of a piece of news. In this context, the (social or ideological) proximity characterizing our relationship with the node in our digital network endorsing a piece of information to which we are exposed becomes much more important than its original source (Messing and Westwood, 2014). Authority and proximity thus overlap almost completely, with the result that the source's expertise -a key element of authority -loses its centrality in how individuals evaluate the trustworthiness of a piece of information. Classic literature on newsmaking has shown that a source's capital of authority influences the credit granted to that information by journalists as well (Gans, 1979;Tuchman, 1978). Such patterns privilege institutional sources over other types of sources since institutional authority is considered a byproduct of the position of power the institution occupies within society. Digital and especially social media have offered institutional and political actors the opportunity for an uninterrupted flow of communication, within which False information can be strategically disseminated, exploiting some of the affordances of these platforms. Just as with everyday citizens, journalists' proximity is likewise indicative of a source's credibility (Gans, 1979): a frequently 'encountered' source becomes familiar and is thus more likely to be trusted. Within contemporary media ecologies, it becomes much easier for journalists to constantly engage with multiple potential sources of news through digital media, allowing them to become proximate with an increasingly diverse range of sources, including nonelite and even anonymous subjects. A second crucial element orienting individuals and media institutions in their evaluations of the truthfulness of a piece of information is its content. Social psychology literature has stressed that people tend to believe true stories that are coherent with their vision of the world and are relevant to them, terming such processes 'confirmation bias' (Nickerson, 1998). Two opposing patterns characterizing news consumption on social media -i.e. high selectivity (Prior, 2013) and incidental exposure (Fletcher and Nielsen, 2017) -both interact with confirmation bias in affecting judgements of truthfulness. Since users craft their information networks on the basis of commonalities related to specific interests and visions, tribalism can emerge (Sunstein, 2017), with in-group members judging content truthfulness on the basis of its capacity to reinforce community bonds. At the same time, since social media are 'news-finds-me' environments (Gil de Zúñiga et al., 2017), they can expose users to highly conflicting information, with confirmation bias becoming the most important principle orienting the way in which information is processed (Zollo et al., 2017). Despite professional standards, congruence (with their own or their audience's vision of the world as well as with the editorial line of their company) and relevance (for the public debate at a given moment) can also be regarded as key to the process through which journalists and news organizations evaluate a story's newsworthiness (Golding and Elliott, 1979). Finally, contextual factors influence people and institutions in their assessments of the truthfulness of a piece of information. Here we refer to the situation within which information is processed. The informational exuberance (Chadwick, 2009) characterizing contemporary information ecosystems can result in what has been described as 'information overload' (Austin et al., 2012), a condition in which content is very difficult to process, negatively impacting the benefits individuals derive from it. The result is a limited attention (boyd, 2010) to processing information, potentially influencing how individuals assess truthfulness. Contextual factors also impact the ways in which professional actors process information and assess its truthfulness. When an extremely relevant story unexpectedly breaks out, professional actors can experience an information overload due to the highly augmented density of information flowing online. In this manner, as it has been described both in the literature and in journalistic accounts (Schifferes et al., 2014), inaccurate or even fabricated information produced and shared by known and unknown sources passes professional newsmaker gatekeepers. The resulting practice has been described by Bruno (2011) as 'tweet first, verify later'.",
          "Multiple judgements (true/false) on the information circulating within the system and sharing decisions place actors in relationships with one another and with information content, generating multiple and multifaceted chains of propagation. We will discuss the possible structures of these chains as a whole in the next section; we focus here on the links in the chain, i.e. the individual propagations or the meso level of our analytical framework. At this level, the observer-dependent perspective introduced above becomes relevant. Since we aim to consider multiple, and frequently conflicting, judgements regarding the truthfulness of information produced and/or shared, recognizing the unavoidable subjectivity characterizing the whole process -including the process of the researcher studying the cycle -is very important. Indeed, it is the observer's perspective that draws the boundary of the system by determining that we are dealing with a cascade of propagations of False content (here, as we have already explained above, the capitalized initial letter distinguishes the outcome of the researcher's judgement from the outcomes of the propagators' judgement). Starting from these premises, the possible combinations of the judgement of the original author of the False information (injector) and the judgements of further propagators and between various propagators when the circulation cascade is activated generate a matrix of four possible scenarios (Table 1). In the first case (1), both actors are aware of the false nature of the information but nevertheless decide to share it. While this pattern can be observed in the propagation of humoristic and satirical content, it can also be the case of two subjects who, for strategic reasons (e.g. propaganda), deliberately produce and share false information. We define this as 'pure disinformation' propagation. Conversely, when a piece of information originally injected as true is shared by a propagator who thinks it is false (2), we witness a case in which misinformation is exploited to become disinformation. In most of the cases in which False information is produced as true, the creator unintentionally injects misleading information into the system. A third option occurs when a piece of information is devised as false by the injector but perceived as true (3) by the propagator: we are observing a case of disinformation propagated through misinformation. In this case, an actor produces information knowing it is False, but other users perceive it as true and share it as such. Gross examples of this often result in embarrassing moments for the propagator (especially when the subject is a news organization) who mistakenly understood as true fabricated false information. Finally, when False information is perceived as true by the injector and by the propagator (4), we are witnessing what we could assume to be a classic process of propagation of misinformation. It is the case with the most common conspiracy theories, which flourish thanks to the content created and shared by members of polarized online communities that firmly believe in what they communicate. Other times, False information inadvertently injected into the system by an authoritative source (e.g. a respected news media organization) keeps propagating as a legitimate piece of content until someone proves it False. Given the context described above, it should be clear that trying to understand the nature of contemporary disinformation by focusing solely on the initial step of the process -when someone introduces information into the system -is at the very least an oversimplification. Over 60 years ago, Katz and Lazarsfeld (1955) argued that mass information flows depended on the personal influence that opinion leaders exercised upon other citizens, showing how, in a process, the 'first step' is not necessarily the most important one. Testing the two-step flow theory in the context of social media, Hilbert et al. (2017: 456) empirically demonstrated how, by considering the multiple perspectives of all actors involved in information cascades, the model best describing information flows can vary from a simple single step to a 'some kind of intricate network-step flow'. Similarly, Messing andWestwood (2014: 1058) contend that, within social media ecologies, the agenda-setting power that was originally concentrated in newsrooms is now diffused across social networks. The multiple actions of propagation, guided by single judgements and decisions, have thus become the backbone of news flows within the contemporary hybrid information system. Information cycles are thus better represented by a dynamic growing tree-like structure of propagation cascades, conceptually similar to those studied in the field of complex propagation (Centola et al., 2007). Conceptualizing False information cycles in terms of propagation cascades has two major consequences. First, a cycle/cascade is very unlikely to exclusively include propagations of a single type. Every real-world cycle is most likely a combination of multiple semi-independent types of propagation (Table 1) in which individual assessments and decisions are constantly influenced but not determined by the local dynamics discussed above. This process is illustrated in Figure 1. A central consequence of this way of understanding information cycles is that, despite each information cycle being a set of diverse propagations, it is still possible to theoretically describe 'global cascades' as an emergent and thus autonomous phenomenon. Within this perspective, the emergence of spreading dynamics on a systemic level (e.g. news and misinformation propagating through viral and apparently unstoppable dynamics) is coherently built upon several individual processes, thus the study of specific cases of propagation can be used to understand underlying dynamics and how these micro and meso dynamics merge into the macro evolution of a single information cycle. Keeping this is mind, it is important to stress that while every information cycle is most probably composed of several coexisting types of propagation, it is likely that every cycle will be dominated by a single type or by a combination of types.",
          "On 15 November 2015, the Spanish newspaper La Razon published on its front page the face of one of the terrorists responsible for the attacks that had hit Paris a few days earlier. Unfortunately, the face did not belong to one of the terrorists involved in the terrible attack on the French capital but instead to Veerender Jubbal, a young Canadian man. Before the attacks, Veerender Jubbal -who is Sikh and wears a turban -had posted a selfie taken with his iPad, asking his Twitter followers to wish him 'good luck'. After the attack, someone easily edited the photograph by replacing the iPad with a Quran and Photoshopping a suicide vest onto his shirt. The fake picture went viral online and was shared by various news organizations, even making it to the front page of the print edition of La Razon, before it was finally debunked and people began listening to Veerender's complaints. The origin of the Photoshopped image is in some sense largely irrelevant to the study of the cycle of False information. The picture was created and shared with the knowledge that it was a fake, but it was quickly picked up by several sources who judged it true. The diffuse proximity allowed by social media, confirmation bias dynamics (terrorists must wear turbans, and it does not matter if it is a dastar, a Sikh turban) and the 'tweet first, verify later' logic can perhaps explain how a photograph of a smiling man from the internet can be judged as the true picture of a suicide bomber, taken just before an attack. Nevertheless, once the picture was online, it was quickly shared as a legitimate picture of a terrorist, and it took over 24 hours to stop the dissemination of the False information. On 18 November 2016, Chris Lamb wrote in the US edition of The Huffington Post a satirical piece in which he imagined the US president Donald Trump suggesting the removal of the Statue of Liberty because it encouraged immigration. Despite the obvious satirical content of the article, several Italian newspapers (including the leading Il Corriere della Sera and La Repubblica) and TV channels (including La7 and RAI News) reported the news that 'Trump attacked the Statue of Liberty'. The news got great attention in Italy and was widely shared as a story confirming the unpredictability and unorthodox behaviour of the US president, before being retracted by the same newspapers that initially shared it. On 9 November 2016, the day after Republican presidential nominee Donald Trump claimed an unexpected victory in the US presidential race, a grainy picture began circulating on social media, along with the claim that the Ku Klux Klan were openly marching in Mebane, North Carolina to celebrate the win. According to investigations by the website Snopes.com (Palma, 2016), the people marching were indeed supporters of the conservative candidate celebrating the victory, but what the original creator of the picture mistook as a robe was in fact a flag. Nevertheless, the photo circulated widely on social media and partisan websites as an evidence of the connection between the conservative president-elect and the Ku Klux Klan. These are just three minor stories that illustrate key elements of the system we have described: a. The intention of the injector does not determine the future evolution of the False information cycle. Given the contemporary hybrid media system and the complex nature of information cycles, False and misleading information need not be born as such. The real goal behind the production of a piece of False information may be largely unknown (as in the case of Veerender Jubbal), can be satirical (as in the case of Lamb's article) or may be born from an honest mistake (as in the case of the picture taken in North Carolina). b. False news can only be understood as a process. Information will be constantly assessed by a number of different actors, which will act in accordance with their individual criteria to establish its truthfulness. If and when these individuals share the information, this will generate a nearly unique chain of types of propagations, defined by the model presented in Table 1. c. Information equilibrium does not imply consensus. One consequence of the model and the first two points above is that societal consensus regarding the nature of any information is somewhat unlikely. When information is constantly assessed by a number of independent actors acting in accordance with the aforementioned principles, every operation aimed at establishing the real nature of a piece of content (e.g. debunking or fact checking a piece of news) will itself be judged as true or false and ultimately accepted only by a limited group of actors.",
          "In this article, we have proposed a new approach to studying False information. Building upon an interdisciplinary theoretical background, we have suggested moving beyond a simple focus on the initial step of the news cycle (when news is actually produced and regarding the producer's intentions) to instead observe the cycle as an emergent whole influenced but not determined by individual assessments (concerning the nature of the information) and decisions (concerning whether to propagate the information). We have suggested that this is necessary due to the hybrid nature of contemporary media systems, and we theoretically grounded our approach in a second-order perspective. We have suggested that the propagation of False information should be investigated especially at the micro level (actors' judgements) and meso level (combinations of actors' decisions), while we contend that the processes characterizing the macro level should be seen as the result of the multiple combinations characterizing the previous two levels. We have argued that, at the micro levels, all actors in the hybrid media system operate, at their very core, in the very same way (despite the different levels of perceived responsibility, ethical concern, and ultimate goals characterizing them). These key processes, when connected within a relational model, produce a set of four typologies of propagation that can explain all the various types of actual propagation, including those that have always been highly problematic for pre-existing models (e.g. satire, bullshit). We therefore suggest that 'theorizing about the propagator' is beneficial to understand, mutatis mutandis, how both False and True news circulate. Certainly, we are not the first to suggest placing more focus on propagation processes. Academic research on rumours and conspiracy theories has gone beyond the generative moment of information, looking at the process of disseminating misleading information. And yet the scenarios that emerge from this wide body of research have focused only on a type of propagation in which -at times clashing with the initial goals of the authorspeople share False information while believing it to be true. A more coherent focus on the propagation process would allow us to address a number of cases that would otherwise be difficult to explain with an appropriate theoretical model, such as the case of propagators of False information who recognize its falsehood but nevertheless decide to diffuse it. This also applies to the cases of satire and parody, which may be generated with positive intentions but may nevertheless be judged true and diffused as such as the result of an honest yet dangerous mistake. Adopting this approach allows us to observe diffusion of False information as a much more nuanced phenomenon that, as shown in our examples, does not require a single understanding of the news or assume a single possible reaction. When information is constantly assessed by a number of independent actors, who then act in accordance with their assessments, a situation of coexisting opposing beliefs is the normal status of the system rather than the exception. Finally -much like Niklas Luhmann's idea of communication as an ephemeral event -a propagation is, by definition, an event in a chain of propagations and a specific cascade of propagations with its emergent properties becomes observable as an autonomous phenomenon. As such, societal communication can reference this whole case in terms of codes other than true/false. Different systems in a functional differentiated society (Luhmann, 2012) can thus address a cascade as a whole using their own perspective. While true/false is the code used by science, the media system would deal with this case in terms of its informativeness (information/non-information) and the law system using its own binary code (legal/illegal). The term 'fake news' as it is now commonly used covers completely such reality, which is why we provocatively argue that only a liar can breezily use it."
        ],
        "ground_truth_definitions": {
          "disinformation": {
            "definition": "Misleading information deliberately aimed at deceiving others.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          },
          "misinformation": {
            "definition": "False or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases.",
            "context": "Scholars working in the field of the philosophy of information have differentiated between disinformation and misinformation on the basis of the source's intent to deceive (Floridi, 1996). The concept of 'disinformation' refers to misleading information deliberately aimed at deceiving others, while 'misinformation' implies false or inaccurate information circulating as a result of honest mistakes, negligence, or unconscious biases (Fallis, 2015). This conceptual distinction has been highly successful within and beyond the philosophy of information (Habermas, 1989 [1962]; Jack, 2017).",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "You are a term‑definition extraction assistant.  \nGiven a single section of a scientific paper, you must:\n\n1. **Identify all explicit term‑definition pairs** that appear in the section.  \n   * A pair consists of a term (the concept being defined) and a definition sentence that explains the meaning of that term.  \n   * The definition must appear in the same section and be written in the paper’s own words.  \n   * Do not invent definitions or use information from outside the section.  \n   * If a term has synonyms defined in the same sentence, list them together separated by “/” in the term field (e.g., `hateful/swearing`).  \n   * If a term is defined more than once, keep the first clear definition.  \n\n2. **Provide a short, one‑sentence definition**.  \n   * Remove any introductory phrases such as “is defined as”, “means”, or “refers to”.  \n   * Keep the definition lowercase except for proper nouns.  \n\n3. **Include context**: capture one full sentence immediately before and one after the definition sentence (if available).  \n   * Preserve the original wording and formatting exactly as it appears in the section.  \n\n4. **Mark the type** of definition:  \n   * `explicit` if the sentence directly states the meaning (e.g., “A term is defined as …”).  \n   * `implicit` if the meaning is inferred from surrounding text but the term is not explicitly introduced.  \n\n5. **Output format**:  \n   * First, write a short chain‑of‑thought **Reasoning** that explains how you identified each definition and why you chose its type.  \n   * Then output `Extracted Terms:` followed by a JSON array of objects, one per pair, with the keys `term`, `definition`, `context`, and `type`.  \n   * Do **not** add any other text after the JSON.  \n\n6. **No definitions**: If the section contains no explicit term‑definition pairs, return an empty JSON array `[]` and a brief reasoning stating that no definitions were found.\n\n**Example prompt**  \n```\nSection: The term \"sharing of fake news\" is defined as the act of posting any link from a source that has a history of publishing falsehoods. This measure helps us distinguish fake from real content.\n```\n\n**Expected output**  \n```\nReasoning: ...\nExtracted Terms: [\n  {\n    \"term\": \"sharing of fake news\",\n    \"definition\": \"the act of posting any link from a source that has a history of publishing falsehoods\",\n    \"context\": \"The term \\\"sharing of fake news\\\" is defined as the act of posting any link from a source that has a history of publishing falsehoods. This measure helps us distinguish fake from real content.\",\n    \"type\": \"explicit\"\n  }\n]\n```\n\nFollow these rules exactly. If you are unsure whether a sentence is a definition, err on the side of *no* to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "We follow best practices (Guess, Nyhan and Reifler 2020, Grinberg et al. 2019, Lazer et al. 2018) and operationalize \"sharing of fake news sources\" as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news. To this end, we cross-referenced all tweeted URLs against a list of 608 fake news sources, constructed by journalists and scholars (for details, see SM Section 10 and Guess et al. 2019, Allcott, Gentzkow andYu 2019). The list was compiled to document the prevalence of fake news during the 2016 U.S. presidential election but has been updated to account for fake news publishers that have emerged since. To the extent possible, the list creators have coded the most popular fake news sources for partisan slant: 12 of the most popular were *pro*-Democratic fake news sources while 30 were *pro*-Republican sources. While this number may seem low, note that almost 86% of all shared fake news sources originated from one of these 42 domains for which we have partisan ratings. (See SM Section 6 for the complete list of fake news domains and Figure SM 2b for the 20 fake news domains that were shared the most.) As a robustness test, supplementary analyses discussed below group the fake news sources according to a more fine-grained coding scheme proposed by Grinberg and colleagues (2019) that distinguishes among three classes of fake news sources differing in their likelihood of publishing misinformation. In SM Section 8, we also replicate the main results using a different, but partially overlapping list of fake news sources. Our measure of fake news sharing records actual online behavior and thus does not suffer from biases that can plague self-reported measures of online activity. Further, while the present study is certainly not the first to couple survey and social media data (e.g., Guess, Nagler and Tucker 2019), it has the most detailed set of psychological measures; see below. Still, it is important to keep in mind when interpreting our results that we cannot tell if the specific stories people share are true or false: Our fake news measure is at the publisher level, not the story level. To measure the \"sharing of real news sources,\" we cross-referenced participants' tweets against a list of real news publishers obtained from the AllSides organization. AllSides seeks to help citizens navigate the online media landscape by providing \"balanced news and civil discourse.\" Their web page (www.allsides.com/media-bias/) contains a list of 260 real news publishers with ideological ratings: \"Left\" (labeled \"Strong Democratic\" below, n = 45), \"Lean Left\" (\"Leaning Democratic\", n = 63), \"Center/Mixed\" (n = 88), \"Lean Right\" (\"Leaning Republican\", n = 29) or \"Right\" (\"Strong Republican\", n = 39). While AllSides combines different methods for estimating ideological bias (e.g., editorial reviews, third-party judgments), SM Section 7 shows that their ratings correlate highly -Pearson's r = .90with network-based measures of ideological alignment of media sites (Bakshy, Messing and Adamic 2015). See SM Section 7 and Figure SM 2b for the 20 most shared real news domains. Our measures of real and fake news sharing account for ideological and partisan slant at the source level and are thus useful for understanding the content people care about when sharing news. To gain further leverage on testing the relevance of content for sharing behavior, we also built two unique data sets of news headlines (see SM Section 11 and 12 for details). We focus on headlines because these are more accessible than full texts and because prior research demonstrates that simply seeing a headline can exert large effects on decisions to share news on social media (Effron and Raj 2019). The first data set was based on the headlines of stories shared by our participants, which gives us a clear representation of the content they deemed sufficiently important to share. To construct the data set, we first retrieved the full URLs from all 85,084 tweeted news links shared by our participants. Based on these URLs, we parsed the news domains' HTML code and successfully extracted the headlines of 75,560 news articles (89% of all article headlines). To construct our second data set, we used the Archive.org web page -an indexing system for the Internet's web pages for historical archiving -to scrape headlines that had appeared on the daily front pages of the most popular (among our participants; see Figure 5 below) news sites in the period 2016-2019 (i.e., covering roughly the same period for which we have tweets from our participants). Compared to our Twitter headlines, this data set contains a much larger number of news story headlines (we successfully scraped a little over 500,000 headlines). More importantly, the front page headlines offer a glimpse of the universe of news stories our participants could potentially see and read before deciding which stories to share. In this way, it sidesteps issues of \"selection\" biases in sharing decisions due to psychologically motivated reasoning; a point we return to during the analyses. We subjected these headlines to two types of automated analyses. To measure the sentiment of the headlines, we relied on the sentimentR R package, which includes a sophisticated algorithm for calculating text polarity sentiment. This algorithm improves the performance of more traditional dictionary-based methods by calculating sentiment on the sentence level and considering valence shifters (e.g. not, very, hardly). To identify the political affiliations of those mentioned in the headlines, we created two custom dictionaries of prominent Republican and Democratic elites. Our primary dictionary is based on YouGov's list of the 100 most famous Democrats and Republicans. 2 We replicate our results with an alternative dictionary listing all members of the US Congress, Senate and the Trump Administration in the supplemental materials. Finally, we matched these dictionaries to the headline data sets and recorded whether a given headline mentions anyone from the Republican or Democratic party. We use these more detailed content measures in the final part of the analysis, whereas the initial parts focus on the crude distinction between fake and real news sources.",
        "reasoning": "The section contains an explicit definition of the term “sharing of fake news sources”. It states: “operationalize 'sharing of fake news sources' as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news.” This includes the cue “operationalize... as” and clearly explains the meaning of the term, satisfying the definition criteria.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "",
        "reasoning": "No section content was provided, so there's nothing to analyze for a definition. Without text, we cannot identify any definition cues such as “is defined as,” “refers to,” or “means.” In the absence of evidence, we must err on the side of “no.”",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "So far we have only considered tasks in which the cost of information gathering and the availability of information are the same for +tests and -tests. However, several studies have looked at hypothesis testing in situations where tests are costly. Of particular ecological relevance are those tasks in which one must learn from the outcomes of one's actions. As mentioned earlier, studies by Tschirgi (1980) and Schwartz (1982) suggest that when test outcomes determine rewards as well as information, people attempt to replicate good results (reinforcement) and avoid bad results (nonreinforcement or punishment). This encourages +Htesting, because cases consistent with the best current hypothesis are believed more likely to produce the desired result. Einhorn and Hogarth (1978; see also Einhorn, 1980) provide a good analysis of how this can lead to a conflict between two important goals: (a) acquiring useful information to revise one's hypothesis and improve long-term success, and (b) maximizing current success by acting the way you think works best. Consider the case of a university admissions panel that must select or reject candidates for admission to graduate school. Typically, they admit only those who fit their hypothesis for success in school (i.e., those who meet the selection criteria). From the point of view of hypothesis testing, the admissions panel can check on selected candidates to see if they prove worthy (+Htests). It is much more difficult to check on rejected candidates (-Htests) because they are not conveniently collected at your institution and may not care to cooperate. Furthermore, you would really have to admit them to test them, because their outcome is affected by the fact that they were rejected (Einhorn & Hogarth, 1978). In other words, -Htests would require admitting some students hypothesized to be unworthy. However, if there is any validity to the admissions committee's judgment, this would have the immediate effect of reducing the average quality of admitted students. Furthermore, it would be difficult to perform either kind of Ttest in these situations. -l-Ttests and -Ttests would require checking known successes and known failures, respectively, to see whether you had accepted or rejected them. As before, information about people you rejected is bard to come by and is affected by the fact that you rejected them. The net result of these situational factors is that people are strongly encouraged to do only one kind of tests: +Htests. This limitation is deleterious to learning, because +Htests reveal only false positives, never false negatives. As in Wason's 2,4,6 task, this can lead to an overly restrictive rule for acceptance as you attempt to eliminate false-positive errors without knowing about the rate of false negatives. On the other hand, our analyses suggest that there are situations in which reliance on + Htesting may not be such a serious mistake. First, it might be the case that you care more about false positives than false negatives (as suggested earlier). You may not be too troubled by the line you insert in rejection letters stating that \"Regrettably, many qualified applicants must be denied admission.\" In this case, +Htesls are adequate because they reveal the more important errors, false positives. Even where both types of errors are important, there are many circumstances in which +Htests may be useful because false positives are more likely than false negatives (see Table 1). When P(t) = P(h) andp(t) < .5, for example, the false-positive rate is always greater than the false-negative rate. In other words, if only a minority of applicants is capable of success in your program, and you select about the right proportion of applicants, you are more likely to be wrong about an acceptance than a rejection. As always, the effectiveness of a +test strategy depends on the nature of the task. Learning from +Htests alone is not an optimal approach, but it may often be useful given the constraints of the situation.",
        "reasoning": "The passage discusses various concepts such as hypothesis testing, +Htests, -Htests, and their application in a university admissions context. However, it does not provide a clear, standalone definition of any particular term. There is no phrase like “is defined as,” “means,” or “refers to” that introduces a specific word or concept with a formal explanation. The discussion mainly elaborates on the implications and use cases of existing concepts rather than defining a new term. Therefore, the section does not contain an obvious definition.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "",
        "reasoning": "No section content was provided, so there's nothing to analyze for a definition. Without text, we cannot identify any definition cues such as “is defined as,” “refers to,” or “means.” In the absence of evidence, we must err on the side of “no.”",
        "is_definition": false
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
