{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience. Whereas once audiences were more or less passive consumers, they are now actively participating in the distribution, and sometimes even the production of news. People today can start their own blogs, live-stream from political events they participate in (e.g. protests), and may even contribute to small media organizations often catering to niche market segments. To be sure, just as with legacy media, online audiences still face a choice of exposure: which news do they choose to consume. But on top of that, they also face a new choice of which news to share with their friends and followers. These trends, we contend, have implications for the psychological dynamics underlying the way audiences engage with this media, the novel step of news sharing potentially recruiting new psychological mechanisms. These trends have pushed to the forefront questions about information veracity. In advanced democracies, strong editorial norms and standards for good reporting meant that citizens could -at least most of the time -trust legacy media outlets to publish stories that accurately reflected the world. For most citizens and for most legacy media, then, it made sense to pay more attention to the political usefulness of news stories and, thus, it is not surprising that this news dimension has been critical in driving selective exposure. Yet, on the internet, editorial norms have lost power, and information circulating on social media will to a much larger extent vary on both dimensions of usefulness and veracity. Truth cannot be taken for granted and veracity judgments are thus, potentially, more important than ever. Yet, these two information dimensions -veracity and usefulness -are not unique for social media users. In everyday conversations when, for example, discussing or relaying a rumor, people must consider both whether they believe it and how useful they think it is. Psychologists have studied the psychology of rumor transmission for decades, highlighting the importance of both accuracy-oriented and goal-oriented motivations (for an overview, see DiFonzo and Bordia 2007). For example, people may share rumors they believe are true to be cooperative, to signal competence, or to get others to affirm the accuracy of the rumors. Sharing a rumor that turns out to be wrong, in contrast, may damage people's reputations (Altay, Hacquin and Mercier 2019). Nonetheless, goal-oriented processes may still take precedent, notably under intense social conflict. Some of the most well-studied, and most extreme, examples come from ethnic conflicts, where rumors are heavily used to mobilize the in-group against the out-group. In his comprehensive review of this role of rumors, Horowitz (2000, p. 74) concludes: \"Rumors are (. . . ) embedded in the riot situation, because they are satisfying and useful to rioters and their leaders. (. . . ) Rumor is likely to prevail over accurate information. (. . . ) Rumor prevails because it orders and organizes action-in-process.\" Below we apply these general considerations about the psychology of information sharing to the object of our empirical inquiry: The sharing of news -fake or real -about politics on social media.",
        "reasoning": "I identified all terms in the section that are explicitly defined. The text contains explicit definitions for the following terms: \n- \"Accuracy-oriented motivations\"\n- \"Goal-oriented motivations (sometimes referred to as directional motivations)\"\n- \"Selective exposure\".\n\nEach of these meets the criteria of an explicit, single‑sentence definition. The surrounding sentences (one before and one after) provide the required context. No other terms in the section meet the criteria for an explicit definition.",
        "extracted_terms": [
          {
            "term": "accuracy-oriented motivations",
            "definition": "spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world",
            "context": "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes.",
            "type": "explicit"
          },
          {
            "term": "goal-oriented motivations/directional motivations",
            "definition": "prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes",
            "context": "Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows).",
            "type": "explicit"
          },
          {
            "term": "selective exposure",
            "definition": "the idea that political predispositions propel people to read and watch news from sources that share their political beliefs",
            "context": "For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "microsociological accounts of how knowledge and technologies are constructed to the mesosociological and macrosociological political and institutional organization of scientific knowledge and science policy. Here, analytical concern centers on distributional inequalities in technoscience and the ways that formal and informal manifestations of power, access to resources, relations among organizations, and procedures for rule making create losers as well as winners and explain both institutional stasis and change. For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored. This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance. Three of the four studies cluster in the area of environmental science and technology: the development of alternatives to chlorinated chemicals, better understanding of toxic exposure to air pollution through alternative air monitoring devices, and the environmental etiology of cancer. The fourth study is based on interviews with scientists from a wide range of academic disciplines about forbidden knowledge. Taken together, the research demonstrates the analytic potential of undone science to extend and deepen the new political sociology of science by providing a political sociological perspective on the problem of research agendas and more general issues of the construction of knowledge and ignorance. We begin with a brief review of the existing literature. Our discussion highlights some of the basic contours that the case studies reveal about undone science and that in turn can guide future research. themselves are constituted by agonistic relations between dominant and nondominant networks, even when \"undone science\" is completed, the knowledge may become stigmatized and the credibility and standing of scientists who produce it may suffer (Hess 2007). Contemporary discussions of undone science have various precedents. In some ways, Marx's critique of political economy and his effort to develop an alternative research field of Marxist political economy was an early exploration of undone science, in that Marx both critiqued the assumptions of mainstream economics and developed a framework for alternatives within the field (Marx 1967). In a similar vein, feminist research and multicultural science studies have highlighted the systematic lack of attention paid to gender, race, and related issues in science. Feminist research has also described how gender-laden assumptions shape the development of research programs and, like Marxist scholarship, has proposed alternative research frameworks and programs (e.g., Haraway 1989;Harding 1998;Forsythe 2001). Historical research highlights the institutional constraints of completing undone science. Of particular relevance to the new political sociology of science is the study of how the contours of entire disciplines or research programs have been shaped by military and industrial funding priorities, and consequently how some subfields have been left to wither on the vine while others have been well tended by government and industrial funding sources (e.g., Noble 1977;Forman 1987;Markowitz and Rosner 2002). Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006). Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry. Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying. The sociology of scientific knowledge also examined the role of uncertainty and interpretive flexibility in the generation and resolution of controversies, both within the scientific field and in broader public fora (e.g., Collins 1985Collins , 2002)). In critical analyses of risk assessment and statistical analysis, STS scholars have also brought out the unanticipated consequences of broader forms of ignorance that are not considered within the horizon of standard risk assessment practices (Hoffmann-Riem and Wynne 2002;Levidow 2002). Sociologists have also examined the production of the \"unknowable,\" as occurred when claims were made that an accurate count of ballots for the 2000 U.S. presidential election was impossible (Hilgartner 2001), and \"regulatory knowledge gaps,\" which are among the unintended consequences of the U.S. Environmental Protection Agency's (EPA) environmental testing program in New Orleans following Hurricane Katrina (Frickel 2008;Frickel and Vincent 2007). Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown.1 In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge. In an effort to map in more detail the concept of undone science, this study summarizes four research projects. The four studies are based primarily on semistructured interviews and/or participant-observation, which are appropriate methodological choices given the exploratory nature of the research and the need, at this stage, to understand the dimensions and features of undone science. The following sections summarize the aspect of these four independently designed research projects that have encountered the phenomenon of undone science. Because social movement and other civil society organizations have frequently encountered a deficit of research on health and environmental risks associated with exposure to industrial pollutants, it is not surprising that three of the cases considered here focus on the health and environmental sciences. The question of generalizability across various scientific research fields cannot be resolved in this study; our goal is the preliminary one of mapping and exploring undone science.",
        "reasoning": "The section contains several explicit definition statements introduced by phrases such as “that is,” “identified as,” or beginning with a word and a comma.  \nKey terms that are directly defined are: **undone science**, **agnotology**, **specified ignorance**, **nonknowledge**, **negative knowledge**, and **nescience**.  \nEach of these terms is defined in one of the sentences in the original text; the surrounding sentences (one before and one after, when available) are included verbatim as the context.  The definitions are short, single‑sentence extracts that are reproduced exactly from the source without alteration.  All identified definitions are marked as “explicit” per the requirements.",
        "extracted_terms": [
          {
            "term": "undone science",
            "definition": "areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored",
            "context": "For example, why does science pay dividends more often to some groups than to others? What explains the selection of certain areas of scientific research and technological design choices and the neglect of others? **This shift in focus to the institutional politics of knowledge and innovation brings into sharper relief the problem of \"undone science,\" that is, areas of research identified by social movements and other civil society organizations as having potentially broad social benefit that are left unfunded, incomplete, or generally ignored.** This article brings together four recent studies to elaborate the concept of undone science and move forward the more general project of a political sociological approach to the problem of research priorities and scientific ignorance.",
            "type": "explicit"
          },
          {
            "term": "undone science",
            "definition": "a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge",
            "context": "Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown. **In Gross's terms, undone science is a type of nonknowledge when viewed from the perspective of social movements, but from the perspective of some research communities and elites, it may be viewed as negative knowledge.** In an effort to map in more detail the concept of undone science, this study summarizes four research projects.",
            "type": "explicit"
          },
          {
            "term": "agnotology",
            "definition": "social production of ignorance",
            "context": "Historians and others have also offered detailed investigations of the dynamics of intellectual suppression and purposeful policy decisions to avoid some areas of research, usually research that would challenge powerful industrial interests (MacKenzie and Spinardi 1995;Zavestoski et al. 2002;Martin 2007). **In the emerging literature on the social production of ignorance or what some historians have called \"agnotology\" (Proctor and Schiebinger 2008), additional studies of particular relevance examine the industrial funding of contrarian research to generate a public controversy and scientific dissensus (Proctor 1995), the role of the government and industry in rendering knowledge invisible by producing classified knowledge and trade secrets (Galison 2004), and problems of imperceptibility for chemically exposed groups (Murphy 2006).**",
            "type": "explicit"
          },
          {
            "term": "specified ignorance",
            "definition": "knowledge that researchers have about topics that deserve further inquiry",
            "context": "Functionalist and constructivist sociologies of science have also contributed indirectly to the understanding of undone science, primarily through discussions of the epistemic status of ignorance and uncertainty. **Merton (1987) identified \"specified ignorance\" as knowledge that researchers have about topics that deserve further inquiry.** Zuckerman (1978) also noted that theoretical commitments, or what Kuhnians would call \"paradigms,\" could result in decisions by scientists to characterize some areas of specified ignorance as not worth studying.",
            "type": "explicit"
          },
          {
            "term": "nonknowledge",
            "definition": "known unknowns that are considered worth pursuing",
            "context": "Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown. **Nonknowledge, or known unknowns that are considered worth pursuing**. In an effort to map in more detail the concept of undone science, this study summarizes four research projects.",
            "type": "explicit"
          },
          {
            "term": "negative knowledge",
            "definition": "knowledge deemed dangerous or not worth pursuing",
            "context": "Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown. **Negative knowledge, or knowledge deemed dangerous or not worth pursuing**. In an effort to map in more detail the concept of undone science, this study summarizes four research projects.",
            "type": "explicit"
          },
          {
            "term": "nescience",
            "definition": "a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown",
            "context": "Gross (2007Gross ( , 2009) ) has drawn on the general sociology of ignorance to distinguish various forms of scientific ignorance, including nonknowledge, or known unknowns that are considered worth pursuing; negative knowledge, or knowledge deemed dangerous or not worth pursuing; and \"nescience,\" or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown. **Nescience, or a lack of knowledge about the unknown, a form of ignorance that is a precondition for a surprise because it is an unknown unknown**. In an effort to map in more detail the concept of undone science, this study summarizes four research projects.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Over the past 30 years, there have been scores of studies on the nature of hypothesis testing in scientific investigation and in everyday reasoning. Many investigators talk about confirmation bias, bat this term has been applied to many different phenomena in a variety of contexts. In our review of the literature, we find that different kinds of \"confirmation bias\" can be understood as resulting from a basic hypothesis-testing heuristic, which we call the positive test strategy. That is, people tend to test hypotheses by looking at instances where the target property is hypothesized to be present or is known to be present. This +test strategy, in its various manifestations, has generally been regarded as incompatible with the prescription to seek disconfirmation. The central idea of this prescription is that the hypothesis tester should make a deliberate attempt to find any evidence that would falsify the current hypothesis. As we show, however, +testing does not necessarily contradict the goal of seeking falsification. Indeed, under some circumstances, +testing may be the only way to discover falsifying instances (see Fig- ure 3). Furthermore, in probabilistic environments, it is not even necessarily the case that falsification provides more information than verification. What is best depends on the characteristics of the specific task at hand. Our review suggests that people use the +test strategy as a general default heuristic. That is, this strategy is one that people use in the absence of specific information that identifies some tests as more relevant than others, or when the cognitive demands of the task preclude a more carefully designed strategy. Our theoretical analyses indicate that, as an all-purpose heuristic, +testing often serves the hypothesis tester well. That is probably why it persists, despite its shortcomings. For example, if the target phenomenon is relatively rare, and the hypothesis roughly matches this base rate, you are probably better off testing where you do expect the phenomenon to occur or where you know the phenomenon occurred rather than the opposite. This situation characterizes many real-world problems. Moreover, +tests may be less costly or less risky than -tests when realworld consequences are involved (Einhorn & Hogarth, 1978;Tschirgi, 1980). Like most general-purpose heuristics, however, +testing can lead to problems when applied inappropriately. In rule discovery, it can produce misleading feedback by failing to reveal a whole class of important falsifications (violations of necessity). In propositional reasoning (e.g., the four-card problem), +testing leads to superfluous tests of necessity (+Ttests) and neglect of some relevant tests of sufficiency (-Ttests). In a variety of tasks, including concept indentification, intuitive personality testing, and contingency judgment, a +test strategy can lead to inefficiency or inaccuracy by overweighting some data and underweighting others. The consequences of using a +test strategy vary with the characteristics of the task. Our task analyses serve two major functions. First, they nighlight some of the structural similarities among diverse tasks in the broad domain of hypothesis testing. This permits integration of findings from different subareas that have so far been fairly isolated from each other. Second, our approach provides a framework for analyzing what each task requires of the subject, why people make the mistakes they do, and why changes in the structure and content of tasks sometimes produce significant changes in performance. These questions are central to understanding human hypothesis testing in the larger context of practical and scientific reasoning.",
        "reasoning": "The section contains a single explicit definition: the “positive test strategy” is defined in sentence 4. No other terms are given a formal definition within this excerpt. Thus only one term‑definition pair is extracted.",
        "extracted_terms": [
          {
            "term": "positive test strategy",
            "definition": "people tend to test hypotheses by looking at instances where the target property is hypothesized to be present or is known to be present.",
            "context": "In our review of the literature, we find that different kinds of \"confirmation bias\" can be understood as resulting from a basic hypothesis-testing heuristic, which we call the positive test strategy.\npeople tend to test hypotheses by looking at instances where the target property is hypothesized to be present or is known to be present.\nThis +test strategy, in its various manifestations, has generally been regarded as incompatible with the prescription to seek disconfirmation.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "In order to understand how NFD rules influence the CSR activities of firms, we conducted statistical analyses that compared firms in three 'early adopter' countries (France, the UK and Denmark) with other OECD countries, as well as taking into account the changes within these countries over time. These three countries were first among the OECD to introduce NFD regulation with an explicit focus on the social dimension. In this context, mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately-owned stock exchange-listed corporations. We excluded cases where regulation applied only to state-owned companies, as in Sweden since 2007. We also excluded disclosure regulation limited to hazardous environmental risks, as found in the Netherlands or the US. Spain is an ambiguous case, since its 2011 government regulation only recommends (rather than mandates) disclosure (González and Vílchez, 2015). Similar requirements were also introduced by Norway in 2013, but this only covers one year during our observation period. In the EU, Directive 2014/95/EU on non-financial disclosure required all member states to transpose the Directive into their national legislation before December 2016. This development will change the picture of CSR disclosure and makes our analysis timely. Our statistical analysis makes use of the Thomson Reuters Asset4 ESG database, which measures firm-level corporate responsibility activities. This database has already been validated in the CSR literature (Eccles et al., 2014;Cheng et al., 2014;Hawn and Ioannou, 2016;Rathert, 2016). Recently, it has increasingly been used in studies published in the Journal of Business Ethics (Al-Shaer and Zaman, 2017; Aouadi and Marsat, 2016;Benlemlih et al., 2018). Asset4 functions as a financial intermediary providing investment information related to environmental, social, and governance (ESG) issues; it systematically collects information from company reports, company websites, and other sources, such as newspapers and nongovernmental organizations. In addition, firm-level financial data were obtained through Thomson Reuters DataStream. Our sample of firms consisted of stock exchange-listed corporations during the period 2002-2014. This covers the time before and after the implementation of non-financial disclosure rules in Denmark and the UK. However, the very early introduction of policies in France during the 1980s predates the existence of any source of comparable international CSR data. We reported our results for 24 OECD countries that had a sample size of 10 firms or more per year during the period of observation. Moreover, and to make the countries more comparable, we restricted our sample to companies responsible for 90% of total market capitalisation per year in countries with more than 100 companies, namely Australia, Canada, Japan, the UK, and the US. Table 2 shows the countries and average annual number of firms included in our analysis. The annual number of companies used in the analysis varied slightly from year to year owing to missing values. In total, our sample contains 19,709 firm-year observations. For our analysis, and as suggested by (Fransen, 2013), we examined CSR activities in a disaggregated fashion, focusing solely on those related to social issues. We devised a CSR measurement based on 36 items published by Asset4 that measure the presence or absence of policies related to social issues, as well as whether or not companies have taken specific action to implement these policies. 1 We included corporate activities across seven sub-dimensions of the social pillar: prod- Moreover, we created a second index based on a coding of corporate controversies and scandals across a similar range of issues (see Error! Reference source not found.). Examples of social pillar controversies include irresponsible activities linked to business ethics in general, political contributions, bribery, and corruption. Whereas CSR data are largely based on self-reporting by companies and tend to be almost complete, the Asset4 ESG Controversy Score measures companies' exposure to irresponsible activities and negative events (and their progress) as reflected in global media (e.g. scandals, lawsuits, ongoing legislative disputes or fines). Thus, though all ethical questions are discursive, some are already discursively more settled and might be involved in court cases, whereas others are still 'only' being discussed in the media. Consequently, nearly 50% of firms scored zero as regards CSiR, reflecting a potential underreporting bias. The resulting variables for CSR and CSiR ranged from 0 to 100, expressing the percentage of measured activities in relation to the possible maximum. Missing data was excluded from the analysis. We tested H1a and H1b using the CSR index, and H2 using the CSiR index. We also calculated one additional dependent variable. To test H3a and H3b, we created a measure of variation in the CSR activities between firms. A ratio of the spread of CSR activities reflects the distance between each firm and the median CSR activity level in each country in each year. The main independent variable was a country-level indicator of NFD policies developed by using information published by the Hauser Institute for Civil Society. The institute has collected cross-national information on NFD regulation going back to the 1970s. We used this to build a binary NFD variable indicating whether a country had implemented mandatory non-financial disclosure requirements (1) or not (0). As mentioned earlier, we focused explicitly on the impact of NFD legislation on the social activities of a wide range of stock exchange-listed companies. In our analyses, we included several firm-level control variables (Jackson and Apostolakou, 2010;Koos, 2012;Padgett and Galan, 2010). First, we expected that CSR activities would increase with firm size, measured here by the logged number of employees. Second, financial performance might positively influence CSR through the availability of resources, so we measured financial performance according to return on assets (pretax income as a percentage of total assets, winsorized values in the 1 st and 99 th percentile). We also included a ratio of total debt to total assets to capture financial constraints on CSR activities owing to indebtedness. Third, CSR might also be influenced by the degree of innovation, in view of greater future business opportunities or the need to justify new products in the eyes of customers. Thus, we included the level of R&D spending as a percentage of total assets. To acknowledge possible reverse causality of firm characteristics, these variables were lagged by one year. In general, the coefficients of the control variables were consistent with previous studies. Moreover, we included industry fixed effects based on a supersector industry classification benchmark (ICB) to control for sectoral differences that affect CSR activities and year dummies to account for the observed trend towards increasing levels of CSR. Finally, we added country dummies to control for wider institutional differences between countries. All firm-level control variables were obtained from financial data collected by Thomson Reuters DataStream. A key consideration in our study was whether our CSR index would be suitable to capture the effects of NFD regulation. Broad requirements for disclosure do not necessarily lead to the adoption of specific policies or implementation activities. However, a com-pany might previously have adopted a CSR-related policy, but only begin to provide explicit information to the public about this after disclosure has become mandatory. In this sense, the measurement would capture the shift from an implicit to a more explicit form of CSR, but not necessarily a change in corporate activities. In order to control for this type of confounding, we controlled for firm-level adoption of GRI (Global Reporting Initiative) standards as an indicator of the quality of reporting. Consequently, we argue that it is unrealistic to reduce differences in our measured CSR index to differences in reporting, but we have interpreted these to reflect changes in actual CSR activities. To model the impact of NFD regulation between the years 2002 and 2014, we adopted different general linear squares (GLS) random effects models; these enabled us to test our hypotheses using robust standard errors clustered by company. Random effects estimations are appropriate since our key explanatory variable for NFD regulation does not vary much over time (Wooldridge, 2010). All estimations were carried out using STATA 12.1 and 15.0.",
        "reasoning": "The section contains only one explicit definition, namely for the term \"mandatory disclosure\". It is introduced in the sentence \"mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately‑owned stock exchange‑listed corporations.\" The surrounding sentences provide context but contain no other explicit definitions of new terms.",
        "extracted_terms": [
          {
            "term": "mandatory disclosure",
            "definition": "regulation concerning the disclosure of social activities that applies to a broad segment of privately‑owned stock exchange‑listed corporations",
            "context": "These three countries were first among the OECD to introduce NFD regulation with an explicit focus on the social dimension. mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately‑owned stock exchange‑listed corporations. We excluded cases where regulation applied only to state‑owned companies, as in Sweden since 2007.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Peña, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Peña's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fernández, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fernández, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fernández, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10462-022-10338-7",
        "sections": [
          "Recent years have shown a growing interest in the application of natural language processing techniques for extracting, summarizing, and creating new data from text. Today's digital media ecosystem generates massive streams of unstructured data, such as news articles, web pages, blog posts and tweets, thus posing a set of challenges related to their automatic understanding. Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narrative can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time. Although several attempts have been conducted in the domain of narrative extraction, some problems remain unsolved, which are not easily solved by the currently available Natural Language Processing (NLP) and Information Extraction (IE) techniques. Important challenges in this domain involve defining annotation schemes that are comprehensive enough to include the relevant features of the narrative elements, but, at the same time, not too cumbersome to avoid overloading the process of extraction; automatically extracting cohesive narratives from unstructured data with high effectiveness; detecting and characterizing storylines over single and multiple documents; identifying narrative segments within large portions of text; dealing with informal language and with different types of documents (e.g., social media and micro-blogs); coping with different human languages and with combinations of languages; defining meaningful formal representation of narratives as an intermediate step to visualization schemes; devising a standard evaluating narrative framework (made of datasets, baselines and metrics) as an entry-point for researchers interested on properly evaluating their methods; and contributing with the development of specialized linguistic resources for low-resource languages, such as narrative annotated data, to avoid creating niches. To help the research community address these challenges, different techniques can be applied depending on the type and scale of the narrative, e.g. single news articles versus whole fiction books. In this paper, we aim to survey existing approaches and techniques related to the process of extracting narratives from a text, an emergent theme of research within the field of computational linguistics and artificial intelligence. To the best of our knowledge, there is currently no survey addressing the full process of narrative extraction from textual data. Hence, collecting, organizing, documenting and describing fundamental concepts of this process is of paramount importance for different related areas, where the extraction of narratives plays an important role. Domains such as journalism (Caselli et al. 2016), finance (El-Haj 2022), health (Sheikhalishahi et al. 2019) , information access in digital libraries (Kroll et al. 2022), and other fields that require analyzing narratives over a common topic using NLP approaches, are among the main beneficiaries of this survey. Such communities are very active in the area, which makes it quite hard for a researcher to be aware of all the relevant contributions. It should be noted that in the different contexts where narratives can be extracted, there are common problems; however, there is also terminology, notation and specificities for each one. While in a financial narrative, there is a need for structure extraction (El-Haj et al. 2019) given the format in which the data are generally available, in narratives related to historical events (Lai et al. 2021), there is no such need. In this work, such particularities will not be discussed; instead, we will seek to focus on tasks that are common to narrative extraction from a general point of view. In an attempt to provide a thorough account of relevant research developed within the field of computational linguistics and artificial intelligence (AI), we conduct a survey where the different approaches are organized according to the pipeline of tasks one can find in the extraction of narratives from text. The development of this study began with the selection of fundamental articles understood as key to the subareas that make up the extraction of narratives. From these articles, we apply the snowball procedure (Wohlin 2014) to expand the pool of articles by considering relevant research published in high-quality conferences and journals in the fields of natural language processing and artificial intelligence. The remainder of this paper is organized as follows. Section 2 gives an overview of the process of extracting narratives from a text by introducing the fundamental concepts, the data acquisition and annotation effort, and the narrative extraction pipeline. Section 3 presents the initial step of narrative extraction that comprises the pre-processing and parsing of a text. Section 4 introduces research developed on the identification and extraction of narrative components, namely lexical and syntactical components. Section 5 presents the techniques found in the literature to establish the linkage between the identified narrative components. Section 6 refers to research work focused on the representation of narratives structures. Section 7 provides a snapshot of the metrics and the datasets behind the evaluation efforts. Section 8 promotes a discussion of some important aspects of narratives research including open issues. Finally, Sect. 9 concludes this paper by presenting its final remarks.",
          "In this section, we begin by presenting a detailed definition of the concept of narratives and its associated terminology. Following, we discuss important steps related to data acquisition and annotation. Finally, we describe the general pipeline behind the process of extracting narratives from textual data through the lens of a computational scope. The pipeline here introduced will define the structure of the rest of this paper.",
          "Narratives have long been studied in the field of linguistics. One of the first authors to introduce a formal definition was Adam (1992), who considers narratives to be a prototypical sequence obeying a thematic unity, regarding a chronological succession of events involving characters. The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theatrical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community. Riedl (2004) considers narratives as a cognitive tool for situated understanding, i.e., a structure designed to better understand the world around us. Motta (2005), instead, understands narratives as forms of relationships that are established due to culture, the coexistence between living beings that have interests, desires, and that are under the constraints and social conditions of hierarchy and power. As the author states, whoever narrates has some purpose in narrating, therefore no narrative is naive. The same narrative can be seen from a different point of view, i.e., from a focal point for seeing, hearing, smelling, and experiencing the accessible. Stripping the data of unnecessary information and making sure that relevant metadata is kept is also key during this process. A case in point related to narrative extracted from news is the publication date, which is relevant to determine its timeline. What follows is designing a suitable annotation scheme that is simultaneously tailored to encompass all the particularities of the target language(s), and comprehensive enough to be applied to other datasets or even be broaden. Bearing in mind, on the one hand, the diversity of annotation frameworks, and, on the other hand, the usefulness of establishing comparisons between annotated corpora from different genres in the same language, but also across languages, many proposals try to achieve this balance utilizing acknowledged standards, which have resulted in, for e.g., Ontologies of Linguistic Annotation (OLiA) (Chiarcos 2014), and ISO 24617-Language resource management-Semantic annotation framework. The decision about the annotation framework, and about the different layers of annotation, depends necessarily on a variety of factors, like the annotation purpose, the texts genre, among others (Pustejovsky et al. 2017). In the case of the narratives annotation, since it is relevant to feature participants, events, time and space, as well the relationships between them, the annotation scheme can be designed to include several intertwined semantic layers enabling temporal, referential, thematic, and spatial annotations (see for example Silvano et al. (2021)). The adequacy of the annotation tool is also of great relevance for the efficiency of different tasks, namely creation, browsing, visualization, and querying of linguistic annotations. Although one can choose to tailor a tool to the specific features of one's project, it may be labor-saving to resort to the existing ones and, if necessary, proceed with some modifications. Some of the existing annotation tools that enable annotating different markables with several attributes, as well as establishing links between those markables are the following: MMAX2 (Müller and Strube 2006), MAE and MAI Stubbs (2011), BRAT (Stenetorp et al. 2012), or ANNIS (Krause and Zeldes 2014). During the process of narrative extraction, some degree of human participation is necessary, either to verify the feasibility of the annotation scheme, to annotate linguistic aspects for which automatic models are insufficient, or to supervise the automatic annotation. Different strategies can be adopted depending, namely, on the type of annotators that the project wants or needs, with or without linguistic training, specifically: crowdsourcing (Estellés-Arolas and de Guevara 2012), class sourcing (Christopher Blackwell 2009) or gamification (Stieglitz et al. 2016). When using human annotation, calculating interannotator agreement (IAA) is crucial to ensure its reliability and accuracy. The two most used metrics in computational and corpus linguistics are: Cohen's Kappa (and its variation, Fleiss's Kapppa) and Krippendorff 's Alpha (Pustejovsky and Stubbs 2012).",
          "The study of narratives from the perspective of natural language texts can be summarized into five major stages: (1) Pre-Processing and Parsing; (2) Identification and Extraction of Narrative Components; (3) Linking Components; (4) Representation of Narratives and (5) Evaluation. Each of these tasks gives rise to the structure adopted in the rest of this survey. An overall picture is shown in Fig. 1, and a detailed description of each task can be found in the following sections.",
          "Pre-processing and parsing of a text comprise a set of lexical and syntactic tasks. Lexical activities aim to split the text into basic units, called tokens, and to normalize them into different forms; syntactical analysis identifies the grammar class of tokens, and identifies the dependency between the tokens, producing trees or chunks that relate the tokens to each other. The following subsections detail both tasks.",
          "Lexical tasks seek to standardize the input text as a way to better prepare the data content for the following steps (Sun et al. 2014). Despite being a simple procedure, they are considered to be a key factor to achieve effective results (Denny and Spirling 2018). The main lexical tasks required for the narrative extraction process are: (1) Sentence segmentation, i.e., dividing the whole text into sentences and tokens to gather the lexicon of the language (Palmer 2007); for this task, using deep learning (DL) methods Schweter and Ahmed (2019) achieved SOTA results through the use of a multi-lingual system based on three different architectures of neural networks. (2) Text tokenization, applied to break sentences into tokens, the smallest unit of a text (Vijayarani et al. 2016); and (3) Text Cleaning and Normalization, which may involve a number of optional cleaning steps, including removing numbers, punctuation marks, accent marks, or stopwords, as well as apply stemming (Jabbar et al. 2020) and lemmatization (Bergmanis and Goldwater 2018) to observe other possible discrepancies between equal or similar words. These are foundational NLP tasks; hence they will not be explored in detail in this article. More details regarding these tasks can be found in Raina and Krishnamurthy (2022).",
          "The lexical analysis precedes a more profound observation, which aims to identify the grammar class of the words and to infer the relationship between words in a sentence to help understand the meaning of a text. To go over this process, we use the text presented in Fig. 2 as a running example in the remainder of this survey. ",
          "The first step in this stage is to assign parts-of-speech tags to each word of a given text (e.g., nouns, verbs, adjectives) based on its definition and its context, which is called the Part-of-Speech (PoS) tagging task. The current state-of-the-art is settled by Bohnet et al. (2018) in several languages. In this work, the authors used recurrent neural networks with sentence-level context for initial character and word-based representations, achieving an accuracy of 97.96% on top of the Penn Treebank dataset Marcus et al. (1993). With respect to narrative texts, since their elements can be typically associated to different word classes (for instance, nouns to participants, verbs to events), POS tagging plays a fundamental role (Palshikar et al. 2019;Quaresma et al. 2019;Yu and Kim 2021). Figure 3 shows the result of applying the Stanzafoot_0 library (Qi et al. 2020) PoS tagger to the first sentence of our running example. In the figure, PROPN refers to a proper noun, PUNCT to punctuation, DET to determiner, VERB to a verb, and ADP to adposition. These tags are defined under the Universal Dependencies guidelines (de Marneffe et al. 2021).",
          "Following PoS tagging, the parsing of the text can be conducted. The chunking task (a.k.a, shallow parsing) is responsible for identifying constituent parts of the sentences (nouns, verbs, adjectives, etc.) and linking them to higher-order units that have discrete grammatical meanings (noun groups or phrases, verb groups, and others). Figure 4 illustrates the results of applying chunking on top of the first sentence of our running example through the use of the benepar library (Kitaev et al. 2019). By looking at the figure, we can observe that both \"a\" (determiner -DT), as well as \"rabbit\" (noun-NN), belong to a higher group of noun phrases (NP). Note that adpositions (tagged as \"ADP\" in Fig. 3) are now represented in Fig. 4 with the \"IN\" tag. As in other tasks, the application of deep learning approaches (Hashimoto et al. 2017;Zhai et al. 2017;Akbik et al. 2018) has brought important improvements to this particular task. The next step of the pipeline is to understand how all the words of the sentence relate to each other, which is done by a dependency parsing analysis. The objective, as referred by Jurafsky and Martin (2009), is to assign a single headword to each dependent word in the sentence through labeled arcs. The root node of the tree, that is, the head of the entire Fig. 2 Running example Fig. 3 Results of applying the stanza PoS tagging processor to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" structure, will be the main verb in the sentence (e.g., \"pulled\" in our running example). This task can enhance the performance of named entity recognition (further discussed in Sect. 4.2) models and be used to increase information extraction (de Oliveira et al. 2022). Figure 5 shows a dependency analysis of the first sentence of our running example alongside the type of relationship predicted to occur between two tokens. The results obtained stem from applying the Stanford CoreNLP Parser (Chen and Manning 2014), one of the most well-known tools in this regard. A quick look at the figure emphasizes the association between several terms, among which we highlight the relationship hierarchy between \"John\" and \"magician\". The current state-of-the-art of this task, in the English and Chinese languages, is settled by Mrini et al. (2020). The model proposed by the authors used a combination of a label attention layer with Head-driven Phrase Structure Grammar (HPSG), and pre-trained XLNet embeddings Yang et al. (2019). In a narrative context, the application of these tasks can be particular useful to identify relations between narrative components (further discussed in Sect. 4). Fig. 4 Results of applying the benepar chunking model to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Fig. 5 Results of applying the Stanford CoreNLP Parser Dependency to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\"",
          "Coreference resolution, also known as anaphora resolution, uses information about the semantic relation of coreferent expressions to infer the reference between them (Jiang and Cohn 2021). In addition to that, all the terms that reference the same real-world entity are marked (Mitkov 2014). For example, the word \"presentation\" (found in sentence 2 of our running example) refers to \"show\" (which can be found in sentence 1). Coreference resolution is essential for deep language understanding, and has shown its potential in various language processing problems (Poesio et al. 2016). Bringing it to the context of narrative extraction, some works highlight the importance of this task in the most different scenarios, such as clinical narratives (Jindal and Roth 2013a), newswire (Do et al. 2015), and also in violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System (Uppunda et al. 2021). In a general domain, several approaches have been proposed over the years (Pradhan et al. 2012;Mitkov 2014;Lee et al. 2013), but recently there has been a shift towards adopting transformers models (Ferreira Cruz et al. 2020;Kantor and Globerson 2019) to improve the connection between textual elements and, as such, enhance the coreference task. The current SOTA of coreference resolution is marked by Attree (2019) and Kirstain et al. (2021), both sticking to deep learning methods.",
          "Table 1 presents the current status of the tasks covered in this section. This includes lexical tasks (sentence segmentation, text tokenization, data cleaning, normalization, stopwords removal, stemming, lemmatization) and syntactic (PoS tagging, chunking, dependency parsing, coreference resolution). Herein, however, we only list those that have been undergoing constant improvements over the most recent years. In this table, the effectiveness of each study is shown according to the measures reported by the authors.",
          "The next step in the narrative extraction pipeline is to identify and extract the main elements that compose a narrative. This step comprises tasks like the detection and classification of events, the recognition and the classification of named entities (participants), the extraction of temporal information and of spatial data.",
          "Finding the events mentioned in the text is an essential step towards the extraction of narratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person. Xie et al. (2008) went a little bit further, and suggested describing real-world events through the use of the 5W1H interrogatives: when, where, who, what, why, and how, anchored on journalistic practices. Based on this reference, an event such as the one referenced in our running example might be depicted along with the six aspects: who-John, the magician; when-2022 (taking into account the date of writing the text); Table 1 State-of-the-art in pre-processing and parsing tasks Task Ref. Method Effectiveness Benchmark dataset Sentence segmentation Schweter and Ahmed (2019) BiLSTM 98.62 F1-score (English) Europarl Corpus Koehn (2005) 99.70 F1-score (Greek) SETimes Alperen and Tyers (2010) 99.86 F1-score (Arabic) Leipzig Corpus Goldhahn et al. (2012) Text tokenization Graën et al. (2018) Language-specific and language independent token identification rules 1% Error rate SMULTRON Treebank Volk et al. (2015) Lemmatization Bergmanis and Goldwater (2018) Standard encoder-decoder architecture + character-level sentence context 92.4 Accuracy (ambiguous words) Universal Dependency Treebank v2.0 Nivre et al. (2020) 84.30 Accuracy (unseen words) Part-of-speech tagging Bohnet et al. (2018) Recurrent network model 97.96 Accuracy Penn Treebank Marcus et al. (1993) Chunking Akbik et al. (2018) Neural character-level language modeling 96.72 F1-score Penn Treebank Marcus et al. (1993) 96.34 F1-Score Dependency parsing Mrini et al. (2020) Label attention layer + HPSG + XLNet 97.39 UAS Penn Treebank Marcus et al. (1993) 96.29 LAS Coreference resolution Attree (2019) Neural attention model 92.50 F1-Score GAP Dataset Webster et al. (2018) where -XYZ contest realized in Edinburgh; what-the magician pleased the audience during his presentation; how-taking a rabbit out of his hat; why-his presentation was better than the last one. The task of event detection has its roots in the late 90's when the Topic Detection and Tracking (TDT) project emerged (Allan et al. 1998) as the first attempt to cope with the rising of huge volumes of data. Among the entire project, the detection of new events, also known as the first story detection (FSD) (Kontostathis et al. 2004), was a subtask of the TDT project, concerned with the detection and subsequent tracking of the first and coming stories of a given news event. One way to detect events is to rely on event triggers, cues that express an event's occurrence, most often single verbs or phrasal verbs, as referred by Boroş (2018), but also nouns, noun phrases, pronouns, adverbs, and adjectives. According to Araki (2018), 95% of the ACE (Automatic Content Extraction) event triggers consist of a single token. This is the case of our running example, where the events pulled, loved, presented are triggered by verbs, and show, contest and presentation are also events represented by nouns. Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argument role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 subtypes, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example. The ACE 2005's predefined event scheme is presented in the left-hand part of the figure. The event trigger (\"pulled\"), the 3 argument roles (\"Arg-Entity\"), and 2 modifiers (\"Arg-Time\" and \"Arg-Place\") identified for the type \"Contact-Meet\" type are illustrated on the right-hand side of the figure. In this visualization, the time-reference \"this year\" is normalized to 2022 based on the document creation time (DCT). The current state-of-the-art in event extraction task for the English language is provided by Feng et al. (2018), who developed a language-independent neural network. The authors tested on top of the ACE 2005 English event detection task (Walker et al. 2006) data, achieving an F1-score of 73.40%. More detailed information about the event detection task can be found in Saeed et al. (2019) and a recent survey from Xiang and Wang (2019). In recent years, the task of monitoring streams of data and detection events has made a shift towards microblogs platforms, such as Twitter (Atefeh and Khreich 2015), a very different scenario when compared to the TDT era, when algorithms were developed to track news stories over time from traditional media like newspapers (Kalyanam et al. 2016). The detection of events also plays a fundamental role in different kinds of applications, domains, and languages, as is the case of clinical narratives (Jindal and Roth 2013b;Adams et al. 2021), and historical events (Lai et al. 2021). Despite the significant advances in the last few years, the application of event extraction techniques in the context of narrative representation has been quite limited (Metilli et al. 2019), and only recently a few works began to emerge. Metilli et al. (2019), for example, made use of numerous novel discourse and narrative features, besides common relations, such as the time at which the event takes place, or coreference events, which detects whether two mentions of events refer to the same event (Araki 2018). Aldawsari and Finlayson ( 2019) presented a supervised model to automatically identify when one event is a sub-event of another, a problem known as sub-event detection or event hierarchy construction. Another strand tackles the problem of nested event structures, a common occurrence in both open domain (not limited to a single topic or subject) and domain specific (dedicated to a particular problem representation or solution) extraction tasks (McClosky et al. 2011). For instance, a \"crime\" event can lead to an \"investigation\" event, which can lead to an \"arrest\" event (Chambers and Jurafsky 2009). The research of Metilli et al. (2019) applied a recurrent neural network model to event detection applied to biography texts obtained from Wikipedia. The model used a set of event categories (e.g., birth, conflict, marriage, and others) to depict a narrative. In the context of news stories, Zahid et al. (2019) proposed the development of heuristics to segment news according to a scheme that defines the organization and order of the events. The strategy followed in that research employed mechanisms that journalists usually exploit to compose news.",
          "Another essential category of narrative elements is the participants. They are the \"who\" of the story, relevant to the \"what\" and the \"why\". Participants, sometimes also referred to as actors, are involved in events in a varied number of ways and often correspond to entities in NLP. These, are usually identified through Named Entity Recognition (NER), a generic task aiming at seeking, locating and categorizing the entities mentioned in the text into predefined categories. Although the three most common named entities are person, organization, and location, further types such as numeric expressions (e.g., time, date, money, and percent expressions) can also be considered (Nadeau and Sekine 2007). In narrative terms, participants are more frequently restricted to the person and organization categories, though other categories such as animals may also be used. In our running example this would result in annotating \"John\" and \"audience\" as a person, and \"rabbit\" as an animal as can be observed in Fig. 7. Overall, research on named entity recognition can be categorized into four main categories: rule-based approaches, unsupervised learning approaches, feature-based supervised learning approaches, and deep-learning-based approaches (Li et al. 2022). Beyond this, other aspects, such as textual genres, entity types, and language, are also considered by research conducted on named entity identification (Goyal et al. 2018). The current stateof-the-art in named entity recognition was obtained by Wang et al. (2021) on the CoNLL 2003NER task (Tjong Kim Sang and De Meulder 2003). To automate the process of finding better concatenations of embeddings for structured prediction tasks, the authors proposed Automated Concatenation of Embeddings (ACE) based on a formulation inspired by recent progress on neural architecture search. The method achieved an F1-score of 94.60%. In narrative contexts, Lee et al. (2021) present a proposal that goes beyond the identification of entities in narratives, proposing a multi-relational graph contextualization to capture the implicit state of the participants in the story (i.e., characters' motivations, goals, and mental states). Piper et al. (2021) point out that NLP works focusing on agents have emphasized broadening the understanding of characters beyond named entities. This has been done through the concept of animacy detection, as in agents like \"the coachman\" or \"the frog\", while also distinguishing characters from other named referents (Piper et al. 2021). According to Oza and Dietz (2021)'s proposal, selecting a set of relevant entities for a story construction could be best achieved by using entity co-occurrences in retrieved text passages -especially when the relative relevance of passages is incorporated as link strength. In line with what the authors claim, their proposal is between 80% and 30% more effective than the best link-based approach.",
          "Temporal references are of the utmost importance for the understanding of the narrative and its timeline by anchoring each event or scene at a point in time. The process of identifying temporal references is commonly referred to as Temporal Tagging, and can be split into two sub-tasks: extraction and normalization (Strötgen and Gertz 2013). The former aims to correctly identify temporal expressions, and can be seen as a classification problem. The latter aims to normalize the identified temporal expressions, and can be seen as a more challenging process where different temporal expressions, carrying out the same meaning, need to be anchored at the same time-point. In this regard, three temporal taggers take the lead, Heideltime (Strötgen and Gertz 2013), SuTime (Chang and Manning 2012) and GuTime (Mani and Wilson 2000), which support the four basic types of temporal objects defined by Pustejovsky et al. (2005) in TimeML, the standard markup language for temporal annotation containing TIMEX3 tags for temporal expressions: Dates (e.g., \"December 3, 2021\"), Time (e.g., \"5:37 a.m.\"), Durations (e.g., \"four weeks\", \"several years\"), and Sets (e.g., \"every day\", \"twice a month\"). Alongside with these, other realizations of temporal expressions can be found in a text (Strötgen et al. 2012;Campos et al. 2017), which pose additional challenges. We refer to explicit (e.g., \"April 14, 2020\"), implicit (e.g., \"Christmas day 2019\"), and relative temporal expressions (e.g., \"yesterday\"), a kind of temporal expression that requires further knowledge to be normalized. A discussion of the challenges associated with each one of them can be found in Strötgen andGertz (2013, 2016). Other researchers (Jatowt et al. 2013) devised methods concerning document dating of non-timestamped documents. The rise and continuous development of temporal taggers has paved the way to the emergence of upstream research at the intersection between information extraction and several NLP tasks, where temporal expressions play an important role. For example, in the information retrieval domain, temporal information (Campos et al. 2014) can be used for temporal clustering of documents (Campos et al. 2012) or temporal ranking (Berberich et al. 2010;Campos et al. 2016); in question answering to query knowledge bases (Sun et al. 2018); in document summarization to construct timeline summaries (Campos et al. 2021); and in web archives to estimate the relevance of past news (Sato et al. 2021). At this stage of identification and extraction of narrative components, several works address the intersections between the tasks of identifying and extracting events, entities, and temporal information. Strötgen and Gertz (2012) explored event-centric aspects, considering related temporal information. In particular, they introduced the concept of event sequences, a set (or sequence) of chronological ordered events extracted from several documents. In their work, the geographic dimension played an important role by mapping event-sequences onto a map as well. In contrast, studies such as Agarwal et al. (2018) and Rijhwani and Preotiuc-Pietro (2020) explored the task of recognizing named entities considering time-aware aspects. The incipient annotation of documents to support related research has also led to the development of markup languages (e.g., TimeML (Pustejovsky et al. 2005), a formal specification language for temporal and event expressions anchored on TIMEX3 tags), annotated corpora (e.g., TimeBank corpus (Pustejovsky et al. 2006), a standard set of English news articles annotated with temporal information under the TimeML 1.2.1 (Saurı et al. 2006) guidelines) and research competitions [e.g., TempEval series (Lim et al. 2019)].",
          "The problem of extracting geographic references from texts is longstanding (Martins et al. 2008), and is highly related to the field of Geographic Information Retrieval (GIR) (Purves et al. 2018). Much of this information can be found in unstructured texts through references to places and locations (Purves et al. 2018). Many difficulties arise when attempting to comprehend geographic information in natural language or free text (e.g., under-specified and ambiguous queries) (Purves et al. 2018). Challenges related to space involve the detection and resolution of references to locations, typically, but not exclusively, in the form of place names, or more formally toponyms, from unstructured text documents (Jones and Purves 2008). In our running example, to extract the location of the main event, one can consider \"contests near Edinburgh\", which consists of three important parts, a theme (contests), a spatial relationship (near), and a location (Edinburgh). However, in the example, it is unclear whether there is only one contest in the city or multiple ones, and, if so, to which one the text is pointing to. The spatial relationship \"near\" also brings more questions than answers, making it difficult to understand whether it refers to downtown Edinburgh or some constrained space within the state of Edinburgh. Spatial relationships such as these may be both geometric (obtained using coordinate systems imposed on the real world, such as latitude and longitude) and topological (spatially related, but without a measurable distance or complete direction) (Larson 1996). The current SOTA in toponym detection and disambiguation was achieved by Wang et al. (2019) on the SemEval 2019 Task 12 (Weissenbacher et al. 2019), taking as a basis PubMed articles. In their approach, called DM_NLP, the authors made use of an ensemble of multiple neural networks. In a different approach, Yan et al. (2017) proposed a solution called augmented spatial contexts that learns vector embeddings and uses them to reason about place type similarity and relatedness. When considering space in a narrative context, other issues arise. Geographic maps, landscape paintings, and other representations of space are not always narratives, yet all narratives presuppose a world with spatial extension, even when spatial information is concealed (Ryan 2014 [Online]). Generally speaking, a narrative space is a physically existing environment in which characters live and move. This space is usually identified as a setting, that is, the general socio-historic-geographical environment in which the action takes place (Ryan 2014 [Online]), i.e., grounded by referents in the real world (e.g., the Edinburgh city in our running example) or entirely fictional (Rohan kingdom in The Lord of Rings). Aiming to identify places, some works (Bamman et al. 2019;Brooke et al. 2016) have been exploring entity recognition and toponym resolution in the context of narratives, making it possible to recognize named locations, facilities, and geopolitical entities (Edinburgh, Rohan) within this kind of text (Piper et al. 2021). Another problem is related to coreference resolution. Unlike coreference resolution of named locations, long-document coreference of common items (e.g., the house and the room), which form the narrative universe for many fictional creations, can be tough. As indicated by Piper et al. (2021), many questions can only be addressed by calculating the distance between locations described in the text: how far do Frodo and Sam go on their journey? Systems that allow better inference on spatial information within narratives could provide important insights to get the reader closer to the narrative universe.",
          "Table 2 brings a summary of the current status of the tasks covered at the identification and extraction stage discussed in this section. In this table, the effectiveness of each study is shown according to the measures reported by the authors. The type column specifies the type of task addressed in the works. All the works discussed here refer to a semantic analysis of the text. This contrasts with Table 1 where semantic tasks, but, above all, lexical and syntactic, were addressed. In our supplementary material, made available with this survey, one can find additional benchmark datasets relating to the topics covered in this section. This material has datasets available that can be used as a reference for carrying out the tasks described here. Some of these datasets are available in other languages and might help identify and extract narrative components also in these languages.",
          "After the process of identifying and extracting narrative components, it is crucial to extract the link relations between such pieces. Linking narrative components comprises the core of the extraction of narratives from the text. More than extracting separate elements from texts, establishing relations and detecting structures from them becomes essential to fully understand their meaning. In this stage, extracted pieces of information are connected, structuring the narrative at a global level. Thus, temporal and event linking, entity and  ",
          "Despite concentrating the research community's attention, predicting temporal and causal relations between events and temporal references, and investigating the evolution of an event over time as a whole (Kalyanam et al. 2016), remains a major challenge of the text understanding task (Leeuwenberg and Moens 2018) that goes beyond merely identifying temporal expressions in text documents. Ultimately, understanding a text with regards to its temporal dimension requires the following three steps: (1) identification of temporal expressions and events; (2) identification of temporal relations between them; and (3) timeline constructions. An overview of each one of them is shown in Fig. 8 for our running example. Given an input text, the system determines the temporal expressions and the events (top). Next, it assigns TimeML annotations (middle). Finally, it produces a timeline as an output (bottom). Figure 8a begins by showing the input text. By looking at it, one can easily detect \"this year\" temporal expression (normalized to 2022 assuming that the Document Creation Date-DCD-refers to November 21st, 2022) and the chronology of the events. In our example, the event pulled is carried out by a magician called John. During the show, he pulled a rabbit out of a hat. The presentation was more welcomed by the public than another previous presentation. Despite all the information collected, it is not clear whether the \"last one presented\" (event) was held during the present edition or last year's contest, or even if the \"show\" consisted of only \"pulling a rabbit out of a hat\". Ambiguity in the text might also raise doubts about whether the previous presentation refers to John or another magician. In Fig. 8b, we can observe a possible interpretation of the relation phase, where temporal links are established between the temporal expressions and the events. Finally, in Fig. 8c, we can see the generated timeline. In this visualization, one can observe a reference to the document creation time, the present time (reading time), and the ordering of the events according to the time-reference \"this year\" normalized to 2022. This apparently simple example shows how difficult it may be to understand a text in detail. In the following subsections, we present research tackling the identification of temporal expressions, events and their relations, before exploring timeline constructions. Mostafazadeh et al. (2016b), in their work, proposed a pipeline strategy for extracting events and linking them temporally, without building a timeline. The authors demonstrated that events that follow a protocol or script could be related to each other through temporal associations. These relationships are based on temporal reasoning, and provide a broad understanding of language. In another study, Roemmele and Gordon (2018) applied a neural encoder-decoder to detect events before predicting the relations between adjacent events in stories. The authors assumed that subsequent events are temporally related, and as such the extraction of events and their respective relations can be inferred. The goal is to model events that are the cause of other events and also events that are the effect of other events. Similarly, Yao and Huang (2018) deemed that the order of events in a text is the same as that of the temporal order of them in a narrative. Based on this assumption, the authors used a weakly supervised technique to propose a novel strategy to extract events and temporal relations between events in news articles, blogs, and novel books across sentences in narrative paragraphs. In another proposal, Han et al. (2019a) introduced a strategy where the extraction of events and temporal relations was jointly learned. Hence, the information about one task is employed to leverage the learning of the other. This strategy selected shared representation learning and structured prediction to solve the tasks. The results achieved were superior to the previous works. Ning et al. (2017) suggested a structured learning approach to identifying temporal relations in natural language text. Their solution was evaluated on top of the TempEval-3 data (UzZaman et al. 2013), achieving a temporal awareness [metric provided by UzZaman and Allen (2011)] of 67.2%. In the health domain, Tang et al. (2013) developed a temporal information extraction system capable of identifying events, temporal expressions, and their temporal relations found in clinical texts. In this same domain, Leeuwenberg and Moens (2020) proposed an annotation scheme for extraction of implicit and explicit temporal information for clinical reports identified events, which provides probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. A comprehensive overview of the research conducted over the years on identifying events, time references, and properly connected them is given by Derczynski (2016).",
          "",
          "The next step, after establishing the relationships between events and the temporal entities, is to arrange temporal information in such a manner that the remaining narrative components can be organized with regards to time (Leeuwenberg and Moens 2019). One important aspect of this, as stated by Leeuwenberg and Moens (2019), is the temporal reasoning task, which refers to the process of combining different temporal cues into a coherent temporal view. This is of the utmost importance as previous works have shown that the information presented in the text may not match the elapsed order (Mostafazadeh et al. 2016b). Getting to know the correct order of the events may also enable to dive deeply in the analysis of particular components. Antonucci et al. (2020) employed this notion to investigate the evolution of characters of books over time. The authors trained word embeddings related to different characters in distinct parts of literary texts (e.g., chapters). These embeddings are also called dynamic or temporal embeddings (Bamler and Mandt 2017) since, from them, it is possible to analyze relations between concepts over time. This work demonstrated that characters with strong friendships have similar behavior; therefore, such characters evolve similarly over time. Another outcome of the temporal reasoning task is building the network of events in chronological order, i.e., the text's narrative. Over the years, different researches (Bethard et al. 2007;Mani and Schiffman 2005) have been proposed with one such goal. In Leeuwenberg and Moens (2018), the authors proposed two models that predict relative timelines in linear complexity and new loss functions for the training of timeline models using TimeML-style annotations. On the sidelines, Jia et al. (2021) proposed the use of temporal information as a way to provide complex question answering related to temporal information through knowledge graphs. An overview of the temporal reasoning task, on the extraction, and on how to combine temporal cues from text into a coherent temporal view is presented in a survey of Leeuwenberg and Moens (2019).",
          "Capturing the linking between the pairs of identified entities (or participants) is an important aspect of the narrative understanding. Known as relation extraction task (Qin et al. 2021), it attempts to determine a semantic link between two named entities in a sentence. Performing this task on top of our running example would result in identifying an instance of a pulled relation between \"John\" and \"a rabbit\". Such semantic relations are usually structured in the form of < e 1 , rel, e 2 > triples, where e 1 and e 2 are named-entities, and rel is a relationship type (Batista 2016 [Online]). In our case, it could be exemplified by the triple: < John, pulled, a rabbit > . Considering that in narratives, events are often directly linked, extracting such relationships is also of paramount importance. Semantic relations also encompass objectal relations (i.e., relations between discourse entities seen as extra-linguistic concepts). These relations aim to state how two discourse entities are referentially related to one another (ISO 24617-9:2019 (E) 2000). Other NLP tasks, like Question-Answering Systems (Li et al. 2019), and the creation of Knowledge Graphs (Zhang et al. 2019b), can benefit from identifying such a relation. An example of entity relation extraction employed to build Knowledge Graphs is provided in Han et al. (2019b). In this work, the authors present OpenNRE, an open-source and expandable toolkit for implementing neural models for relation extraction. Using this tool, one can train custom models to extract structured relational facts from the plain text, which can be later used to expand a knowledge graph. The traditional relation extraction can be solved by applying one of the following approaches: (1) rule-based; (2) weakly-supervised; (3) supervised; (4) distantly supervised; and (5) unsupervised. In addition to this type of relation extraction, one can also extract semantic relationships by following Open Information Extraction approaches (OIE) (Batista 2016 [Online]), a research field that obtains domain-independent relations from a text. This task is supposed to extract all kinds of n-ary relations in the text (Xavier et al. 2015). Two methods can be applied by the Open Information approaches (1) rule-based; and (2) data-based. The former relies on hand-crafted patterns derived from PoS-tagged text or dependency parse tree rules. The latter generates patterns based on training data represented as a dependency tree or PoS-tagged text. Over the last few years, neural networks have also made their path in the particular domain of relation extraction (Qin et al. 2021), through different neural network approaches (Baldini Soares et al. 2019;Hendrickx et al. 2010;Nadgeri et al. 2021). Exploiting the narrative structure, (Tang et al. 2021) proposed a Multi-tier Knowledge Projection Network (MKPNet). The strategy was designed to leverage multi-tier discourse knowledge and present a knowledge projection paradigm for event relation extraction. According to the authors, such a paradigm can effectively leverage the commonalities between discourses and narratives for event relation extraction. This study had a focus on the projection of knowledge from discourses to narratives. (Lv et al. 2016) also proposed a strategy that takes advantage of the narrative structure. The authors applied an autoencoder to a set of features, including word embeddings, in a clinical narrative dataset, the 2010 I2B2 relation challenge (Uzuner et al. 2011). Using the same dataset, the 2010 I2B2 relation challenge, and the 2018 National NLP Clinical Challenges (N2C2) dataset (Henry et al. 2020), Wei et al. (2019) proposed two different models to extract relations from clinical narratives. The models used a language model based on transformers to extract the features, and BI-LSTM neural network attention to detect the relations. Another example comes from finances related narratives, in this context a shared task was proposed by Mariko et al. (2022a) with the intend to extract causality between existing relations. In this shared task experiments were done on top of FinCausal dataset (Mariko et al. 2022b), a dataset extracted from different 2019 financial news. The best results were achieve by a team that developed an ensemble of sequence tagging models based on the BIO scheme using the RoBERTa-Largemodel, which achieved an F1 score of 94.70 to win the FinCausal 2022 challenge. In addition to these types of connections, entities can also be also linked to other kinds of data. Knowledge databases can furnish a unique identity to entities, which can enrich the narrative with information, and aid to disambiguate the entities as well. The task of linking knowledge base-entities and narrative entities is called entity linking. In the following section, we will discuss this task.",
          "Over the years, efforts have been developed to explore relations between entities in texts (Freitas et al. 2009;Hasegawa et al. 2004). This willingness to recognize individuals in a document is a significant move towards understanding what the document is all about (Balog 2018). Truly understanding a text requires, however, linking these individuals with other pieces of information. The temporal connection between events is one way to achieve this. Another way is to relate the events of the narrative with entities from a knowledge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008). Formally, the process of linking entities can be understood as the task of recognizing (named entity recognition), disambiguating (named entity disambiguation [Eshel et al. 2017)], and linking entities (named entity linking) with unique entity identifiers from a given reference knowledge-base, such as DBpedia (Auer et al. 2007), or Yago (Suchanek et al. 2007). Given the sentence \"Bush was a former president of the USA from 1989 to 1993\", the idea is to determine that \"Bush\" refers to George H. Bush, an American Politician who served as the 41st president of the United States from 1989 to 1993, and not to \"Bush\" a British rock band formed in London, England in 1992. When connecting an entity with Wikipedia data, such as Wikidata (Vrandečić and Krötzsch 2014), this task is known as Wikification. As emphasized by Szymański and Naruszewicz (2019), when a document is Wikified, the reader can better understand it because related topics and enriched knowledge from a knowledge base are easily accessible. From a system-to-system view, the meanings of a Wikified document's core concepts and entities are conveyed by anchoring them in an encyclopedia or a structurally rich ontology. In the context of a narrative, it might mean a more knowledge-rich narrative. Figure 9 illustrates the results of applying entity linking on top of our running example. By looking at the figure, one can observe the linkage of the detected entities or events in the text with their presence on external sources, such as knowledge bases. One can get insights into who is \"John\", \"Edinburgh\" or the \"XYZ contest\". This information might be helpful not only to add more knowledge but also to disambiguate the case of entities that are associated with more than one concept. In the scope of entity linking efforts, Raiman and Raiman (2018) achieved the state-ofthe-art focusing on a cross-lingual approach, a type system on an English dataset supervised with French data. In particular, they constructed a type system and used it to constrain a neural network's outputs to respect the symbolic structure.",
          "Understanding narrative stories involves the ability to recognize events and their participants, as well as to recognize the role that a participant plays in an event, as \"'who ' did 'what' to 'whom\"\", and 'when', 'where', 'why', and 'how' (ISO 24617-4:2014 ). Generally, this can be achieved by Semantic Role Labeling (SRL). This task assigns semantic roles to the constituents of the sentence (Aher et al. 2010). Contrary to syntactic analysis Fig. 9 Overview of entity linking task result when applied to our running example (such as the ones conducted on chunking, dependency parsing, etc.), it acts on a semantic level, and is responsible for capturing predicate-argument relations, such as \"who did what to whom\"' (He et al. 2018), towards making sense of a sentence's interpretation. SRL recovers the latent predicate-argument structure of a sentence, providing representations that answer basic questions about a sentence's meaning. Figure 10 illustrates the results of applying SRL on top of the first sentence of our running example, through the use of AllenNLPfoot_1 . model, which is the implementation of Shi and Lin (2019). The relation established between the different parts of the sentence can be observed by looking at the figure. In this relation, all the parts are linked by the verb \"pulled (out)\": \"John, the magician\", \"a rabbit\" and \"of a hat\" are the core arguments, while \"at the show in Edinburgh\" and \"this year\" are modifiers, the former conveying spatial information and the latter temporal information. The current state-of-the-art of this task is settled by He et al. (2018) with an F1-score of 85.5%, using a deep learning approach based on a BiL-STM neural network to predict predicates and arguments on top of the OntoNotes benchmark (Pradhan et al. 2013). In a narrative-focused approach, Mihaylov and Frank (2019) proposed the use of linguistic annotations as a basis of a discourse-aware semantic self-attention encoder for reading comprehension on narrative texts. In this work, the authors adopted the 15 fine-grained discourse relation sense types (see section 5.5 for more information about discourse relations) from the Penn Discourse Tree Bank (PDTB). According to what is mentioned by the author, combining such an annotation scheme with self-attention yields significant improvements. Following this approach, this study's results indicate that SRL significantly improves who and when questions and that discourse relations also improve the performance on why and where questions. These results are illustrated on top of the Narra-tiveQA (Kočiský et al. 2018) reading comprehension.",
          "When reading or listening to a text, the reader/listener establishes relations of meaning between the different parts, be they clauses, sentences, or paragraphs. These relations are discourse relations (DRels)-also known as rhetorical relations or coherence relationsand are crucial to explain how discourse is organized. For that reason, they have been the basis of several frameworks, such as Rhetorical Structure Theory (RST) (MANN and Thompson 1988), Segmented Discourse Representation Theory (Asher et al. 2003).",
          "Semantic labeling result when applied to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Looking at our running example, since no connective is present, the identification of the DRel would be very challenging using automatic methods. However, taking into consideration lexical and other semantic information, as well as our world knowledge, one can infer that the two sentences are related by the DRel Result, because loving the magician's presentation is a consequence of him pulling a rabbit out of the hat. The second sentence can be divided into two arguments/ text unities related by Comparison. Figure 11 illustrates the annotation of this example using an RST markup tool (O 'Donnell 2000). In NLP, the process of uncovering the DRels between text units is called discourse parsing, a very complex task, despite a few advances over the past years. One of the most recognized discourse parsers is the one used by the news corpus Penn Discourse Tree Bank (PDTB) (Prasad et al. 2018). This end-to-end discourse parser (LIN et al. 2014) follows the same steps as PDTB human annotators: identification of the discourse connectives, arguments segmentation, labeling them Arg1 and Arg2, and recognition of the explicit DRel. When no explicit relation is extracted, the second step labels the implicit DRels. The last step consists in labeling the attribution spans (see Potter (2019) for more information about attribution relations), which are text units that reveal if the content should be attributed to the writer or to another participant. The 2021 edition of the DISRPT Shared Task (Zeldes et al. 2021) added the task of Discourse Relation Classification across RST, SDRT and PDTB, to the two existing tasks from the previous edition (Zeldes et al. 2019), Elementary Discourse Unit Segmentation and Connective Detection. The Shared Task was performed in relation to 16 datasets in 11 languages. Overall, the system with the best results was DisCoDisCo (Gessler et al. 2021) with a Transformer-based neural classifier, which was able to surpass state-of-the-art scores from 2019 DISRPT regarding the first two tasks and to obtain a solid score on the 2021 benchmark for the third task. Establishing DRels between events, such as cause, result, or temporal sequence, is of paramount importance to understand the narrative. Much has been achieved with manual annotation of datasets, which has been the basis for the development of some Discourse Parsers, namely within shared tasks (see LI et al. (2021), for a review and future trends). However, many issues need further research so that Discourse Parsing can fully contribute to narrative extraction, in particular concerning the identification of implicit DRels and the specification of the high-level discourse structures. Table 3 State-of-the-art in information linking tasks Task Ref. Method Effectiveness Benchmark dataset Temporal and event linking Ning et al. (2017) Structured Learning 67.20 Temporal Awareness TempEval-3 UzZaman et al. (2013) Named entity linking Raiman and Raiman (2018) (1) Type system, and (2) use it to constrain the outputs of a neural network to respect the symbolic structure. 94.88 micro-precision CoNLL 2003 NER task Tjong Kim Sang and De Meulder (2003) 90.85 micro-precision F1-Score Relation extraction Baldini Soares et al. (2019) BERT-based Model 89.5 F1-score SemEval-2010 Task 8 Hendrickx et al. (2010) Semantic role labeling He et al. (2018) BiLSTM 85.50 F1-score OntoNote Pradhan et al. (2013) Discourse relation parsing Gessler et al. (2021) Transformer-based neural classifier EDU segmentation on Treebanked data -91.48 F1-score (average) Connective detection on Treebanked data -91.22 F1-score (average) Relation classification on Treebanked data -61.82 F1-score (average) DISRPT 2021 Zeldes et al. (2021)",
          "Table 3 brings a summary of the current status of five of the tasks (temporal reasoning, entity relation extraction, entity linking, semantic role labeling, and discourse relation parsing) covered at the information linking stage discussed in this section. The type column specifies the type of task addressed in those works. In this table, each study's effectiveness is displayed according to the measures reported by the authors. All the discussed tasks are related to semantic analysis at this pipeline stage. In our supplementary material, made available with this survey, one can find a list if additional benchmark datasets relating to the topics covered in this section. The datasets mentioned there can be used as a reference for carrying out the tasks described here. Some of these datasets are available in languages other than English and are useful for the tasks of linking narrative components in these other languages.",
          "The representation of narratives can be categorized in two levels, the conceptual and the visual level. The first one considers that the elements in a narrative are codified as concepts that allow abstracting the meaning. This codification allows the exploration and detection of the elements of a narrative, smoothing the process of analysis of a narrative. The second level of representation is built using visual elements like lines, graphs, icons, pictures, or other graphical resources used to depict the narrative. This level of representation is more accessible for people of different degrees of general expertise. Next, we detail both levels of representation.",
          "Ontologies are a flexible and complex framework in computer science that allows building schemes to represent several kinds of concepts. They aid the multi-layered meanings of a narrative to be pictured as faithfully as possible (Ciotti 2016) since the main elements are conceptually identified. Over the years, some ontologies have been proposed as a means to representing narratives. Khan et al. (2016), for example, proposed an ontology for narratives applying it to Homer's Odyssey. Damiano and Lieto (2013), in turn, described an ontology for narratives for the hero's journey, which is a traditional archetype of the histories of the western culture. The ontology is applied in digital archives for artwork, and then to aid the browsing through the digital artifacts. Tests were made in a small dataset, and some information about artworks was obtained by reasoning. In another proposal for cultural and historic events, Meghini et al. (2021) depicted the Narrative Ontology (NOnt). The authors built an ontology that comprises some rules, like laws of physics. Then, they applied it to heritage crafts, and case studies were conducted to validate the proposed model. Although, ontology is a flexible framework, building an ontology that encompasses all the elements in a narrative is a cumbersome task. Thus, few proposals undertake this challenge.",
          "Representing and learning common sense knowledge for the interpretation of a narrative is one of the fundamental problems in the quest for a profound understanding of language (Mostafazadeh et al. 2016a). Logic-based representations, like DRT (Kamp and Reyle 1993) and Abstract Meaning Representation (AMR) (Banarescu et al. 2013), express recursive formal meaning structures that have a model-theoretic interpretation, although following different formalizations. DRT adopts a dynamic and compositional perspective, which entails determining how the meaning of a sentence can change the context, and enables a straightforward and elegant representation of not only anaphoric relations, both nominal and temporal, but also of other linguistic phenomena. Within DRT's framework, the processing of the discourse is performed one sentence at a time in Discourse Representation Structures (DRSs) in an incremental manner. Each sentence is represented in the DRSs by discourse referents, which represent entities in the discourse, always displayed at the top of the box, and by conditions, which establish a relation of identity between the discourse referents and the corresponding element of the sentence, typically displayed below the universe constituted by the discourse referents. These DRSs are recursive formal meaning structures that have a model-theoretic interpretation, and can be translated into firstorder logic (FOL). Asher (1993) and Asher et al. (2003) extended DRT with the inclusion of discourse relations and with contributions from other dynamic semantic proposals and pragmatics, naming it Segmented Discourse Representation Theory (SDRT). The aforementioned semantic representations are used by the Groningen Meaning Bank (GMB) (Basile et al. 2012;Bos et al. 2017) and the Parallel Meaning Bank (PMB) (Abzianidze et al. 2017). While PMB adopts DRT as a single semantic formalism to fully represent the different annotated linguistic phenomena, GMB goes further and enhances DRT semantics with discourse relations between DRSs, using Boxer (Bos 2015;Curran et al. 2007), a system which utilizes -calculus to generate the meaning representation of texts. Another logic-based representation, but for sentences, is the AMR (Banarescu et al. 2013). The sentence is represented as a single-rooted graph (Damonte et al. 2017). The syntax of AMRs can be defined recursively, and it is possible to specify a systematic translation to first-order logic. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The AMR Bank (Banarescu et al. 2013), based on this form of representation, is a set of English sentences paired with simple, readable semantic representations in some cases manually constructed by human annotators, but also generated in a semi-automatic manner (Gruzitis et al. 2018). It is possible to represent narratives using AMR as well. Droog-Hayes et al. (2018), for instance, employ AMR to represent Russian folktales and extract the narrative structure.",
          "Narrative visualization addresses the interplay between narrative and visualization. This includes perspectives that range from the use of visualization elements to enrich traditional narratives, to the exploration of narrative techniques in visualization-rich artefacts. In this section, we focus on the visual and structured representation of a narrative as an important step, not only as a final output of the narrative extraction pipeline, but also as a machinereadable representation that can be used as a tool for human inspection and validation of each of the pipeline steps. Broadly speaking, narrative visualization can be mapped into 7 genres (Segel and Heer 2010): magazine-style, annotated chart, partitioned poster, flow chart, comic strip, slide show, and film/video/animation. As stated by the authors, such genres vary primarily in terms of the number of frames -distinct visual scenes, multiplexed in time and/or space -that each contains, and the ordering of their visual elements. None of these genres are mutually exclusive: they can act as components and be combined to create more complex visual genres. A less explored area is the visualization of narratives themselves, a perspective of particular interest in the context of this survey, where the focus is on the representation of the extracted narrative as closely as possible to its original form. Metilli et al. (2019) developed a semi-automatic software that can import knowledge from Wikidata to allow users to construct and visualize narratives, based on a proposed ontology to annotate narratives. Baikadi et al. (2011), in turn, proposed a framework for visualizing a narrative, and introduced an environment designed to explore narrative visualization to support novice writers. Milon-Flores et al. (2019), instead, applied algorithms to extract emotions and characters involved in literary work to propose a methodology that is able to generate audiovisual summaries by the combination of emotion-based music composition and graph-based animation. The representation of narrative participants and events over time can be accomplished by projecting these elements in two-dimensional layouts. Munroe (2009) introduced the concept of a narrative chart, a visual representation that encodes narrative elements and interactions in a representation where lines flows from left (past) to right (future) representing each participant, and events are depicted using ellipsis to which participants are connected. Kim et al. (2018) presented the Story Explorer, a visualization tool to explore and communicate nonlinear narratives through the representation of story curves, which contrast story order (y-axis) with narrative order (x-axis). Story Explorer is used to analyze and discuss narrative patterns in 10 popular nonlinear movies. Less elaborate visual schemes can also be used to represent the elements from a narrative and provide an accessible understanding of it. A simpler, but powerful resource to represent the elements of narrative, their relationships, and the chronological order of them is based on the use of knowledge graphs (Ehrlinger and Wöß 2016) and their linkage to knowledge-bases (von Landesberger et al. 2011). Li et al. (2018), for instance, employed a graph structure to extract and link narrative events. Amorim et al. (2021) in their work proposed the Brat2Viz tool to build a visual representation of a knowledge graph from a DRT representation of a narrative. A useful visual representation, in this context, is also the Message Sequence Chart (MSC) (Harel and Thiagarajan 2003), a diagram created to specify systems requirements. Due to its flexibility, MSCs are also suitable to schemetize other kinds of processes. In Brat2Viz tool (Amorim et al. 2021), the authors generates a MSC from a DRT as well. Further initiatives also explore MSC for narrative visualizations (Palshikar et al. 2019;Hingmire et al. 2020). A discussion of the challenges associated with narrative visualization, and the general field of information visualization, can be found in de Ponte Figueiras (2016), and also in Tong et al. (2018). The work of Edmond and Bednarz (2021) also addresses the multidisciplinary nature of the field and outline possible trajectories based on the emphasis put on the narrative itself or the data visualization components used.",
          "With the growing maturity and understanding of the narrative extraction process, new procedures have come into play to help formalizing the narrative extraction evaluation step. In particular, shared tasks were proposed to establish a common experimental setup and overcome the lack of datasets. In this section, we discuss the evaluation carried out by studies developed in the area. Representations made of the extracted narratives need to be evaluated for their level of abstraction to the application (e.g., health care systems, games, news, etc.). The evaluation of the extracted narrative is an essential step of its understandability (Riedl and Young 2010) because it aims to observe the content's understanding by the final consumers and specialists, which can provide insights and possible improvements. However, computational evaluation presents hard challenges. In the literature, few studies evaluate narratives computationally extracted or generated since this is a subjective and application-specific task. In cases where the evaluation is considered, a manual evaluation method is usually adopted. In this case, the narrative result is delivered to specific (usually hired) people, who evaluate the generated result from pre-defined domain criteria, as it is done in Motwani et al. (2019), and give their feedback and overall perception of it. However, ways of automatically evaluating the extracted narratives are sought. Goyal et al. (2010), for example, evaluated the representation of narratives texts through plot units measuring the F-score achieved by the model. Other works (Metilli et al. 2019;Zahid et al. 2019) also assessed the tasks developed in the context of narratives in terms of this measure. Research works that use this measure have the advantage of observing a given task's performance isolated, i.e., separately. Narratives, however, need to be evaluated as a whole. Narrative Cloze Test is a common evaluation framework for script learning introduced by Chambers and Jurafsky (2008). Based on the cloze task (Taylor 1953), the narrative cloze evaluation approach consists of a sequence of narrative events in a document where one event has been removed. The evaluation is then done by predicting the missing verb and typed dependency. In the context of narratives, this approach became widely used to explore the commonsense reasoning of narratives outcome. In a similar approach, Mostafazadeh et al. (2016a) proposed the Story Cloze Test. The authors presented this measure as a generic story understanding evaluation framework that can also evaluate story generation models (e.g., by calculating the log-likelihoods assigned by the story generation model to the two ending alternatives), which does not necessarily indicate a requirement for explicit narrative knowledge learning. According to the authors, models that perform well in the Story Cloze Test reveal some deeper understanding of the story. In this work, Mostafazadeh et al. (2016a) also presented a corpus of 50k five-sentence commonsense stories (ROCStories (Mostafazadeh 2016)) developed to enable a brand new framework for evaluating story understanding -bringing a great contribution to the narratives field. Another form of evaluating narratives was proposed by Kočiský et al. (2018) in the Nar-rativeQA Reading Comprehension Challenge. In this challenge, one is presented to a dataset and a respectively set of tasks in which questions about stories must be answered. As pointed out by the authors, these tasks are designed so that successfully answering their questions requires understanding of the underlying narrative rather than relying on shallow pattern matching or salience. Approaches like these are essential to the reasoning of narratives, in the sense that they aim to provide understandable narratives to the end-user focusing on the outcome and in the causality between the events. Another aspect that should be considered when evaluating a narrative is from whose point of view the story is being told (Brahman et al. 2021). As discussed in the beginning of this survey, having access to appropriate datasets is one of the most important steps behind any evaluation procedure. In this context, we make available a summarized list of the most important datasets in several languages that suit the evaluation tasks approached in this survey. We refer the interested reader to check the the supplementary material of this paper.",
          "As exposed in the previous sections, the extraction of narratives comprehends a series of interrelated tasks related to different areas, which amount to an intricate and complicated enterprise. As such, and although much has been accomplished, different challenges need to be met. In this section, we refer to the most prominent, organizing them into general and narrative extraction-oriented.",
          "",
          "Human statements frequently contain ambiguity, error, implicit information, and other sources of complexity. Therefore, the creation of cognitive agents with human-level natural language understanding capabilities involves mimicking human cognition (McShane 2017). As narratives are composed of human declarations, the study of narratives presents a high level cognitive challenge. Some of the problems inherent to narratives are coreference resolution, for instance, when the subject is null, polysemy (i.e., the multiplicity of meanings of a word or phrase), synonymy (i.e., the expression of the same idea with different terms), ambiguity, related, not only to polysemy, but also to syntactic structure, presuppositions or sarcasm. Natural Language Understanding, a research area of NLP, can aid in the resolution of some of these issues. However, a model able to deal with the aforementioned issues, and to perform at a level proximate to human reasoning is still far from being developed.",
          "As demonstrated in this survey paper, narratives are formed by a series of links between involved participants and events, and organized according to their causality over time, which may mean that different documents that report the same event can compose a familiar narrative. However, few works found in the literature of the area explore the extraction of narratives from multiple documents. Future research should propose new methods to automatically identify, interpret, and relate the different elements of a narrative, which will likely come from various sources. This dilemma is related to the fact that current models are often centered on recurrent neural networks (which, although can represent contexts longer than other types of networks, still have limitations for this type of strategy). Working in vast environments (i.e., contexts) is closely connected to natural language and requires scaling up internal processes before larger documents can be handled.",
          "Although narratives are present in a set of diverse languages, most of the data available, however, is mainly in English. One consequence of this is that low-resource languages end up receiving far less attention. For example, while the task of temporal information extraction has drawn much attention in recent years, research efforts have mostly focused in the English language (Mirza 2016), and the development of new solutions for less known languages ends up being compromised by the lack of properly annotated datasets.",
          "Pre-processing and parsing, and the identification and extraction of narrative components, are two of the components that present the best results in the process of narrative extraction. This stems from the fact that the tasks covered by these processing stages have many applications and have been studied across several NLP tasks over the years. The remaining tasks of the narrative extraction pipeline, however, still present many challenges for further enhancements. In the following, we describe some of those challenges, highlighting future directions whenever appropriate.",
          "The annotation process is a key element when working on narrative extraction. Since a significant number of the tasks from the narrative extraction pipeline hinge on the existence of large annotated datasets, the effort of annotation is huge. One of the biggest issue is precisely the lack of manually annotated datasets available with the necessary information. Checking the manual annotations can also be troublesome because, often, the multilayer annotation is so dense that the annotator can barely unravel what was annotated. A useful solution to this is to resort to visual representations such as knowledge graphs and message sequence charters to carry out the supervising task. Nonetheless, further developments on these visual representations are needed. Another problem is related to the inter-annotator agreement. As stated before, narratives are complex and not always straightforward, which reinforces the relevance of assessing the level of agreement between annotators, both during the process of creating the annotated datasets and evaluating the annotation performed by means of automatic methods.",
          "Several challenges related to the temporal domain may be understood as possible reasons that prevent the development of more elaborated solutions in different languages. For instance, the free-text temporal expression (Strötgen and Gertz 2016), which deals with general phrases not covered by standard definitions, the resolution of implicit or relative temporal expressions, or the problem posed in normalizing temporal expressions across different time-zones, are among some of the most well-known reasons. Thus, the most promising approaches rely on cross-lingual transformer language models and cross-lingual sentence embeddings that leverage universal language commonalities.",
          "From the linguistic point of view, some texts have a stereotypical structure already well established in the area (as is the case of medical reports). The knowledge of the domain is essential to understand the structure and context of a document to improve the reasoning process. The integration of different knowledge might provide, for example, a hierarchical structure of terminology that can help to identify whether two different statements refer to the same event. For a semantic and pragmatic analysis, other computational tools, such as automated reasoning systems, are useful. These tools may help to explore how inference processes can clarify pragmatic phenomena such as conversational implications and more explicitly context-based understanding.",
          "Most text processing tools concentrate on extracting relatively simple constructs from the local lexical context and focus on the document as a unit or even smaller units such as sentences or phrases, rather than on relations of different elements within the document or even cross-document relations. Correctly inducing semantic relations, between participants, participants and events or between text spans, within a natural language story is also an open issue in narrative studies. For instance, the task of discourse parsing has still a long road to go (Morey et al. 2017), to achieve discourse relation identification beyond local dependencies.",
          "Implementing a combination of three relation extraction techniques (machine learning, heuristics, and a hybrid combination of both) may be an interesting approach to be explored in the context of narratives. As narratives result from a semantic structuring of information extracted from a text, which presents n-ary relations among themselves, the Open IE exploration might provide meaningful insights to narratives studies. As far as we know, nothing of this kind has yet been proposed.",
          "The narrative perspective, also known as the point of view, is the vantage point through which the events of a story are filtered and then transmitted to the audience. Thus, the same story might have different point of views (POV) depending on the person (narrator/ character) who narrates the story or the angle from which one looks (Al-Alami 2019; Brahman et al. 2021). Extracting narratives from different points of view using the same dataset, combining them in a coherent representation of the story or deciding on which are the most relevant are still open challenges. Considering different narrative points of view also influences the task of evaluating narratives.",
          "Without using some form of a semantic representation that offers an abstraction from the details of lexical and syntactic realizations, comprehension is arguably unattainable. Representing embedded stories -stories told inside a story-is also a complex task that remains open. Gervás (2021) proposed a simple model to represent them. Nevertheless, the author stated that there is substantial further work in the depicted model. Narrative visualization is an active area of research, mostly as a result of a strong community focused on interactive storytelling and the use of data visualization techniques to improve narrative understanding. In contrast, there is ample opportunity for further research in the visual representation of narratives themselves. Existing research is mostly focused on the representation of narrative elements such as participants and events, whereas work on commonly used narrative techniques-e.g., focalization, allegory, personification, amplification-is scarce and represents a challenging opportunity for future research. With the development of the field, research on the proposal of common visual vocabularies and patterns for recurring visual solutions also constitute a pertinent opportunity for further research. Another aspect where research opportunities exist is the study of user interaction with narratives-which degree of manipulation is useful to improve narrative understanding? Which elements and dimensions of a visual representation should be open to user interaction? Finally, evaluating the effectiveness of the visualization of a narrative typically resort to user studies involving adhoc tasks-an opportunity for the development of reference evaluation benchmarks and guidelines exists.",
          "Coming up with a framework that evaluates the narrative extraction pipeline as a whole is a crucial step for further developments. One of the difficulties refers to the process of creating a gold standard dataset, a labor intensive task that is highly dependent on the subjective interpretation of several factors, such as the emphasis put on each narrative participant, or the level of detail to include. Raters may also have differing viewpoints on annotating data or disagree about what details should be kept, contributing to a lower consensus between raters and intra-raters. On the application level, the purposes of evaluation may vary according to the context addressed. In general, the proposed systems should answer questions like: What is this narrative about? Does the system accurately extract the required elements to tell a story from it properly? How accurately and efficiently does the present system represents the narrative in terms of clarity and amount of data to the application context? The processing of narrative in large-scale or from multiple documents also bumps the issue that supervision is expensive to obtain. When considering the evaluation metrics for natural language processing, another issue is related to how well these methods match and generalize to the real complexity of human languages and how many more interesting natural language inference datasets can be generated.",
          "Narratives are an essential tool for communication, representation, and understanding information. Computational systems that are able to identify narrative elements and structures can naturally interact with human users. Such systems understand collaborative contexts as an emerging narrative and can express themselves through storytelling (Riedl 2004). This survey paper provides simultaneously an account of the study of narrative extraction and a roadmap for future research. To this end, we propose a narrative extraction pipeline, defining the key tasks involved in this process identified in the literature. By doing this, we set a common ground for further studies, highlighting the different stages of the narrative process and the most prominent approaches. During the course of this survey paper, we also pointed out extensive literature focused on extracting narratives and supporting NLP tasks. Nonetheless, and despite several recent advances, there are still important open issues demonstrating that narrative extraction is a rich and promising research area that requires multidisciplinary knowledge in the crossroads between linguistics and computation."
        ],
        "ground_truth_definitions": {
          "composition of a narrative": {
            "definition": "is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events.",
            "context": "Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narra- tive can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time.",
            "type": "implicit"
          },
          "Computational Narratology": {
            "definition": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations",
            "context": "The study of narratives from the computational perspective is carried out through a study area called Computational Narratology (Mani 2014). Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "type": "explicit"
          },
          "narrative extraction": {
            "definition": "the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "context": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources. A closely related term is Computational Narrative Understanding (Piper et al. 2021), which broadens the perspective to social, personal and cultural dimensions.",
            "type": "explicit"
          },
          "event argument": {
            "definition": "a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event mention": {
            "definition": "a sentence or expression that explains an event, including a cause and multiple arguments.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event trigger": {
            "definition": "the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "argument role": {
            "definition": "the link between an argument and the event in which it is involved.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "ontologies": {
            "definition": "formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation.",
            "context": "Another way is to relate the events of the narrative with entities from a knowl- edge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of repre- sentation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008).",
            "type": "explicit"
          },
          "discourse": {
            "definition": "The expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc.",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "explicit"
          },
          "story": {
            "definition": "the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting)",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "implicit"
          },
          "event": {
            "definition": "something significant happening at a specific time and place with consequences.",
            "context": "Finding the events mentioned in the text is an essential step towards the extraction of nar- ratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "2025.acl-long.1513",
        "sections": [
          "As online news and social networks have radically altered the media landscape, understanding the dynamics of news propagation and the narratives that news convey is more crucial than ever. Online news outlets make it possible for recurring narratives to appear, often with very different phrasings, in multiple articles and propagate with very high velocity across audiences, languages, and countries. This is especially problematic with manipulative narratives, potentially containing strong biases, mis/disinformation, propaganda, or harmful content. Such a risk is even more pervasive when it is related to divisive issues and can have large societal implications, including destabilization, conflict, hate, or incitement to violence. Various definitions of narrative can be found in the literature, depending on the context and the goal of the analysis. In this paper, we define narrative as a recurring, repetitive (across and within articles), overt or implicit claim that presents and promotes a specific interpretation or viewpoint on an ongoing (and frequently dynamic) news topic. Narratives may be embedded in thousands of articles across multiple topics, themes, languages, and news genres (e.g., news reporting or opinion). They can manifest using very different vocabularies, media frames (Entman, 2007), and tones, which makes their automatic detection and extraction very challenging. Moreover, multiple narratives can be present in the same article, or even in the same paragraph or in the same sentence. An example is shown in Fig. 1, where the narrative \"Downplaying Climate Change\" occurs in two articles from very different domains: finance vs. auto enthusiasts. Thus, the effective automatic detection, extraction, and analysis of narratives in news articles is an important challenge and remains an open problem ( §2). Such capacity is crucial for analyzing the news landscape, identifying media bias, and detecting attempts to influence readers. It can also be used to warn and educate media consumers to improve their media literacy. Large Language Models (LLMs) have made the need for such capacity even more urgent, as they are vulnerable to (Xu et al., 2024) and prone to generating manipulative narratives (Vykopal et al., 2024) with modest effort. In this work, we focus on the problem of narrative detection, treating it as multi-label paragraphlevel classification problem ( §3). The biggest challenge is the lack of annotated datasets (Nunes et al., 2024), especially for long news articles (as opposed to micro-blogs or snippets). Existing narrative classification datasets for long articles ( §2) have been annotated at the document level and assign only a single label per annotation. Narratives, however, are often conveyed by small parts of the articles, and thus it is important to pinpoint those parts, which is why we model the problem as a paragraph-level classification. We consider two news domains of current relevance ( §3.1): climate change (CC) and the military conflict between Ukraine and Russia (URW). Unlike previous work on narrative classification, where the labels are flat and coarse (Li et al., 2023) ( §2), we developed a two-level taxonomy with coarse-and fine-grained narrative labels (hereafter narratives and sub-narratives) for each topic ( §3.2). We collected news articles in four languages-English, Bulgarian, Portuguese, and Russian-related to the two domains ( §4.1), and manually annotated them at the paragraph level with labels from the corresponding taxonomies ( §4.2). We report statistics about the resulting dataset ( §4.3) and discuss its quality ( §4.4). We make the dataset, dubbed PolyNarrative, publicly available, along with experimental results ( §5) and code of several strong baselines that assign narrative labels to news articles at the paragraph or the document level; in the latter case, we use the union of the paragraph-level ground-truth labels of each document as the correct prediction. Our contributions are as follows: • We propose a new two-level hierarchical taxonomy for narrative extraction. We further develop a novel datasetfoot_0 that uses this taxonomy with news article annotations from two levels (coarse-and fine-grained), at the paragraph level (previous work was document level), using multi-label annotation (previous work was mostly single-label) across two current domains (previous work was single-domain), and four languages (previous work was for a single language). • We provide a comprehensive overview of the data acquisition and annotation process, and highlight the challenges and measures taken to address them. • We perform evaluation using several strong multi-label classification methods, and draw conclusions that can benefit future work. 2 Related Work",
          "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023;Silvano et al., 2024) and capturing narrative components in the form of semantic relationships between entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012;Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes. In an effort to synthesize various formulations of the concept, Dennison (2021) proposed a refined definition of narratives as \"selective depictions of reality across at least two points that can include one or more causal claims, and are generally generalizable and can be applied to multiple situations, as opposed to specific stories\". This formulation, although well structured from a theoretical perspective, was not sufficient to properly operationalise the concept for our intention to provide a concrete taxonomy. Denmark Punishing Farmers for Cow 'Emissions' to 'Fight Global Warming' Denmark has become the first country to force farmers to comply with the goals of the World Economic Forum's (WEF) \"Net Zero\" agenda, to supposedly \"fight global warming.\" ...",
          "Although carbon dioxide is typically blamed for causing \"climate change,\" globalists claim that methane traps about 87 times more heat on a 20-year timescale but top scientists have debunked these claims as a hoax. A recent peer-reviewed study provided conclusive scientific evidence proving that carbon dioxide (CO2) emissions in Earth's atmosphere cannot cause \"global warming.\" Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change However, not only did they find that higher levels of CO2 made no difference, but they also proved that it simply isn't possible for increases in carbon dioxide to cause temperatures to rise. Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change Nevertheless, unelected globalists at the United Nations Environment Program claim that livestock accounts for about 32% of \"human-caused methane emissions.\" ... Thus, our definition of narrative ( §1), while informed by the above general formulation, is based on previous efforts to create taxonomies and datasets for practical use (Kotseva et al., 2023;Li et al., 2023;Coan et al., 2021).",
          "",
          "To apply modern NLP methods, as highlighted by Santana et al. (2023), extraction of narratives has been approached as hierarchical event detection (Glavaš et al., 2014), as frame bias detection (Zaghouani et al., 2024), as spatiotemporal entity relationship detection (Nunes et al., 2024), and as multi-class classification (Li et al., 2023). Li et al. (2023) presented a social media dataset based on short texts from microblogs and trained a BERT-based narrative classifier detecting COVID anti-vaccine claims from and using a flat taxonomy, while Weigand et al. (2022) focused on conspiracy narrative detection by applying a binary classifier to detect the presence of conspiracy theories and applied a topic modeling segmentation without a pre-defined taxonomy. Coan et al. (2021) released an English-only dataset using a two-level taxonomy on climate change denial, annotated at the snippet level from website fragments and using a single label per snippet, while Kotseva et al. (2023) provided a multi-lingual dataset annotated at document-level using a single label per document. While datasets with individual characteristics (long-articles, hierarchical taxonomies, multilinguality) have been released before, to the best of our knowledge, there has been no other dataset that provides a multi-lingual corpus, annotated at paragraph level using a hierarchical taxonomy in a multi-label multi-class fashion.",
          "Several narrative taxonomies have been created to classify recurring argumentation in online news. Kotseva et al. (2023) created a three-level narrative taxonomy on COVID-19 and used it to classify and to analyze trends over time. Hughes et al. (2021) presented a taxonomy of common anti-vax narratives, organized on several common tropes and rhetorical strategies. Coan et al. (2021) presented a two-level taxonomy for common cases of climate change denial in short snippets. Amanatullah et al. (2023) presented a flat taxonomy of common pro-Russian narratives found in the alleged pro-Kremlin influence campaigns related to the war in Ukraine. In this work, we used the last two taxonomies as a starting point for the two domains of interest.",
          "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, \"CO 2 is beneficial\" is a sub-Narrative of the \"Climate Change is beneficial\" Narrative of the Climate Change (CC) taxonomy. A (sub-)Narrative can appear in a variety of contexts and topics, and can manifest using subtle and indirect argumentation or framing, and thus its detection can be very challenging. For example, the sub-Narrative \"Western sanctions will backfire\" could be expressed throughout a financial reporting article, or in a small paragraph at the end of an opinion article commenting on military matters. Thus, in order to better tackle this challenge, our approach focuses on highlighting specific segment(s) of each news article that expresses each (sub-)Narrative. In this sense, we consider a paragraph as a minimal segment. If a (sub-)Narrative is expressed in multiple paragraphs of an article (or the entire article), all the corresponding paragraphs need to be annotated accordingly. Moreover, one paragraph can express more than one Narrative and more than one level of Narratives (Narratives and sub-Narratives). In these scenarios, the paragraph needs to be annotated with all the applicable (sub-)Narratives. Given this formulation, the task becomes a hierarchical multi-label multi-class classification problem at the paragraph level.",
          "We selected two domains (topics) that currently receive extensive news coverage, are presented through several different perspectives, and are highly susceptible to manipulation through the creation of artificial narratives. The first domain is Climate Change (CC) and the second one is the Ukraine-Russia War (URW). In both cases, we have numerous accounts of repeated argumentation in the form of narratives. We are particularly interested in potentially manipulative narratives, and this is reflected in the choice of the (sub-)Narratives of our taxonomies. However, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily a case of mis/disinformation. This is important to highlight, as mixing legitimate with mis/disinformative claims is a frequent manipulative practice (Goel et al., 2023).",
          "To create the two taxonomies, we started from existing narrative taxonomies for URW (Amanatullah et al., 2023) and CC (Coan et al., 2021). We asked media analysts to suggest modifications based on their media monitoring experience. These modifications included additions (new (sub-)Narratives), elaborations (splitting existing (sub-)Narratives into multiple different ones), modifications (refining definitions and names) and merging (grouping into common (sub-)Narratives). We asked, when possible, to phrase each (sub-)Narrative as a con-crete claim (e.g., \"Renewable energy is dangerous\" or \"Ukraine is a puppet of the West\"). When the argumentation regarding a (sub-)Narrative was very diverse or fragmented, we gave a more general descriptive label (e.g., \"Criticism of Institutions\" or \"Speculating war outcomes\"). We ended up with 38 and 36 sub-Narratives, grouped into 11 and 10 Narratives for URW and CC, respectively. The top level of the two taxonomies (Narratives) is shown in Figure 3. The full taxonomies, including sub-Narratives and definitions, can be found in Appendix A. We note that the two taxonomies are not complete; in fact, they are not intended to be complete, rather they are representative of the practical experience of the media analysts, and different valid formulations are possible. We stress again that although our taxonomies focus on potentially manipulative narratives, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily mis/disinformation or indicates strong bias. UKRAINE-RUSSIA WAR (URW) Blaming the war on others rather than the invader Discrediting Ukraine Russia is the Victim Praise of Russia Overpraising the West Speculating war outcomes Discrediting the West, Diplomacy Negative Consequences for the West Distrust towards Media Amplifying war-related fears Hidden plots by secret schemes of powerful groups CLIMATE CHANGE (CC) Criticism of climate policies Criticism of institutions and authorities Climate change is beneficial Downplaying climate change Questioning the measurements and science Criticism of climate movement Controversy about green technologies Hidden plots by secret schemes of powerful groups Amplifying Climate Fears Green policies are geopolitical instruments ",
          "",
          "To obtain news articles for annotation, we used an in-house online news scraping and indexing tool to retrieve articles on the two selected domains in the four target languages. 2 We considered both mainstream and \"alternative\" news sources (that is, sources known to repeatedly publish content with misinformation). We also developed ad-hoc scoring methods, outlined below, to heuristically evaluate the relevance of each article to the corresponding domain. More specifically: 1. We created keyword-based queries for the two domains in all the languages, and used them to retrieve a large number of articles from the news article index of our in-house tool. For the URW domain, we included documents from 2021 to 2024. For the CC domain, the documents ranged from 2015 to 2024. 2. To assess the relevance of the resulting articles, we formulated another set of key phrases, corresponding to the (sub-)Narratives of the taxonomies (e.g., \"Ukraine is corrupt\", \"Science is debunked\") or other aspects we wished to emphasize or de-emphasize. We then used bart-large-mnlifoot_2 to perform zero-shot classification with the title and the first 300 characters of each article, resulting in a classification score per article and key phrase. 3. We used an XLM-RoBERTa-basedfoot_3 multilabel classifier, trained on the Persuasion Techniques dataset (Piskorski et al., 2023a,b), and used the approach of Nikolaidis et al. (2024) to produce Persuasiveness Score metrics per article. For the CC domain, we also used a climate change denial classifier, based on the work of Coan et al. (2021), to further filter and reduce the number of articles to review. 4. We used a linear combination of the resulting scores (relevance score per key phrase, Persuasiveness Score metrics and the score from the climate change denial classifier) to automatically rank articles from most to least likely to contain relevant (sub-)Narratives. A member of the team who was familiar with the news landscape of the language, then manually inspected the resulting set of articles, focusing on articles that seemed relevant to the defined taxonomies. Since manual inspection was timeconsuming, we used the different scores computed above to reorder the articles and focus mainly on highly ranked articles.",
          "In order to clarify the annotation process, including the meaning of key concepts such as Narrative, sub-Narrative, the precise meaning of each label of the taxonomies, and good practices, we created a document with detailed annotation guidelines. This included specific definitions and examples for each label, together with a detailed description of the annotation process to be followed. The definitions and examples of the taxonomies can be found in Appendix A. As we wanted to produce a multilingual dataset with four languages, we had multiple annotator teams, each focused on one language. We assigned two annotators per news article and a curator who handled consolidation and quality assurance before the final version. We held regular calls in which hard cases were highlighted, comparisons across annotations from different languages were made, and issues with ambiguities of the guidelines were discussed. To annotate the documents, we used the Inception annotation tool (Klie et al., 2018). Each language was assigned a coordinator who had the responsibility to sort through the collected articles, assign annotators and curators, and create annotation batches incrementally. The coordinator was also responsible for training the annotators. We instructed the annotators to read each news article paragraph by paragraph, and to annotate each of them with all the applicable Narratives and sub-Narratives before moving to the next one. When no Narrative or sub-Narrative was found, the annotator simply moved to the next paragraph. Because the sub-Narrative labels were many and difficult to memorize, we configured Inception so that for each paragraph, the annotator would first select all the applicable Narratives, and then all the applicable sub-Narratives from a menu displaying the sub-Narratives of the selected Narratives only. When a paragraph was deemed to contain a Narrative, but none of its sub-Narratives, the \"[Narrative]: Other\" label was selected at the sub-Narrative level. Similarly, the \"Other\" label was selected when no Narrative was applicable. During the annotation process, several issues were highlighted. First, the distribution of labels was very uneven and varied noticeably across languages (Table 2). This reflects the reality of the news discourse, as some Narratives are more frequent in some countries than others. After each annotation batch, the collected label statistics were used by the coordinator to optimize the selection of articles for the next batch. Second, some labels were semantically very close, which led to frequent confusion. We used frequency metrics and cura-tor feedback to discuss such cases in the regular meetings, where we reiterated and further refined the guidelines. For example, the sub-Narratives \"Ukraine is a puppet of the West\" and \"The West does not care about Ukraine, only their interests\" were frequently confused. To resolve this ambiguity, we added guidelines instructing the annotators to select the former when Ukraine was the subject and the latter when the West was.",
          "Table 1 shows statistics about the documents of the PolyNarrative dataset, broken down per language, and aggregated over both domains. We can see that, except for Russian, all languages have approximately 400 annotated training documents; the test documents are approximately 35 for each language. We aimed at articles of relatively long text with a median length of 500 words and a minimum length of 250 words. Even after the rigorous data selection process, the final label distribution after annotation is highly imbalanced (see Table 2 and Appendix B for a breakdown per language). Additionally, the distribution of labels varies noticeably from language to language, due to differences in media interest across countries, as already discussed. For example, for URW, Russian is a notable outlier in terms of distribution, with \"Praise of Russia\" being one of the most common Narratives. One note to highlight, is that after the article selection process, the proportion of mainstream and alternative news varies widely across languages and could have an impact on label distribution. Moreover, 44% and 54% of the documents in CC and URW, respectively, are assigned more than one label; for paragraphs, the corresponding percentages are 12% and 15%, respectively. This observation justifies our choice to perform multi-label annotation.",
          "Table 3 reports the inter-annotator agreement scores for Narratives (coarse labels) and sub-Narratives (fine-grained labels), per language, collectively for both domains, measured as Krippendorff's α at paragraph level using the simpledorfffoot_4 library. The agreement is under the recommended value of 0.667 but is higher than the IAA of tasks of similar complexity (Piskorski et al., 2023b(Piskorski et al., , 2024)). TRAIN Language #documents #paragraphs #sent. #char. BG 400 3,951 6,728 937,150 EN 394 3,683 8,012 1,176,513 PT 395 3,877 5,739 971,356 RU 133 583 1,630 206,124 TEST Language #documents #paragraphs #sent. #char. BG 35 325 542 70,291 EN 41 584 1,106 144,401 PT 35 330 578 86,132 RU 32 154 478 56,872 Total 1,465 13,487 24,813 3,649k Table 2: Distribution statistics for coarse-grained labels (Narratives), assigned to paragraphs and documents, in both TRAIN and TEST splits. Statistics for fine-grained (sub-Narratives) are shown in Appendix B. As one would expect, the agreement on finer labels is lower, because of their subtler differences. We noticed that the CC domain caused more confusion between the annotators than URW. In both cases, we could see that the confusion was skewed by a small set of under-agreed labels (5 for URW and 7 for CC) that achieved disagreement above 40% and 60%, respectively. If we excluded these labels, the IAA for all languages rises to 0.567 and 0.560 for the coarse and 0.452 and 0.516 for fine-grained, for CC and URW respectively. There were some sub-Narratives that were commonly confused. For example, in the URW subset \"Discrediting the West, Diplomacy: West is tired of Ukraine\", \"Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests\", and \"Discrediting Ukraine: Ukraine is a puppet of the West\". Overall, on a positive note, the majority of the disagreement between the two annotators in the sub-Narrative labels was between labels of the same Narrative (e.g. sub-Narratives under \"Discrediting the West, Diplomacy\" were frequently confused with one another) On average, out of all the individual annotator disagreements (paragraphs where the two annotators picked a different sub-Narrative), 67% was with sub-Narratives of the same Narrative. A detailed breakdown of the disagreements statistics for each label can be found in Appendix E.",
          "",
          "To provide baseline results for the new PolyNarrative dataset, we performed experiments with commonly used multi-label classification methods. We assessed the following configurations: • Label granularity: We assessed the performance in both coarse (predicting Narratives) and fine-grained (predicting sub-Narratives) settings. Additionally, we assessed how the extra information about the fine-grained predictions impacted coarse-grained predictions. • Paragraph vs. document: We assessed the performance when the models were trained and evaluated to classify paragraphs vs. entire documents. The document-level labels were produced by taking the union of the labels of all of the paragraphs contained in each document. • Prompting vs. fine-tuning: We also assessed the capability of (open-source) LLMs to annotate (sub-)Narratives without further training, given instructions similar to the guidelines given to the human annotators, comparing performance to fine-tuned models. We experimented with the following models: • XLM-RoBERTa large (Conneau et al., 2020), in two versions: -XLM-RoBERTa-coarse: trained using only the coarse labels. -XLM-RoBERTa-fine: trained using the fine-grained labels. • Llama3.1-70b (Grattafiori et al., 2024), without fine-tuning, in four versions: -Llama3.1-70b-0shot-labels: Prompted with the names of the labels of the taxonomy only, in a zero-shot fashion. -Llama3.1-70b-guidelines: Prompted with the full annotation guidelines, which include hand-picked examples for each (sub-)Narrative. -Llama3.1-70b-guidelines-labels: Prompted with the names of the labels, followed by the full annotation guidelines. -Llama3.1-70b-hierarchical: two-step hierarchical prompting, where the model is instructed to pick the coarse-grained labels first, and then given these coarsegrained predictions, decide on the finegrained label; full guidelines are also provided. When using Llama, after receiving the model's response, we performed a series of post-processing operations, where the generated output labels were filtered and normalized to remove hallucinated labels and to handle cases where the model generated labels in a slightly different form (e.g., different capitalization, punctuation errors). To extract paragraph-level annotations from Llama, we inserted extra \"[paragraph_N:]\" tags at the beginning of each paragraph, N being the number of the paragraph, and modified the prompts to instruct the LLM to output each extra tag followed by the relevant labels of the corresponding paragraph. After the call to the LLM, we recombined the outputs with the input paragraphs in a post-processing phase, using regular expressions. The exact prompts used are given in Appendix C. The XLM-RoBERTa (Conneau et al., 2020) models were trained at the token level. For each token, a multi-hot vector equal in size to the number of labels was provided as ground-truth, and each token inherited the ground-truth labels of its paragraph. At inference time, we obtained a similar predicted multi-hot vector from each token, and each label was assigned (or not) based on the majority vote of the paragraph's tokens. To bypass the 512 token limitation of XLM-R, we used a sliding window with a 50% overlap. We also report the results for a naïve stratified random baseline, where we randomly draw labels from a multinomial distribution, while respecting the frequency statistics in the training set.",
          "Since our task is multi-label multi-class classification with a large number of classes, and noticeable class imbalance, we opted to use suitable metrics that are robust in these conditions. Table 4 presents the F 1 macro and sample-averaged F 1 scores for all models in both domains (CC, URW). Sample-average F 1 score is calculated by calculating the F 1 score on each instance and subsequently averaging over all instances. We observe that the XLM-RoBERTa large classifier outperforms Llama3.1-70B in the fine-grained case both for document-and paragraph-level predictions, especially in sample F 1 , where the difference is very pronounced. The Llama3.1 prompted model seems to be more competitive in coarse-grain evaluation. Interestingly, including the full guidelines in the prompt generally achieves worse results than using the taxonomy labels only. When we add both the taxonomy labels and the guidelines, performance improves, compared to using only the guidelines, but it is unclear whether it is better to include only the taxonomy labels. Hierarchical prompting does not seem to yield competitive results. In terms of per-label performance, the weakest model performance was measured on the sub-Narratives under \"Downplaying climate change\" and \"Green policies are geopolitical instruments\" Narratives in the CC domain and \"Russia is the Victim\" and \"Blaming the war on others rather than the invader\" in the URW domain. One key observation is that models exhibited relatively lower performance for the \"[Narrative]: Other\" labels, that is, the paragraphs that contained a Narrative but none of the listed sub-Narratives, that may include a very diverse set of argumentation, coming from a set of heterogeneous sub-Narratives that our taxonomy does not cover. Performance also seemed to vary noticeably per language. In case of Russian, Llamma3.1-70b models seemed to perform rather poorly on a per-sample level. On XLM-RoBERTa, English consistently exhibited poorer performance compared to other languages, with Bulgarian and Portuguese showing different performance per domain. We provide a detailed breakdown of the performance (by label and by language) of two models (XLM-RoBERTa f ine and Llama3.1-70bguidelines-labels) on Appendix D.",
          "We present PolyNarrative, a new multi-lingual, multi-label, multi-domain dataset for extraction of narratives at the paragraph level from long news articles. The dataset comprises 1,476 articles totaling 13,625 paragraphs in 4 languages (Bulgarian, English, Portuguese, and Russian), annotated using two expert-refined Narrative hierarchical taxonomies with 38 and 36 fine-grained and 10 and 11 coarse-grained narratives for each domain. We describe in detail the data acquisition and annotation process and highlighted noteworthy issues. We present preliminary experimental results using both fine-tuned encoder (XLM-R) and prompting-based decoder (Llama 3.1) Language Models in multiple granularities (fine, coarse), levels (paragraph, document), and configurations (prompting strategies). We also highlight interesting findings regarding model performance. We hope that this dataset will catalyze new research directions in narrative detection and extraction, and stimulate the development of new methods and techniques. We highlight that the taxonomies are based on expert real-world experience and they are not meant to be perceived as complete or to capture the whole breadth of the media discourse around the two domains. We encourage the research community to propose extensions that potentially capture more diverse perspectives. We encourage the direction of developing methods that are domain-and potentially taxonomy-agnostic, able to perform narrative detection in a completely unsupervised fashion, removing the dependence on human subjectivity. We believe that a multi-domain and highly granular dataset such as this one can facilitate experimentation towards this goal. Regarding future work, we plan to experiment with ways to perform generation of new narrative labels and automatic revision of taxonomy and guidelines. One way to approach this is by analyzing the structure of narrative elements in the text relate them with narrative labels and identify new structures. We also intend to use retrieval strategies to perform narrative retrieval within large news corpora.",
          "Intended Use and Misuse Potential The main drive behind the creation of our dataset was to advance research on automated narrative classification and the detection of deceptive content across multiple languages and domains. However, given that possible risk of exploiting the dataset to boost the production of biased manipulative disinformation attempts, we advise responsible use. In this context, whoever develops a Narrative detection system is also responsible for deciding which Narratives to detect, in an ethical manner. Furthermore, this research in direction can contribute to the development of independent tools such as browser plugins or independent publicly available services that could help users contextualize the information they consume, contributing to their awareness and improving citizens' media literacy. Environmental Impact The deployment of LLMs might have a large carbon footprint, especially when training new models. In the context of the reported experiments, we did not train any new LLMs, but only used existing trained models in an in-context zero-shot scenarios, which is relatively cheap in terms of computing. Fairness The majority of the annotators, primarily researchers with linguistic background and prior annotation experience, come from the institutions of the co-authors of this manuscript. They were fairly remunerated as part of their job. The remaining part of the annotator pool consisted of (a) some students from the respective academic organizations, (b) few external experienced analysts paid at rates set by their contracting institutions, and (c) experts from a contracted a professional annotation company, who were compensated according to rates based on their country of residence.",
          "Taxonomies and Dataset Representativeness Our taxonomies were edited by experienced media analysts, active in the study of misinformation and fact-checking. As such, the taxonomies overrepresent Narratives of interest of media analysts from Western institutions. The selection should not be perceived as covering the complete discourse of the two domains, but rather what such analysts encounter in practice. The original taxonomy for URW used before the heavy revision from the analysts, was not from a peer-reviewed publication but from a technical report. The dataset presented covers two widely discussed domains around the world and a wide range of media sources. However, it should not be considered as representative of the media in any specific country, nor should it be considered as balanced in any way. Also, it is important to note that while this work focuses on narratives potentially containing mis/disinformation, the operationalized definition of Narrative can also be used to detect neutral or desirable viewpoints. Biases Although the annotators were trained and made acquainted with the specifics of the two domains of interest for our task and cross-language quality control mechanisms have been put in place in the annotation process, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset. Consequently, models trained on this dataset might exhibit certain biases. and Han Qiu. 2024. The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16259-16303, Bangkok, Thailand. Association for Computational Linguistics. Wajdi Zaghouani, Mustafa Jarrar, Nizar Habash, Houda Bouamor, Imed Zitouni, Mona Diab, Samhaa El-Beltagy, and Muhammed AbuOdeh. 2024. The FIGNEWS Shared Task on News Media Narratives. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 530-547, Bangkok, Thailand. Association for Computational Linguistics.",
          "This section introduces the taxonomies of narratives and subnarratives for the two domains. A.1 Ukraine-Russia War (URW) 1. Blaming the war on others rather than the invader: statements attributing responsibility or fault to entities other than Russia in the context of Russia's invasion of Ukraine. Example: \"The economic crisis is due to Western sanctions.\" Example: \"Ukraine's actions provoked this conflict.\" Sub-Narratives: (a) Ukraine is the aggressor: Statements that shift the responsibility of the aggression to Ukraine instead of Russia and portray Ukraine as the attacker. Example: \"Ukraine secretly provoked the war because it was harassing Donbass province citizens.\" (b) The West are the aggressors: Statements that shift the responsibility for the conflict and escalation to the Western block. Example: \"The real perpetrators were US/EU. They sabotaged Minsk II agreement only to force Russia to invade.\" 2. Discrediting Ukraine: Statements that undermine the legitimacy, actions, or intentions of Ukraine or Ukrainians as a nation. Example: \"Ukraine's government is corrupt and ineffective.\" Example: \"Ukrainian soldiers are committing atrocities.\" Example: \"Ukrainian identity does not exist\" Sub-Narratives: (a) Rewriting Ukraine's history: Statements that aim to reestablish history of Ukrainian nation in a way that discredits its reputation. Example: \"Ukraine is not a real nation, it was a fabrication to split Russia and ally with Hitler.\" (b) Discrediting Ukrainian nation and society: Statements that aggressively undermine the legitimacy and reputability of Ukrainian ethnicity and people (c) Discrediting Ukrainian military: Statements that aim to undermine the capabilities, professionalism or effectiveness of the Ukrainian armed forces. (d) Discrediting Ukrainian government and officials and policies: Statements that seek to delegitimize the Ukrainian government, its leaders, and its policies, portraying them as corrupt or incompetent. (e) Ukraine is a puppet of the West: Claims that Ukraine is controlled or heavily influenced by Western powers, particularly the United States and European Union. (f) Ukraine is a hub for criminal activities: Allegations that Ukraine is a center for illegal activities such as human trafficking, drug smuggling, or organized crime (g) Ukraine is associated with nazism: Accusations that Ukrainian society or government has ties to or sympathies with Nazi ideology, often referencing historical events or extremist groups. (h) Situation in Ukraine is hopeless: Statements that portray Ukraine as having no viable perspectives or no potential positive future. Example: \"Ukraine should just give up, it is all over debt and will be exploited by the West anyway.\" 3. Russia is the Victim: Statements that portray Russia as being unfairly targeted or victimized. Example: \"Russia is being unfairly sanctioned.\" Example: \"The West is ganging up on Russia without justification.\" Example: \"Russia is doing what every country would do (e.g. protect its interests/honour etc.)\" Sub-Narratives: (a) The West is russophobic: Statements that claim that the negative reaction to Russia's actions are because of the negative perspective of western countries instead of Russia's own actions. Example: \"Politicians in the West blame Russia for everything, instead of looking at their mistakes.\" Example: \"In Country X, they banned Tchaikovsky ballets and Chechov's plays because they cannot stand Russia and its culture.\" (b) Russia actions in Ukraine are only selfdefence: Statements that justify Russia's action solely as legitimate self-defence and not a deliberate action. Example: \"There was no other way than war to defend the Russian-speaking people in Donbass.\" (c) UA is anti-RU extremists: Statements claiming that Ukraine is comprised of extremist elements that are vehemently opposed to Russia. 4. Praise of Russia: Statements that positively highlight Russia's actions, policies, or character Example: \"Russia is leading the way in international diplomacy.\" Example: \"The Russian economy is resilient and strong.\" Example: \"Glorifying mentions of Russia's weapon systems and military might.\" Sub-Narratives: (a) Praise of Russian military might: Statements that positively highlight Russia's military institutions, equipment and scale. Example: \"Russia has far more tanks and powerful artillery that US/EU would only dream of.\" (b) Praise of Russian President Vladimir Putin: Statements that present Vladimir Putin positively, including his personal and leadership qualities. Example: \"Any country would want such a strong leader as Putin to lead the way.\" (c) Russia is a guarantor of peace and prosperity: Statements that portray Russia solely in a positive manner, emphasising their potential to provide peace and prosperity to those that cooperate. Example: \"Take a look at Africa, Russia supports countries and turns them into independent nations guided by their people's interests where Western countries colonised brutally.\" (d) Russia has international support from a number of countries and people: Statements that emphasise the popularity and acceptance of Russia in the international stage. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" (e) Russian invasion has strong national support: Statements that emphasise the popularity and acceptance of the invasion inside Russia and on Russian-speaking populations. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" 5. Overpraising the West: Statements that excessively and unduly laud or extol the virtues, accomplishments, and moral superiority of Western countries, particularly in the context of international relations and military. Sub-Narratives: (a) NATO will destroy Russia: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) and its allies are capable or already in the process of eradicating Russia. (b) The West belongs in the right side of history: Statements that portray Western nations and their actions as morally superior and aligned with progress and justice and possess moral superiority. (c) The West has the strongest international support: Statements that emphasize or claim widespread backing for Western policies and actions from the international community, potentially downplaying opposition or criticism. 6. Speculating war outcomes: Statements that predict or make assumptions about the potential results or consequences of a conflict Sub-Narratives: (a) Russian army is collapsing: Statements that suggest or claim that the Russian military is experiencing a significant decline in its effectiveness, strength, or morale. (b) Russian army will lose all the occupied territories: Speculative statements that predict or assume the potential outcomes of the conflict, specifically regarding the possibility of the Russian military losing control of all the territories it currently occupies. (c) Ukrainian army is collapsing: Statements that suggest or claim that the Ukrainian military is experiencing a significant decline in its effectiveness, strength, or morale. 7. Discrediting the West, Diplomacy: Statements that criticize the Western countries, or international diplomatic efforts. Example: \"The West is hypocritical in its foreign policy.\" Example: \"Western diplomacy has failed in resolving conflicts.\" Example: \"International organizations will not solve anything because...\" Sub-Narratives: (a) The EU is divided: Statements that present the EU as a set of divided entities and interests, usually unable to take actions. Example: \"The European Council will never vote on sanctions for Russia, since they cannot agree on even the simplest of the issues.\" (b) The West is weak: Statements presenting the West overall as a non-potent group of countries (that is not as powerful as it used to be). Example: \"The weakened West is once again impotent to act in front of the will Russia.\" (c) The West is overreacting: Statements that claim that the West and its institutions are reacting to Russia's actions in a disproportionate manner. Example: \"Putin did not invade the EU but Ukraine. Imposing harsh sanctions is not the way to deal with it, dialogue and debate is.\" (d) The West does not care about Ukraine, only about its interests: Statements that claim that the West is only interested in Ukraine for its own benefits, disregard-ing the country's fate. Example: \"The West has indebted Ukraine more than XX bln of dollars, a lucrative deal for western companies to exploit.\" Example: \"NATO's actions are endangering global security.\" (e) Diplomacy does/will not work: Statements discrediting the potential of ongoing or potential diplomatic efforts. Example: \"Diplomats are desperately trying to figure out solutions but now it's too late, they have failed and Russia is free to do whatever.\" (f) West is tired of Ukraine: Claims that Western countries, particularly the United States and European nations, are becoming fatigued or disinterested in supporting Ukraine and its efforts. 8. Negative Consequences for the West: Statements that highlight or predict adverse outcomes for Western countries and their interests. Example: \"Sanctions against Russia will backfire on Europe.\" Example: \"The West is headed for an economic downturn.\" Sub-Narratives: (a) Sanctions imposed by Western countries will backfire: Statements that catastrophize on the possible negative effects for Western sanctions of Russia. Example: \"The winter is going to be cold and with current gas prices, we are talking of societal unrest.\" (b) The conflict will increase the Ukrainian refugee flows to Europe: Statements that catastrophize on the possible refugee outflows due to the conflict. Example: \"Like we did not have refugees from the Middle East, now we will have Ukrainians stressing our housing and healthcare problems.\" 9. Distrust towards Media: Statements that question the reliability or integrity of media organizations. Example: \"Western media is spreading propaganda.\" Example: \"You can't trust what the news says about Russia.\" Sub-Narratives: (a) Western media is an instrument of propaganda: Statements that discredit the media institutions of the West and claim that they are instruments of propaganda. Example: \"... but you wouldn't hear this on a western channel, only the party line from State Department.\" (b) Ukrainian media cannot be trusted: Statements that discredit the media institutions of the Ukraine and claim that they should not be trusted for reporting on the war. Example: \"Ukraine is conducting its own propaganda using their TV channels, news and social media.\" 10. Amplifying war-related fears: Statements that evoke fear or anxiety about potential threats, dangers or reactions. Example: \"The West is pushing us towards World War III.\" Example: \"It is a matter of time before war speads on the West\" Example: \"Nuclear war is imminent\" Sub-Narratives: (a) By continuing the war we risk WWIII: Statements that warn against upsetting Russia's and its leadership, evoking fear of causing WW3. Example: \"The Western elites with their fixation on Russia are sleapwalking towards WW3\" (b) Russia will also attack other countries: Statements that claim that it is imminent that Russia will attack other countries. Example: \"... and be sure, Ukraine is the first not the last country to be invaded. Others will follow.\" (c) There is a real possibility that nuclear weapons will be employed: Statements that evoke fear or anxiety about the use of nuclear weapons. Example: \"... and if Western hypocrisy continues to provoke, Putin might be forced to press the red button. . . for good\" (d) NATO should/will directly intervene: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) ought to or will take direct military action in a conflict, potentially implying a shift in policy or strategy. 11. Hidden plots by secret schemes of powerful groups: Statements that suggest hidden plots or secretive actions by powerful groups related to the war. Example: \"There's a secret plan by the elites to control global resources.\" Example: \"The war is just a cover for something much bigger.\" A.2 Climate Change (CC) 1. Criticism of climate policies: Statements that question the effectiveness, economic impact, or motives behind climate policies.",
          "(a) Climate policies are ineffective: Statements suggesting that climate policies fail to achieve their intended environmental goals. (b) Climate policies have negative impact on the economy: Statements claiming that climate policies lead to negative economic outcomes. (c) Climate policies are only for profit: Statements that argue climate policies are driven by financial or corporate gain rather than genuine environmental concerns.",
          "Statements that challenge the competence, integrity, or intentions of various institutions and authorities in relation to climate change. Sub-Narratives: (c) Temperature increase does not have significant impact: Statements claiming that the increase in temperature is not going to have any noticeable effect in nature. (d) CO2 concentrations are too small to have an impact: Statements claiming that the concentrations of CO2 will have a negligible effect. (e) Human activities do not impact climate change: Statements that support that climate change is not caused by human activity. (f) Ice is not melting: Statements claiming that there is not melting of ice. (g) Sea levels are not rising: Statements denying that sea levels have risen (or will rise). (h) Humans and nature will adapt to the changes: Statements claiming that whatever the changes in climate humans or nature will manage to find solutions to adapt. 5. Questioning the measurements and science: Statements that raise doubts about the scientific methods, data, and consensus on climate change. Sub-Narratives: (a) Methodologies/metrics used are unreliable/faulty: Statements claiming that the scientific methodologies and metrics used to measure climate change are flawed or unreliable. (b) Data shows no temperature increase: Statements asserting that available data does not support the claim of global temperature increase. (c) Greenhouse effect/carbon dioxide do not drive climate change: Statements asserting that available data does not support the claim of global temperature increase. (d) Scientific community is unreliable: Statements discrediting scientists, the scientific community and their actions. 6. Criticism of climate movement: Statements that challenge the motives, integrity, or impact of the climate movement. Sub-Narratives: (a) Climate movement is alarmist: Statements suggesting that the climate movement exaggerates the severity of climate change for dramatic effect. (b) Climate movement is corrupt: Statements alleging that the climate movement is influenced by ulterior motives, by corruption or by unethical practices. (c) Ad hominem attacks on key activists: Statements attacking the reputation of key figures (such as scientists, activists, politicians or public figures). 7. Controversy about green technologies: Statements that express skepticism or criticism of environmentally friendly technologies. Sub-Narratives: (a) Renewable energy is dangerous: Statements claiming that renewable energy sources pose significant risks or dangers. (b) Renewable energy is unreliable: Statements asserting that renewable energy sources are not dependable for widespread adoption. (c) Renewable energy is costly: Statements asserting that renewable energy sources are too expensive, inefficient and worth adopting for widespread use. (d) Nuclear energy is not climate friendly: Statements asserting that nuclear sources are or should not be considered as good for the climate. Statements that minimize the urgency of addressing climate change by suggesting that any action taken at this point is futile or too late to make a meaningful impact. 10. Green policies are geopolitical instruments: Statements claiming that that environmental policies and initiatives are used as tools for geopolitical power and influence rather than genuine environmental concern.",
          "(a) Climate-related international relations are abusive/exploitative: Statements criticizing international relations related to climate change as exploitative or economically abusive. (b) Green activities are a form of neocolonialism: Statements suggesting that green initiatives are a way for developed countries to exert control and influence over developing nations, a modern form of colonial practices.",
          "Figures 4 and 5 report the normalized count of coarse labels per language. There are substantial differences, notably between RU and other languages for URW and between EN and other languages for CC. These differences could be due either to bias in media of each country, or be due to bias in the sampling of the articles to annotate. The reason RU is absent for CC is that it was not possible to find enough articles to annotate. In Table 5, we report on the difficulty of annotating labels. We report the proportion of each label to have a disagreement between annotators both with the Other label specifically, or with any other label. Based on the numbers, we categorise each label in a class of difficulty.",
          "In this Section we provide the different prompts used in the experiments. C.1 0shot (only taxonomy) You are an experienced analyst making labeling articles with labels specific to the war in Ukraine. Labels: {codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the above list. The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst labeling articles with labels specific [\"to the war in Ukraine\"/ \"Climate Change\"] You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below. ",
          "",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You are given the coarse label and you are asked to return the fine label. You will apply a label if the article presents at any point a narrative that is given in the annotation guidelines given below.  D Selected model performance per label and language We provide detailed breakdown of the performance of our top-performing model on fine-grained level (sub-Narratives). We provide both the detailed scores per-label (Figures 6, 7) and per-language (Figures 8,9,10,11) for the two domains (CC and URW). ",
          "We report on the detailed dissagreement statistics of the annotators per each fine-grained label, and highlight the difficulty of each label, in Table 5.    label count %conf %other mcl difficulty 1: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor 169 0.309 0.315 5 Medium 2: URW: Blaming the war on others rather than the invader: The West are the aggressors 260 0.377 0.390 7 Medium 3: URW: Discrediting Ukraine: Rewriting Ukraine's history 23 0.272 0.111 28 Medium * 4: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society 38 0.380 0.053 6 Medium * 5: URW: Discrediting Ukraine: Discrediting Ukrainian military 316 0.314 0.217 1 Medium * 6: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies 502 0.215 0.253 8 Easy 7: URW: Discrediting Ukraine: Ukraine is a puppet of the West 219 0.235 0.165 28 Easy * 8: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities 127 0.368 0.243 6 Medium * 9: URW: Discrediting Ukraine: Ukraine is associated with nazism 97 0.189 0.080 6 Easy * 10: URW: Discrediting Ukraine: Situation in Ukraine is hopeless 107 0.303 0.220 6 Medium * 11: URW: Russia is the Victim: The West is russophobic 167 0.343 0.427 2 Medium 12: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence 130 0.518 0.273 1 Hard * 13: URW: Russia is the Victim: UA is anti-RU extremists 34 0.477 0.406 11 Hard * 14: URW: Praise of Russia: Praise of Russian military might 466 0.140 0.346 5 Easy 15: URW: Praise of Russia: Praise of Russian President Vladimir Putin 100 0.157 0.348 17 Easy 16: URW: Praise of Russia: Russia is a guarantor of peace and prosperity 257 0.352 0.385 1 Medium 17: URW: Praise of Russia: Russia has international support from a number of countries and people 228 0.160 0.245 16 Easy 18: URW: Praise of Russia: Russian invasion has strong national support 20 0.333 0.250 17 Medium * 19: URW: Overpraising the West: NATO will destroy Russia 11 0.263 0.400 20 Medium 20: URW: Overpraising the West: The West belongs in the right side of history 36 0.263 0.133 28 Medium * 21: URW: Overpraising the West: The West has the strongest international support 24 0.254 0.308 7 Medium 22: URW: Speculating war outcomes: Russian army is collapsing 51 0.313 0.476 23 Medium 23: URW: Speculating war outcomes: Russian army will lose all the occupied territories 10 0.312 0.200 22 Medium * 24: URW: Speculating war outcomes: Ukrainian army is collapsing 93 0.242 0.118 5 Easy * 25: URW: Discrediting the West, Diplomacy: The EU is divided 105 0.164 0.435 28 Easy 26: URW: Discrediting the West, Diplomacy: The West is weak 145 0.301 0.267 14 Medium * 27: URW: Discrediting the West, Diplomacy: The West is overreacting 34 0.395 0.294 11 Medium * 28: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests 211 0.381 0.262 7 Medium * 29: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work 109 0.550 0.434 28 Hard * 30: URW: Discrediting the West, Diplomacy: West is tired of Ukraine 43 0.583 0.171 28 Hard * 31: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire 79 0.338 0.723 12 Medium 32: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe 12 0.000 NaN Easiest 33: URW: Distrust towards Media: Western media is an instrument of propaganda 104 0.180 0.452 11 Easy 34: URW: Distrust towards Media: Ukrainian media cannot be trusted 23 0.500 NaN 6 Hard 35: URW: Amplifying war-related fears: By continuing the war we risk WWIII 135 0.285 0.339 37 Medium 36: URW: Amplifying war-related fears: Russia will also attack other countries 133 0.211 0.270 2 Easy 37: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed 292 0.259 0.486 2 Medium 38: URW: Amplifying war-related fears: NATO should/will directly intervene 63 0.305 0.273 36 Medium * 39: CC: Criticism of climate policies: Climate policies are ineffective 74 0.407 0.174 44 Hard * 40: CC: Criticism of climate policies: Climate policies have negative impact on the economy 96 0.337 0.333 45 Medium * 41: CC: Criticism of climate policies: Climate policies are only for profit 58 0.508 0.069 68 Hard * 42: CC: Criticism of institutions and authorities: Criticism of the EU 54 0.275 NaN 45 Medium 43: CC: Criticism of institutions and authorities: Criticism of international entities 104 0.434 0.196 44 Hard * 44: CC: Criticism of institutions and authorities: Criticism of national governments 225 0.375 0.274 45 Medium * 45: CC: Criticism of institutions and authorities: Criticism of political organizations and figures 190 0.487 0.172 44 Hard * 46: CC: Climate change is beneficial: CO2 is beneficial 19 0.071 NaN 51 Easiest 47: CC: Climate change is beneficial: Temperature increase is beneficial 13 0.266 0.500 54 Medium 48: CC: Downplaying climate change: Climate cycles are natural 36 0.294 0.300 50 Medium 49: CC: Downplaying climate change: Weather suggests the trend is global cooling 13 0.406 0.308 70 Hard * 50: CC: Downplaying climate change: Temperature increase does not have significant impact 5 0.736 0.286 56 Hardest * 51: CC: Downplaying climate change: CO2 concentrations are too small to have an impact 18 0.500 0.143 59 Hard * 52: CC: Downplaying climate change: Human activities do not impact climate change 34 0.387 0.083 60 Medium * 53: CC: Downplaying climate change: Ice is not melting 18 0.117 NaN 62 Easy 54: CC: Downplaying climate change: Sea levels are not rising 2 0.500 0.500 47 Hard 55: CC: Downplaying climate change: Humans and nature will adapt to the changes 5 1.000 0.400 50 Hardest * 56: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty 51 0.333 0.118 59 Medium * 57: CC: Questioning the measurements and science: Data shows no temperature increase 10 0.562 0.333 56 Hard * 58: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change 5 0.750 1.000 Hardest 59: CC: Questioning the measurements and science: Scientific community is unreliable 45 0.540 0.185 56 Hard * 60: CC: Criticism of climate movement: Climate movement is alarmist 67 0.619 0.386 61 Hardest * 61: CC: Criticism of climate movement: Climate movement is corrupt 35 0.869 0.463 60 Hardest * 62: CC: Criticism of climate movement: Ad hominem attacks on key activists 78 0.234 0.217 60 Easy * 63: CC: Controversy about green technologies: Renewable energy is dangerous 18 0.421 0.250 65 Hard * 64: CC: Controversy about green technologies: Renewable energy is unreliable 42 0.344 0.333 45 Medium * 65: CC: Controversy about green technologies: Renewable energy is costly 27 0.512 0.100 45 Hard * 66: CC: Controversy about green technologies: Nuclear energy is not climate friendly 3 1.000 0.750 45 Hardest * 67: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites 47 0.438 0.217 68 Hard * 68: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives 74 0.548 0.094 45 Hard * 69: CC: Amplifying Climate Fears: Earth will be uninhabitable soon 51 0.432 0.188 70 Hard * 70: CC: Amplifying Climate Fears: Amplifying existing fears of global warming 864 0.124 0.477 71 Easy 71: CC: Amplifying Climate Fears: Doomsday scenarios for humans 104 0.435 0.239 70 Hard * 72: CC: Amplifying Climate Fears: Whatever we do it is already too late 18 0.264 0.111 70 Medium * 73: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative 7 0.785 0.273 68 Hardest * 74: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism 10 0.375 0.333 68 Medium * Table 5: Label difficulty, %conf is the percentage of annotation with that label that results in an inconsistency, while %other is the percentage that resulted in an inconsistency with label other. \"mcl\" is the most common label with which a specific label is confused. \"difficulty\" make assess the difficulty of the label based on %conf: Easiest (≤ .1), Easy (≤ .25), Medium (≤ .4), and Hard (≤ .4) or Hardest (> 0.6), a star indicate that there is more confusion within the labels of the taxonomy than with the Other class, meaning that it is extra difficult for the annotator"
        ],
        "ground_truth_definitions": {
          "narrative in the context of news articles": {
            "definition": "descriptions of spatiotemporal events and capturing narrative components in the form of semantic relationships between entities.",
            "context": "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity.",
            "type": "implicit"
          },
          "Opinion Mining": {
            "definition": "task to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity.",
            "context": "This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes.",
            "type": "explicit"
          },
          "sub-narrative": {
            "definition": "lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative.",
            "context": "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, “CO2 is beneficial” is a sub-Narrative of the “Climate Change is beneficial” Narrative of the Climate Change (CC) taxonomy.",
            "type": "implicit"
          },
          "Narratives": {
            "definition": "overt or implicit claims, recurring across articles and languages, promoting a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinformation.",
            "context": "We present PolyNarrative, a new multilingual dataset of news articles, annotated for narra- tives. Narratives are overt or implicit claims, recurring across articles and languages, promot- ing a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinfor- mation. We developed two-level taxonomies with coarse- and fine-grained narrative labels for two domains: (i) climate change and (ii) the military conflict between Ukraine and Russia.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "frai-06-1225093",
        "sections": [
          "Similarly, Visual Question Answering (VQA) systems are known to learn heavy language priors (Agrawal et al., 2016). Thus, a lack of transparency can affect the decision-making process and may lead to the erosion of trust between humans and Artificial Intelligence (AI) systems. This can further jeopardize users' safety, ethics, and accountability if such a system is deployed publicly (Madsen et al., 2021). Considering the utilization of NLP in healthcare, finance, and law domains, all of which can directly affect human lives, it can be dangerous to blindly follow machine predictions without fully understanding them. For instance, a physician following a medical recommendation or an operation procedure for a patient without full knowledge of the system can do more harm than good. In addition, systems employing Machine Learning (ML), such as most current NLP methods, are prone to adversarial attacks where small, carefully crafted local perturbations can maximally alter model predictions, essentially misguiding the model to predict incorrectly but with high confidence (Finlayson et al., 2019). The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model's decision-making process of arriving at a prediction and the ability to be verified and audited. Therefore, models with explainability are interpretable and complete. In this survey, the focus is on explainability and mainly on the outcome explanation problem where Guidotti et al. (2018) describe explainability as \"the perspective of an end-user whose goal is to understand how a model arrives at its result\". In NLP, there exist various explainable techniques such as LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016), Integrated Gradients (Sundararajan et al., 2017), and ",
          "One of the emerging explainable techniques for NLP applications is rationalization (Atanasova et al., 2020). Rationalization provides explanations in natural language to justify a model's prediction. These explanations are rationales, which present the input features influencing the model's prediction. The reasoning behind the prediction could be understood simply by reading the explanation/rationale, thereby revealing the model's decision-making process. Rationalization can be an attractive technique because it is human-comprehensible and allows individuals without domain knowledge to understand how a model arrived at a prediction. It essentially allows the model to \"talk for themselves\" (Bastings et al., 2019;Luo et al., 2021). This technique is a part of a subset of explainability because it enables models to be interpretable and complete, as shown in Figure 1. Specifically, rationalization provides a local explanation since each prediction has a unique explanation rather than one for the entire model. Local explanations can be categorized into two groups: local post-hoc and local self-explaining. Danilevsky et al. (2020) present local post-hoc methods as explaining a single prediction after the model predicts and local self-explaining methods as simultaneously explaining and predicting. Rationalization in NLP was first introduced in 2007 (Zaidan et al., 2007). As described in Section 4.6, the objective was to use annotator rationales to improve task performance for text categorization. Interestingly, explainability was not the core objective. However, explainability is an advantage of rationalization because it makes the model inherently explainable even if used in the context of task improvement (Gurrapu et al., 2022). Our literature review found that rationalization can be further divided into two major groups: abstractive and extractive (El-Kassas et al., 2021). In extractive rationalization, important features or sentences from the input data are extracted as rationales to support the prediction. In contrast, abstractive rationalization is a generative task in which novel sentences are ). (B) An example of the Abstractive Rationalization approach (Rajani et al., ). generated using new words or paraphrasing existing sentences. This is typically accomplished through the use of a language model such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), T5 (Raffel et al., 2020), or GPT (Generative Pre-trained Transformer) (Radford and Narasimhan, 2018). Figure 2 demonstrate the usage of the two explanation types with examples. Recent advances in explainable NLP have led to a significant increase in rationalization research. Further, at present, the field of rationalization is disorganized. Thus, the motivations for this survey are-(a) formally define rationalization, (b) present and categorize the well-cited techniques based on NLP tasks, and (c) discuss current trends and future insights on the field. Thus, our primary goal is to provide future researchers with a comprehensive understanding of the previously scattered state of rationalization. The key contributions of this paper are as follows. 1. First literature to survey the field of rationalization in NLP.",
          "Explainable AI (XAI). 3. A comprehensive list of details on available rationalization models, XAI evaluations, datasets, and code are provided to guide future researchers. 4. Presents NLP Assurance as an important method for developing more trustworthy and reliable NLP systems.",
          "Table 1 indicates the related survey papers published in recent years. Danilevsky et al. (2020) note that previous surveys in XAI are broadly focused on AI without a specific narrow domain focus. Their work primarily focuses on surrogate-based explainability methods. NLP publications in recent years further demonstrate that this distinction is less relevant and valuable in the NLP domain because \"the same neural network can be used not only to make predictions but also to derive explanations.\" Therefore, surveying the field of Explainable AI (XAI) in NLP requires NLP-specific methods that are different from the standard XAI methods that are widely known. Thayaparan et al. (2020) survey the use of explanations specifically in Machine Reading Comprehension (MRC). The authors describe MRC papers that support explanations and provide a detailed overview of available benchmarks. Further, Madsen et al. (2021) briefly discuss rationalization and natural language explanations using a question and answer approach, CAGE (Commonsense Auto-Generated Explanations) (Rajani et al., 2019) as an example. Thus, this raises the questionhow can rationalization be generalized and applied to other tasks in the NLP domain? However, until now, no comprehensive literature review on rationalization has been available for the prominent NLP tasks. Thus, through this survey paper, we attempt to address this need.",
          "To provide clarity and distinguish terms that are typically used interchangeably in published literature, a list of definitions is provided in Table 2. These terms are used throughout the paper.",
          "The following are the inclusion-exclusion criteria for our publications collection methodology. The first known use of rationalization in NLP was in the year 2007. Our survey focuses on the domain of NLP from 2007 to early 2022. We have included peer-reviewed publications within this range that include a significant rationalization component as a method to provide explainability. We defined significance as rationalization being the main component of their research methodology and approach. We have eliminated a number of publications that are either not entirely in the NLP domain or do not contain a significant rationalization component. For identifying and selecting articles, the range of keywords and topics was limited to the following in the NLP domain: rationalization, explanation, justification, and explainable NLP. Thus, this survey includes reviews of the articles from journals, books, industry research, dissertations, and conference proceedings from commonplace AI/NLP venues such as ACL, EMNLP, NAACL, AAAI, NeurIPS, and others. Finally, these articles are categorized by important NLP tasks, as shown in Table 3. In recent years, there has been an increase in focus on explainability in NLP after a rise in deep learning techniques (Danilevsky et al., 2020). Due to this, a majority of the papers collected were from recent years (2016 and onwards), as illustrated in Figure 3. The availability of relevant articles was limited. After following the above approach, 33 articles were downselected to be the primary focus of this paper's discussion. Instead of providing a broad yet surface-level understanding of the work, we focus on demonstrating in-depth the most important approaches and TABLE Three related survey papers on explainability and interpretability in NLP. No. Survey title Year Papers reviewed 1 A Survey on the state of Explainable AI for Natural Language Processing (Danilevsky et al., 2020) 2020 50 2 A survey on explainability in machine reading comprehension (Thayaparan et al., 2020) 2020 69 3 Post-hoc interpretability for neural NLP: a survey (Madsen et al., 2021) 2021 27 TABLE List of common terms that are used interchangeably in published literature.",
          "",
          "A \"machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans\" (Guidotti et al., 2018).",
          "Interpretability \"aims at developing tools to understand and investigate the behavior of an AI system\" (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
          "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
          "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model's output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
          "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain. progress made in each NLP task. Overall, we selected six articles in multiple NLP domains, five on Machine Reading Comprehension and Sentiment Analysis, four on Text Classification, Fact-Checking and Commonsense Reasoning, and three on Natural Languages Inference, and two articles on Neural Machine Translation (NMT). .",
          "In this section, we discuss relevant papers and their rationalization techniques categorized by the NLP tasks listed in Table 3. Tables with important information on the papers for each subsection are presented at the beginning. TABLE Details on seven NLP tasks and their definitions which are surveyed in this paper.",
          "",
          "Enabling a model to answer questions regarding a given context (Baradaran et al., 2022).",
          "Going beyond pattern recognition to make inferences using world knowledge (Apperly, 2011;Sap et al., 2020).",
          "Determining if a hypothesis entails or contradicts a premise (MacCartney, 2009).",
          "Classifying if a claim is either true or false based on evidence (Vargo et al., 2018;Gurrapu et al., 2022).",
          "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
          "Categorizing textual data by automatically assigning labels (Minaee et al., 2021).",
          "Translating languages using deep neural networks (Sutskever et al., 2014).",
          "MRC enables a model to answer questions regarding a given context (Baradaran et al., 2022). For this reason, it also frequently referred to as Question Answering (QA) Systems. As shown in Table 4, for MRC applications, we found five recent articles from which three articles provide novel datasets (Ling et al., 2017;Mihaylov et al., 2018;Xie et al., 2020) and the remaining articles Sharp et al. (2017) and Lakhotia et al. (2021) each propose a new MRC framework. The first article, published in 2018, presented a new questionanswering dataset based on the open book exam environment for elementary-level science-OpenBookQA (Mihaylov et al., 2018). This dataset consists of two components-(i) Questions (Q): a set of 5,958 multiple choice questions and (ii) Facts (F): a set of 1,326 diverse facts about elementary level science. This dataset was further tested for evaluating the performance of existing QA systems and then compared with the human performance. The results indicated that human performance was close to 92%, but many existing QA systems showed poor performance close to the random guessing baseline of 25%. Additionally, the authors found that simple neural networks achieved an accuracy of about 50%, but it is still not close to the human performance, about 92%. Recently an extension of the WorldTree project (Jansen, 2018), i.e., WorldTree V2 (Xie et al., 2020), is presented. The main goal of this project is to generate a science domain explanation with a supporting semistructured knowledge base. The WorldTree project is a part of TABLE Selected Machine Reading Comprehension papers. References Name Year Explanation Models XAI Metric Dataset Code Sharp et al. (2017) -2017 Extractive TF-IDF, FFNN -AI2 science, Aristo Mini -Ling et al. (2017) -2017 Extractive LSTM, Seq2Seq -AQuA Mihaylov et al. (2018) OpenBookQA 2018 Abstractive BiLSTM Max-out -OpenBookQA, Xie et al. (2020) WorldTree V2 2018 Abstractive TF-IDF, BERT -WorldTree V2 Lakhotia et al. (2021) FiD-Ex 2021 Extractive T5, BERT-to-BERT -Natural Questions - explainable question answering tasks that provide answers to natural language questions and their human-readable explanations for why the answers are correct. Xie et al. (2020) notes that most multi-hop inference models could not demonstrate combining more than two or three facts to perform inference. However, here the authors merge, on average, six facts from a semi-structured knowledge base of 9216 facts. Thus, this resulted in the WorldTree V2 corpus for standardized science questions. This corpus consists of 5100 detailed explanations to support training and instrumenting multi-hop inference question answering systems. Lakhotia et al. (2021) demonstrates a new MRC framework called FiD-Ex (Extractive Fusion-in-Decoder). It has been noted that seq2seq (Sequence to Sequence) models work well at generating explanations and predictions together. However, these models require a large-labeled dataset for training and bring a host of challenges such as fabricating explanations for incorrect predictions, the difficulty of adapting to long input documents, etc. Thus, to tackle these challenges, the Fid-Ex framework includes sentence markers to encourage extractive explanations and intermediate fine-tuning for improving fewshot performance on open-domain QA datasets. This new framework is tested on ERASER (Evaluating Rationales And Simple English Reasoning) datasets and their benchmarks for evaluations (DeYoung et al., 2020). This experiment concludes that FiD-Ex significantly improves upon prior work on the explanation metrics and task accuracy on supervised and few-shot settings. Sharp et al. (2017) proposes a new neural network architecture that re-ranks answer justifications as an intermediate step in answer selection. This new approach alternates between a max pooling layer and a shallow neural network (with ten nodes, glorot uniform initializations, tanh activation, and L2 regularization of 0.1) for providing a justification. This approach contains three components: 1) retrieval component, which retrieves a pool of candidates' answer justification, 2) extractor, which extracts the features and 3) scores, which perform the scoring of the answer candidate based on the pool of justifications. The authors used 8th-grade science questions provided by Allen Institute for Artificial Intelligence (AI2) for evaluations. The training set includes 2,500 questions with four options, and the test set consists of 800 publicly released questions. Further, a pool of candidate justifications corpora containing 700k sentences from StudyStack and 25k sentences from Quizlet is used. The top 50 sentences were retrieved as a set of candidate justification. For model tuning, the authors used five-fold cross-validation, and during testing, the model architecture and hyperparameters were frozen. The authors compared results using two baselines: IR baseline and IR++. They concluded that this new approach showed better accuracy and justification quality while maintaining near state-of-the-art performance for the answer selection task. Ling et al. (2017) presented a dataset and an approach that provides answer rationales, sequences of natural language, and human-readable mathematical expressions for solving algebraic word problems. The authors proposed a sequence-to-sequence model which generates a sequence of instructions and provides the rationales after selecting the answer. For this purpose, a twolayer LSTM (Long Short-Term Memory) with a hidden size of 200 and word embedding with a size of 200 is utilized. Further, the authors also built a dataset containing 100,000 problems in which each question is decomposed into four parts -two inputs and two outputs. This new dataset is used for generating rationales for math problems and for understanding the quality of rationales as well as the ability to obtain a correct answer. Further, the authors used an attention-based sequence to sequence model as a baseline and compared results based on average sentence level perplexity and BLEU-4 (Bilingual Evaluation Understudy). The authors noted that this new approach could outperform the existing neural models in the ability to solve problems and the fluency of the generated rationales.",
          "Commonsense knowledge helps humans navigate everyday situations. Similarly, commonsense reasoning in NLP is the ability for a model to go beyond pattern recognition and use world knowledge to make inferences (Apperly, 2011;Sap et al., 2020). As shown in Table 5 on commonsense reasoning, we found four articles, and all of them provide unique solutions that contribute to the development of commonsense reasoning frameworks. Sap et al. (2020) demonstrates a solution for commonsense reasoning using LSTM encoder and decoder. The main goal was to convert the actions of an autonomous agent into natural language using neural machine translation. For this purpose, the authors built a corpus of thoughts of people as they complete tasks in the Frogger game which are then stored as states and actions. In the next step, LSTM encoder and decoder are used to translate actions as well as states into natural language. Lastly, the authors used the BLEU score to calculate sentence similarity and assessed the accuracy for selecting the best rationale. The authors also conducted a survey to evaluate the rationales based on human satisfaction. The Frogger experiment is concluded with Encoder-Decoder framework outperforming the baselines and demonstrates that the use of game theory approaches for generating rationales is a promising technique (Chang et al., 2019(Chang et al., , 2020;;Yu et al., 2019;Li et al., 2022). Further, it is noted that deep learning model performance is poor when used in tasks that require commonsense reasoning due to limitations with available datasets. To tackle this problem, Rajani et al. (2019) developed the Commonsense Auto-Generated Explanations (CAGE) framework for generating explanations for Commonsense Question Answering (CQA). The authors also created a new dataset-Common Sense Explanations (CoS-E)by collecting human explanations for commonsense reasoning and highlighting annotations. From this paper, the authors concluded that CAGE could be effectively used with pre-trained language models to increase commonsense reasoning performance. Recently, Majumder et al. (2021) and Tang et al. ( 2021) presented novel solutions for commonsense reasoning. Majumder et al. (2021) focused on the Natural Language Expiations (NLEs), which are more detailed than Extractive rationales but fall short in terms of commonsense knowledge. In this solution, the authors proposed a self-rationalizing framework RExC (Rationales, Explanations, and Commonsense). RExC first extracts rationales that act as features for the prediction then expands the extractive rationales using commonsense resources. In the last step, RExC selects the best suitable commonsense knowledge for generating NLEs and a final prediction. The authors tested RExC on five tasksthree natural language understanding tasks and two vision language understanding tasks. Overall, the results indicated improvement in the quality of extractive rationales and NLEs that bridges the gap between task performance and explainability. On the other hand, Tang et al. (2021) focused on Visual Commonsense Reasoning (VCR). They focused on a problem when a question with a corresponding input image is given to the system, and it attempts to predict an answer with a rationale statement as the justification. To explore this, author presented a multi-model approach by combining Computer Vision (CV) and NLP. Their approach leverages BERT and ResNet50 (Residual neural network) as the feature representation layer and BiLSTM (Bidirectional LSTM) and Attention for the multimodal feature fusion layer. These layers are then concatenated into an LSTM network for the encoder layer before passing into the classifier for the prediction layer. This was tested on the benchmark VCR dataset and it indicated significant improvements over existing methods and it also provided a more interpretable intuition into visual commonsense reasoning.",
          "Natural Language Inference (NLI) task helps with identifying a natural language hypothesis from a natural language premise (MacCartney, 2009). For this application, as shown in Table 6, we found three articles. The first article presents a new dataset-e-SNL (explanation-augmented Stanford Natural Language Inference) (Camburu et al., 2018)-and the other two articles discuss approaches that can improve NLI. Camburu et al. (2018) extended the Stanford NLI (SNLI) (Bowman et al., 2015a) dataset by providing human-annotated explanations for the entailment relations. This new dataset-e-SNLI-is used in a series of classification experiments involving LSTM-based networks for understanding its usefulness for providing human-interpretable full-sentence explanations. The authors also evaluated these explanations as an additional training signal for improving sentence representation and transfer capabilities of out-of-domain NLI datasets. Thus, from these experiments, the authors conclude that e-SNLI can be used for various goals mentioned above and also be utilized for improving models as well as asserting their trust. Another issue with NLI is the faithfulness of the generated explanations, tackled by Kumar and Talukdar (2020) and Wiegreffe et al. (2021). Kumar and Talukdar (2020) mentioned that existing methods do not provide a solution for understanding correlations of the explanations with the model's decision-making and this can affect the faithfulness of the generated explanations. Considering this problem, the authors proposed and presented a new framework -NILE (Natural language Inference over Labelspecific Explanations). The NILE framework can generate natural language explanations for each possible decision and process these TABLE Selected commonsense reasoning papers. References Name Year Explanation Models XAI metric Dataset Code Ehsan et al. (2018) -2018 Extractive LSTM, Seq2Seq ---Rajani et al. (2019) CAGE 2019 Abstractive GPT, BERT -CoS-E, CommonsenseQA Majumder et al. (2021) RExC 2021 Extractive Transformer -ComVE, e-SNLI, COSe, e-SNLI-VE, VCR -Tang et al. (2021) DMVCR 2021 Extractive LSTM, BERT -VCR TABLE Selected Natural Language Inference papers. References Name Year Explanation Models XAI metric Dataset Code Camburu et al. (2018) e-SNLI 2018 Abstractive BiLSTM, Seq2Seq -e-SNLI Kumar and Talukdar (2020) NILE 2020 Abstractive GPT-2, RoBERTa -e-SNLI Wiegreffe et al. (2021) -2020 Abstractive T5 -CoS-E, SNLI explanations to produce a final decision for the classification problems. To test this approach, the authors used two datasets -SNLI and e-SNLI -and compared NILE with baseline and other existing approaches based on explanation accuracy, in-domain evolution sets (SNLI), and on out-of-domain examples (train on SNLI and test on MNLI) (Williams et al., 2018). Based on the first 100 SNLI test samples, the results indicated that NILE variants are comparable with the ETPA (Explain Then Predict Attention) baseline, and NILE explanations generalize significantly better on out-of-domain examples. For out-of-domain examples (MNLI ), results showed that the percentage of correct explanations in the subset of correct label predictions was significantly better for all the NILE variants. Thus, the authors concluded that NILE is an effective approach for accurately providing both labels and explanations. Further, Kumar and Talukdar (2020) also focused on the need for faithfulness for denoting the model's decision-making process by investigating abstractive rationales. The author proposed two measurements -robustness equivalence and feature importance agreement -to investigate the association of the labels and predicted rationales, which are required for a faithful explanation. This investigation was performed on CommonsenseQA (Talmor et al., 2019) and SNLI dataset using T5based models (Narang et al., 2020). The results indicated that stateof-the-art T5-based join models demonstrate desirable properties and potential for producing faithful abstractive rationales. .",
          "Fact-checking has become a popular application of NLP in recent years given its impact on assisting with misinformation and a majority of the work has been with claim verification (Vargo et al., 2018;Gurrapu et al., 2022). Based on a paper published in 2016, there are 113 active fact-checking groups and 90 of which were established after 2010 (Graves and Cherubini, 2016). This indicates the growth of the fact-checking application. Considering the scope of this literature review, as shown in Table 7, we found four articles on fact-checking. Two of the studies in this section present novel datasets, and the remaining two provide new techniques to improve fact-checking. In 2017, a large dataset for the fact-checking community called LIAR (Wang, 2017) was introduced, including POLITIFACT data. Most works on this data were focused on using the claim and its speaker-related metadata to classify whether a verdict is true or false. The evidence-an integral part of any fact-checking processwas not part of the LIAR and was overlooked. Thus, in Alhindi et al. (2018) extended the LIAR dataset to LIAR-plus by including the evidence/justification. The authors treated the justification as a rationale for supporting and explaining the verdict. Further, they used Feature-based Machine Learning models (Logistic Regression and Support Vector Machine) and deep learning models (Bi-Directional Long Short-term Memory (BiLSTM) and Parallel-BiLSTM) for binary classification tasks to test the data. The results demonstrated a significant performance improvement in using the justification in conjunction with the claims and metadata. Further, Hanselowski et al. (2019) introduced a new corpus for training machine learning models for automated fact-checking. This new corpus is based on different sources (blogs, social media, news, etc.) and includes two granularity levels-the sources of the evidence and the stance of the evidence toward the claim-for claim identification. Authors then used this corpus to perform stance detection, evidence extraction and claim validation experiments. In these experiments, a combination of LSTMs, baseline NN, pretrained models have been used, and their results are compared based on precision, recall, and F1 macro. The results indicated that fact-checking using heterogeneous data is challenging to classify claims correctly. Further, the author claims that the fact-checking problem defined by this new corpus is more difficult compared to other datasets and needs more elaborate approaches to achieve higher performance. It has been noted that the fact-checking systems need appropriate explainability for the verdicts they predict. The justifications that are human-written can help to support and provide context for the verdicts, but they are tedious, unscalable, and expensive to produce (Atanasova et al., 2020). Considering this issue, Atanasova et al. (2020) proposed that the creation of the justifications needs to be automated to utilize them in a large-scale fact-checking system. The authors presented a novel method that automatically generates the justification from the claim's context and jointly models with veracity prediction. Further, TABLE Selected fact-checking papers. References Name Year Explanation Models XAI metric Dataset Code Alhindi et al. (2018) LIAR-PLUS 2018 Extractive SVM, BiLSTM -LIAR-PLUS Hanselowski et al. (2019) -2019 Extractive BERT -FEVER Atanasova et al. (2020) -2020 Extractive DistilBERT -LIAR-PLUS -Rana et al. (2022) RERRFACT 2022 Extractive RoBERTa, BioBERT -SCIFACT this new method is then tested on the LIAR dataset (Wang, 2017) for generating veracity explanations. The results indicated that this new method could combine predictions with veracity explanations, and manual evaluations reflected the improvement in the coverage and quality of the explanations. Another important domain in which fact-checking is useful is Science. Researching and providing substantial evidence to support or refute a scientific claim is not a straightforward task. It has been seen that scientific claim verification requires in-depth domain expertise along with tedious manual labor from experts to evaluate the credibility of a scientific claim. Considering this problem, Rana et al. (2022) proposed a new framework called RERRFACT (Reduced Evidence Retrieval Stage Representation) for classifying scientific claims by retrieving relevant abstracts and training a rationaleselection model. RERRFACT includes a two-step stance prediction that differentiates non-relevant rationales then identifies a claim's supporting and refuting rationales. This framework was tested on the SCI-FACT dataset (Wadden et al., 2020) and performed competitively against other language model benchmarks on the dataset leaderboard. .",
          "Sentiment Analysis is a subset of the text classification field (Minaee et al., 2021). It focuses specifically on the \"computational study of people's opinions, sentiments, emotions, appraisals, and attitudes toward entities such as products, services, organizations, individuals, issues, events, topics and their attributes\" (Zhang et al., 2018). The use of rationales to support sentiment analysis models in NLP is widely used compared to other NLP tasks. For this task, as shown in Table 8, we identified five papers in this field. In 2016, Lei et al. (2016) pioneered rationalization in sentiment analysis by proposing a problem: \"prediction without justification has limited applicability\". To make NLP outcomes more transparent, the authors propose an approach to extract input text which serves as justifications or rationales for a prediction. These are fragments from the input text which themselves are sufficient to make the same prediction. Their implementation approach includes a generator and an encoder architecture. The generator determines which can be potential candidates for a rationale from the input text. Those candidates are fed into the encoder to determine the prediction and the rationales are not provided during training. They employ an RCNN (Region-based Convolutional Neural Network) and an LSTM architecture and when compared with each other the RCNN performed better. The experiment was conducted on the BeerAdvocate dataset. The paper's approach outperforms attention-based baseline models. They also demonstrate their approach on a Q&A retrieval task indicating that leveraging rationales for sentiment analysis tasks is very beneficial. Similarly, Du et al. (2019) claim that explainability alone is not sufficient for a DNN (Deep Neural Network) to be viewed as credible unless the explanations align with established domain knowledge. In essence, only the correct evidences are to be used by the networks to justify predictions. In this paper, the authors define credible DNNs as models that provide explanations consistent with established knowledge. Their strategy is to use domain knowledge to improve DNNs credibility. The authors explore a specific type of domain knowledge called a rationale which are the salient features of the data. They propose an approach called CREX (Credible Explanation), which regularizes DNNs to use the appropriate evidence when making a decision for improved credibility and generalization capability. During training, instances are coupled with expert rationales and the DNN model is required to generate local explanations that conform to the rationales. They demonstrate it on three types of DNNs (CNN, LSTM, and self-attention model) and various datasets for testing. Results show that the CREX approach allows DNNs to look at the correct evidences rather than the specific bias in training dataset. Interestingly, they point that incorporating human knowledge does not always improve neural network performance unless the knowledge is very high quality. Many papers published in the rationalization field indicate that a machine learning system learning with human provided explanations or \"rationales\" can improve its predictive accuracy (Zaidan et al., 2007). Strout et al. (2019) claim that this work hasn't been connected to the XAI field where machines attempt to explain their reasoning to humans. The authors attempt to show in their paper that rationales can improve machine explanations as evaluated by human judges. Although automated evaluation works, Strout et al. (2019) believe that since the explanations are for users, therefore humans should directly evaluate them. The experiment is done by using the movie reviews dataset and by having a supervised and an unsupervised CNN model for a text classification task. They use attention mechanism and treat the rationales as supervision in one of the CNN models. Results indicate that a supervised model trained on human rationales outperforms the unsupervised on predictions. The unsupervised is the model where the rationales/explanations are learned without any human annotations. The selective rationalization mechanism is commonly used in complex neural networks which consist of two componentsrationale generator and a predictor. This approach has a problem of model interlocking which arises when the predictor overfits to the features selected by the generator. To tackle this problem this paper proposes a new framework A2R which introduces TABLE Selected sentiment analysis papers. References Name Year Explanation Models XAI metric Dataset Code Lei et al. (2016) -2016 Extractive LSTM, RCNN -BeerAdvocate, AskUbuntu Du et al. (2019) CREX 2019 Extractive CNN, LSTM -BeerAdvocate, MovieReview -Strout et al. (2019) -2019 Extractive RA-CNN, AT-CNN -MovieReview -Yu et al. (2021) A2R 2021 Extractive BiGRU -BeerAdvocate, MovieReview Antognini and Faltings (2021) ConRAT 2021 Extractive CNN, BiGRU -AmazonReviews, BeerAdvocate a third component for soft attention into the architecture (Yu et al., 2021). The authors have used BeerAdvocate and MovieReview for understanding the effectiveness of the framework. The authors compared results from A2R with the original rationalization technique RNP (Rationalizing Neural Predictions) along with 3PLAYER, HARD-KUMA and BERT-RNP. For implementation authors have used bidirectional Gated Recurrent Units (GRU) in the generators and the predictors. Furthermore, they performed two synthetic experiments using BeerAdvocate dataset by deliberately inducing interlocking dynamics and then they performed experiments in real-world setting with BeerAdvocate and MovieReview. From the results they made two conclusions-(1) A2R showed consistent performance compared to other baselines on both the experiments, ( 2) A2R helps to promote trust and interpretable AI. In the future, the authors would like to improve A2R framework for generating casually corrected rationales to overcome the lack of inherent interpretability in the rationalization models. Existing methods in rationalization compute an overall selection of input features without any specificity and this does not provide a complete explanation to support a prediction. Antognini and Faltings (2021) introduce ConRAT (Conceptbased RATionalizer), a self-interpretable model which is inspired by human decision-making where key concepts are focused using the attention mechanism. The authors use the BeerReviews dataset to not only predict the review sentiment but also predict the rationales for key concepts in the review such as Mouthfeel, Aroma, Appearance, Taste, and Overall. ConRAT is divided into three subodels, a Concept Generator which finds the concepts in the review, a Concept Selector that determines the presence or absence of a concept, and a Predictor for final review predictions. ConRAT outperforms state-of-the-art methods while using only the overall sentiment label. However, Antognini et al. (2021) have further demonstrated that attention mechanism usage can contribute to a tradeoff between noisy rationales and a decrease in prediction accuracy.",
          "Text classification, also commonly known as text categorization, is the process of assigning labels or tags to textual data such as sentences, queries, paragraphs, and documents (Minaee et al., 2021). Classifying text and extracting insights can lead to a richer understanding of the data but due to their unstructured nature, it is challenging and tedious. NLP techniques in text classification enable automatic annotation and labeling of data to make it easier to obtain those deeper insights of the data. For this task, as shown in Table 9, we identified four papers in this sfield. Traditionally, rationales provide well-defined kinds of data to nudge the model on why a prediction is the way it is given the data. Moreover, they require little additional effort for annotators and yield a better predictive model. When classifying documents, it is beneficial to obtain sentence-level supervision in addition to document-level supervision when training new classifications systems (Zhang et al., 2016). Previous work relied on linear models such as SVMs (Support Vector Machines), therefore, Zhang et al. (2016) propose a novel CNN model for text classification that exploit associated rationales of documents. Their work claims to be the \"first to incorporate rationales into neural models for text classification\". The authors propose a sentence-level CNN to estimate the probability that a sentence in a given document can be a rationale. They demonstrate that their technique outperforms baselines and CNN variants on five classification datasets. Their experimentation task uses Movie Reviews and the Risk of Bias (RoB) datasets. On the movie review dataset, their technique performs with a 90.43% accuracy with the RA-CNN (Recurrent Attention Convolutional Neural Network) model and similar strong results are also indicated on the RoB datasets. It seems intuitive that more data or information can lead to better decision-making by the neural networks. Zaidan et al. (2007) propose a new framework to improve performance for supervised machine learning by using richer \"kinds\" of data. Their approach is called the \"annotator rationales\" technique and it is to leverage a training dataset with annotated rationales. The rationales highlight the evidence supporting the prediction. Zaidan et al. (2007) test their approach on text categorization tasks, specifically, sentiment classification of movie reviews and they claim that these rationales enable the machine to learn why the prediction is the way it is. Rationales help the model learn the signal from the noise. ML algorithms face the \"credit-assigment problem\" which means that many features in the data (X) could have affected the predicted result (Y). Rationales provide a \"shortcut\" to simplifying this problem since they provide hints on which features of X were important. Zaidan et al. (2007) used a discriminative SVM for experimentation and the results indicate that this technique significantly improves results for the sentiment classification and they hypothesize that leveraging rationales might be more useful than providing more training examples. Recently, rationales have been a popular method in NLP to provide interpretability in the form of extracted subsets of texts. It is common to have spurious patterns and co-varying aspects in the TABLE Selected text classification papers. References Name Year Explanation Models XAI Metric Dataset Code Zaidan et al. (2007) -2007 Extractive SVM -MovieReview -Zhang et al. (2016) -2016 Extractive SVM, RA-CNN -Risk of Bias -Liu et al. (2019b) GEF 2019 Extractive CNN, LSTM -PCMag Reviews, Skytrax User Reviews Plyler et al. (2022) CDA 2021 Extractive RL, RNN -TripAdvisor Reviews, RateBeer dataset due to which rationale selectors do not capture the desired relationship between input text and target labels. Considering this problem this paper proposes CDA (Counterfactual Data Augmentation) framework to aid rational models trained with Maximum Mutual Information (MMI) criteria (Plyler et al., 2022). CDA consists of transforms-for rational and classificationsbecause of their effectiveness over RNNs in NLP. The authors used TripAdvisor.com and RateBeer datasets for testing CDA with three baselines -MMI, FDA (Factual Data Augmentation), and ANT (simple substitution using antonyms). The results of the rational models were compared using precision and the accuracy of the classifier is reported based on the development set. From the results, authors concluded that the models trained using the CDA framework learn higher quality rationales and it doesn't need human intervention. In the future, the authors would like to explore more on counterfactual predictors and on CDA framework that could connect with other rationalization strategies. Similarly, Liu et al. (2019b) proposed a novel Generative Explanation Framework (GEF) for classification problems that can generate fine-grained explanations. The motivation behind this explanation framework is to provide human-readable explanations without ignoring finegrained information such as textual explanations for the label. For understanding the accuracy of explanations, the authors conducted experiments on two datasets-PCMag and Skytrax User Reviewswhich were processed by the Stanford Tokenizer. Further, the authors used Encoder-Predictor architecture in which they used Conditional Variational Autoencoder (CVAR) as a base model for text explanations and Long Short-Term Memory (LSTM) for numerical explanations. The experimental results indicated that after combining base models with GEF the performance of the base model was enhanced along with improving the quality of explanations. Further, the authors also used human evaluation for evaluating the explainability of the generated text explanations. The authors noted that for 57.62% of the tested items GEF provided better or equal explanations compared with the basic model.",
          "With the advent of deep learning, Neural Machine Translation (NMT) became the successor to traditional translation methods such as Rule-based or Phrase-Based Statistical Machine Translation (PBSMT) (Yang et al., 2020). NMT models leverage Deep Neural Networks architecture to train the model end-to-end to improve translation quality and only require a fraction of the storage memory needed by PBSMT models (Sutskever et al., 2014). The use of explanations to support NMT model's prediction is relatively new, however, there has been some pioneering work to provide more explainability. For this task, as shown in Table 10, we identified two relevant papers in this area. Quality Estimation (QE) models perform well at analyzing the overall quality of translated sentences. However, determining translation errors is still a difficult task such as identifying which words are incorrect due to the limited amounts available training data. The authors explore the idea that since QE models depend on translation errors to predict the quality, using explanations or rationales extracted from these models can be used to better detect translation errors (Fomicheva et al., 2021). They propose a novel semi-supervised technique for word-level QE and demonstrate the QE task as a new benchmark for evaluating feature attribution (the interpretability of model explanations to humans). Instead of natural language explanations, their technique employs various feature attribution methods such as LIME, Integrated Gradients, Information Bottleneck, causal, and Attention. It was shown that explanations are useful and help improve model performance and provide better explainability. Deep learning models are black-boxes because they involve a large number of parameters and complex architectures which makes them uninterpretable. Considering this problem and to bring interpretability in deep learning models (Alvarez-Melis and Jaakkola, 2017) propose a model-agnostic method for providing explanations. The explanations provided by this method consist of sets of inputs and output tokens that are causally related in the black-box model. Further, these causal relations are inferred by performing perturbations on the inputs from the black-box models, generating a graph of tokens, and then solving a partitioning problem to select the most relevant components. To test the methods via runtime complexity analysis with respect to the number of LM (Linear Model) forward/backward passes. Finally for performance standard dataset-specific metrics accuracy (for SST and CoS-E) and macro-averaged F1 (for Movies) are used. The results indicated that UNIREX allows effective trade-off between performance, faithfulness, and plausibility to identify better rationale extractions. Further, authors also mention UNIREX trained rationale extractors can generalize to unseen datasets and tasks. With many ML systems demonstrating performance beyond that of human across many applications, the field of XAI is advancing techniques to improve transparency and interpretability. Das and Chernova (2020) explores XAI in the context of a question previously unexplored in ML and XAI communities: \"Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human?\". The authors investigate this question through the game of Chess where computational game engines performance surpass the average player. They present an automated technique for generating rationales for utilitybased computational methods called the Rationale-Generating Algorithm. They evaluate this with a user study against two baselines and their findings show that the machine generated rationales can lead to significant improvement in human task performance. They demonstrate that rationales can not only be used to explain the system's actions but also instruct the user to improve their performance. Sharma et al. (2020) explores an of rationalization in the mental health support field and understanding empathy. Empathy is important for mental health support and with the rise of text-based internet platforms, it becomes crucial to understanding empathy in only communication. The paper presents a computational approach to understanding empathy by developing a corpus of 10,000 pairs of posts and responses with supporting rationales as evidence. They use a multi-task RoBERTabased bi-encoder model to identify empathy in conversations and extract rationales for predictions. Their results demonstrate that their approach can effectively identify empathic conversations. To improve interpretability for NLP tasks, recent rationalization techniques include Explain-then-Predict models. In this technique, an extractive explanation from the input text is generated and then a prediction is generated. However, these models do not use the rationales appropriately and consider the task input as simply a signal to learn and extract rationales. Zhang et al. (2021) propose a novel technique to prevent this problem with their approach called ExPred where they leverage mult-task learning on the explanation phase and embed a prediction network on the extracted explanations to improve task performance. They experiment with three datasets [Movie Reviews, FEVER (Fact Extraction and VERification), MultiRC] and conclude that their model significantly outperforms existing methods.",
          "This section compares extractive and abstractive rationalization techniques. It can be observed from Figure 4 that there is more interest and focus on extractive rationalization techniques compared to abstractive. There are multiple reasons for this, and the progress in the Automatic Text Summarization (ATS) domain can help explain.",
          "In most extractive rationalization approaches, generating a rationale is similar to text summarization. These rationales contain the salient features of the input text, which users need to understand as the most influenced features of the model's prediction. Next, two steps are implemented while performing the task-(i) irrelevant information is ignored, and (ii) most crucial information is selected based on a scoring algorithm. This approach is a common foundation of summarization techniques. In extractive summarization, meaningful sentences are extracted to form a summary of the original text while still retaining the overall subject matter (El-Kassas et al., 2021). The critical difference with rationalization is that it is able to justify a neural network's prediction with evidence. In a way, extractive rationalization uses extractive summarization's fundamentals and takes it further. It frames the task as can we rationalize the output prediction where rationalize means to understand the prediction process and reason with supporting evidence. This introduces an interdependent relationship between the rationale and the prediction. This process is close to how humans rationalize with a sequence of reasons to justify a decision. This can be implemented in the NLP process to make models more explainable. As interest in ATS systems grew in the past few decades, researchers have mainly focused on extractive summarization due to its simplicity, and reliability (El-Kassas et al., 2021). The abstractive summarization needed reliable natural language generation; thus, it was in its infancy from the 2000s to the early 2010s. Therefore, an increasing body of knowledge on extractive techniques is available, which researchers interested in rationalization could leverage and build on. This intuition behind extractive summarization paves the way for extractive rationalization. The stark difference between extractive and abstractive in Figure 3 is expected and reasonable, and the fields of summarization and rationalization follow similar paths. However, summarization approaches should purely be used for inspiration -following the identical methods for rationalization would be insufficient, and it does not provide reliable model explainability. Chan et al. (2021) notes that for appropriate explainability, the desiderata for the rationale is that-(i) it must reflect the model's reasoning process (faithfulness), (ii) be convincing to the nonexpert (plausibility), and (iii) the rationale extraction should not hurt task performance. Thus, there is more work than simply extracting sentences as rationales. Moreover, extractive rationalization is insufficient because extracted sentences themselves are insufficient to provide full explainability. Humans do not fully understand without context and a coherent and logical explanation.",
          "The extensive research in extractive summarization reached its maturity, has peaked in terms of performance, and now the progress is stagnated (Gupta and Gupta, 2019). Recent advances in deep learning and the advent of the Transformer architecture in 2017 have led to more reliable and influential language models (Vaswani et al., 2017;Devlin et al., 2019). In 2019, Liu and Lapata (2019)  Abstractive techniques allowed novel words and phrases to be generated instead of extracting spans from the input. Due to these advances, the research focuses gradually shifted from extractive to abstractive summarization. It is expected that rationalization will follow a similar trend. Abstractive rationalization is still relatively new, with limited research available. However, there have been promising and pioneering approaches such as Rajani et al. (2019) and Kumar and Talukdar (2020). Almost every paper discussed with an abstractive rationalization technique in Section 4 leveraged some implementation of the Transformer architecture, such as BERT, GPT-2 (Radford et al., 2019), and T5, amongst others. BERT was the most frequently used language model. When BERT was released in 2018, it achieved SOTA results on many NLP tasks and surpassed human performance on tasks such as question answering and commonsense reasoning (Devlin et al., 2019). It made a giant leap in terms of performance compared to other language models of its time. This led to wide adoption and variations of BERT for the tasks where the Transformer-based model was required. Recently introduced models such as BART (Lewis et al., 2020), GPT-3 (Brown et al., 2020), and T5 demonstrate promising results and surpass BERT in some tasks. This is due to language models growing exponentially, and they continue to improve and perform incredibly well at natural language generation (Sanh et al., 2019). For example, in some cases, text produced by GPT-3 is almost on par if not better, than human-written text. This enables more opportunities for research in abstractive rationalization, which is needed. By leveraging SOTA language models, explanations can become more comprehensive and convincing when illustrating a model's decision-making process. As mentioned in Section 2, it is almost as if the models are \"talking for themselves\". We believe that significant progress can be made in rationalization by focusing more on improving abstractive techniques.",
          "In this section, we discuss insights from the literature reviewed, challenges, and potential future directions to propel progress on rationalization. Most importantly, we introduce a new XAI subfield called Rational AI.",
          "In Section 1, we have seen the need for explainability and the available methods in NLP. The numerical methods, such as SHAP values (Lundberg and Lee, 2017b) or Attention scores, visualization methods, such as LIME (Ribeiro et al., 2016), and saliency heatmaps, all require specialized domain knowledge to understand. At the same time, with increasing interactions with NLP-based systems, the nonexpert also deserves to know and understand how these black-box systems work because it has some degree of influence on their lives. This is formally called the right to an explanation, a right to receive an explanation for an algorithm's output (Miller, 2019b). A classic example is a bank system with an NLP model that automatically denies a loan application. In this situation, providing the loan applicant with SHAP values or saliency heatmaps to justify the bank's algorithms is not very meaningful. Thus, explainability methods are truly explainable and helpful if the nonexpert can understand them (Mittelstadt et al., 2019). We introduce Rational AI (RAI) as a potential solution.",
          "Rationalization techniques come the closest to this goal because they are built on natural language explanations (NLEs). NLEs are intuitive and human comprehensible because they are simply descriptive text. The textual information can be easily understood and translated into other languages if needed. Across all of the NLP tasks discussed in Section 4, we have seen the benefits of NLEs and the accessibility it provides to the nonexpert. We believe there is a critical need to focus on explainability techniques with NLEs. Considering these factors, we propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI. Further, rationality should not be confused or used interchangeably with the general AI term of a rational agent Russell and Norvig (2002). These are distinct topics with similar names. In this survey, RAI and rationality are purely considered in the context of model explainability. We also have not seen any usage or previous definitions of RAI within this context. We compare rationality to the other fields shown in Figure 5. Models with interpretability are interpretable, while those with explainability are interpretable and complete, as described in Section 1. Models with rationality are interpretable and complete and can rationalize their behavior through an NLE. The explainability methods described earlier in this subsection explain, but they do not justify in a way that is accessible and comprehensible to the nonexpert. In recent years, language models have become powerful and incredibly good at language generation tasks, but we have yet to see their full potential. As they continue to grow exponentially, we predict this is the beginning of explainability techniques using NLEs. The intuition behind RAI is that rationalization is one such technique, and many are yet to be developed. This calls for a more organized field to improve research focus and the need for RAI to exist.",
          "Although RAI arises from the need for better explainability for NLP tasks, it is potentially applicable in general AI and other fields in AI. Other fields, such as Computer Vision, Speech, and Robotics, could leverage rationalization methods to improve their model explainability. For example, rationalization in Computer Vision can help explain through an NLE which visual features contributed the most to an image classifier prediction in place of complex explainable techniques (Sundararajan et al., 2017;Tjoa and Guan, 2021). Many promising opportunities exist for researchers to apply rationalization in other disciplines.",
          "We have seen that rationalization is a relatively new technique, and with it, various challenges exist. In this subsection, we share challenges and potential solutions to improve the current state.",
          "No standard statistical evaluations exist currently for rationalization. There is a wide variety of metrics that are in use, such as Mean Squared Error (Lei et al., 2016), Accuracy (Zaidan et al., 2007;Du et al., 2019;Rajani et al., 2019), F1 Score (Alhindi et al., 2018;Rana et al., 2022), ANOVA (Analysis of variance) (Das and Chernova, 2020), and Precision (Plyler et al., 2022). We have observed that the most preferred statistical metric is accuracy. It is reasonable for evaluation metrics to be task-dependent and focused on the prediction. However, those alone are insufficient because the accuracy of the NLE also needs to be considered. For example, if the task prediction had high accuracy, but the NLE was unclear and incomprehensible, then it is not helpful. Metrics such as the BLEU (BiLingual Evaluation Understudy) score by Papineni et al. (2002) and the ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation) score by Lin (2004) exist for evaluating open-ended machine-generated texts. However, we have seen limited use in the literature review, such as Camburu et al. (2018). The scores work by comparing the generated text with a set of ground-truth reference texts, and often these are human-written references. These scores are helpful, especially for abstractive rationalization, where explanations can be open-ended; however, they come with limitations since the evaluation is effectively token-level matching. Since an NLE is the typical outcome of systems with rationalization, adopting a standard evaluation metric can help improve research progress. Consistent evaluations also make it easier to compare different experiments and approaches.",
          "The availability and the need for more diversity of appropriate datasets is also a problem hindering progress. Availability: Data collection is an expensive and timeconsuming task. It is possible to repurpose existing datasets, but modifying them requires manual human labor. Thus, researchers often build their datasets for a specific task they are working on. Camburu et al. (2018) developed the e-SNLI dataset by modifying the SNLI dataset from Bowman et al. (2015b). Camburu et al. TABLE Human-Centered Evaluations (HCE) are performed in out of papers surveyed in this review. Year References Method name Venue NLP domain HCE performed? 2007 Zaidan et al. (2007) -NAACL Text Classification No 2016 Ribeiro et al. (2016) LIME NAACL Multiple Domain Yes 2016 Lei et al. (2016) -EMNLP Sentiment Analysis No 2016 Zhang et al. (2016) -EMNLP Text Classification No 2017 Sharp et al. (2017) -CoNLL Machine Reading Comprehension No 2017 Ling et al. (2017) -ACL Machine Reading Comprehension No 2017 Alvarez-Melis and Jaakkola (2017) SOCRAT EMNLP Neural Machine Translation Yes 2018 Ehsan et al. (2018) -AIES Commonsense Reasoning Yes 2018 Alhindi et al. (2018) LIAR-PLUS FEVER Fact-Checking No 2018 Mihaylov et al. (2018) OpenBookQA EMNLP Machine Reading Comprehension Yes 2018 Xie et al. (2020) WorldTree V2 LREC Machine Reading Comprehension Yes 2018 Camburu et al. (2018) e-SNLI NeurlPS Natural Language Inference No 2019 Rajani et al. (2019) CAGE ACL Commonsense Reasoning Yes 2019 Hanselowski et al. (2019) -CoNLL Fact-Checking No 2019 Du et al. (2019) CREX ICDM Sentiment Analysis No 2019 Strout et al. (2019) -BlackboxNLP Sentiment Analysis Yes 2019 Liu et al. (2019b) GEF ACL Text Classification Yes 2020 Atanasova et al. (2020) -ACL Fact-Checking Yes 2020 Lakhotia et al. (2021) ERASER ACL Multiple Domain Yes 2020 Das and Chernova (2020) RGA IUI Multiple Domain Yes 2020 Sharma et al. (2020) EPITOME EMNLP Multiple Domain No 2020 Kumar and Talukdar (2020) NILE ACL Natural Language Inference Yes 2020 Wiegreffe et al. (2021) -EMNLP Natural Language Inference No 2021 Majumder et al. (2021) RExC ICLR Commonsense Reasoning Yes 2021 Tang et al. (2021) DMVCR DaWaK Commonsense Reasoning No 2021 Tang et al. (2021) FiD-Ex EMNLP Machine Reading Comprehension No 2021 Chan et al. (2021) UNIREX ICML Multiple Domain No 2021 Zhang et al. (2021) EXPred WSDM Multiple Domain No 2021 Fomicheva et al. (2021) -ACL Neural Machine Translation No 2021 Yu et al. (2021) A2R NeurlPS Sentiment Analysis Yes 2021 Antognini and Faltings (2021) ConRAT ACL Sentiment Analysis Yes 2021 Plyler et al. (2022) CDA NeurlPS Text Classification No 2022 Atanasova et al. (2020) RERRFACT AAAI Fact-Checking No (2018) achieved promising results on their task, demonstrating how their dataset can enable a wide range of new research directions by altering and repurposing existing datasets. Diversity: Without enough datasets, new research in rationalization will be limited. Researchers will be constrained to the existing datasets to make new progress. This trend is evident in the literature reviewed in MRC and Sentiment Analysis compared to NMT. In MRC, the datasets are very diverse. In sentiment analysis, most papers rely on either the BeerAdvocate (McAuley et al., 2012) or MovieReviews (Maas et al., 2011) datasets to perform their experiments. In both domains, we discovered five publications each. For a domain such as NMT, progress seems limited, and we found only two publications. The lack of appropriate rationalization datasets for NMT tasks is a possible reason for this. As we observed in our literature review, there is a direct relationship between dataset availability and the progress made. More work in creating new datasets for rationalization can help improve diversity and the progress of certain domains lagging behind, such as NMT. New datasets across all domains, in general, will increase the interest and work in rationalization because researchers will have more flexibility in designing new techniques and experimenting with a wide variety of data. Stamper et al. (2010) has organized the largest repository of learning science datasets called DataShop, and it led to improvements in research progress. Similarly, an organized central repository for rationalization supporting datasets can be beneficial. Without a centralized model evaluation and development system, reproducibility and accessibility will remain low.",
          "NLP has direct applications in many disciplines. For example, MRC and commonsense reasoning are helpful in the education discipline. Our literature review indicates using Q&A tools and commonsense injection to generate explanations for educational needs (Mihaylov et al., 2018;Li et al., 2019). Further, NLP has also been used to enhance human task performance, as we saw in Das and Chernova (2020), and to provide support for mental health (Sharma et al., 2020). Additionally, fact-checking is another application, and it is crucial in social media, fake news detection, and law (Alhindi et al., 2018). It has become common to interact with these systems, and they may have a significant influence on all aspects of our society. Due to this, the European Union recently passed a regulation that requires algorithms to provide explanations that can significantly affect users based on their userlevel predictions (Doshi-Velez and Kim, 2017).",
          "The explanations provided by the NLP systems must provide enough information to the user to help them understand its decision-making process (Putnam and Conati, 2019). Considering these aspects, the human-machine partnership is essential for evaluating and generating accurate explanations. This calls for better methods to evaluate the explanations generated. The field of HCE addresses this problem, and Sperrle et al. (2021) defines it as a \"field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios.\" In this literature survey, we found 15 out of 33 papers in which HCE is performed, and a summary is provided in Table 12. Sperrle et al. (2021) shares the increasing trend of HCE since 2017 compared to the previous years. While conducting this literature survey, this trend was not observed in the rationalization domain. Overall, we found that HEC is incorporated in most of the papers on Machine Reading Comprehension (2 out of 5), Commonsense Reasoning (3 out of 4), Fact-Checking (1 out of 4), Natural Language Inference (2 out of 5), Neural Machine Translation (1 out of 2), Sentiment Analysis (3 out of 5) and Multiple Domain (3 out 6). From our observations, researchers give more attention to performance while evaluating AI algorithms and ignore human factors such as usability, user intentions, and user experience. Thus, along with the accuracy of AI algorithms, it is also essential to focus on the interpretability and reliability of the explanations generated by AI algorithms. The articles in which HCE is used are primarily performed via crowdsourcing using Amazon Mechanical Turk, and the focus is on user-based evaluations or annotations. This pattern necessitates conducting expert evaluations to understand users' needs better because it can help improve trust in AI algorithms. HCE is a subset of the Human-Computer Interaction (HCI) field, which is integrated with the AI paradigm after the algorithmic performance evaluations as shown in Figure 6. This integration can be regarded as human-centered AI, and Riedl (2019) claims this as an AI/ML perspective that intelligent systems are part of a more extensive system that also includes human stakeholders. The literature (Hwang and Salvendy, 2010) on usability, and user experience testing demonstrated three widely used methods to perform HCE -Think Aloud (TA), Heuristic Evaluation (HE), and Cognitive Walkthrough (CW). The TA method is a standard method and can be more effective considering the evaluations of explanations in the NLP domain. In the TA method, evaluators are asked to \"think aloud\" about their experience while an experimenter observes them and listens to their thoughts (Fox, 2015). This way, an HCE method can be used in the final step to understand usability, user intentions, and user experience. This may lead to a better understanding of the interpretability and reliability of the explanations generated by rationalization. Therefore, in addition to statistical evaluation techniques, we strongly encourage researchers to integrate HCE as part of their evaluations.",
          "It is critical to perform rigorous testing and validation of NLP systems at all stages before their deployment. For example, it should be ensured that the data is unbiased, models are interpretable, and the process of arriving at the outcome is explainable to a nonexpert. In the last step of this process, it would be beneficial to use RAI techniques. Integrating rationalization with human-centered evaluations and elements of NLP Assurance can invoke human-AI trust and safety with the systems -with the recent rise of chatbots such as ChatGPT, the need for more rigorous validation is more important than any other time. This process may also transform black-box systems into white-box systems and make NLP models more comprehensible and accessible for nonexpert users."
        ],
        "ground_truth_definitions": {
          "abstractive rationalization": {
            "definition": "generating a novel explanation",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "extractive rationalization": {
            "definition": "extracting text fragments from the input",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Interpretability": {
            "definition": "Interpretability aims at developing tools to understand and investigate the behavior of an AI system.",
            "context": "Interpretability “aims at developing tools to understand and investigate the behavior of an AI system” (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to “explain or to present in understandable terms to a human” what the AI system is performing.",
            "type": "implicit"
          },
          "Rationalization": {
            "definition": "justifies a model’s output by providing a natural language explanation.",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Black-box Model": {
            "definition": "A machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans",
            "context": "Given in a Table",
            "type": "explicit"
          },
          "Explainability": {
            "definition": "the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction.",
            "context": "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
            "type": "explicit"
          },
          "interpretable": {
            "definition": "i.e., allowing humans to understand the internals of a model",
            "context": "The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models.",
            "type": "explicit"
          },
          "NLP Assurance": {
            "definition": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users.",
            "context": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain.",
            "type": "explicit"
          },
          "Completeness": {
            "definition": "The ability to accurately describe the operations of a system that allows humans to anticipate its behavior better.",
            "context": "However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model’s decision-making process of arriving at a prediction and the ability to be verified and audited.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "24589ef86242ff182e7bae495b94e7c11931f20c",
        "sections": [
          "Whitehouse: Listening to Birds in the Anthropocene / 55 The post-war Great Acceleration had its symbol in the loss of something meaningful and beautiful, but also common enough to have once been taken for granted. Listening to birds, Carson implied, was no longer something conceivable as straightforward or inevitable. Carson exemplified this shift by quoting an Alabama resident writing about the effects of a massive spraying programme to control fire ants: Our place has been a veritable bird sanctuary for over half a century. Last July we all remarked, \"There are more birds than ever.\" Then, suddenly, in the second week of August, they all disappeared … There was not a sound of the song of a bird. It was eerie, terrifying. What was man doing to our perfect and beautiful world? Finally, five months later a blue jay appeared and a wren. 8 Life in such a perfect world was thus accompanied by the presence of birds, a presence made most readily and delightfully manifest in their songs. When once familiar companions no longer accompanied local residents, the shock was unexpected and unnerving. The Anthropocene is still a new and largely academic term but one that relates to concerns that have been more widely felt, particularly since they were so clearly articulated by Carson. First, there is the concern that everyday experiences of birds and other wildlife are under threat from everyday human activities. Second, there is a belief that local changes and local activities can have global causes and effects. Finally, there is the concern that while things might seem okay in the present, nothing can be taken for granted about the future-not even the near future and what now seems commonplace. The Anthropocene is thus both a term that relates to real and observable changes in the local worlds people perceive around them and to semiotic elaborations on those perceptions that draw together local and global, human and non-human, present and future, into anxiety-laden narratives. There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"9 Anxious semiotics is central to listening to birds in the Anthropocene not only directly in the face of loss but in the tensions of daily experiences that might seem, on the face of it, to be positive. Fluctuations and changes in what can be heard lead back to possible human causes, perhaps even our own activities. Anxious semiotics can even emerge in response to an increase in some species, perhaps spreading in the wake of anthropogenic changes, or to birds singing earlier in the season, perhaps because of climate change. Anxiety points towards potential, as well as actual, loss. It also points towards concerns about human culpability for change, both locally and more generally. Conservationists monitor the fortunes of birds and their habitats assiduously, enjoying the support of much of the public as they do so. But growing desires to attend to and to care for birds and their ecology seem to be continually outstripped by our capacity to disrupt and endanger. The environmentalism spawned by Silent Spring has constantly drawn attention to the ways that ecosystems, both local and global, are affected by human activity. What prospers and what disappears are causally bound together with human actions in ways that are sometimes readily apparent and sometimes barely perceptible. I explore the associations that people perceive as emerging between bird sounds and environmental changes. Following the semiotic approach of Kohn,10 I argue that the symbolic and moral connotations of listening to birds in the Anthropocene follow from their iconic and indexical grounding in places, producing an anxious semiotics in which even positive associations can have portentous or uncertain implications. My argument progresses from recent claims made by Bernie Krause about the evolution of soundscapes and their disruption by humans to a series of narratives contributed to the Listening to Birds project: an anthropological study of people's relations to birds through sound. Finally, I invite the reader to listen to four recordings and consider the sounds they hear as a means of sensing life in the Anthropocene.",
          "Harmony and balance was a state that Carson considered to be under threat in the modern world. She began Silent Spring with the nostalgic statement, \"There was once a town in the heart of America where all life seemed to live in harmony with its surroundings.\"11 This harmony was not just something that could be seen or measured but that could also be heard in the varied mix of sounds in the environment. This point finds elaboration in the work of musician and scientist Bernie Krause, most notably in his book The Great Animal Orchestra. 12 Krause's argument is that places have evolved their own acoustic ecology and that in a natural environment the sounds will tend to fit together rather than disrupting or competing with one another. He begins The Great Animal Orchestra by speculatively describing the soundscape of the American plains, 16,000 years ago and prior to the arrival of humans in North America. An array of now extinct mammals, along with more familiar birds, insects and amphibians, fill the air with their sounds. But despite this complex mix of sounds, each animal is able to make itself heard. Inspired by Murray Schafer's soundscape approach, Krause argues that this is because each species has a sound that occupies a distinct bandwidth and so can exist in harmony with the other sounds that tend to occur around it, be they from other species or from physical factors such as running water. \"This is,\" he writes, \"the tuning of the great animal orchestra.\" 13 This tuning can be rendered visible through the use of spectrograms, which translate sound into a graph plotting time against frequency. As Mundy has pointed out, spectrograms have played a significant role in creating an \"image of evolution,\" but she argues that, more typically, this takes the form of a stable, visible object that is readily comparable and can thus be more easily incorporated into larger taxonomies.14 In Krause's argument, each spectrogram is an image of evolution because it is assumed that the sound of each species is indexically related to the other sounds in the same soundscape, i.e. there is a causal relation between the sounds in an environment and the \"bandwidth\" occupied by the sounds made by each species. This set of inter-relating sounds occupying a distinct sonic niche has emerged, according to Krause, through evolutionary processes. The nesting of these sounds is rendered as a visible object in the spectrogram. Krause divides up sound into three categories. First is the geophony, sounds made by the physical environment such as waves, wind or rain. Second is the biophony, which consists of sounds made by animals, plants and other organisms. Finally is the anthrophony or humangenerated sound. While this classification is ostensibly based on the origins of the sounds rather than what they actually sound like, Krause emphasises the evolutionary emergence of the different types of sound, arguing that: The sounds of geophony were the first sounds on Earth-and this element of the soundscape is the context in which animal voices, and even important aspects of human sonic culture, emerged. 15 So here we have a sense of the sounds of the world developing in relation through the geological eras and into the present day. The different types of sound that Krause outlines might themselves be regarded as sonic epochs in which a particular category of sound emerged or tended to predominate. As such, Krause is implying that we have now entered the epoch of anthrophony and that human sounds are drowning out the biophony and geophony in many parts of the world. The Anthropocene has also ushered in a new kind of anthrophony, with the sounds of industry, machinery, combustion engines and electronic amplification being a rather different and more disruptive type of human-induced sounds than those that would have predominated in the pre-industrial era. As such, I would argue that one can differentiate between pre-and post-Anthropocene anthrophony, the former tending to integrate more closely with other sounds and the latter often disrupting or dominating them. Indeed, Krause's main concern is not with the effects of anthrophony in general but the effects of what he calls electromechanical sounds. Problematic sounds don't so much originate in human bodies themselves but in Anthropocene technologies used by humans. Humans, Krause argues, are not just having an effect on the biophony by drowning out the sounds of other species with their own but also by disrupting ecosystems. Here his concerns are clearly aligned with those of Carson. He is troubled by the loss of sounds but also by the disruption to the harmonious sonic ecosystem. It is not simply a spring of silence that is worrying but one of disharmony. Humans have effects on other sounds not just through their own sounds but through their effects on habitats. These changes reduce the quantity of sound but also cause disruption to the relative harmony of the soundscape. When a biome is compromised, spectrograms will lose both density and diversity, along with the clear bandwidth discrimination among voices that is otherwise visible in nonstressed-habitat graphic displays. Biophonies from stressed, endangered, or altered biomes tend to show little organisational structure. 16 This emphasis on considering sounds within the context of the environments in which they emerge is a point sometimes forgotten in the analysis of bird sounds by scientists and other scholars, who take recorded sounds as discrete, decontextualized objects. Biophonic sounds are thus indexical of the conditions within which organisms live and have evolved. Krause also defines noise in relational terms: I think of noise as an acoustic event that clashes with expectation-loud heavy metal music in an intimate restaurant is noise … A straight-piping motorcycle gunning its way through the delicate landscape of Yosemite Valley shatters the numinous experience for both visitors and animals.17  Noise is thus a conflict that emerges from perceived disjunctions of one set of sounds and another, or between one aspect of the world as experienced and another. It is dissonant rather than resonant, and Krause's argument is that dissonance and noise have increased greatly to the detriment of the biophony and to human well-being. As sounds are the products of activities, this also points to increasing dissonance between human activities more generally and the activities of other species in our environment. These conclusions lead Krause to lament that, \"In order to hear the wild biophonic world, we need to get to places free from human noise.\" 18 The world as it should sound, for Krause, is not just pre-Anthropocene, like the rural idyll Carson draws on, but pre-human. It is a world not of companions but of strangers. How then should one listen to birds in the Anthropocene-in a world in which humans are not just present but dominant and noisy and in which our activities are often dissonant with those of other species? What signs are people listening for and how are they listening? The Anxious Semiotics of Humans and Other Species Krause's work provides support for what many have long realised: that the sounds of birds and other animals are importantly sounds in place. Sound-making happens in relation to other sounds and to the acoustic conditions. For birds, sound-making is also place-making; it is an act of territorialising space, of making relations with other birds and continually re-weaving the context of their lives. In order to do this, as Krause argues, birds make sound in relation to other sounds and both their lives and their evolution enable them to be responsive to signs within their environment. This emphasis on sound and communication as emerging relationally and across species boundaries can be aligned with the recent proliferation of \"more-than-human\" approaches in the social sciences. 19 These new ways of thinking about life have created fresh impetus in the environmental humanities and social sciences, but few authors in this field have explicitly addressed how to analyse the sorts of meanings and communication that emerge in more-than-human relations. A recent exception has been Eduardo Kohn, who has argued that the grounding for how humans and non-humans perceive and understand their world is similar across species. 20 Drawing on the varied works of Peirce, Bateson and von Uexküll, Kohn argues that the semiosis of life is grounded in iconic and indexical signs. The tendency of anthropology and other social sciences has been to emphasise symbolic signs, particularly language, as the primary means of communication. The apparent absence of symbolic communication in non-humans has fuelled an emphasis on human communicative exceptionalism that appears to forge a gulf in human and non-human semiotics. Kohn argues that symbols should instead be seen as nested within a semiotic field of icons and indexes from which their meaning emerges. In this respect, the ways that humans find meaning in the world is not so different to the ways that other organisms do: they are all grounded in the iconic and indexical. 21 Birds perceive the representations of other organisms, including those of their own kind, and their sound-making emerges in association while, for humans, the symbolic only \"works\" because it is emergent from the same sorts of associations. As Kohn puts it, \"The semiosis of life is iconic and indexical. Symbolic reference ... is an emergent dynamic that is nested within this broader semiosis of life from which it stems and on which it depends.\" 22 What follows from this grounded and emergent concept of the symbolic is morality. \"The moral is also distinctively human, because to think morally and to act ethically requires symbolic reference. It requires the ability to momentarily distance ourselves from the world and our actions in it to reflect on our possible modes of future conduct … This distancing is achieved through symbolic reference.\" 23 While I am more hesitant than Kohn to exclude ethical consciousness from non-humans, I agree that both the symbolic and the ethical emerge from the iconic and indexical semiosis of being-in-the-world 24 and that the symbolic also provides the potential for a conceptual, and usually partial, distancing from this immersion. Being-in-the-world is, for humans at least, double-edged because it entails a constant oscillation along a spectrum of mindfulness 25 that runs between engagement and detachment (see also the work on scientists and naturalists by Candea, MacDonald, and Ellis), 26 between Dwelling and Skill (London: Routledge, 2000); S. Eben Kirksey and Stefan Helmreich, \"The Emergence of Multispecies Ethnography,\" Cultural Anthropology 25, no. 4 (2010): 545-576. 20  Kohn,How Forests Think. 21 Ibid.,[38][39] Ibid., 55. 23 Ibid., 133. 24 By \"being-in-the-world\" I wish to convey a relational sense of life that is derived from thinking of the organism-in-its-environment as indivisible and emergent. \"Being\" cannot be separated from \"the world.\" the real and the symbolic, between is and ought. Focusing on this oscillation is, I argue, essential to understanding how people listen to birds in the Anthropocene. In order to clarify my argument, I first need to describe the semiotics of being-in-theworld that I have in mind. Developing Kohn's argument, I bring his semiotic approach into conversation with Tim Ingold's claim that life is best realised as a meshwork. By doing so, I aim to add some flesh to Kohn's semiotics and give a sense of how meanings emerge from the entanglements of the meshwork. Ingold introduces his understanding of meshwork thus: Every… trail [of movement or growth] discloses a relation. But the relation is not between one thing and another-between the organism 'here' and the environment \"there.\" It is rather a trail along which life is lived. Neither beginning here and ending there, nor vice versa, the trail winds through or amidst like the root of a plant or a stream between its banks. Each such trail is but one strand in a tissue of trails that together comprise the texture of the lifeworld. This texture is what I mean when I speak of organisms being constituted within a relational field. It is a field not of interconnected points but of interwoven lines; not a network but a meshwork. 27 As such, any organism is a tangle of interconnecting life lines and thus is emergent from these relations and associations. \"Organisms and persons … are not so much nodes in a network as knots in a tissue of knots, whose constituent strands, as they become tied up with other strands, in other knots, comprise the meshwork.\" 28 For Ingold then, life is characterised by movement and constant birth; it is a process out of which forms emerge but are never fixed. A question that arises from such a view of life is how, in such a fluid and entangled world, can any phenomenon be knowable enough to be dealt with or represented? How can living beings make a good enough \"guess\" at what is emerging to keep on living in a form that is iconic of what it has been before? It is in answer to these questions that Kohn's employment of Peirce's semiotics can be helpful. According to Kohn, Peirce \"strove to situate his entire philosophical project … within a special kind of realism that could encompass actual existence within a broader framework that would account for its relationship to spontaneity, growth, and the life of signs in human and nonhuman worlds.\" 29 This \"broader framework\" might be better thought of as a meshwork-a way of thinking about the world as emergent, fluid and entangled that chimes with both Kohn's and Peirce's ontological claims. The lines, knots and texture of the meshwork are themselves semiotic. Iconic forms are continually emerging in the continuous rebirth of the world; materialised indexical associations are what \"things\" are made of. The meshwork is semiosis made flesh. Kohn develops these themes by drawing on Peirce's concepts of \"firstness,\" \"secondness\" and \"thirdness.\" Firstness is that which is spontaneous and ephemeral-a glimpse of possibilities that are as yet unrealised and unconnected. Kohn describes the tantalising whiff of cinnamon from a freshly opened passion fruit as an example of firstness. Secondness is more startling and sometimes shocking; secondness can be unpredictable or a jolt to habits. For Kohn, a crashing palm tree exemplifies secondness, as a sudden disruption in the meshwork that might realign its relations and draw attention to troubling indexical causation. Thirdness, finally, is more general, regular and habitual. It can take the form of behaviour but can also be seen in terms of certain general tendencies of life such as selforganisation and resistance to entropy. As Kohn argues, \"This tendency is what makes the world potentially predictable and what makes life as a semiotic process, which is ultimately inferential, possible. For it is only because the world has some semblance of regularity that it can be represented. Signs are habits about habits.\"30 Thirdness is the sort of regularity that Krause hears represented in a relatively uncompromised ecosystem in which each bird responds to the regularities of other sounds and acoustic conditions, and it is essential to semiosis more generally. All signs contain aspects of firstness, secondness and thirdness but tend to amplify one of these more than the others. They are all relational. Humans and other beings can sometimes be shocked (secondness) by what is encountered, but being shocked draws deeper attention to habits and regularities. We know the world and our movements within its meshwork differently through shock, and listening to birds in the Anthropocene is habitually shocking. In examining perceptual practices in the meshwork, Ingold writes not of shock but instead marks a distinction between astonishment and surprise. 31 Astonishment, he argues, is a response to a world that is open and continually being remade. In this, even the mundane can be astonishing. Surprise is a response to a world conceived of as closed and predictable, which thus creates the possibility for surprising deviations. Astonishment involves \"riding the crest of the world's continued birth\"32 whereas surprise involves grasping the world \"within a grid of concepts and categories.\" 33 The former Ingold associates with animism and the latter with the sciences. However, I do not see the two sorts of response as being wholly inimical. Kohn's argument that the symbolic realm, in which the world might be conceived of as closed, predictable and thus potentially surprising, is itself emergent from an openness to the world, in which entanglement, continuous birth and astonishment are endemic. Moving too far into the closed, symbolic world can be pathological. As Kohn points out when describing his own temporary breakdown, symbolic semiosis that is divorced from an iconic and indexical grounding can easily induce panic: Panic and its dissipation reveal these properties of symbolic semiosis. They point both to the real dangers of unfettered symbolic thought and to how such thought can be regrounded. Watching birds regrounded my thoughts, and by extension my emerging self, by recreating the semiotic environment in which symbolic reference is itself nested. Through the artifice of my binoculars I became indexically aligned with a bird, thanks to the fact that I was able to appreciate its image now coming into sharp focus right there in front of me. 34 In his account, Kohn describes a movement from being deeply alienated and ungrounded in his semiosis while travelling on a bus to the dissipation of this panic when he takes time to look at a tanager while out walking. This move marked a 're-astonishment' with the world, while the panic was a shock that revealed more clearly his regular habits of being. The move was not so much between engagement and detachment but between immersion and alienation. This is what can happen when symbolic thoughts drift free from their anchoring in the indexical and iconic conditions of being-in-the-world. As Candea argues in relation to his work with scientists studying meerkats, detachment is not the opposite of engagement but a particular manifestation of engagement, one that enables scientists to understand the world of the meerkats more deeply and sensitively. 35 It is, in fact, a manifestation of the oscillation I described earlier between the real grounding of being-in-the-world and the symbolic and ethical. The surprise of a scientist is not so much a product of disengagement but of the oscillation towards an emergent symbolic semiosis in which the world is conceptualised as a closed model. However, in producing such a model the scientist needs to be fully engaged with a world of complex movement and constant form generation. They need to have consciously dwelt in the meshwork in order to build the model. In order to be surprised, one first needs to be astonished. The shocking surprises of the Anthropocene are most readily perceived by those who find the world constantly astonishing. My reason for discussing these relational semiotic approaches is to emphasise that the symbolic and ethical associations and interpretations of listening to birds in the Anthropocene are grounded in an experience of being-in-the-world; they are not simply cultural models that are imposed onto what is perceived. Any such models or symbolic associations are emergent from being-in-the-world rather than prior to it. Likewise moral ideas are emergent from the iconic and indexical grounding of the meshwork. Following Ingold and Kohn I argue that it is not possible to have symbolic and moral ideas without first being instantiated in the world in a meshwork of relations and associations that are at once both material and semiotic. As such, the ways that humans listen to their surroundings are not fundamentally different in their grounding to the ways that birds listen to theirs. In the narratives that follow a range of moral and symbolic ideas are represented, but these draw on the indexical associations of place and temporality. While the approach that I have outlined above is more broadly about how humans and other species perceive the world, there are some particularities to listening to birds in the Anthropocene that require elaboration. I argue that the semiotic context of the Anthropocene is peculiarly anxious because, as I pointed out earlier, it emphasises our separation from the rest of life just at the moment we connect with it. As Aldo Leopold knew, \"One of the penalties of an ecological education is that one lives alone in a world of wounds.\" 36 The Anthropocene world is increasingly not a world of reassuring thirdness and regularity but of a shocking secondness-such as Carson wrote about in Silent Spring-in which the causes of deleterious change seem to lead us back to our own actions. As the indexical and iconic grounding of our being-in-the-world shifts, those wounds become more apparent to those that are the least alienated from other forms of life. The moral and symbolic fallout is the anxious semiotics of the Anthropocene.",
          "I derive three principles from the above discussion that together provide insights into the particularities of listening to birds in the Anthropocene: 1. The semiotic grounding of perception is an indexical and iconic meshwork, both for birds and for humans. Although the meshwork is constantly emerging, there is still potential for regularities (thirdness) that are responded to by organisms in their actions and that provide a basis for the continuance of their lives. 2. The symbolic emerges from this grounding and provides potential for an oscillation between different kinds of mindfulness e.g. engagement and detachment, real and ideal etc. Human perception often involves this kind of oscillation. 3. The semiotic grounding of the Anthropocene is particularly unstable and the causes of this instability can often be traced to human action. The perception of instability and uncertainty thus provokes a distinctly anxious semiotics that can shock and unsettle senses of place and time, as well as provoking moral disquiet. To add some flesh to the bones of these principles, I turn now to the Listening to Birds project, an anthropological study of people's relations with birds through sound. Through this research I received many stories from people narrating their own experiences of listening. 37 Most came from respondents in the UK but many were also contributed from other parts of the Englishspeaking world, including Australia and New Zealand. I take these narratives as a means by which people highlight what sounds they notice, why those sounds are significant and how they respond to changes in what they hear. They also illustrate broader moral and symbolic dimensions of people's relations with their environment and the role that the sound-making of birds plays in this. What I find most striking about many of the stories is that they concern listening to birds in familiar places around where people live. They are about birds that live alongside people and inhabit their gardens and towns. These are not stories of distant soundscapes in remote and wild places but of companion species that make their own places through sound in the places that people make. What also becomes clear in these narratives is the importance to people of a sense of resonance with birds and with their environment more generally. By this I mean a kind of iconic resemblance or indexical association that arises between human activities and those of birds, as represented in their sound-making. The activities of birds are widely understood to resonate with the rhythms of time and season and, following from this understanding, many 37 These were received via the project website, which invited contributions of people's experiences of listening to birds. The website was widely publicised when the project was covered by the BBC News website. respondents expressed feelings of well-being in response to hearing birds. This seems most apparent when people's lives also resonate with these rhythms,38 as Rob from Droitwich wrote: As a farmer I'm up well before dawn at this time of year but even now there are birds singing in the morning. I heard my first dunnock yesterday. They have already started pairing up and the little owls have been amusing us with their bright chirrups every evening and morning. They certainly brighten up the dark mornings and evenings. As we move into spring I'm lucky enough to hear the dawn chorus every day and it does give me a real lift. Lambing time can be magical when you see new-born lambs taking their first steps to a sometimes deafening suite of blackcaps, robins, chiffchaffs, willow warblers and the rest. If you can't appreciate that then there's something seriously wrong with you. When people like Rob attend to the activities of birds as they go about their lives they are given, as he puts it, \"a real lift.\" This lift, I argue, stems from a sense of resonance that comes from this sympathetic attention to the activities of other beings around us. The sort of attunement of activities that Rob describes resonates, but it also requires that he be sensitive to the lives of those around him. Rob is, according to his account, astonished on a daily basis by listening to birds. His engagement with his surroundings enables him to notice the daily and seasonal changes and regularities. The signs of new and burgeoning life that he perceives serve to exemplify the stability of the place where he lives and the life he leads in it. This resonant grounding even draws Rob into making a moral point about the need for others to appreciate such an experience, perhaps reflecting an anxiety that many do not. His sensing of place and time and of his own resonance with birds is grounded in his being-in-the-world but also draws forth this generalised reflection. As Ingold argues, resonance is analogous to the rhythmic interplay of musicians, who may be playing different sounds but who are following the patterns of sounds around them in ways that are complementary and harmonious. 39 As Krause implies, resonance is also integral to the listening and sound-making of birds and other animals, as they make themselves heard through resonating with the acoustics of their own worlds. The apparent regularity of home, as signalled to Rob by the sound-making of birds, is important in forming many people's sense of the temporality of days and seasons. Rachel Carson clearly understood this connection and its power. Indeed, her book might better have been called Silenced Spring. Spring as a period on the calendar might not be silent without the birds, but for many it would scarcely have been spring either. Bird song does more than simply herald the spring; in an important sense it is spring. As Carson realised, the Anthropocene was making home seem less stable and unproblematic. Many narratives describe the effects of this instability in how home sounds through its birds. A respondent from northern England wrote: I grew up in a suburban area of a northern industrial town and was used to seeing lapwings flitting away across the fields, pee-witting as they did so. I haven't seen a lapwing anywhere near the area for years now and it saddens me to think that my daughters won't get that simple joy of seeing and hearing something otherworldly on their doorstep. Whitehouse: Listening to Birds in the Anthropocene / 65 In this example, an integral sight and sound of the rural-urban fringe has disappeared, probably because of the changes wrought by modern agriculture and our attempts to produce more food more cheaply. The silencing of lapwings is felt anxiously by a father who wants his daughters to experience some continuity with his own sense of place. Again, the grounding of sights and sounds comes to have symbolic connotations of home and of this respondent's own biography, but the differences between his own experience and that of his daughters provokes shock. Other narratives describe new birds that have moved in, like the Asian ring-necked parakeets that now add colour and sometimes discordant sound to the south-eastern suburbs of England. A respondent from Surrey writes: Over the last year or so I started hearing strange squawking in my garden. Normally the bird songs are of the usual garden variety, so this was interesting. It turns out that we have ring-necked parakeets living in the area and it was them creating the squawks and screeches. Now I listen out for them as it's lovely to see the bright green parakeets flying around and \"terrorizing\" the local woodpigeons. The ordinary suburban lives of the English Home Counties are enlivened by these exotic birds, terrorising the woodpigeons and contrasting with bird songs \"of the usual garden variety.\" Sometimes the apparent stability and quietude of suburbia needs something to shake it up; the shock of the new serves to emphasise familiar regularities. But whether these stories concern disappearing species or incoming exotics they speak of the sorts of connections with place that people make through bird sounds and of the acoustic ecologies of places. This grounding leads on to moral pronouncements about how people would like places and times to sound and of the sort of resonance they seek. In this final example, a respondent from Cumbria articulates memories of the place where he grew up before raising concerns over the increasing dissonance between farming and the activities of the birds, a dissonance that has led to the silencing of one particular bird: I grew up in Ireland and lived about three miles from Foxford, County Mayo. In the summer, and especially in the late evenings and at night, one could hear the corncrake call-lots of them-some near, and others far away. The call told me that summer was at last here. I would dearly love to hear that sound again or for my children and grandchildren to hear it too. Sadly the corncrake went into decline because of silage making-farmers cutting grass while the corncrake and other wild birds are nesting on the ground. I would love to see a comeback. The resonance that my respondents described in relation to the sound-making of birds is, I argue, an emergent ethic that draws lives and activities together. It is integral to how they experienced their surroundings but also to how they idealised experience and how they wanted their relations with a place to be. The desire for corncrakes to make comeback in County Mayo derives from this sort of idealisation of how a place should be experienced, and this ethical concern is thrown into sharp relief by their disappearance since the advent of Anthropocene agricultural intensification. This desire for resonance is not just about how things sound but about fitting in with the activities of birds in a way that is sometimes iconic and sometimes complementary. Listening to birds, for my respondents, is not simply about apprehending the sound but about trying to understand how sounds relate to their own activities and the places and temporalities they inhabit. When people move to new areas or other countries they often find the different bird sounds they encounter to be unsettling, to precipitate a sense that they don't belong and that their activities no longer resonate with the right kind of bird sounds. Even without the abruptness of moving to a new area, changes in local bird populations and the loss of species and their sounds can foster a sense of anxiety and loss. But silence, as Carson and Krause both realised, is more unsettling still. My respondents are, I think, revelling in perceiving the activities of other species as they become manifested in sound. Life makes sound and movement and their lack can easily come to be understood as a kind of death. The silence and stillness of birds killed by pesticides or motor vehicles, or habitats depleted by logging or drainage, promote an anxious semiotics of death and loss typical of the Anthropocene, when change and loss is readily attributed to disruptive human actions, sometimes perhaps even the listener's own. Silence and discordance are symptomatic of the Anthropocene.",
          "To explore these issues further, I now invite the reader to listen to four sound recordings of birds that were made in four different countries. These span a range of situations from urban gardens, to islands, to a celebrated rural location for birds, to a rainforest remnant where an enigmatic bird lingers on the edge of extinction. I offer a description of my own listening as a starting point for the reader and as a guide to what is being heard. These are not intended as exhaustive interpretations but to facilitate closer listening. I aim to assist the reader in understanding why these places sound the way they do and to encourage them to listen more closely to what they hear as they go about their lives more generally. The anxious semiotics that is never far away only emerges through active listening. Finally, I add as a proviso that listening to these recordings is not the same experience as \"being there.\" Listening is not simply a process in which sound is heard but is a whole bodily experience of being in place in which sound is a focal point. Recordings may \"transport\" one somewhere but they do not furnish the entire semiotic context and sense of being-in-the-world. Capertee Valley, New South Wales: June 2013 40 I spent a week writing a draft of this paper in the, to me, unfamiliar surroundings of the Australian bush. The Capertee Valley is a renowned area for birds a couple of hours from Sydney: a mix of woodland, fields, hills and valley. Certainly there is abundant life here: a constant twitter of fairy-wrens, thornbills and red-browed finches; the chirrups of a Jacky winter; a garrulous cry from a kookaburra or a currawong; the eerie calls of Australian magpies and ravens; the distant squawks of a flock of cockatoos and the agitated bleating of masked lapwings. What do these sounds tell me about the place where I am? At times it seems chaotic, with a whole range of disparate and similar sounds competing with one another. But I am, unusually, in a place where birds, together with a few insects and amphibians, are almost all I can hear. If this place sounds of anything it sounds of its birds. It is far from silent, or even quiet, but does seem to have what Krause calls tranquility. 41 Silence, he argues, is not a desirable state but quite disorienting. In silence, as Carson probably realised, it is hard to find a sense of place. Tranquility, on the other hand, is a state between noise and silence in which a kind of endorphin-rich serenity is attained. The only significant disruption to the sounds of birds comes when a plane flies high overhead. I likely flew over this area on my own journey to Australia and so the sound of aircraft is, in part, enabling me to hear this place. But air travel also seems to be contributing to climate change and the increasingly severe summer heat that regularly leads to devastating fires in this part of Australia. The valley is, to use Haraway's phrase, \"full of bumptious life\" but the strains of Anthropocene anxieties and the ironies of global travel still encroach. 42 Mata da Balbina, Brazil: August 2008 43 The Atlantic forest of eastern Brazil has fragmented enormously over the last century. Many of its bird species have disappeared or become extremely scarce and localised. Some are so rare they are almost like ghosts. One such bird is Stresemann's bristlefront, a thrush-sized bird with a long tail and mass of bristles around the base of its bill from which it takes its name. Until the mid-1990s this was a species only known from two museum specimens. It seemed like a shadowy myth of the past but was then discovered fleetingly in the coastal forests before disappearing as soon as it had appeared. Then in the early 2000s it was found again in a fragment of hilltop forest called Mata da Balbina close to the border of the Bahia and Minas Gerais provinces. Here the small population lingered and I was able to visit in 2008, together with another British birder. We arrived at the forest knowing little about the bird and our knowledge of its singing came from a single online recording made the previous year. The field guides that we used had no description of the sound at all, presumably because it was unknown when the guides were produced. After some time searching, a liquid, cascading song rose up from the thick undergrowth. It was soon close enough to make a recording, which we carefully played back, quietly and at long intervals, to try to entice the bird to show itself. 44 The bird we could hear in the forest sounded slightly different to the online recording because it added in an alternating ending to the song. Curiously, it seemed to respond more enthusiastically to the recording we had downloaded rather than the playback of its own song. It overlapped with the recording almost as if in a duet. The lack of scientific information on the singing of Stresemann's bristlefront rather liberated my thinking about what I was listening to; the singing had no \"facts\" to conform to. Almost an hour passed in the midst of the dense tangle of branches before we saw a female creep nervously across the forest floor, its singing encircling us enigmatically. It was a 41 Krause, The Great Animal Orchestra, 216. 42 Donna Haraway, The Companion Species Manifesto: Dogs, People and Significant Otherness (Chicago: Prickly Paradigm Press, 2003), 32. 43 See audio clip #2 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 44 The ethical issues surrounding playback of bird sounds are complex. For further discussion see David Sibley, \"The Proper Use of Playback in Birding,\" accessed 27 February 2015 http://www.sibleyguides.com/2011/04/the-proper-use-of-playback-in-birding/. beautiful but disconcerting experience; I wondered if the recording I made of its song might outlive the species itself. Stewart Island, New Zealand: November 2008 45 The small harbour of Oban is named after a port in the west of Scotland but this Oban is the main settlement on Stewart Island, an hour or two's boat ride from the southern tip of New Zealand's South Island. It's only my second day in the country, after a lengthy flight from the UK. Jet-lagged and weary, I had already had the strange experience of finding myself on the other side of the world, switching from early winter to early summer as I did so. And yet the birds were much the same as at home in Britain, thanks to a range of familiar garden birds that had been brought to New Zealand by acclimatisation societies in the late 19 th century. The impoverished avifauna of the main islands is swamped by these settler birds, most of which are well-regarded songbirds back in Europe. The sense of similarity to home is oddly disorienting. I am, in part, shocked by the familiarity of what I hear. I arrived by boat in Oban and began to hear some of the \"real\" New Zealand birds: the voluptuous warbles of tuis and the raucous gabble of kakas. These exotic sounds contrasted with the more prosaic chatter of house sparrows and the descending chirruping of chaffinches. This mix of the recognisable sounds of home and the new sounds of the Antipodes jarred, both in terms of familiarity and in the quality of sound. A duck flying over was again familiar, although most ducks in the area were a hybrid mix of introduced European mallards and native grey ducks. An oystercatcher and a gull were both familiar sounding too, much like those that careen about the coastlines of Scotland, but these were subtly different native species rather than introduced birds. Human history and the sounds of birds are inextricably bound together in New Zealand. The relatively short period of human settlement has shaped the avifauna and ecology in dramatic and readily perceptible ways. As much as the presence of the birds I hear, I sense the absence of those that have disappeared since Europeans arrived: bush wren, piopio, huia and many others. In Krause's terms it sounded like an ecosystem that has yet to become sonically integrated, with disparate types of sound, meant for different acoustic and ecological conditions, in uneasy coexistence. Aberdeen, Scotland: March 2014 46 A bright morning, just as winter is turning to spring. The winter has been the wettest on record, perhaps an effect of global warming; the switch to mild, dry weather seems to encourage a rush of activity in the botanical gardens. I'm reminded of the hubbub of sounds from the Capertee Valley: a rush of twittering finches, tits, dunnocks, treecreepers, goldcrests and robins. A great spotted woodpecker taps loudly at a trunk and, being near the coast, the wail of a herring gull cuts through the air. While the gardens seem like a green oasis in the city, the 45 See audio clip #3 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 46 See audio clip #4 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with an Olympus LS11. Whitehouse: Listening to Birds in the Anthropocene / 69 distant thrum of traffic is ever-present and soon the stream of birdsong is engulfed by a passing helicopter heading out to serve the offshore oil rigs. These garden and suburban sounds seem homely enough and the vibrancy of early spring exudes from the tumble of sounds. The clashes and contrasts are most obvious with the rumble of traffic, both aerial and terrestrial. I wonder if the birds are adapting their soundings to the ambient conditions, just as great tits are purported to have done in noisy urban areas. 47 Is the loud exuberance of singing a response to the noisy environment as much as the onset of spring warmth? The above descriptions reveal something about the sounds on the recordings and the context of their making. They also reveal a lot about how I listen to birds and how my own Anthropocene anxieties inform how I do this. My listening practices are somewhat specialised, given that listening to birds has been a central part of my life, but there are certain aspects of my own process of listening that I consider to be exemplary of how many people listen to birds in the Anthropocene. Much of what might be deemed distinctive about my own practices derives from a fine-grained knowledge of what kinds of birds I am hearing. Although in some situations I was not entirely familiar with the local avifauna, I could, with a little practice, put a name to many of the sounds. This was a result of careful listening and comparison to recordings but also through visual identification, facilitated by field guides illustrated with systematically arranged iconic images and descriptive texts. 48 But, while I was doing this to a rather precise degree, the naming of what is being heard is common to any narrative of listening. Even if the listener only thinks of the sound as being a bird then they are entering into the symbolic process of relating a perceptual encounter to a linguistic marker. This process of naming enters the experience into other sets of relations. Some named sounds might be redolent of place and time; others might be out of place or exotic. Some will be familiar and expected; others could be strange and unexpected. Like many people, I am astonished by the liveliness of the birds I listen to, but I can also be shocked, both by the strange case of a ghostly bird clinging tenuously to existence and by the familiarity of the birds I hear in another hemisphere. When people name aspects of what they directly encounter, they enter those phenomena into a whole range of other narratives, relations and reflections, which are in turn re-entered into the way one listens to one's surroundings. In the Anthropocene those new relations often lead us back to ourselves and the actions of humans, leading us into listening for discordance, disruption and absence. Even in hearing familiar and comforting sounds there is an anxiety in knowing that these can shift abruptly, or are competing with ever encroaching noise and habitat destruction.",
          "The more we care about our world and the more we pay attention to it, the worse things seem to get. This is a powerful anxiety for many in the Anthropocene. Engaging with and reflecting upon the world is essential to our own sense of well-being, but it brings with it the realisation of our own destructiveness. The more we listen to birds the more we notice the loss of birds from pesticides, the destruction of habitat, the encroaching dominance of Anthrophonic sounds, the sounds that are out of place and the ecosystems that are dissonant. There is nothing new about the way that the semiotics of our listening oscillates between our being-inthe-world and our reflection upon it. The semiotics of the Anthropocene, however, destabilises the groundings of our perceptions and draws our reflections anxiously towards our own disruptiveness. Silence and loss hang heavy in the Anthropocene, but this does not mean that our ongoing relations with birds and their sound-making are not fruitful or that we should desist attending to them to avoid the inevitable unease. These lives are ones that people often hope to resonate with, that is, to attend to them in a way that is ethical, that is aesthetic. As Donna Haraway puts it: \"All ethical relating, within or between species, is knit from the silk-strong thread of ongoing alertness to otherness-in-relation.\" 49 The \"ongoing alertness\" to the sounds of birds that my respondents told me about is just this kind of ethical relating, but it is a relating that is importantly grounded in the same kinds of semiotic processes through which birds listen to their own world. I argue that listening to birds in the Anthropocene should not encourage a separation of human activities from those of birds but should instead ground the development of relations of companionship. It elicits not simply a narrative of encroaching loss and the ever present threat that humans pose to non-humans, but one of enskillment, of how we learn to listen to birds and to the rest of our world and how we learn to make ourselves, however uneasily, at home in it. Andrew Whitehouse is an anthropologist and birder teaching at the University of Aberdeen, Scotland. His work has explored conservation conflicts in western Scotland, landscape and anthropology and, more recently, people's relations with birds through sound. He is co-editor of the volume Landscapes Beyond Land: Routes, Aesthetics, Narratives published by Berghahn in 2012."
        ],
        "ground_truth_definitions": {
          "anxious semiotics": {
            "definition": "Sense of uncertainty and concern over potential human culpability in loss.",
            "context": "There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "eb29476dd81aefedf2896db42f039f003a0ec5bf",
        "sections": [
          "Intense competition from large-scale growers has forced small-scaled farmers to find new niche markets for their commodities through value-added marketing. To discover the right niche market is a complicated task because demand is highly segmented among consumers who may be concerned with different attributes such as local, organic, and other specialty types. A recent survey of 500 Twin Cities consumers showed that consumers buy locally grown produce primarily because of its high quality and freshness (King, 2007). Previous studies that focused on consumers' choices at farmers' markets in other states showed that today's consumers support local farmers' markets because they expect higher quality (for example, freshness and taste) and lower prices (Brown, 2002(Brown, , 2003)). Organically grown produce is considered to be healthy and environmentally friendly because of the use of less-damaging pesticides (Magnusson et al., 2001;Thompson and Kidwell, 1998). For some consumers, premium price is the major obstacle in buying organic foods (Magnusson et al., 2001). Yet, ''locally grown'' produce is not necessarily ''organically grown'' produce. Locally grown produce can be produced using conventional production methods. Organic produce purchased by Minnesota consumers might be from California or New Zealand. Finding out why certain consumers prefer organically grown or locally grown produce and their willingness to pay (WTP) for fresh produce with different attributes is important for stakeholders such as producers and marketers in making their production and marketing decisions because it helps them figure out what type of fresh produce to grow and sell, what to emphasize in marketing efforts, and what are reasonable prices to charge. The literature on consumer preferences for and marketing segmentation of organically and locally grown fresh produce is sparse. Loureiro and Hine (2002) studied consumers' WTP for local (Colorado-grown), organic, and GMO-free potatoes. They found that the ''Colorado-grown'' attribute afforded potato producers the highest consumer acceptance and premium compared with organic and GMO-free potatoes. However, only consumers' attitudes toward potato freshness and nutrition were included in their estimation. Important consumer attitudes toward certain quality attributes such as appearance, taste, food safety, and so on (Yue et al., 2007(Yue et al., , 2008) ) were not included in their study. The traditional hypothetical survey method is preferred by most academic researchers in consumer preference studies. The hypothetical method asks individuals to choose the values they are willing to pay for certain products without asking them to exchange money and real goods. Although widely used, a question about this approach that has been aroused is whether values elicited from hypothetical experiments reflect the amounts individuals would actually pay. It is well documented that consumers are less reluctant to spend money in stated preference studies than in markets with real economic commitment. In the literature, this is known as the hypothetical bias (see, for example, Alfnes et al., 2006). Previous studies also show that there is significant hypothetical bias when consumers' WTP is elicited using pictures instead of real products in hypothetical studies in which participants do not need to pay for the products shown in the pictures (see, for example, Yue et al., 2008). Lusk and Schroeder (2004) found that there is no significant difference between marginal WTP elicited from hypothetical and nonhypothetical experiments when real products instead of pictures are used in both experiments. The objective of this study was to investigate consumers' WTP for organically grown and locally grown fresh produce and the marketing segmentation of these two types of produce. The results of this research are very important for small-scale farmers, market organizers, and sponsoring agencies in making their production and marketing decisions. This study also contributes to the literature on consumers' choice and demand segmentation between organically grown and locally grown produce. In addition, our research is different from previous studies in that we used and compared two methods to investigate consumer preference for and WTP for organically and locally grown fresh produce. We used both the traditional hypothetical experiment and the nonhypothetical experiment with real economic incentives. The use of a choice format is new in the consumer preference and WTP literature. Lusk and Schroeder (2004) developed the choice format with posted prices in different scenarios to investigate consumers' WTP for food quality attributes. Alfnes et al. (2006) used a nonhypothetical choice experiment to investigate consumers' WTP for salmon with different colors. The hypothetical experiment and nonhypothetical choice experiment with real economic incentives give data in similar formats and are complimentary to each other; one gives hypothetical WTP data and the other gives real WTP data. The use of the two methods helps eliminate the hypothetical bias that exists in traditional hypothetical experimental methods and get consumers' real WTP for fresh produce with different attributes.",
          "We tested the data from the hypothetical experiment and the nonhypothetical choice experiment for similarities and used the two data sources in a joint estimation. We tested if the use of real products instead of pictures can eliminate hypothetical bias. The details of both experiments are presented subsequently. Attributes. The fresh produce used for the study was tomatoes because they are widely grown locally and organically in Minnesota. In our experimental design, there were two qualitative attributes (organically grown and locally grown) and one quantitative attribute (price). Each qualitative attribute had two levels-yes or no. The number of levels for the quantitative factor, price, was six. The price ranged from $1.25 to $2.50 with $0.25 as intervals based on real tomato prices in different stores in the Twin Cities area when the experiments were conducted. Table 1 summarizes the levels of attributes. There were four combinations of the two qualitative attributes as shown in Table 2. In our experiment, we named the locally grown (no) and organically grown (no) as ''conventional,'' locally grown (no) and organically grown (yes) as ''organic,'' locally grown (yes) and organically grown (no) as ''local,'' and locally grown (yes) and organically grown (yes) as ''organic + local.'' The tomatoes were purchased from a farmers' market, a supermarket, and an organic farm in St. Paul, MN. Other product attributes such as tomato size, color, shape, size, firmness, and ripeness were not included in the design. However, the tomatoes were of the same type and when we made purchases, we carefully compared tomatoes' color, size, shape, ripeness, and firmness to make them be very similar to each other in these attributes. Therefore, the tomatoes' possible differences in these attributes were controlled and participants said that they were unable to distinguish between them. Scenarios. In the hypothetical and nonhypothetical experiments, participants were presented with a series of pricing scenarios and asked to choose one alternative in each pricing scenario. If none of the alternatives was of interest, they could choose ''none of these.'' To lessen the cognitive burden on participants, only two alternatives were usually included in each scenario in a choice experiment. In our experiments, each scenario had two alternatives described by conventional, organic, local, or organic plus local attributes and price. Because it was not practical to ask each participant to choose from all possible scenarios, a fractional factorial design was developed to minimize scenario number and maximize profile variation. The design was developed based on four principles: level balance (levels of an attribute occurred with equal frequency), orthogonality (the occurrences of any two levels of different attributes were uncorrelated), minimal overlap (cases in which attribute levels did not vary within a scenario were minimized), and utility balance (the probabilities of choosing alternatives within a scenario were kept as similar as possible) (Louviere et al., 2000). The fractional factorial design gave each participant sets of 12 scenarios to evaluate. The clearly dominated alternatives were eliminated. For a further discussion of factorial design, see Louviere et al. (2000). The following is an example of one of the scenarios: Please examine the tomatoes on the table. Consider a situation in which you are buying 1 lb of fresh tomatoes. From the following pairs of alternatives, please choose one alternative that you would like to purchase (you can choose ''none'' if you do not want either of them): Scenario 1-conventional $1.75; organic + local $2.25; none of these. Next to organic tomatoes, we put a USDA Certified Organic label; beside local tomatoes, we put a Minnesota Grown label; and beside locally grown, organic tomatoes, we put both the USDA Certified Organic label and the Minnesota Grown label. Hypothetical experiment. For the hypothetical experiment, participants were not asked to buy anything after they made choices. Real products were used in the hypothetical experiment. Participants were asked to answer a series of questions such as what kind of retailers they were most likely to patronize and a series of questions designed to extract information on their attitudes toward and preference for the organic and local attributes of fresh produce. Nonhypothetical experiment. For the nonhypothetical choice experiment, one of the pricing scenarios was randomly drawn as binding, i.e., the choices made in that scenario would be implemented. Participants would receive the products they chose and were required to pay the prices posted for the corresponding products in the binding scenario. The nonhypothetical choice experiment had the following five steps-Step 1: The experimental procedure was explained to participants and some information about the alternatives (the three attributes) was provided. Step 2: Participants studied the alternatives in scenarios and marked on a choice scheme which of the alternatives in each scenario they wanted to buy. Step 3: After participants completed all scenarios, each participant drew one card with a number determining his or her binding scenario. Step 4: Each participant received the fresh produce he or she chose in his or her binding scenario. Step 5: Participants went to the cashier and paid for their fresh produce. After participants made their choices, they were asked to answer the same set of questions as in the hypothetical experiment. Sampling. A booth was set up at the Minnesota State Fair to conduct the hypothetical and the nonhypothetical choice experiments in Aug. 2008. The Minnesota Sate Fair is the largest U.S. state fair in terms of average daily attendance (Minnesota State Fair, 2008). The sociodemographics of fair attendees are close to Minnesotan sociodemographics except that a slightly higher proportion of those aged between 35 and 44 years attend the Fair who live in Minnesota (Minnesota State Fair, 2008;U.S. Census Bureau, 2004). Therefore, we expected to get a representative sample of Minnesota consumers. State Fair participants were randomly selected to participate in the experiments. Each participant only participated in one type of experiment, either the hypothetical experiment or the nonhypothetical experiment. A small amount of cash or gifts worth $6 to $8 was given to participants to compensate for their time. To ensure that the sample was representative, only one family member per household was allowed to participate in the experiments. Most participants at the Minnesota State Fair came as families. When we recruited participants, we asked the primary food purchaser of the household to participate in the experiments. In total, 365 participants were surveyed with 115 participating in the nonhypothetical experiment and the remaining 250 participating in the hypothetical experiment. Some of the data were not included in the analysis because of one or more missing values. Observations of 343 participants with 110 participants from the nonhypothetical experiment and 233 from the hypothetical experiment were included in the analysis. Statistical model. Consumers' satisfaction derived from choice and consumption of dif-  (Train, 2003). We used the mixed logit model to capture all possible correlations for responses from the same participant and scenario. The statistical model was: where U nis was the utility of individual n from alternative i in scenario s; x nis were vectors of observed variables relating to alternative i and individual n; b was a vector of fixed coefficients; h was a vector of normally distributed random terms with mean zero used to capture the possible correlations; and nis was an identical and independent extreme value error term. The standard logit model is a special case of the mixed logit model where h has zero variance. The density of h was denoted by f (hjV), where V was the fixed parameter vector of the distribution. For a given h, the conditional choice probability of alternative i was a standard logit: Consequently, the unconditional choice probability, P, in the mixed logit model was the logit formula integrated overall values of h with density of h as weights: This integral should be solved analytically and was approximated through simulation (Brownstone and Train, 1999). The maximum likelihood estimation method was used to estimate coefficients with Stata 10.0 software (StataCorp, College Station, TX).",
          "The summary statistics of the sample are shown in Table 3. Approximately 84.51% of participants purchased fruits and vegetables more than once a week and only 0.28% of participants purchased fruits or vegetables less than once a month. These results, derived from the data, indicated that the sample of participants was representative of consumers who are regular fresh produce consumers. The average age of participants was 45 years old. Seventy-three percent of participants lived in urban areas and 23% of them belonged to certain environmental groups. Fourteen percent of participants had children younger than 12 years old in the household, 66% of participants were female, and the average education level was a college degree. On average, there were two to three people in a household and the average income level was $69,000. A large proportion of participants (68%) purchased most of their fresh produce from supermarkets, 27% from a roadside stand or farmers' market, 16% from high-end specialty stores, 11% from cooperatives, 9% from mass merchandisers, and 7% from discount stores. In the questionnaire, we also asked participants how important certain characteristics of locally grown and organically grown fresh produce were to their purchase decision using a Likert scale with 1 being not important and 5 being very important. The percentage of participants who considered different characteristics at various levels of importance are listed in Tables 4 and5. The characteristics were chosen based on previously published studies (Loureiro and Hine, 2002;King, 2007;Yue et al., 2007Yue et al., , 2008)). For locally grown fresh produce, 83% of participants considered freshness as ''very important'' and 72% of them rated safe to eat as ''very important.'' Forty-eight percent of participants thought lower price ''somewhat important'' and 17% of them regarded it as ''very important.'' Supporting the local econ-omy was a good reason for people to choose locally grown fresh produce as indicated by the fact that 74% of participants chose 4 or 5 on the Likert scale. Environmentally friendly was also considered to be important for 61% of participants. Carbon footprint was a concern for 54% of participants, whereas 25% of participants did not regard it as a major concern. For organically grown fresh produce, 52% of participants considered good for health to be ''very important'' and 26% of them thought it to be ''important'' (they chose 4). Forty percent of them regarded lower price as ''somewhat important'' and 42% of them chose 4 or 5, which means that these people thought that lower price was ''important'' or ''very important'' for organic fresh produce. Approximately 83% of participants thought safe to eat as ''important'' or ''very important'' for organic fresh produce and 67% of them considered environmentally friendly as ''important'' or ''very important.'' These results were consistent with previous studies showing that consumers associate organic production methods with a reduced health risk and may choose to reduce the risk from pesticide residues by switching to organically grown products (Magnusson and Cranfield, 2005;Williams and Hammitt, 2001). Recent survey evidence shows that consumers purchased organic foods because they perceived the foods to be fresh (68.3%), better for health, and a better source of nutrition (67.1%; Whole Foods Market, 2005). Furthermore, over 70.3% of the con- The numbers in parentheses were the percentages of participants who belong to the corresponding categories. x Participants were told to choose all that applied. sumers said they bought organic food or beverages to avoid pesticides. To get some idea of participants' past purchases of locally grown and organically grown produce, we asked participants ''When you buy fruits and vegetables, how often do you buy locally grown (organically grown) fresh produce when locally grown (organically grown) options are available?'' For locally grown produce, 14% of participants chose ''always,'' 40% chose ''most times,'' 38% chose ''sometimes,'' and 8% chose ''seldom'' or ''never.'' For organically grown produce, 6% chose ''always,'' 15% chose ''most times,'' 39% chose ''sometimes,'' and 40% chose ''seldom'' or ''never.'' The estimation results of the mixed logit model are shown in Table 6. The P value for the likelihood ratio test was <0.01, which indicated that the model fit the data well. Although some variables have a naturally interpretable metric, others do not, especially ordinal variables and interaction effects (McCall, 2001). Therefore, to simplify interpretation of parameters associated with the quality attributes, variables except organic, local, price, and real and the associated interaction effects among them were standardized with a mean of zero and a SD of one. The standardization was done by subtracting the respective variable's mean and dividing by its SD. We found that the P values for the correlations among the scenarios and the responses from the same participant were pretty large (>0.95). Therefore, the correlations were deemed to be highly insignificant. Price premium for local and organic attributes. The intercept and coefficients of organic, local, price, and organic*local were highly significant. The negative sign of the coefficient of price indicated that the higher the price of the product, the lower the proba-bility that participants would be likely to choose the product. The intercept divided by the coefficient of price is the consumer WTP for 1 lb of conventional tomatoes and the amount was $1.18. The coefficient of organic divided by that of price is the premium the consumer was willing to pay for 1 lb of organic tomatoes, which was $0.72. Similarly, the premium for the local attribute was $0.73. The premium for locally grown, organic tomatoes was not the sum of the two premiums, which was indicated by the highly significant negative coefficient of organic*local. Instead, the premium was $1.16 (the sum of coefficients of organic, local, and organic*local divided by the coefficient of price). Hypothetical bias. The marginal significant coefficients of real*price (P = 0.09) indicates that there might exist hypothetical bias for the WTP elicited using the hypothetical experiment compared with the nonhypothetical choice experiment. By calculation, we found that the hypothetical bias for organic attributes was 7.5% of the premium elicited using the nonhypothetical choice experiment. The hypothetical bias for the premium of the local attribute was 9% of the premium elicited using the nonhypothetical choice experiment. Relative to the $1.16 premium for the local plus organic attribute in the hypothetical experiment, participants were willing to pay a premium of $1.07 for 1 lb of locally grown, organic tomatoes in the nonhypothetical choice experiment. In this respect, our results are in line with the large literature on hypothetical bias in valuation studies. Previous studies have shown that the difference between the real and hypothetical study is not fixed with the changes in WTP (Yue et al., 2008). Some previous studies have found that hypothetical bias is quite high when pictures are used in a hypothetical experiment compared with a nonhypothetical experiment using real products (see, for example, Alfnes, 2003;Harrison and Rutström, 2005;Yue et al., 2008). In contrast, we found that hypothetical bias was not that high when real products were used in the hypothetical experiment. The results indicated that even if hypothetical bias might not be completely eliminated as found by Lusk and Schroeder (2004), it can be significantly decreased when real products were used to elicit consumer WTP for certain attributes in a hypothetical experiment. The findings have very important implications for future studies on consumers' WTP literature. Nonhypothetical experiments should be used; if the employment of a nonhypothetical experiment is not practical, real products rather than pictures of products should be used to lower any possible hypothetical bias. Sociodemographic background of consumers. We found that consumers' sociodemographic backgrounds affected their choice and WTP for conventional, organic, local, or locally grown, organic tomatoes. The interaction effect between age and organic was significant and negative, which indicated that older participants were less likely to choose organic tomatoes compared with younger participants. Older participants were more likely to purchase local tomatoes compared with younger participants, but the coefficient was not highly significant (P = 0.13). The significant and positive coefficient of income* organic meant that wealthier participants were more likely to purchase organic tomatoes. This result was in line with other studies on organic attributes of fruits and vegetables (see, for example, Yue et al., 2008). The coefficient of gender*local was significant and negative, which means that female participants were more likely to purchase locally grown tomatoes compared with male participants. In the questionnaire, we asked participants if they belonged to any groups associated with the environment. The estimation results showed that those participants who belonged to environmental groups were least likely to purchase conventional tomatoes and were most likely to purchase locally grown tomatoes followed by organic tomatoes. These results were similar to those by Yue et al. (2007), in that consumers who belonged to environmental groups were more likely to purchase organic apples. Participants' educational backgrounds affected their choice of tomatoes with different attributes. Participants with higher educational levels were more likely to purchase conventional tomatoes and less likely to purchase organic tomatoes. Participants who live in urban areas were most likely to purchase organic tomatoes followed by locally grown tomatoes and were least likely to purchase conventional tomatoes compared with participants from rural areas. One possible explanation is that consumers who live in urban areas may be less likely to grow fresh produce than those who live in rural areas. People experienced with growing products know that there is not as large a quality difference between locally produced fruits and nonlocally produced fruits compared with those who have no growing experience (Yue et al., 2007). Hartman (1997) also showed that the consumption of organic food was skewed in urban areas. We found that household size did not significantly differentiate participants in their choice of tomatoes with different attributes. However, participants with children younger than age 12 years had a higher chance of choosing locally grown tomatoes even if the coefficient was not highly significant (P value was 0.13). Also, these participants were more likely to choose organic tomatoes and less likely to choose conventional tomatoes as indicated by the positive sign of the coefficient of child*organic and the negative sign of the coefficient of child, even if the estimated coefficients were not statistically significant. Store categories. The estimation results showed that participants bought different types of tomatoes from different types of stores. Participants who purchased most fresh produce from mass merchandisers were most likely to choose conventional tomatoes, less likely to choose locally grown tomatoes, and least likely to choose organic tomatoes. Participants who purchased fresh produce mainly from cooperatives had the highest probability of purchasing organic tomatoes, followed by locally grown tomatoes, and they had the lowest probability of purchasing conventional tomatoes. Participants who bought the most fresh produce from specialty stores were more likely to purchase organic tomatoes and less likely to purchase locally grown tomatoes, but the results were not statistically significant. Those participants who often patronize supermarkets were most likely to purchase conventional tomatoes, followed by organic tomatoes, and they were least likely to purchase locally grown tomatoes. Participants who visited discount stores most often were less likely to purchase organic tomatoes. Those participants who often patronized roadside stands and farmers' markets were most likely to purchase locally grown tomatoes, less likely to purchase organic tomatoes, and least likely to choose conventional tomatoes. Marketing implications. These results have important marketing implications for fresh produce growers, wholesalers, and retailers. A large proportion of consumers consider ''freshness'' and ''safe to eat'' as ''very important'' for purchasing locally grown fresh produce, which indicates that these attributes should be stressed by local growers when they promote their products in advertisements, newsletters, and so on. As for organic fresh produce, ''good for health'' and ''safe to eat'' are their selling points and should be emphasized in promotional materials. There exists a group of consumers who view ''lower price'' as important for local and organic fresh produce, which was consistent with findings of previous studies that premium price was the major obstacle to buying organic foods for some consumers (Magnusson et al., 2001). To promote to this price-sensitive segment, emphasis on ''affordability'' by decreasing the price premium to a certain level will work to increase sales of organic and local fresh produce. The sociodemographics of consumers who choose conventional, local, or organic attributes have important implications for choices of target markets for fresh produce with differentiated attributes. Older female consumers with lower than average education levels who shop in urban markets are the right target market to promote locally grown fresh produce, whereas younger consumers with above-average income who live in urban areas are the main target market for organic produce. The average price premiums for organic, local, and organic plus local attributes of tomatoes were $0.67, $0.67, and $1.06, respectively, compared with conventional tomatoes after correction of hypothetical bias. These price premiums give growers guidelines when they decide what prices to charge for tomatoes with different attributes. Comparing prices that growers, wholesalers, and retailers actually charge with these premiums can provide these parties with information about whether their charged prices were consistent with consumers' perceptual values of these different attributes. Based on this information, the associated parties can make adjustments on their pricing strategies to attract more potential consumers. Finding out these price premiums can also help growers choose the right production methods and profitable value-added attributes to promote by comparing the premiums with the associated production and marketing costs. Interestingly, we found that the average premiums consumers were willing to pay for organic tomatoes and locally grown tomatoes were about the same. However, the profitability of the two types of produce might not be the same as a result of variation in production costs. Local growers can get a higher premium if they grow organic tomatoes and promote them as both locally grown and organically grown instead of organically grown alone in local markets. The results show that consumers actually patronize different stores to buy fresh produce with different attributes. Therefore, growers and wholesalers need to choose the right stores to sell their products, and retailers need to source the right types of produce for these parties to make profits. Mass merchandisers and supermarkets are the major places consumers purchase conventional produce, which means that conventional fresh produce growers need to choose these retail outlets as their major venues to sell their products. Cooperatives are where most consumers purchase organic produce, indicating that organic fresh produce growers might be able to sell more of their products and can charge a relatively higher price if they choose to sell their products at cooperatives. In contrast, discount stores and mass merchandisers where people expect to get low-priced products are not the major place for growers to sell organic produce. Supermarkets might be another choice for selling organic fresh produce. For locally grown fresh produce, consumers are most likely to purchase them from farmers' markets and roadside stands. These direct marketing channels are the best venues for local growers to sell their products. Cooperatives are also good venues to sell locally grown products, whereas mass merchandisers and supermarkets might not be able to gain enough profits by selling locally grown fresh produce. From the supply perspective, the availability of organic and locally grown produce in different stores also contributes to consumer patronage of these stores in purchasing fresh produce with different attributes. Fresh produce provides the greatest chances for direct sales to consumers through farmers' markets and roadside stands that are commonly used by small-scale growers. Given the fact that most local growers are small scale in Minnesota, it is not surprising that consumers are most likely to choose direct marketing channels to purchase locally grown produce. The growth of organic food has given rise to new marketing outlets in addition to traditional organic outlets such as cooperatives. Mass market groceries and chain stores entered the organic market place at a time of low prices for commodities at the farm gate. However, it has been documented that approximately one third of organic products are sold through mass market grocery stores with a disproportionately higher percentage of processed food and dairy products instead of fresh produce (Klonsky, 2000). In recent years, some mass merchandisers like Walmart started to source and advertise organic fresh produce and they could get a significant market share if they are able to keep prices low enough (Business Week, 2006) The hypothetical and nonhypothetical experiments were conducted at the Minnesota State Fair. One issue to consider is whether the results are representative of consumers in general (Mitchell and Carson, 1989). In our survey, the sample chosenthose who participated in the Minnesota State Fair-does not necessarily represent all fresh produce consumers in the United States, and in this regard, there is some population choice bias. The ''often'' variable indicated that most of participants were regular fresh produce consumers, so we expect the sample selection bias to be minor. We thus view our sample respondents as representing a population of the central Midwest instead of representing the total U.S. population. Given the potential biases, the extrapolation of our findings to other populations should be made with caution. We interpret the findings relative to the population attending local and regional fairs and markets.",
          "To assist small-and medium-sized growers who want to capitalize on growth in demand for differentiated produce, this article presents consumer WTP for the organic, locally grown, and locally grown organic attributes, the characteristics consumers perceive as important when they make purchase decisions of these differentiated fresh produce, and the various types of retail venues where consumers purchase these different products. A combination of the traditional hypothetical experimental method and nonhypothetical choice experiment was used to investigate consumer preference for valueadded fresh produce. The combination of these two methods can help eliminate any possible hypothetical bias in the hypothetical experimental method. We found that hypothetical bias existed using the hypothetical experimental method, but the bias was not high if real products were used in that survey. We elicited consumer WTP for local and organic attributes for fresh tomatoes. We found consumers perceived various characteristics as important for their purchase decision of organic and local fresh produce. Consumers' sociodemographic backgrounds affected their choice of fresh produce with different attributes and consumers patronized different types of stores to purchase conventional, organic, and local fresh produce."
        ],
        "ground_truth_definitions": {
          "hypothetical bias": {
            "definition": "the difference between what people say they will pay and what they would actually pay",
            "context": "We found that when real products were used in the hypothetical experiment, the hypothetical bias (the difference between what people say they will pay and what they would actually pay) was not high.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "304b9778edf07175e6964dfa3f37cacec6006197",
        "sections": [
          "Airports are symbols of mobility. They have become emblematic of our post-modern world. Yet in the early days of airports, there was no such thing as the rivers of passengers that flow through the sculpted steel and glass façades of contemporary terminal buildings. The airport consisted of a few tents and an airfield, remnants of air travel's military beginnings. Airports now cater for the millions of people who flow in, out and within countries each year. The huge mile long terminal structures are carefully built to facilitate the mobility of passengers, baggage, and cargo to their destination. To ensure the security of the country the flows are entering, while at the same time to protect the very means of their travel, these flows must be watched and controlled. The airport is well and truly a space under surveillance. And yet, the airport's role in surveillance has gone largely unnoticed by surveillance studies, reflecting the invisibility of these transient 'non-places' within the social sciences (see Crang, 2002;Gottdeiner, 2001;Rosler, 1998;Augé, 1995). Nonetheless, since the TWA flight crash in 1996 and the more recent September 11th attacks, countries such as the United States are pushing through increased measures to improve airport security and safety, launching airports into the public consciousness. Security itself has become an all-encompassing term, spreading into the current vocabulary. Widespread gun crime at schools has highlighted the 'airport-style security' measures of metal detectors and baggage checks that are now common place (BBC News Online, 1999). This paper explores several aspects of airport surveillance. In themselves, airports offer interesting case studies of how mobility may become watched and controlled. The heavy government presence in these spaces, the possibility for the invasion of privacy and the disregard of human rights must be questioned. We may also learn useful lessons at airports. For as recently discussed by Lyon (2003) airports may be seen as \"possible microcosms of wider societal surveillance trends\" (20) signposting possible surveillance developments in other spaces and places. In this paper, I shall therefore explore a number of the most recent surveillance techniques for monitoring and controlling passengers at the airport. These practices are argued to closely match designations of a 'panoptic sort' (Gandy, 1993) or surveillant sorting (Lyon, 2002b) from the surveillance studies literature. I discuss the implication of these techniques in terms of privacy issues and discrimination. In-turn, the second section to this paper examines the surveillance of non-human mobilities at airports. Informed by an actor network theory (ANT) approach and recent turns towards the agency of 'objects' and 'things', the paper provides a brief over-view of the surveillance of baggage, possessions and aircraft -indicative of a lack of research in this area and a trend towards the surveillance of non-humans. Firstly, let me contextualise this discussion within current turns towards mobility and security. I examine the theorisation of mobility within surveillance studies and trends towards surveillant sorting, before moving on to illustrate the increasing need for an ANT approach towards the subjects of surveillance.",
          "Recent progress towards the understanding of movement within surveillance studies probably reflects the current interest in mobility from disciplines such as sociology (Urry, 2002;2000), geography (Cresswell, 2001;Thrift, 1995) and cultural studies (Morris, 1988). This is no-doubt indicative of the increasingly mobile world in which we live, and must also owe its momentum to the popular fluid and mobile thought of philosophers such as Gilles Deleuze, Felix Guattari, Ian Chambers and Paul Virilio. This new mobility paradigm has moved beyond static idealizations of society towards theories that are marked by terms such as nomadism, displacement, speed and movement. Mobility has become as central as the tenets of society, space and power that fill much of our thinking. In many cases, mobility has often been understood to be an escape from a social order and as an act of resistance (Cresswell, 1993). De Certeau's (1984) tactics and Deleuze and Guattari's (1988) nomad-power transgress the fixity of the state. Gypsies and travellers cross physical boundaries and so become recognized as 'out of place' by authorities and local residents (Sibley, 1995). In many cases, physical mobility is often seen to signify the metaphorical movement from, and transgression of social norms. Mobility is therefore often viewed in terms of risk to the safe and static containers of space, territory and social order. Bauman's (1993) stranger and vagabond provide the perfect figures of risk to security as spatial and social boundaries are crossed. This is not to argue that mobility is always taken as deviant. Obviously, tourists and business travellers do not necessarily contradict social norms. Taking a relational (Cresswell, 1996) view of mobility allows us to see movement merely injected with meaning in different socio-spatial circumstances -it means different things, to different people, in different places and at different times. Therefore, where a tourist may be welcomed into a country a migrant may not be. However, the spaces at which mobility may be channelled provide the most obvious points for these social differentiations. Henceforth, it is at the borders, at airports -where movement and distinct spatial boundaries coexist -where undesirable mobilities may be distinguished from the desirable. This is increasingly achieved by surveillance. Surveillance is one method used to control mobility. As the growth of information and communication technologies allow data packets to be transferred between databases and other surveillant technologies, surveillance itself has become more mobile (Lyon, 2002b). For Lyon andothers (2002a, Phillips andCurry, 2002) however, surveillance may also struggle with mobility by stasis. What Lyon describes as the 'phenetic fix' characterizes surveillance techniques that attempt to capture the essence of movement -taking a snapshot of movement, bodies and identities. Information abstracted from moving bodies may be appropriated and fitted into neat categories and definitions. In this paper, I want to examine a particular form of the 'phenetic fix' labelled 'surveillant sorting' that is probably most illustrative of airport and border surveillance. Surveillant sorting is often used to describe the formation of those categories and profiles discussed above. Gandy's (1993) term the panoptic sort being another terminology. Issues surrounding sorting emphasise the possibility for discrimination -treating some better than others. Surveillant sorting is also able to distinguish the mobile. For example, studies have illustrated that the transfer of data packets may become shaped and prioritised, some given quicker speed and access than others (see: Graham and Wood, 2003). Mobile sorting may also work materially. Cars travelling through the Canadian-US border (Sparke, forthcoming) are put into categories of 'good' and 'bad' by the NEXUS system, the good are able to pass through quickly, where 'bad guys' are forced to wait in long queues for questioning. At airports, surveillance, rather than examination, is becoming the key method with which to distinguish passengers (Salter, 2003). In this paper, I examine a number of issues surrounding these procedures. And yet, I also want to explore a form of sorting that has gone on within the wider study of surveillance itself and which airports may be used to highlight. By this, I mean that there seems to be a tendency to limit our understanding of mobility to humans -to sort the human from the non-human. This paper could examine the mobility of passengers passing through airports and the associated methods of surveillance and control. However, to do this would be to ignore the orbiting movement of non-humans that are also subjected to the airport's controlling gaze. It is in this respect that we may turn to Actor Network Theory (ANT), the work of Bruno Latour, John Law and the recent veering of the social sciences towards the role of objects (Pels et al., 2002), things and their mobility (de Laet, 2000). These approaches are somewhat absent from surveillance studies as noted by Wood in a past issue of this journal (Wood, 2003). Wood observes Kirstie Ball's paper (2002) to be one of the only contributions to such a debate. Ball's approach pinpoints the elements of surveillance systems that \"are all socio-technically perpetuated through interactions of artefacts, technologies and people\" (586). Therefore, whilst humans and non-humans may form networks of the elements of surveillance, little emphasis has been placed upon these aspects of the subjects of surveillance. Examining how surveillance deals with mobility need not be limited to the mobile human, but extended to include things, and indeed their amalgamation with people (Haraway, 1991;Urry, 2002). Such a shift in focus is particularly pertinent when we turn to the increasing concentration upon the means of terrorism by governments and borders -shifting surveillance away from the perpetrators. Carter (2001) states that governments need better capabilities for the detection and prevention of terrorist activities. He argues that: This involves surveillance of persons and motives -a delicate matter -but also surveillance of potential means of destruction such as crop dusters, germ cultures and pilot instruction. Surveillance of means raises far fewer civil liberties issues than does surveillance of persons, and it might be much more effective. (15) The surveillance of the means of destruction is a much more attractive option for security. Governments take the view that a bag will not mind if it is x-rayed, CT scanned, and rummaged through. Examining the subjects of surveillance can enable our better understanding of the purpose, workings, and impacts of these systems. This approach is discussed in the final section. Firstly, it is useful to situate these practices within the development of air-travel, the growth of airports, and the rising need for airport security.",
          "The first airport resembling anything that we know today was built in 1922 at Königsberg in East Prussia (Zukowsky, 1996). Königsberg's terminal was a considerable advance from the windswept military landing fields and beaches. The new terminals became bridges for the transition between land and flight. Airports captured the imagination representing a symbol of progress and technological achievement in modernity. Indeed, the driving ambition of Hitler's Nazi regime was manifest in the rebuilding of Berlin's Templehof airport -the new airport reborn from the ashes of the failing past government and decaying economic system (Braun, 1995). From these early beginnings, airport space was a segregated and exclusionary one, being inhabited only by military personnel or the wealthy. Even before the First World War, aviation became an impossible dream fulfilled. Higher society frequented airfields to watch the displays of macho pilots and their flying machines (Wohl, 1994). Flights became social and cultural gatherings for the upper classes, excluding the lower. Soon, the development of the vacation and package holidays saw the growth in trips to Europe; to Spain and Portugal and the beginnings of mass transatlantic travel (Löfgren, 1999b). International air transport became less costly and connected to many destinations. Wide-body jets meant that 200 to 500 people could be carried by one flight eliminating the extortionate costs involved. The further deregulation of the American and British airlines saw airlines multiply, stimulating the competitiveness of seat prices, and also a greater choice of destination for many travellers. These changes also meant radical developments in the complexity of running airports as airlines rapidly grew and folded. Although, the growth of mass travel was still unable to fully democratize this space. VIP lounges developed for business and frequent flyers to enjoy the quiet luxury of a comfortable seat, the amenities of television, and most recently a shower or sauna. The progression of the airport culminated into the enormous multi-terminal, multi-runways sites that see millions pass through each year. They soon became airport-cities employing the work force of urban populations. Manuel Castells (1996) describes airports as a 'space of flows', nodes on the global networks of mobility. But the terminals are also built for a carefully controlled internal mobility. David Pascoe argues that the terminal was \"predicated on confluence, the precisely timed pumping of passengers into and out of [space]\", airlines then \"constructed specially designed terminals to control and direct the masses of passengers…\" (2001: 226). For as passengers increased so did the potential dangers and security risks. This was due partly to the function of airports as borders, but also to the nature of international terrorism that has developed over the past three decades. Perhaps Paul Virilio (1997) is correct where he writes that the border is now at the centre of our cities. Airports act as border zones to the vertical vectors of mobility that cross national and state boundaries. For this, Airports must function in a similar way to the borders that police the boundaries of our countries, regulating the movement of people that enter and leave. For Virilio, \"the doors to the city are its tollbooths and its custom posts are dams, filtering the fluidity of the masses, the penetrating power of the migrating hordes\" (1986: 7). Airports must therefore function as a screening or filter for the threats to a nation, but airports also become screens for quite different threats. Since the 1970's terrorism has grown, looking towards the aviation industry as a target. On the 22nd of July 1968, an El Al flight destined for Tel Aviv was hijacked. The hijacking was a bold political statement where passengers were to be traded for Palestinian terrorists imprisoned in Israel. By attacking the Israeli airline they were effectively attacking the Israeli state (Hoffman, 1998). In addition to this, the intense media coverage given to the hijacking saw the event publicized all over the world. Aviation became the perfect target for terror, which since 1968 has endured a rapid rise of these attacks. Planes were spaces that could be controlled easily; the fear of crashing subduing any passenger resistance. Airports also offered limited surveillance of the throngs of people that were travelling. The likelihood of a successful hijack has even been calculated, at 76% it is an obvious incentive for terrorists (Merari, 1998). Due to the increasing risks from terrorism and illegal immigration, airports had to find a way to take control of the airport space. Developments of airport security and surveillance were initially felt through actual changes to the space -to the architecture of the airport. John Zukowsky notes that, \"Intensified security measures changed the planning of airports, deliberately cutting up the open flow of space \" (1996: 15). The development of the sterile lounge concept (Wallis, 1998) saw baggage and security checks made before entrance to the gate. This removed the chance of threatening objects reaching the plane by person. Security checks could then be completed in a purpose built room far away from the gate to reduce queues disruptive to passengers. These techniques are still regarded as the preferred method, evident in the centralized security 'choke points' in most terminals (NRC, 1999). Surveillance has become therefore, one of the primary means of ensuring that airports are made safe and secure. 'Threat vectors' (NRC, 1999) that is, the paths by which threats may enter the airport and find their way to a plane are identified and monitored. In the next two sections, a number of these approaches and their implications are discussed.",
          "Monitoring passengers equally, proved to be far from the truth of running an international airport (Jenkins, 1998). Airport authorities needed a way of putting passengers under surveillance without having to examine every passenger rigorously. For this, methods of passenger screening and profiling have been developed to effectively sort the most probable threat to security from other passengers.",
          "The most recently publicized and perhaps most worrying form of surveillant sorting at airports has come through a surveillance technique known as profiling. Profiling is the ability for information or data about an individual to be built up. People may be sorted into profiles of particular consumer groups. The psychological profiles crime investigators use is an obvious example. Profiles are then used to predict a person's likely behaviour or the likely characteristics a criminal may embody. Profiling also usually relies upon vast quantities of information gathered about someone that are then stored and shared. In light of the growth of international terrorism, a new form of profiling has been developed called Computer Assisted Passenger Pre-Screening (CAPPS) in the United States. It is possible to discuss here some of the issues surrounding profiling and its potential impacts. Profiling was introduced in the US on the recommendations of the Al Gore led White House Commission for Aviation Safety and Security following the TWA flight crash of 1996. The Commission found that, \"passengers could be separated into a very large majority who present little or no risk, and a small minority who merit additional attention\" (1997). The rationale behind profiling is then to concentrate upon the minority -those likely to be a threat, rather than the majority, which requires much greater resources. The recommendations were based upon the practices of the airline El Al who used profiling techniques to fit the descriptions and likely behaviour of terrorists to passengers. CAPPS is reported to work by matching likely terrorist behaviour with present airline flight information. A flagged 'selectee' will then be subjected to personal checking of possessions and perhaps an interview and questioning. The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\" (1997). Problems are also clear over the secrecy of these profiles and the categories passengers become inadvertently pushed into. The lack of control passengers have over these profiles has led to concerns that innocent travellers may be unable to rid themselves of the 'selectee' status of their sorting. The repercussions of September 11th have seen the reorganisation of US airport security under the Transport Security Administration, itself under the newly formed Department for Homeland Security. Bolstered by this restructuring of governance, President Bush's developing and renamed Terrorist Information Awareness Program and rumours of the testing of CAPPS II have caused concern throughout privacy awareness groups. Posted to the US federal register on January 15th of this year, the TSA introduced a new system of records and amendment of the Privacy Act known as \"Aviation Security Screening Records\" (DOT, 2003) designed to facilitate the CAPPS II system. The proposal was open to comment and provoked a massive response from individuals and privacy awareness groups. The prime concern behind this system is again the discrimination and segregation of passengers, and also the sharing of passenger information between multiple government agencies. However, not only the sharing of Personal Name Records (PNR) are at stake (see Bennett, 1999), but the vague details of the report does not count out the possibility that more detailed banking records, tax histories and other sources of information may become easily passed between airports and other state departments. Fortunately, in Europe, different rules apply. In March 2003, the European Parliament moved to reverse an agreement made between the European Commission and the United States that would have forced PNR records to be transferred from European airlines to US airports. Another version of CAPPS II was posted to the Federal Register on August 1st with several revisions made upon the January version. At present, however, the majority of concerns lie in the current testing of CAPPS II, particularly with regard to the JetBlue controversy. The popular airline JetBlue illegally transferred a large number of passengers' records to an agency working for the US government (Singel, 2003). The information was to be used for the testing of passenger profiling on internal flights. Some of this information became public and was published to the Internet. A similar scheme has also been argued to be taking place through Galileo -the computer reservation system (CRS) that runs a large part of air travel reservation in the US.",
          "Other forms of surveillant sorting are occurring through biometric technologies. Biometry is the measurement of the body. Biometrics effectively treats the 'body as text'; identifying specific body parts such as the iris, face and palm signatures to identify the individual (see Ploeg, 1999Ploeg, , 2002)). The use of biometrics extends far beyond air travel, forensic evidence such as fingerprints are commonly used to identify suspects. Nevertheless, biometrics has also become popular at certain US and European airports. Biometrics work by containing detailed records of a passenger's body information (Delio, 2003), the most popular being retina patterns, but this may also include finger prints, DNA matching and face recognition (Agre, 2001) to name but a few. This information and passengers' identities are then stored on cards or a central database to be compared. Biometric systems are usually referred to as authentication systems, where a positive match to the stored information authenticates the identity (Clarke, 2003). However, other biometric systems, referred to as identification systems, compare the captured body data to large amounts of records kept on other databases. Here, passengers are not proving their identity; rather, airport authorities are identifying them. Biometric systems are of obvious concern to proponents of privacy rights, the critique and defence of biometrics coming from several different philosophical perspectives of technology and human agency (see van der Ploeg, 2003). One particular view is to critique biometrics for their treatment of passengers as objects. Much like a piece of baggage to be identified by a bar code, passengers' bodies offer similar bar codes to be read by authorities. Objections have also been raised over the penetration of bodies as biometric systems scan passenger identifiers. Others m ay analyse biometrics from the perspective of how the information is stored, particularly over the ease by which data may be shared with other 3rd parties. The possibility for data to be hacked and misused by external sources is another issue. Irma van der Ploeg (2003) discusses problems over the security of the American immigration service INSPASS authentication scheme at Los Angeles airport. Recorded hand geometry data was designed to be stored only on the card carried by the passenger. However, van der Ploeg illustrates that the card, if lost, would be quickly replaced by INSPASS thereby revealing the storage, somewhere, of this supposedly private information. But biometrics also has several surveillant sorting implications. For example, we can look to the effects upon the movement of passengers. Schiphol airport, Amsterdam has a fully working biometric system developed by the airport and the Dutch immigration service. The 'Privium' scheme is an 'authentication' scheme that, for 90 euros, allows enrolled passengers to bypass busy queues and check in delays. Passengers are then sorted into those enrolled by the scheme and those not. The logic behind these systems is similar to profiling in that it then gives more time for additional security measures to be placed upon those not enrolled in the biometric scheme. The systems therefore virtually sort passengers' information but also work to materially sort passenger's mobility within the terminal. The business 'kinetic elites' (Graham and Marvin, 2001;Andreu, 1998) may pass through to the VIP lounge at speed. And yet, the average traveller, forced to park long distances from the terminal, is excluded from member's club lounges and has to endure waiting in lengthy 'check-in' and security queues.",
          "Finally, the sorting of passengers has also begun with the introduction of what can be known as digital surveillance, or most commonly algorithmic surveillance (see Norris, 2002;Graham and Wood, 2003). Of course, CCTV has become one of the predominant modes of surveillance within airports as well as within cities, shops and indeed on my own university campus. The ability for CCTV to pick up deviant and threatening behaviour has been well documented, as well as its use in identifying suspects. Within airports, its continued and extended application has become a clear priority of airport security concerns. In the UK, the John Wheeler (2002) report on airport security suggested that valuable improvements have been made in this area. A technology now widely used at airports in the United States is a system called Exit Sentry developed by Cernium. Exit Sentry is able to monitor the direction of movement of passengers walking through the exit corridor of secured areas of an airport terminal such as arrivals. A passenger walking the wrong way, trying to enter the secured area through the exit corridor is warned with a flashing light. If the suspect then persists, a siren alerts security staff, and a recording of the suspects' movements is made. Systems such as Exit Sentry are exemplary of recent algorithmic surveillance technologies that in real time analyze CCTV footage of spaces. These systems understand the differences in movement of individual passengers, and may filter out static background information. Threats are not identified by a particular property of an object; rather, particular movements are inscribed with meanings of what is an allowed movement and what is considered suspicious and deviant. The uses for such technologies have been suggested to recognize the movements of car thieves and even people contemplating suicide at quiet train platforms (Norris, 2002). Indeed, research in progress at Southampton University has developed an approach called gait recognition that may be able to identify the identity of individuals by their distinctive walking styles. CCTV algorithmic surveillance effectively sorts and differentiates between mobility, in Exit Sentry's case, that is accepted (non-threatening) and that is unacceptable (threatening). The meanings are not essential to the mobility picked up by the cameras; rather the algorithms are imbued with these meanings. Sorting objects I now move beyond my discussion of airport surveillant sorting, towards a sorting of a different kind. Here I want to discuss the non-humans that have become placed under surveillance at the airport. As I have discussed airports are obviously places of human mobility. And yet, objects and things also intersect these spaces. Baggage flows through the baggage systems miraculously arriving at our destination. We carry on duty-free, cigarettes, and alcohol. But we are only aware of a tiny spectrum of the surveillance systems that place objects under this scrutiny. Initially, the sorting of objects and things at airports has occurred through the physical arrangement of airport spaces. For instance at the re-building of Templehof, the architect Ernst Sagebiel made the innovation of separating passengers, goods and baggage onto different levels of the airport (Braun, 1995). In addition, freight could arrive by an underground train opposite to the subway from which passengers arrived. This form of sorting is still used today, as automated baggage handling systems operate behind the holes in which our luggage disappears.",
          "The monitoring of our luggage at airports is probably one of the most visible methods of surveillance. Contaminating or illegal goods must be stopped. Airports are also nervous over the possibility of explosives and other weapons that may be used for terrorist activities. The identification of these objects is therefore one of the prime duties of airport security and immigration control as they concentrate upon 'threat vectors' (NRC, 1999). These objects may be identified and put under surveillance in a number of ways. If we first take hand luggage, the classic examples are probably the phase induction (PI) metal detection systems (Jenkins, 2002). Memorable scenes in films depict passengers unable to pass through these archways due to a belt or keys. At this point in a person's journey, hand luggage is usually x-rayed. X-ray machines are able to look inside a person's belongings, where operators search for suspicious looking objects, these include explosives, arms, and organic material. Although these x-ray systems, according to FAA research, could also be used upon passengers (NRC, 1996). X-ray operators could easily identify passengers carrying concealed weapons, without having to resort to personal body searches. Objects such as metals and sharp objects become a threat when held by a passenger. They have the potential to become a risk to the plane or the airport. The agency of a threat becomes possible because of the stable network of actors such as a knife, a passenger and indeed a plane. However, objects may take the shape of a threat independently of a person. For example, timer device explosives may be set in advance or barometric sensor devices respond to changes in pressure. Much like Latour's (1999) example of a sleeping policeman (speed bump), where the agency of a policeman to deter speeding becomes deferred onto the speed bump, here, the terrorists will or agency is given to the bomb. It is this deferred relationship that has led airports to implement baggage reconciliation: Positive Passenger Bag Matching (PPBM) recommended by the White House Commission (1997) in the United States. These systems are designed for the possibility that terrorists are not willing to blow themselves up when detonating a bomb. The terrorist would then check their bags onto a flight without actually boarding the plane. The PPBM system automatically flags up the bag that has been put onto the flight without an owner, the bag may then be pulled from the flight. Such a system relies upon the ability of the airport to know where a passenger's luggage is, through most recently, radio frequency (RFID) tagging and even individual trays that a bag is placed onto. Baggage handling systems most commonly use bar code technology however, so that baggage can be read to determine the identity of its owner and the flight it should be on. Surveillance technology has necessarily become more intense in an attempt to secure aviation from these objects. It is also much easier for airports to scan checked baggage than to actually interrogate a human subject. And yet, similarly to passenger screening, airports have not had the staff or equipment to check every person's bag. As such, the scanning of baggage has been linked to the CAPPS profiling systems so that a 'selectees' luggage is put under increased surveillance and identification for threats. For example, the Explosive Detection Systems (EDS) were initially used to scan 'selectee' passengers' bags for traces of explosives. Here, a 'selectee's' baggage is that scanned by the EDS machines. Since the events of 9/11 efforts have been made to increase the installation of EDS machines to ensure every passenger's bag is scanned, this has culminated in the International Civil Aviation Authority (ICAO) recommending that 100% hold scanning be made by 2006. Other systems use the computer tomography (CT) scanners used in hospitals. A slice or tomography of a bag can then be used to calculate the mass and density of materials that are then matched against explosive and hazardous materials.",
          "Perhaps the object we are most conscious of at airports is the passport. Although, considering the passport's importance to international travel there has been very little written about it in the social sciences (for an exception, see: Torpey, 2000;Caplan and Torpey, 2000;O'Bryne, 2001;Salter, 2003). And yet, the passport is one of the primary tools of states for the surveillance of their population's movements. For Torpey (2000), passports provide the means to govern a population's movement, 'penetrating' the individual to 'embrace' populations. Passports are used to identify a person, making a person legible to the state who may then enforce their authority over movement. Passports are also symbols of nation-states and our allegiance to them (O'Byrne, 2001). Paul Fussell's (1982) Abroad typically articulates the standardization that passports impose upon the traveller: where do people belong, where do they live, what is their hair colour, eye colour. All these aspects of our selves that we continually negotiate and question become squashed into the tight categories of the passport. For Löfgren (1999a): \"as a traveller you now had to live up to your passport identity to be able to prove your identity\" (19). This is nowhere more obvious than at the airport. Identification is then one of the primary means of airport surveillance; the display of the passport marks a 'reading' of the individual that occurs at multiple times during their journey. From supplying the ticket at check in, to security, to boarding the plane with your boarding pass and then landing and undergoing immigration control in the destination country, the passport must be displayed many times in the airport. It has become integral to the ritual of international travel. Airport staff and workers must also supply, not their passport, but a similar identification card to access areas of the airport. Indeed, airport vehicles must also be identifiable, clearly bearing company insignias and paint designs. In terms of mobility this is vital, passports and identity cards govern where a person, airport worker or object may go. Passports are a marker of our identity. Identities are read and given mobile and spatial limits by states and airports as they regulate mobility.",
          "We can also look outside the terminal to airspace for examples of non-human surveillance. Airspaces are territorial units but they also act as highways for the traffic of aircraft, these spaces must also become monitored and controlled for the purposes of security and safety. At the dawn of civil aviation, the organization of plane movement was described as \"decidedly sketchy in nature\" (quoted in Wegg, 1995: 115). The pilot had to rely upon skill and sight, and had no knowledge of local weather conditions. In order for the aviation industry to provide efficient and safe travel, it was essential that more could be known about environmental conditions. The first known aviation weather station in the UK was introduced at Croydon airport in 1921 (Wegg, 1995). This surveillance of weather and the general environment is perhaps an increasingly common trend for surveillance. Brazil's System for the Vigilance of the Amazon (SIVAM) developed by Raytheon provides a huge environmental monitoring system to show signs of illegal drug trafficking; mining and logging that have previously gone undetected in the rain forest. Still, the monitoring of weather is obviously not the only concern of air traffic control. As was quickly found in the 1920's crashes were most likely to occur around airports, where planes in the air and on land found it difficult to avoid each other. The need to orchestrate these movements proved paramount to safety. For this, Air Traffic Control (ATC) developed at airports, reinforcing the idea of an airspace that could be vigilantly monitored and controlled for the purposes of both safety and security. Again, at Croydon airport, the new terminal built in 1928 featured an ATC. The tower, or \"chart house\" scanned for incoming aircraft and maintained radiotelephony (RT) communications with planes. The towers therefore monitored all the positions of incoming and taking off aircraft. Un-identified or enemy planes could also be monitored from an ATC. More sophisticated radar technologies have since developed, the air traffic control radar beacon system (ATCRBS) being one. After takeoff, aircraft turn on their transponders that send signals recognized by equipment at air traffic control that may then monitor not merely the position of an aircraft but also the unique identity of the flight can be gained from the transponder signal. Air traffic control, can then manage and organize the airspace for the most efficient and safe flow of aircraft. A similar system has also developed on the ground at airports so that 'aircraft incursions' -planes wandering onto runways without permission -may be avoided. This has usually been completed using a pair of binoculars; however, problems arise during periods of low visibility in bad weather and particularly fog. Airport surface detection equipment (ASDE) is a radar system able to locate and monitor planes movement, combined with tower automated ground surveillance system (TAGS) aircraft and indeed, any other airport vehicle may also be identified on the ground surface (Wells, 1996). Although, it must be noted that newer airspace surveillance is moving considerably away from the control tower as power is given back to the planes. Instead, decentralised forms of surveillance are becoming evident in the form of intelligent on-board systems that allow each aircraft, and airport land vehicles to monitor each other's position.",
          "This paper has in part attempted to question the broad brushstrokes of 'mobility' and 'flow' that are often used to describe airports. Airports act to monitor these flows and filter out threats, sorting and categorizing mobile bodies. Whilst this may happen spatially in-terms of access to specific places of the airport, it also occurs in terms of movement. As I have shown, people may become discriminately separated into categories that also affect their passage and speed through the airport. These systems have been shown to be increasingly troubling in their invasion of privacy and the possibilities for expansive data sharing. In-turn this paper has examined a conceptual sorting within surveillance studies, using the airport to recognise how non-humans may become monitored and controlled -to explore the surveillance of not only human but also non-human mobility. This approach has been argued to be of particular significance given that governments are increasingly turning to non-human surveillance (Carter, 2001). Now the focus is on the means of terrorism. Objects such as explosives have become actants in the eyes of airport security, where barometric or timer explosives pose threats independent to the whereabouts of their makers. Indeed, the surveillance of weather and the location of the actual aircraft is also key to the safety concerns of flight. The mobility of non-humans therefore, pose new challenges for airport surveillance in a climate where 'terror', 'bomb' and 'explosives' are words on every passenger's minds. At its most basic level, I think an ANT approach can help scholars to acknowledge this monitoring of non-humans. Then we can go further than the mere description of these techniques. ANT's implication for surveillance studies is not the simple mobilisation of supposedly inaminate, non-sentient or nonhuman things that may be monitored. We can do more than merely \"follow the actors\" (Latour, 1993; see also : Hitchings, 2003). In particular, ANT may be used to give greater insight into the relationships or networks of objects that surveillance systems examine. But this exercise may seem somewhat pointless if the relationships between humans and nonhumans are not understood. Indeed, investigating the monitoring of a machine in a factory may seem interesting, but is this important for surveillance studies? Does this even fit our definition of what surveillance is? In my opinion, for now, our starting point must be human. This paper has separated its treatment of humans and non-humans in an attempt to illustrate a tendency for scholars to delimit their focus to the study of the surveillance of humans and its implications. It has not been my purpose to forget the complexity of relationships between people and things, the cyborg 'machinic ensembles' of the plane or car that transport us to our destination (Thrift, 1995;Lyon, 2001). Whilst the separations I have made have been deliberately artificial for ease of illustration, any examination of the surveillance of things, only really becomes important if we are to appreciate the impacts they may have upon the human and indeed the human associations with these things. Certainly, it seems as though the surveillance of non-humans, in the examples I have explored, are used precisely because of the difficulties posed by monitoring the human. Many of us do not wish to have a bar-code printed on us, or to be radio tagged. Therefore, non-humans are monitored instead of humans. In-turn non-human monitoring may also be used as indicators of human activity -as evidence. It is far easier to trace illegal weapons and explosives than to read the minds of passengers to test for the intent of their actions. In other cases, particularly remote monitoring, the material effects of human activity are easily seen by satellite photography. Illegal logging may therefore become recognised on vast areas of the Amazon by its initial impacts, as opposed to the individual actions and movements of a person. Of course, the network metaphor may not always fit. At airports, the relationship of passenger and baggage only really resembles a traditional network when passengers' are in contact with their luggage -when their relationship is static. More fluidic metaphors can be invoked to better describe the often shifting relationships of passenger to bag -a 'mutable mobile'. Furthermore, Law and Mol's (2001) article suggests that the metaphor of fire is useful to describe the oftenflickering presence of actors within a network. This metaphor can be seen in the instance of a passenger not boarding a flight and leaving their baggage behind. As discussed, baggage reconciliation ensures that this separation is not complete given the linkage created between passenger and bag. If a passenger fails to get on their flight, their 'absent presence' flickers into significance to ensure the removal of their luggage from the plane. At the same time, do we also limit our examination to humans, objects and things? Activists may argue that the surveillance of animals should be questioned. At airports in particular, animals are routinely transported illegally. But animals are in-fact still key to the surveillance and security operations at the airport. Animals are not only a subject of surveillance but are part of the surveillance system itself. Sniffer dogs or K9 teams can be trained as vapour detectors to monitor illegal goods such as narcotics, and explosive devices (Clutterbuck, 1994). These practices are not simply completed by people or animals but succeed because of their mutual relationship (Saunders, 1999; see also Haraway, 1991;Wolch, 2002). Questioning these relationships is long overdue in our fixation upon the high-tech. We must ask how do these interactions take place? What implications do they have? However, this is not to suggest that ANT is a magic recipe for success. There are obviously many shortcomings illustrated by critiques that query the pretension to inclusion -the universal enfranchisement of ANT (Lee and Brown, 1994), its traditional ignorance of spatiality (Hetherington and Law, 2000) and indeed the shortcomings of the network metaphor as illustrated above. That said we should not shy away from the many positives. Examining the networks of the subjects of surveillance sheds light upon the relationality of humans and things. An exploration of these networks exposes how an amalgamation of people and things may become illegitimate, worthy of surveillance. For example, a terrorist 'agency' or threat can be understood as the achievement of a person carrying scissors on a plane. Examining surveillance through this lens may reveal much about the very workings of surveillance systems, the norms and assumptions that frame them, and the implications for those they monitor. The control of the airport is in my view, a suitable starting point for such a discussion."
        ],
        "ground_truth_definitions": {
          "racial bias": {
            "definition": "biases being held towards ethnicity, and national origin - a person becoming sorted based on personal prejudice",
            "context": "The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\"(1997).",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "2d1201d181da2166ccbf1e922634b8b74a58f141",
        "sections": [
          "The Power of Political Rhetoric RONALD R. KREBS University of Minnesota, Minneapolis, USA PATRICK THADDEUS JACKSON American University, Washington, DC, USA While scholars of International Relations and comparative politics have usually treated rhetoric as epiphenomenal, one strand of constructivism has recently returned rhetoric to the heart of political analysis, especially through the mechanism of persuasion. We too maintain that rhetoric is central to political processes and outcomes, but we argue that persuasion is theoretically and methodologically problematic. We aver that rhetoric's role may be more usefully conceptualized in the context of coercion, and we advance a stylized model that illustrates how rhetorical coercion operates, explains why it works, and identifies key scope conditions. We subsequently illustrate our model's relevance through a detailed examination of a 'hard' case. This article's agenda is twofold. First, it advises scholars in these fields to avoid focusing on unanswerable questions about actors' motives and to examine instead what actors say, in what contexts, and to what audiences. Second, it lays the groundwork for a 'coercive constructivism', complementing the liberal version so prevalent today. KEY WORDS ♦ coercion ♦ constructivism ♦ discourse ♦ Habermas ♦ Israel ♦ military service ♦ persuasion ♦ rhetoric Talk is often thought to belong to the realm of diplomacy, war to the realm of action. Yet, during the 2003 Iraq War, the Bush administration was nearly as preoccupied with how the combat was portrayed as with the combat itself. Its foreign policy team invariably spoke of coalition forces rather than American forces, the war to liberate Iraq rather than the invasion of Iraq, Saddam's death squads rather than Saddam's fedayeen. Rhetoric is central to politics, even when politics takes the form of war. Yet rhetoric is curiously not central to much scholarship in comparative politics and International Relations. Politics is typically marked by rhetorical competition, but our theoretical frameworks are generally hard-pressed to make sense of its dynamics and outcomes. Whereas the dominant materialist tradition treats rhetoric as epiphenomenal, we argue, following recent constructivist work, that the rhetorical interplay itself provides leverage in explaining outcomes. We are less comfortable, however, with the argument advanced by some constructivists that political actors deploy resonant rhetorical forms and thereby persuade their interlocutors of the correctness of their preferred course of action. Although persuasion undoubtedly does occur in the political arena, it is also rare. Moreover, such mechanisms rest on a strong specification of the subjective motivations of individuals and thus are methodologically intractable. Recent mainstream constructivist research has prompted a refreshing debate on and advanced our understanding of political deliberation and argumentation. By focusing on internalized norms as the driving forces of behavior and policy, however, such research has ultimately diverted attention from the dynamics of rhetoric. Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics. 1  Employing an expansive definition of rhetoric that includes all speech actswhether they are oral or written, whether they take place under conditions approximating Habermas' 'ideal speech situation' or not -we argue that rhetorical maneuver can prove critical to success in political contests even when one's opponents have not internalized the promoted values. While claimants may deploy arguments in the hope that they will eventually persuade, their more immediate task is, through skillful framing, to leave their opponents without access to the rhetorical materials needed to craft a socially sustainable rebuttal. Rhetorical coercion occurs when this strategy proves successful: when the claimant's opponents have been talked into a corner, compelled to endorse a stance they would otherwise reject. Most narrowly conceived, this article suggests and explores a language-focused mechanism of political influence complementing recent scholarship, but it also has a broader agenda. At the level of methodology, it calls on mainstream scholarship to avoid centering causal accounts on unanswerable questions about actors' true motives and to focus instead on what actors say, in what contexts, and to what audiences. At the level of substantive theory, it joins European Journal of International Relations 13(1) other recent work in returning rhetoric to the heart of political analysis. But rhetoric is often both the medium and the locus of political contest, and thus focusing on its role in the context of deliberation can take us only so far. Rhetoric matters, we argue, when human beings power as well as when they puzzle. This article represents a step away from constructivism with a liberal flavor, focused on the transformation of values, toward constructivism with coercive characteristics, focused on the exercise of power. This article proceeds in four main sections. First, we critically review existing approaches to rhetoric in empirical political science, examining in particular the limitations of (liberal or deliberative) constructivist approaches premised on persuasion. Second, we set forth our generalizable mechanism of rhetorical coercion and explicate both its logic and its limits. Third, we illustrate this mechanism's operation by exploring in detail a single 'hard' case: the relatively successful efforts of Druze Arabs in Israel to garner greater citizenship rights by framing them as the just deserts of their military sacrifice. This outcome is not entirely explicable without close attention to the dynamics of competing rhetorical claims, and our model of rhetorical coercion helps make sense of this puzzling and important case. Fourth, we briefly discuss the particular possibilities and limits of rhetorical coercion within international politics.",
          "Most scholars of international and comparative politics disparage rhetoric as epiphenomenal. The very phrase 'mere rhetoric' captures the view that what counts is not the language people use but the material power resources upon which they can draw. This perspective is shared by 'realist' (and especially rationalist) writers, and it accords with a well-established understanding of political power. Disenchanted with narrow materialist approaches, some have in recent years sought to harness the power of ideas. Many have invoked beliefs, ideas, and culture to supply inputs and to serve as coordination devices in games with multiple equilibria (Goldstein and Keohane, 1993;Laitin, 1998). Political psychologists have long argued that ideas influence how actors interpret evidence and sift through information (Jervis, 1976). Students of collective action have suggested that principled commitments can motivate participants and render them unusually insensitive to the costs of protest (Cohen, 1985). However, scholars with an ideational bent have typically relegated rhetoric to the margins. For 'idealists', words matter only insofar as they reflect actors' true beliefs, with private statements seen as more revealing than public pronouncements. Public rhetoric is of causal consequence only from the top down, as leaders of states and social movements deploy resonant language in an effort to mobilize support (Edelman, 1964;Snow et al., 1986;Snow and Krebs and Jackson: Twisting Tongues and Twisting Arms Benford, 1988). While devotees of 'interests' and 'ideas' vigorously debate the relative causal power of their favored variables, scholars in both traditions have little use for public rhetorical contestation. But to treat rhetoric as epiphenomenal is to render much of politics puzzling and to do violence to politics as lived by its participants. Rhetoric is certainly a weapon of the weak, but those holding the reins of power can and must deploy it as well. The acquisition and maintenance of rule ultimately hinge as much on legitimacy as on physical coercion, and such legitimacy can be established only through rhetorical action (Weber, 1968). While political contestants unquestionably further their agenda by exploiting material resources, they generally at the same time 'frame' their political activity, explaining the purposes to which their material power is put (Gamson, 1992). Such contests among state leaders and societal activists should be at the center of the study of politics. Rationalists and political psychologists might both protest that they have done precisely this. Rationalist scholars have pointed out that talk is not always cheap. State leaders who renege on their public rhetorical commitments may bear substantial domestic and international costs, giving them incentives both to tell the truth and to strategically manipulate audience costs (Fearon, 1994;Sartori, 2002). Though important, this insight flattens rhetoric into a purely informational tool, a way of efficiently revealing whether one is a high-cost or low-cost actor. As such, it cannot shed light on the framing competitions that often lie at the heart of politics. Political psychologists working within the area of political communication have demonstrated that speakers who succeed in defining the relevant considerations can shape mass opinion and ultimately policy outcomes. 2 But nearly all framing experiments have abstracted far from reality in exposing subjects to just a single issue frame (Druckman, 2001). Even the exceptions have failed to interrogate fully the dynamics of framing. Recent studies have concluded that the framing effect disappears when targets are exposed to competing frames from equally credible sources and that targets then respond in accord with supposedly invariant preferences (Druckman, 2004;Sniderman and Theriault, 2004). By this account, the intense framing contests endemic to politics are peripheral in that their only effect (given the caveat of equal credibility) is to render frame competition inconsequential, and baffling in that actors continue to expend copious resources on their rhetorical efforts. Moreover, this research program has ironically diverted attention from rhetorical interplay and instead directed it towards attributes of the target (e.g. political knowledge, awareness, expertise) and the source (e.g. credibility, trustworthiness, expert status). Studies of 'message effects' are comparatively rare. 3 Our debt to the psychological literature on framing is clear, but we wish to bring rhetoric itself back into the heart of the analysis.",
          "Most approaches fail to accord public rhetoric per se causal status, but one major exception has been the vibrant constructivist literature in international and comparative politics (Checkel, 1998;Finnemore and Sikkink, 2001). A central problem for constructivists has been explaining how and why new norms emerge and why actors might obey norms despite contrary material pressures. 4 Among the universe of mechanisms through which political actors develop 'shared understandings', and arguably occupying pride of place, is persuasion: 'normative claims ', Finnemore (1996: 141) asserts, 'become powerful and prevail by being persuasive' (see also Payne, 2001;Johnston, 2001). The targets of persuasive rhetorical moves do not grudgingly comply, but rather sincerely internalize new beliefs and consequently adopt new identities and preferences. Through persuasion, 'agent action becomes social structure, ideas become norms, and the subjective becomes the intersubjective' (Finnemore and Sikkink, 1998: 914). Through persuasive rhetoric, national and transnational social movements prompt not merely compliance with but consent to existing norms, and norm entrepreneurs need not resort to coercion to effect change. Persuasion -and public rhetoric as its medium -have emerged as the coins of the constructivist realm (see, among many others, Checkel, 2001;Crawford, 2002;Keck and Sikkink, 1998;Risse et al., 1999;Risse, 2000). Though sometimes only implicitly, these scholars have often drawn on Jürgen Habermas' logic of 'communicative action' to explain the power of rhetoric (Johnson, 1993;Lynch, 1999Lynch, , 2002;;Müller, 2001;Risse, 2000). 5  Habermas acknowledges that political actors often engage in narrowly goaldirected (teleological) action, but what makes his political vision distinctive is its affirmation of the possibility and prevalence of rational dialogue, of openminded deliberation in the quest for truth. Every legitimate use of language, Habermas argues, is premised on mutual understandings, on 'idealizations that transcend any particular language game', and these 'give rise to the perspective of an agreement that is open to criticism on the basis of validity claims' (Habermas, 1990: 199). For Habermas, politics (at least in its ideal form) is less about contest than consensus, less about powering than puzzling, and deliberative exchange consequently takes center stage (Habermas, 1984). Ideally, actors leave power and rank at the door, and they seek to persuade others and are themselves open to persuasion. Experimental studies of deliberation in fact conclude that the key to sustaining reflective discussion lies in the willingness of participants to keep an open mind (Barabas, 2004). We are very much in sympathy with this emphasis on public rhetorical interchange, but we are skeptical as to the utility of privileging persuasion as a causal mechanism. As Habermas himself recognizes, the unconstrained dialogue of 'communicative action' is not an empirically accurate portrait of politics, for power and rank are omnipresent in the political sphere. Actors do not leave their identities at the door when entering into dialogue, and they do not employ language unadulterated by earlier political contestation. Discourse is therefore always structured -and not by reason alone. Rules of exclusion and employment dictate what arguments can be proffered, under what conditions, and by whom (Foucault, 1972). Insofar as empirically oriented constructivists invoke 'communicative action' to explain real-world processes, it is fair to ask whether rational deliberation characterizes actual political debate. Only on the rare occasions when it does can Habermasian discourse ethics yield empirical insight into political contestation. It would, therefore, seem more helpful to adopt a theoretical framework that explicates the power of rhetoric even when politics is not truth-seeking and truth-generating, and a conception of rhetoric that includes all speech, no matter how interlaced with power relations. We are also skeptical that analysts can overcome the methodological hurdles standing in the way of demonstrating that persuasion has occurred. Such an explanation presumes that one can pin down the 'real motives' driving individual choice, so that one can distinguish beliefs sincerely embraced from those adopted for strategic or other reasons (Payne, 2001: 39-41). But conclusive proof of persuasion is elusive, for it requires unmediated access to people's minds (Wittgenstein, 1953: §150-5). Did Gorbachev accede to the reunification of Germany because he had been persuaded by the trump card of self-determination (Risse, 2000) or because he had calculated that the net costs of resisting the United States were prohibitive? Did Europeans abandon their overseas colonies because they had been persuaded that formal empire was illegitimate (Crawford, 2002) or because they had calculated that the net expected costs of retaining the colonies were excessive? It is impossible to say based on the available evidence, nor is it clear what evidence could even in principle clinch the case. 6 Relying on statements delivered in private settings does not escape the problem, since these articulations may be as strategic as their public counterparts (Scott, 1990). Many of these questions have received attention elsewhere. Scholars of transnational activism have drawn attention to the 'mobilization of shame', and they too have concluded that ways of framing claims cannot be fabricated out of whole cloth (Crawford, 2002;Price, 1998Price, , 2003;;Risse and Sikkink, 1999). Elements of William Riker's suggestive work on 'heresthetics ' (1996) are apropos, as is Frank Schimmelfenig's research on 'rhetorical action ' (2004). We seek to build on and systematize these important contributions, but we also depart from them. First, while many constructivists recognize that the targets of activist efforts may comply at first for strategic reasons, they claim that lasting normative change requires internalization. 7 This formulation is problematic because it relies on incompatible microfoundations in stage one (instrumental adaptation) and stage two (internalization). On methodological grounds, we prefer a mechanism of political influence highlighting strategic European Journal of International Relations 13(1) action throughout the entire episode. We agree with Weber that consistency, not completeness, should be the hallmark of sound work in the social sciences (Weber, 1949). Further, we are reluctant to characterize any normative change as 'lasting' in the sense of forestalling normative contestation over the long run. We proceed from the premise that norms are inherently subject to challenge and that the rhetorical arrangements sustaining norms are never fully stabilized and are at best relatively stable. Rhetorical contestation is thus always possible in theory, although it may not always be manifest in practice. Consequently, we focus on specific bounded episodes of contestation (McAdam et al., 2001: 29-30). Finally, existing accounts focusing on the strategic uses of public language -both constructivist (e.g. Risse, 2000) and rationalist (e.g. Riker, 1996)continue to be crafted around assertions about the 'interests' served by various rhetorical deployments and the 'motives' driving both speakers and targets. We seek to place the mechanism of rhetorical coercion on firmer methodological footing and, more broadly, to demonstrate the viability of a non-purposive social science. 8 Our skepticism regarding explanations rooted in the identification of motives is not novel. Hans Morgenthau, for one, declared the search for statesmen's motives 'futile' because 'motives are the most illusive of psychological data' (Morgenthau, 1993(Morgenthau, [1948]]: 5). Scholars in the rationalchoice tradition have offered similar observations (Frieden, 1999). Our 'solution', however, is more controversial. We are not prepared to follow our rational-choice colleagues down the path of assuming and deducing preferences. Rather, we take a less-traveled road and argue that social scientists should not begin by specifying the intentions of actors. Let us be clear: we are not arguing that actors do not possess motives, that those motives do not shape actors' behavior, or that such behavior is irrelevant to political outcomes. But we do aver that purposive accounts are analytically less useful than models in which preferences need not be specified or ranked. We therefore seek to minimize the place of motives as driving forces in our accounts of political processes and outcomes. We do not recommend this course lightly, for it flies in the face of deeply held epistemological commitments among social scientists in general and political scientists in particular. Purging empirical narratives of the language of subjective motivation is consequently difficult indeed. 9 Moreover, we recognize that all methodological choices are problematic, since the modeling of complex social processes requires abstracting from reality. Yet while this path is less traveled, it is by no means virgin ground. Many social scientists have distanced themselves from subjectivist understandings of culture as a coherent system of beliefs that might be ascertained through sophisticated survey techniques. We, like they, conceptualize culture as the always contested and often contradictory intersubjective semiotic practices through which social actors seek to generate meaning (Ross, 1997;Wedeen, 2002). Public semiotic codes constrain social actors regardless of whether they truly 'believe' in these codes' content, and thus the question of 'actual' belief is of little relevance (Weber, 1968;Jackson, 2002). Relational sociologists, most notably Charles Tilly, have maintained that individualist approaches commit the sin of reductionism: even if we did know people's motives, that would not help us explain social processes and outcomes. We follow these scholars in avoiding individual consciousness as a causal factor and in shifting 'the emphasis from consciousness to conversation, from action to interaction, from selves to sociabilities' (Tilly, 1998: 400; see also Emirbayer, 1997;and Shotter, 1993). These foundational theoretical moves serve as the point of departure for our model of rhetorical coercion. 10 We cannot observe directly what people think, but we can observe what they say and how they respond to claims and counter-claims. In our view, it does not matter whether actors believe what they say, whether they are motivated by crass material interests or sincere commitment. What is important is that they can be rhetorically maneuvered into a corner, trapped into publicly endorsing positions they may, or may not, find anathema. Rhetoric affects political outcomes even when all actors are cynical operators with little interest in genuine deliberation. The resolution of political issues through public debate need not imply any significant level of intersubjective consensus. Thinking about public rhetoric in this fashion avoids the crudeness of vulgar materialism, the reductionism of ideational approaches, and the heroic optimism of persuasion. Our perspective, which one might term 'coercive constructivism', both relates to and is different from existing approaches. Unlike both materialist and ideational accounts, we argue that language has a real causal impact on political outcomes. Unlike ideational approaches and liberal constructivism, we are skeptical that politics is more often characterized by puzzling than powering; we are equally skeptical that actors can transcend coercion and participate in fully rational deliberation. 11",
          "Rhetoric lies at the heart of politics. But many accounts of politics fail to accord it much weight, and many models fail to appreciate its explanatory leverage. This section seeks to explain how and why skillful rhetorical maneuvering can underpin a successful political campaign -not by persuading one's opponents of the rectitude of one's stance, but by denying them the rhetorical materials out of which to craft a socially sustainable rebuttal. Rhetorical coercion is a political strategy that seeks to twist arms by twisting tongues. This section proceeds by clarifying what rhetorical coercion is, how it operates, and the conditions under which it can be effective.",
          "We begin with a stylized account of an episode of rhetorical contestation. Seeking to effect change in some policy or ongoing course of action, a claimant (C) directs an argument toward the opposition (O) in view of a public (P). For the purposes of the model, it does not matter whether O has exclusive control over the actions at issue (if O is, for example, the government) or whether C and O share responsibility (if they are, for example, political parties in a legislative assembly). All that matters is that O's accession or resistance is critical to the outcome. Any argument that C puts forward contains two analytically separable parts: a frame (or set of terms) that characterizes the issue at hand and a set of implications that C suggests follows from that frame. For example, C might be a group of students protesting a war; their argument, advanced against the national government O through speeches and signs at a rally, portrays the war both to O and to P as unjust and unnecessary (frame) and calls for the end of hostilities and the withdrawal of troops (implications). In responding to C, O may accept or reject either or both the frame and the implications of C's argument. These alternatives yield four different potential outcomes of this episode of political contestation, reflected in Table 1. In case 1, O accepts both the frame and the implications of C's argument, and policy changes accordingly. 12 For C, this outcome represents an unmitigated triumph. Case 2 is more mixed. In case 2, O accepts the proposed implications and changes the policy, but it rejects C's reasoning. An example would be if an environmental movement (C) urged the government (O) to restrict logging for ecological reasons, and the government agreed to restrict logging but justified the policy shift in economic terms. Though the movement would have won the substantive battle, it might reasonably perceive its victory as less than complete; the government might in the future expand logging rights on economic grounds as easily as it had earlier restricted them on that very basis. It is possible that the environmentalists' rhetorical tactics drove the government's concessions, but determining that would require Krebs and Jackson: Twisting Tongues and  access to the authorities' true motives -something we earlier argued one can never ascertain. Both case 1 and case 2 mark (at least temporary) terminuses; contestation might resume at a later stage, but these cells represent relatively stable outcomes in the short to medium run. In contrast, cases 3 and 4 are cases of continued contestation. In case 3, O accepts the terms of C's arguments, but rejects the implications that C draws. For example, a lobbying group (C) may argue that high taxes are an unfair burden on the citizenry and should be reduced through an across-the-board reduction in tax rates; a political party (O) may agree that high taxes are problematic but suggest instead targeted tax cuts. As this relatively narrow policy debate occurs within a common issue frame, it might be termed an implication contest. In case 4, the two parties disagree about the very terms of debate as well as the policies that follow, and their rhetorical efforts consequently focus on advancing their preferred issue frame in the hope that their political opponents will accept it (along with the concomitant implications). Without a common frame bounding the debate, such rhetorical interchange -a framing contestis far more fluid, wide-ranging, and fundamental than in an implication contest. Politics is replete with such framing contests. Advocates of bilingual instruction invoke diversity and inter-cultural respect, while critics charge that instruction in anything other than the dominant language would threaten national ideals and students' eventual success in the marketplace. Opponents of abortion call the fetus an unborn child, portray the act as murder, and label themselves defenders of the right to life; supporters of abortion rights depict the fetus as an insentient being, portray the act as a medical procedure, and dub themselves the defenders of a woman's right to choose. Engaged in either an implication contest or a framing contest, C seeks through its claims and counter-claims to alter the rhetorical environment within which political battle is waged. While C might ideally prefer to persuade O, it must design a political strategy that can attain its desired ends even if persuasion proves impossible. The public plays a crucial role: both C and O must craft their appeals with an eye to some audience which sits in judgment of their rhetorical moves. If C can shift the rhetorical basis on which O justifies its stance to P, C can limit the range of O's possible responses and policy options, transcending the erstwhile implication or framing contest. Put simply, rhetorical coercion has taken place when this strategy proves successful -when O, regardless of its private beliefs, can no longer sustain its public opposition.",
          "Rhetorical contestation consists of parties attempting to maneuver each other onto more favorable rhetorical terrain and thereby to close off routes of European Journal of International Relations 13(1) acceptable rebuttal. Rhetorical coercion is successful when C's rhetorical moves deprive O of materials out of which to craft a reply that falls within the bounds of what P would accept. In the end, O finds itself, against its better judgment, endorsing (or at least acquiescing in) C's stance regardless of whether O has been persuaded or believes the words it utters. The alternatives -enduring punishment at the hands of P or investing significant resources in creating new terms of debate -would be prohibitively costly and time-consuming. In our model of rhetorical coercion, neither the motives nor the sincerity of the parties is particularly relevant. 13  C can succeed in rhetorically coercing O because of two key facts of social life. First, political actors can rarely take tangible steps or advance policy positions without justifying those stances and behaviors -in short, without framing. Politics may entail coercion or distribution, but at the same time it involves the struggle over meanings. Meanings, however, cannot be imposed unilaterally or through the exercise of material power alone. They are, by their very nature, intersubjective (Laffey and Weldes, 1997), and the effort to forge shared meaning implicates some audience in the process. The debate between C and O does not transpire in a vacuum: both are continually striving to legitimate their positions in P's eyes (Perelman, 1982). Second, speakers may not say just anything they would like in the public arena: rhetoric is not infinitely elastic but is structured. 14 Every community of discourse shares a number of topoi, or rhetorical commonplaces, that both enable and constrain speakers' rhetorical possibilities. These commonplaces are not 'fully predetermined, already decided distinctions', but weakly shared notions that can be 'expressed or formulated in different ways in different, concrete circumstances' (Shotter, 1993: 170-1). While C and O are free to weave together these commonplaces in creative ways, they are not free to deploy utterly alien formulations in the course of contestation: such arguments would fall, almost literally, on deaf ears. The available space for rhetorical contestation is, therefore, locally bounded, and the parties to a contentious episode cannot introduce entirely novel arguments. They must configure their appeals utilizing rhetorical tools drawn from a chest that is, in the short term, effectively fixed (Swidler, 1986). Rhetorical innovation, while possible and even inevitable in the long run, is far less likely in the short run for three reasons. First, while structures of discourse are never fully hegemonic and are continually being reworked, coherent political action would be impossible if rhetorical universes were in a state of continuous deep flux. Relative rhetorical stabilities must emerge to permit the construction of political strategies, and thus, at any given time, 'the terrain of dominant discourse is the only plausible arena of struggle' (Scott, 1990: 102). Second, imagining, formulating, and disseminating a brand-new commonplace requires an unusual level of commitment on the part of a claimant in terms of material resources, time, and effort. Deploying existing commonplaces -which have already been imagined, formulated, and disseminated -is far less costly (McAdam et al., 2001: 47-50). The process of transforming an unusual rhetorical form into a commonplace is necessarily lengthy and uncertain, but political actors normally possess far shorter time horizons. Arguments can prove powerful only when the commonplaces on which they draw are already present in the rhetorical field, which is shaped both by the unintended consequences of prior episodes of contestation and/or by campaigns undertaken in advance with the express purpose of reconfiguring the rhetorical terrain. Third, and related, novel rhetorical resources are likely to be drowned out by existing arguments and frames. Dense networks of communication carry and regularly reinforce established commonplaces, overwhelming proffered alternatives. Rhetorical innovations consequently tend to arise at the margins, where communication networks are less dense. Overcoming such barriers is possible in principle, but it is in reality so improbable that C and O must, in a given contentious episode, play within the rhetorical arena that presents itself to them. And that arena privileges particular frames and places others beyond the pale, regardless of the material resources at the speaker's disposal. If C and O were the only two actors in our stylized story, they might introduce novel arguments costlessly and endlessly, and rhetorical coercion would then be impossible. Their rhetorical maneuvers are constrained, however, because rhetorical contestation is public in nature (Bennett, 1980: 805-6). The continual competition for public legitimation ensures that P occupies a critical position, setting limits to the policy stances that the contestants can in practice advance. The relevant audience sets the contours of rhetorical contestation, and stepping beyond or reshaping them requires an investment not attractive or even feasible in the short run. This focus on the public's role constitutes a point of intersection with scholars in the rational-choice tradition who have ascribed great importance to 'audience costs' (Fearon, 1994). In such models, the actors and the audience have clear and consistent preferences that precede the contentious episode; public pronouncements serve to signal the content and/or intensity of those preferences. By contrast, our model does not begin by identifying and ordering the parties' desires, and the outcome of a particular episode does not depend on these preferences' content or ranking. Instead, the importance of the public P lies in the imperative for appeal frames to draw on rhetorical commonplaces present in the public's everyday deliberations (Shotter, 1993: 65-9; see also Bennett, 1980). Our model presumes not that European Journal of International Relations 13(1) P has a well-thought-out position in advance on the issue being contested, but rather that there is only a limited set of arguments that P would, in principle, find minimally acceptable. 15  In sum, one argument 'wins' not because its grounds are 'valid' in the sense of satisfying the demands of universal reason or because it accords with the audience's prior normative commitments or material interests, but because its grounds are socially sustainable -because the audience deems certain rhetorical deployments acceptable and others impermissible. One cannot, however, fashion strict covering laws regarding the audience's response. Students of conversation and argumentation from across several disciplines (anthropology, communication, sociology, philosophy, psychology) have argued that context is unusually crucial to their field of study (Billig, 1996;Gumperz, 1982;Perelman, 1982;Sawyer, 2001;Shotter, 1993;Toulmin, 1958). Classifying structures of discourse, reconstructing the rhetorical possibilities, exploring the rhetorical interplay: these critical challenges require one to formulate arguments limited in time and space and limited by the arena of rhetorical competition. Thus, for example, we do not ask below what sorts of appeals are most effective in general at compelling authorities to grant oppressed minorities first-class citizenship. Rather, through an interrogation of the Israeli case, we suggest that a specific claim (framed around military sacrifice and the equation of rights and obligations) is more likely to be successful in a particular discursive context (when talk about citizenship is narrowly republican). Such an explanation is modest, yet powerful, and potentially generalizable.",
          "The logic of rhetorical coercion suggests that two factors are most important in explaining when the model will have explanatory value. First, is P a relevant party to C's claims-making? Recall that C's power is rooted in its credible threat, implicit or explicit, to bring P in on its side; C's ability to do so deprives O of the option of refusing to respond to or even acknowledge C's claims. If C cannot issue its threat credibly, then O can safely ignore C's claims, no matter how they are framed. We can readily imagine conditions under which P would not be relevant to the interaction between the protagonists in our stylized story. It may be that P simply cannot hear C -perhaps because C is being actively repressed; perhaps because C lacks sufficient resources to publicize its message; or perhaps because C's claims regard alleged misdeeds that transpire far from the public eye. 16 Moreover, in some social contexts, decisions need not be justified. 17 The prevalence of justificatory behavior in the political arena suggests, however, that we are correct to maintain that the scope of the model's application is quite large.",
          "Second, how limited is the universe of rhetorical commonplaces? We have argued that actors must, in any particular claims-making episode, forge their appeals by drawing upon existing commonplaces that represent the boundaries of legitimate framing. When these boundaries are loose, numerous rhetorical strands are available, ensuring that O can almost always put forward a meaningful rebuttal to C. In such circumstances, the terms of debate are hardly constraining. When these boundaries are relatively restricted, rhetorical coercion is more easily achieved, as O has less rhetorical 'wiggle room' -that is, fewer socially plausible ways of rebutting C's claim. We recognize that verbal appeals are multivocal, that advocates and opponents of particular policies may advance their arguments employing similar terms. But public language, in our view, is never infinitely flexible, and even so capacious a rhetorical formulation as that of 'rights' in the American context disciplines the speaker. Such disciplining is more common and thus rhetorical coercion is more likely when speakers have fewer commonplaces on which to draw. 18",
          "One of the most venerable norms in the West is that linking military service and citizenship. 'Whoever puts himself in the service of the state', observed Otto Hintze, 'must logically and fairly be granted the regular rights of citizenship' (Hintze, 1975: 211), and the call to the colors is widely considered the supreme example of state service (Janowitz, 1976). Thus the leaders of those relegated to second-class citizenship have often counseled in wartime against draft evasion and have even urged voluntary enlistment. Afterwards they have shrewdly contrasted their people's record of loyalty and sacrifice to the reality of entrenched political and social inequity. For instance, African-Americans volunteered in droves for the Union Army in the US Civil War and for the American Expeditionary Force in World War I, wrapping their postwar demands in the bloody flag. The rhetoric of military sacrifice has at times proven effective. Mobilized veterans have most obviously exploited their status to gain both political rights and distributional benefits. Oppressed and marginalized ethnic, racial, and religious groups have, based upon their collective sacrifice, also advanced claims for citizenship rights, albeit with varying degrees of success. African-Americans, for example, have repeatedly found this path to citizenship blocked. But their lack of postwar progress is hardly surprising given the high stakes of citizenship battles. The demands of subordinate groups for first-class citizenship challenge the core of the existing social order, and the opposition to meaningful change is consequently often fierce. It is not African-Americans' repeated failures that require explanation as much as the successes of others -such as American women after World War I, Japanese-Americans European Journal of International Relations 13(1) after World War II, and Sikhs in post-independence India -in deploying the rhetoric of sacrifice to win improvements in their status. The opponents of reform typically occupy powerful positions in political, social, and economic structures, and few would expect them to give way before words alone. The politics of citizenship is thus a 'hard case' for the power of rhetoric, for material interests and resources would appear to provide a ready explanation for which groups' citizenship claims are granted and which are denied. We will argue, however, that the particular rhetorical configurations advanced by Israel's various Arab minorities help explain these groups' differential rates of success in overcoming the barriers to first-class citizenship in this 'ethnic democracy' (Smooha, 1997). Drawing on their record of military service, Druze Arabs argued that equal obligations demand equal rights. Jewish politicians found themselves unable to craft a culturally meaningful rebuttal to Druze demands, and they were compelled to concede Druze claims. In contrast, Christian and Muslim Arabs, who have never been drafted into the Israel Defense Forces (IDF), insisted that Israel abide by its public commitment to liberal democracy. While this frame also drew on Israeli commonplaces, it permitted Jewish politicians to justify discrimination against Christians and Muslims by arguing that those who do not perform civic duties cannot lay claim to equivalent public rights and benefits. Although the Druze were poorer than their fellow Arabs and smaller in number, for many years they were more effective in placing their concerns on the national agenda and in garnering a favorable response from the authorities. Consequently, material factors cannot account for the success of the Druze relative to other Arabs, and Jewish politicians' continual resistance to Druze demands suggests that persuasion was not at work. Israel's relationship with its Arab minorities nicely illustrates the power and limits of rhetorical coercion. 19   Background Israel's Arab population -today well over a million strongis divided by religion, with the Muslims, the Christians, and the Druze constituting the three largest groups. Possessing basic political rights, the entire Arab community has enjoyed great socioeconomic progress since Israel's founding in 1948, particularly relative to Palestinians in the West Bank and the Gaza Strip and to Arabs throughout the Middle East. Yet they have also been openly labeled a fifth column, and they have, often without regard to religious affiliation, been the victims of variegated forms of discrimination on the part of the state and the majority Jewish population. The new Israeli government subjected Arabs to military rule, conducted pervasive surveillance within the Arab communities, limited Arabs' capacity for travel and an independent economy, expropriated the bulk of Arab-controlled land, excluded Arabs from the giant labor Krebs and Jackson: Twisting Tongues and Twisting Arms federation known as the Histadrut, and so on. Jewish leaders typically invoked the language of security to justify these policies, but its broad definition encompassed ideological, economic, and even partisan interests as well as security more narrowly conceived. Much overt discrimination came to an end by the late 1960s, but substantial discrimination persisted in more subtle forms (Kretzmer, 1988;Lustick, 1980;Shafir and Peled, 2002). Starting from this common baseline, however, the paths of Israel's Arab minorities have diverged. While all continue today to endure second-class status relative to Jewish Israelis, the Druze have made greater headway than the larger and wealthier Christian and Muslim communities. Since 1956 the Druze have been able to deploy a set of rhetorical commonplaces unavailable to their Christian and Muslim neighbors. That year, male Druze became subject to the military draft, reversing a policy of exemption that had been applied to all Arabs since the passage of Israel's universal conscription law in 1950 (Peled, 1998). Some have speculated plausibly that the Israeli authorities hoped that Druze service in the IDF would provide proof of Israeli liberalism and thereby neutralize Arab nationalists and satisfy Western critics (Firro, 1999;Lustick, 1980), but even full declassification of the relevant documents would not definitively establish the government's motives. We argue that, regardless of who wanted to draft the Druze and why, conscription helped the Druze prosper in Israeli politics.",
          "In the mid-1960s, the Druze began to mobilize, and they regularly sought a public forum in which to air their grievances, which they consistently framed around their people's military service and the equation of rights and obligations. In 1967 Druze activists circulated widely an appeal insisting that 'since we have done our duties … and see in the lack of equality … an infringement and denial of our rights, we urgently demand correction of this wrong' (Firro, 1999: 187). Surveying the history of Druze claims-making, one scholar noted that, among both Jews and the Druze, 'injustices to individual Druzes are usually attacked as ingratitude toward a man who was willing to shed his blood for his country, but now his country turns against him' (Ben Dor, 1979: 134-5). Even in recent years, the Druze have continued to gravitate to this rhetorical mode. During a series of public protests in the 1990s, mayors of Druze villages and towns 'usually denounce [ed] the Israeli government for its broken promises, and stress[ed] the price paid by the community in the form of hundreds of Druze soldiers killed during their service in the Israeli army' (Yiftachel and Segal, 1998: 487). Even the minority of Druze who have objected to the draft have accepted these basic terms. As a Communist Druze Knesset representative argued in the early 1990s, 'If this is a blood covenant, then give us the feeling that we are living in our state, in our homeland, with full rights like all citizens . . . And if there is to be no equality European Journal of International Relations 13(1) of rights, then do not demand equality of obligations, and allow Druze to choose -to serve or not to serve in the IDF' (Zohar, 1991). In constructing their claims frame, the Druze have drawn on rhetorical commonplaces deeply rooted in Israel's rhetorical traditions. Although participation in the pre-state Jewish community, the yishuv, was necessarily voluntary, it prized individual and group commitment to the public good, defined as the historical mission of the Jewish people to rebuild the land of Israel and epitomized by the kibbutz movement. The new state preserved this emphasis on civic commitment as the basis for membership, grafting a powerful statist ideology onto the earlier discourse (Liebman and Don Yihya, 1983). In an early Knesset debate, Prime Minister David Ben Gurion insisted that women too be subject to the draft, bluntly declaring, 'There are no rights without obligations. And above all is the obligation of security, the obligation of defending our existence, our freedom, our independence, and our growth' (Divre Ha-Knesset, 1950: 3:537). Even dissidents have protested within the terms of this dominant definition of citizenship: as one conscientious objector from the Lebanon War put it, 'military service, paying taxes, and obeying the law . . . That is what makes you a citizen and makes you eligible to enjoy the defense and the fruits that [the state] equally distributes' (Helman, 2000: 324). 20  This civic republican tradition has long sat uncomfortably alongside an ascriptive component in the definition of the Israeli polity. The 1948 Declaration of Independence proclaimed the new country 'the Jewish state' even as it promised that Israel would promote the welfare and protect the basic freedoms of all its inhabitants. The country's formal name, the State of Israel, suggests an abiding commitment to pluralistic secularism, but its historical-religious title, the Land of Israel, remains in popular use (Kimmerling, 1985). Some have concluded that 'Jewish ethnicity is a necessary condition for membership in the political community, while the contribution to the process of Jewish national redemption is a measure of one's civic virtue' (Peled, 1992: 435). Yet this ignores the potential contradictions between these discourses. Such tensions are muted when only Jewish citizens are drafted, for then Israeli leaders can publicly embrace civic republicanism while preserving ethnoreligious priorities. But Druze claims-making framed around collective military sacrifice challenged the coherence of Israel's 'ethnorepublican' citizenship. Confronted with such a challenge, Jewish Israeli leaders had two alternatives: either they could acknowledge their exclusive definition of the political community, or they could open its doors. While the Druze squeezed concessions out of Jewish politicians beginning in the late 1960s, there is no evidence that their appeals were persuasive: both the limited and incremental nature of Israeli concessions as well as the grudging fashion in which they were offered are not consistent with a narrative centered Krebs and Jackson: Twisting Tongues and Twisting Arms around persuasion. Rather, our model of rhetorical coercion can help explain why the Druze made headway in this ethnic democracy. Faced with Druze claims, Jewish leaders could have launched a framing contest by (a) admitting openly the discriminatory nature of the state. Alternatively, they could have conceded the Druze frame but challenged its implications by (b) claiming that the Druze were not relatively disadvantaged or (c) arguing that the Druze community's problems were of its own making. The first alternative frame was unappealing -not just because of the likely international repercussions, but also because it would have contradicted both Israel's self-proclaimed status as the sole democracy in the Middle East and its dominant way of speaking about citizenship. The second put forward a claim widely known to be false, and the third possible response, while perhaps plausible with regard to questions of economic development, could not justify administrative segregation, which was, whether cleverly or fortunately, among Druze activists' first targets. There was, however, a fourth option: Jewish politicians could have refused to reply -as long as relevant audiences were unaware of the Druze's plight. The Druze's capacity to engage in rhetorical coercion was consequently dependent on garnering attention from the Hebrew-language media. As a general rule, the Hebrew press rarely covered the Arab communities, for its readership was almost entirely Jewish. But the Druze arguments were resonant with republican rhetorical practice, producing a narrative that Jewish publics comprehended and embraced. As a result, when the Druze made noise, the press amplified it, compensating for the Druze's lack of political clout and ensuring a hearing in the halls of power (Asa'ad, 2000;Toledano, 2000). In short, the Druze trapped Jewish leaders in a rhetorical cul-de-sac in which an implication or framing contest was unsustainable. They maneuvered their Jewish opponents onto a rhetorical playing field on which the Druze could not lose, for no rebuttal would have been acceptable to key audiences, both domestic and international. Wary of calling punishment upon themselves, Jewish leaders had little choice but to concede. Consequently, 'when the Druze demanded something, the ears were much more open' (Toledano, 2000). Despite the Druze community's electoral insignificance, cases of alleged discrimination against individual Druze received attention even in the Knesset (Ben Dor, 1979: 134-6). The Druze demand in the late 1960s for full membership in the Labor Party was irrefutable: as a prominent Labor functionary wrote in a major newspaper, 'How can we explain to that Druze … that he is good enough to endanger his life for the state but that he is not good enough to be a member of the Labor Party?' (Lin, 1970). Confident that their rhetorical resources have bequeathed disproportionate influence, the Druze have resisted an alliance with their numerous fellow Arabs, with whom they share many grievances. Even those Druze otherwise critical of Israel's policies have often supported continued conscription so that European Journal of International Relations 13(1) the military sacrifice frame would remain available. Lacking abundant political resources, the Druze adopted a strategy aimed at rhetorical coercion, and they demonstrated that it could work. Jewish politicians were compelled to say things they (likely) did not believe and ultimately even to follow through on those promises. Like their Druze counterparts, Christian and Muslim Arabs drew on Israeli commonplaces, arguing that Israel needed to eliminate discriminatory measures to live up to its democratic promise. Yet, in comparison to the Druze, they have encountered far greater public resistance, for Jewish politicians had a rhetorical escape hatch. Arab rights, they often argued, were not sacrosanct, for those who did not fulfill their civic obligations (that is, military service) deserved less protection than those who had made the supreme sacrifice. In seeking to convince a wavering Knesset member to support the military administration in 1962, Ben Gurion protested, 'I have recognized all these years that rights are dependent on obligations . . . I do not understand why I may limit the freedom and steal the time of young Jewish men and women when they are in the army, but why it is forbidden to limit much less the freedom of those who do not serve in the army.' 21 That same Labor functionary who advocated offering the Druze membership in the party justified the exclusion of other Arabs on similar grounds: 'I asked them [close Arab friends] how they could sit in the party central committee … next to a bereaved father or mother, next to a party member who had just returned from the front, next to a member who stands ready to leave the next day for reserve service, while they still do not fulfill this obligation' (Lin, 1970). Faced with claims framed in liberal terms, Jewish leaders had a response at the ready, evading the rhetorical dead-end into which Christians and Muslims might have hoped to maneuver them. For decades, this rhetorical move has frustrated Arab efforts to attain first-class citizenship. As one Arab activist recently complained, 'They are trying to link our rights as citizens to performing military service. That's not how a real democracy works' (quoted in Schechter, 2003). Christian and Muslim claimants have long remained mired in a framing contest, unable to persuade or rhetorically coerce Jewish politicians. 22   Alternative Explanations How else might one explain the relative success of the Druze in attaining increments of Israeli effective citizenship? The most intuitive answer would attribute this outcome to the distribution of material resources. But the Druze's potential voter base has been tiny, their per capita income has remained relatively low, they have historically eschewed violent confrontation with the authorities, and they have often had difficulty creating and sustaining effective lobbying organizations (Firro, 1999). By all standard measures of political power, the Druze should have been less successful than other Arabs, not more so.",
          "A second alternative would invert the first. Precisely because the Druze were so small, so weak, and so moderate, Israel could countenance their inclusion in the IDF and grant them some modicum of effective citizenship without threatening the state's Jewish identity. Had the Druze's population been as large as Israel's Christian or Muslim communities, concessions would have been unthinkable. The Druze's small size made them an attractive target for cooptation (Lustick, 1980). But if this argument were correct, presumably the state should have flung its doors wide open, for the costs would have been negligible and the propaganda advantages substantial. In fact, however, while Jewish decision-makers readily offered rhetorical concessions to the Druze, they resisted implementation: 'To say that if the ears were open, things were done -there is a difference' (Toledano, 2000). The Druze made tangible progress, but they fought hard for those increments of meaningful citizenship. Nor did the Druze act like a model coopted minority. They were not content with petty favors, but rather became deeply dissatisfied with their status and continually presented Jewish leaders with difficult and uncomfortable choices. A third alternative would posit that Israel's conquest of the West Bank and Gaza Strip in 1967 and the addition of a large, politicized Palestinian population gave Jewish politicians incentives to grant Druze citizens' demands so as to prevent the Palestinization of the Druze community. Presumably, however, they faced far greater incentives to accommodate the much larger Christian and Muslim citizen populations, whose members had not yet openly allied with the Palestinian national cause. Nevertheless, the Israeli establishment remained highly resistant to Christian and Muslim claims-making. This argument thus has difficulty accounting for the divergent patterns in Israel's relations with its various Arab communities after 1967 -a difficulty not faced by the mechanism of rhetorical coercion. A fourth alternative account would attribute the Druze anomaly to their predilection for taqiyya, or the art of concealment. 23 Long viewed as heretics by their Muslim neighbors, the Druze have historically sought to shield themselves from persecution by hiding their religious observances. With regard to politics, this ultra-pragmatic doctrine prescribes neutrality when possible and 'bandwagoning' -allying with the likely victor -when necessary (Layish, 1985). One would thus have expected the Druze to avoid challenging stronger parties, but in fact the Druze displayed a rebellious streak during Ottoman and European colonial domination of the region. Despite a long history of communal tension, they made common cause with their fellow Arabs in the struggle for Lebanese autonomy early in the 20th century. And, frustrated that one foreign ruler had simply replaced another, their leadership initiated a nationalist revolt against the French in the mid-1920s -with disastrous consequences (Tauber, 1993a(Tauber, , 1993b;;Firro, 1999). The Druze were not proto-nationalists, but they were at times willing to set aside intra-Arab differences, compelling one to ask why they European Journal of International Relations 13(1) did not cast their lot with other Arabs in Israel in the decades after 1948. Taqiyya is obviously of little help in answering this question. Equally important, it lacks analytical bite. The doctrine is so flexible that it is less an explanation for Druze behavior than a tidy description of it.",
          "The Druze's success in compelling Jewish Israeli politicians to grant them a modicum of first-class citizenship illustrates nicely the operation of rhetorical coercion. Although this single case study cannot of course prove that the mechanism is generalizable, we view rhetorical coercion as among the set of 'widely operating explanatory mechanisms that combine differently and therefore produce different outcomes in one setting or another ' (McAdam et al., 2001: 13-14). The Druze case suggests the applicability of rhetorical coercion to domestic politics, but is the mechanism operative in the international arena? Extending the earlier theoretical discussion leads to the conclusion that rhetorical coercion is more likely as social ties in political communities, domestic and even international, grow increasingly extensive and dense. On the whole, however, rhetorical coercion will operate less effectively and less frequently in international settings. Key factors highlighted by the model are often present in international relations: state leaders speak to international audiences, they are sensitive to those audiences' reactions, and justificatory speech acts are prevalent. Indeed, while not systematically elaborating the mechanism's dynamics, others have shown rhetorical coercion to be of critical explanatory importance in such divergent contexts as Cold War crisis bargaining (Bially Mattern, 2004), intra-Arab alliance dynamics (Barnett, 1998), eastward enlargement of the European Union and NATO (Fierke and Wiener, 1999;Schimmelfenig, 2004), and Soviet premier Mikhail Gorbachev's proposals for nuclear disarmament (Evangelista, 2001). Yet even these examples suggest the limits of rhetorical coercion in international politics. Rhetorical coercion, like other forms of political communication, is premised on a political community that shares at least some understandings of the boundaries of acceptable discourse. The more tightly linked the community, the greater the possibilities for rhetorical coercion. Thus it is not surprising to find this mechanism primarily operative among regional groupings with relatively strong requirements for legitimation. The Soviet Union had long legitimated its leadership of the communist bloc by casting itself as the true force for global peace, and its routine calls for nuclear disarmament sustained that claim; Soviet generals could not publicly dispute Gorbachev's proposal without simultaneously undercutting Soviet authority among communist regimes. Similarly, Egypt had, under Nasser, positioned itself as the leader of the Arab world Krebs and Jackson: Twisting Tongues and Twisting Arms based on a pan-Arab discourse that represented Israel as its inherently opposed Other; Nasser reluctantly accepted the risk of war with Israel in 1967 because deviating from the norms of Arab nationalist discourse would have imperiled Egypt's status as the spokesman for Arab nationalism and might have jeopardized his regime's stability. Finally, the European Union, the most tightly bound regional arrangement on the globe today, could not have denied membership to consolidated democracies in Eastern Europe without contradicting the community's legitimating discourse which had intertwined the European concept with liberal democracy; to have vetoed or even vocally opposed eastward enlargement would have challenged the basis for community and threatened individual members' credibility. Such dynamics thus depend on the strength and density of the ties binding the community's members, and such ties vary substantially across the international system. 24 Where such ties are strong and dense -that is, within such regional groupings of states, where international anarchy (as even realists might concede) is most attenuated -rhetorical coercion remains distinctly possible. The cosmopolitan dream of a 'global public' remains today just that, although there are emerging global standards of legitimacy in several arenas and increasingly robust mechanisms through which norm violators might be held accountable (Grant and Keohane, 2005). Where such ties are weak and sparse -as in relations across such regional groupings and as in the many arenas lacking shared bases for legitimation -rhetorical coercion will be far more difficult. As the metaphor of the two-level game implies, political elites in such circumstances are likely to be engaged with and responsive to multiple and different audiences simultaneously, and these audiences are unlikely to share a single conception of what rhetorical moves are acceptable and appropriate. The more complex the social environment, the greater the opportunities for actors to evade the consequences of rhetorical interchange, and thus the more imposing the obstacles to rhetorical coercion. Where such ties have never existed or have been ruptured -perhaps as a consequence of the limits of communications technologies or perhaps as a result of enduring interstate rivalry -communication (including rhetorical coercion) is impossible, and diplomacy is a dialogue of the deaf. As a consequence, it is not the imposing capacity of foreign publics, let alone some elusive global public, to sanction state action that makes rhetorical coercion potentially successful on the international scene. When rhetorical coercion is operative internationally, the critical punishing agent is more often a domestic audience prepared to exact costs for violations of standards of rhetorical appropriateness. While foreigners have disapproved of American unilateralism in recent years, it is not clear that this opposition has driven their governments' policies (Brooks and Wohlforth, 2005), and while anti-Americanism has unquestionably been prevalent, its political consequences European Journal of International Relations 13(1) appear to have often been exaggerated (Keohane and Katzenstein, 2006). The George W. Bush administration could dismiss foreign opposition to the Iraq War, particularly because cooperation on matters of joint concern such as trade and terrorism (narrowly construed) continued seemingly unabated. But the American public's increasing disenchantment with the war has been a far greater cause for concern for the administration, as it is this public that might be directly brought into a political contest over the war -a political contest within which rhetorical coercion is far more likely to operate. 25",
          "Machiavelli famously advised that 'a ruler need not have all the positive qualities …but he must seem to have them… He must seem, to those who listen to him and watch him, entirely pious, truthful, reliable, sympathetic, and religious' (Machiavelli, 1994: 55). Realists, not inaccurately, read Machiavelli as warning against allowing moral considerations to impede actions necessary for the pursuit of political power. But, as coercive constructivists, we would add that these lines imply that social identity has an effect on social and political outcomes, regardless of whether or not the actor internalizes the components constituting this identity. A ruler pursuing policies incapable of public justification would find her path strewn with numerous practical obstacles. This lack of rhetorical resources might even compel her to alter her course of action. In accord with other constructivists, we aver that social norms and social identities matter. 26 In accord with political realists, we argue that whether or not a social actor has internalized a particular set of normative principles is not causally relevant. In accord with the broader cultural turn in the social sciences, we combine these positions by focusing on public language, on how normative principles are deployed in public debate, and by exploring the causal impact this deployment can have on what people say and consequently on what they do -not on the impossibly elusive question of what they believe. Our model of rhetorical coercion can helpfully capture the dynamics of framing contests, when there is controversy about how an issue should be characterized, and of implication contests, when narrower (yet no less important) battles are waged over the consequences of some accepted framing. Our analysis draws attention to two key conditions affecting the applicability of the model. First, rhetorical coercion can occur only when the public is a party to the debate or when the claimant can credibly threaten to involve the audience. Second, not all structures of discourse are equally conducive to rhetorical coercion. This mechanism is more likely to operate when these structures are relatively restricted (and constraining) than when they are relatively loose (and permissive). Furthermore, our model cannot illuminate the process of rhetorical contest in two circumstances: first, when an opponent accepts a claim without argument, and, second, when an opponent accepts the implications of an argument but not its terms. We contend, however, that much social and political life lies within our model's scope. 27  In short, this article seeks to invigorate the study of political rhetoricbut in a fashion relatively unfamiliar to recent debates in international and comparative politics. We argue that rhetorical contestation shapes policy outcomes and that the approaches most common in these subfields miss a significant portion of political and social reality. We further suggest that it is possible to make causal claims without trespassing into the murky waters of subjective motivation and without relying on problematic mechanisms like persuasion. Many IR constructivists would be comfortable with the first of these claims, many realists and students of political culture with the second, few from either camp with their combination. This article has sought to demonstrate that their conjunction is both logically sustainable and potentially productive.",
          "material interests' problematic and have instead embraced a linguistic focus that is not dissimilar to our own. 5. Among those drawing on social psychology are Checkel (2001); Crawford (2002: 26-7);and Johnston (2001). Others have remained agnostic as to how persuasion operates (Finnemore and Sikkink, 1998). We limit ourselves here to discussion of the Habermasian version of the persuasion argument, since in our judgment it is the most clearly articulated mechanism. For an alternative treatment of Habermas in international relations, see Mitzen (2005). 6. On the lack of evidence of changed minds in Crawford (2002), see Dessler and Owen (2005: 602). 7. We are grateful to Kathryn Sikkink for helpful discussion on this point. 8. For treatments closer to our own, see, among others, Barnett (1998); Bially Mattern (2004); Fierke (1998); Goddard (2006). 9. Such language even slips on occasion into our own case study, for the conventions of historical writing insist on subjects who possess agency, articulate goals, and pursue motives. The Foucauldian alternative -interrogating the central discursive formations -might help us escape this trap, but it would come at the cost of the narrative form, which better captures the tensions and surprising reversals of actual political contest. Such traces of motivational language are consequently inevitable, but they should not cloud or draw attention from the rhetorical dynamics themselves. We are grateful to an anonymous reviewer for pressing us on this point. 10. The distinction between persuasion and coercion as modes of political influence is by no means original, but our conceptualization of rhetorical contestation as potentially coercive is more unusual. Scholars, especially those working within a psychological framework, sometimes treat persuasion as synonymous with all attitude change, citing even nuclear deterrence as an example. In contrast, we define coercion as the imposition of constraint on choice, whereas normative persuasion seeks ideally to bypass choice. Both coercion and persuasion may lead the target to assent, but the coerced target merely complies, while the persuaded target consents. Put differently, persuasion speaks to change in preferences, while coercion seeks to alter the social environment confronting an actor while leaving preferences untouched. 11. For a related discussion of 'realist constructivism', see Jackson and Nexon (2004). 12. Note that 'acceptance' here does not imply agreement, but refers only to O's public reaction to C's argument. 13. While we make no strong assumptions regarding the content of the actors' motives, the model does rely on the thin, almost trivial, assumption that actors are fundamentally strategic -that they pursue policies that they believe will further their goals (McAdam et al., 2001). 14. For related arguments, see Cruz (2000); Spillman (1995); Steinberg (1999). 15. In this stylized model, we presume that both actors must legitimate their stance in the eyes of the same audience, but the reality of both international and domestic politics is often more complicated. Speakers in both arenas may be playing to entirely different audiences or may be competing to define which audience is in fact relevant. The dynamics of such contestation are not well addressed by this paper and will be taken up in future articles. We are grateful to anonymous reviewers for raising this question. 16. This is not infrequently the case when O is a government whose commands are implemented by a secretive bureaucracy. A potential weakness of rhetorical coercion is its implication that rhetorical concessions translate smoothly into policy change. O may renege on its commitments, particularly during implementation when there may be little need for public justification. While such breakdowns are likely, we maintain that violations will prove difficult to sustain should C hold O's feet to the public fire in a new episode of rhetorical contestation. 17. Superiors often do not explain their logic to subordinates, and exasperated parents often tell children, 'Just do as I say!' In such circumstances, rhetorical contestation is not likely to be effective. We are grateful to Robert Jervis for pointing this out. 18. For more on the logic of rhetorical coercion, see Jackson (2006); Krebs (2006). 19. For a more extensive treatment of these cases, see Krebs (2006: Chs 3-5). 20. Military service occupies so prominent a site in the Israeli cultural complex that 'civic virtue has been constructed in terms of and identified with military virtue' (Helman, 2000: 320). 21. Ben Gurion to M.K. Yizhar, 12 February 1962, State Archives (Jerusalem, Israel), Office of the Prime Minister Files, C 6304/1086. 22. Christians and Muslims could not control the rhetorical playing field, but they nevertheless made significant gains in the mid-1980s thanks to their raw political power. After the 1977 breakdown of Labor Zionism's stranglehold over the political system, the electoral competition tightened, and the Arab population was actively wooed. Candidates from across the political spectrum stumped for the first time in Arab villages, and the major parties committed themselves to improving the Arabs' lot. 23. A different version of the cultural argument, which Druze and Jews alike have relished, argues that the two peoples are natural allies as fellow minorities in the Muslim Middle East. But the assertion of a natural Jewish-Druze affinity is propagandistic rhetoric that has, since the founding of Israel, served the interests of both Israeli officials and Druze leaders. Druze religious texts are typically dismissive of other religions, and while particularly disparaging of Islam -the dominant regional religious tradition responsible for their oppression -they are hardly admiring of Jews. See Parsons (2001: 70-6); and Dana (2003: 43-51). 24. As this formulation suggests, we believe that network analysis provides a more useful set of tools for grasping such variation than do metaphors regarding, for example, the 'thickness' of the 'normative environment' (Barnett, 1998: 39). For an insightful application of network logic to imperial dynamics, see Nexon and Wright (forthcoming). 25. Consider also the examples discussed earlier. That audiences in the communist bloc outside the USSR might have questioned Soviet leadership or that Arab elites might have challenged Egypt's did matter, but primarily indirectly. Had the Soviet generals or Nasser anticipated that domestic audiences would respond to the prospective loss of regional or ideological authority with collective indifference, they would have perceived themselves to be operating with a far freer hand. That domestic audiences were sensitive to their country's standing in the relevant community of nations, and were believed to be prepared to act on that basis, was crucial in these cases. 26. The language of identity has not figured explicitly in this article, but it is our view that rhetoric and identity are inseparable. Identity is not the mental property of an individual, nor can collective identity be apprehended by aggregating the attitudes of the constituent individuals. Rather, identities are necessarily relational, and thus they are forged through the process of their articulation. Political communities are defined by a common rhetorical lexicon and coalesce around particular rhetorical configurations. 27. Our model cannot, however, capture the long-term processes through which new commonplaces enter the rhetorical space."
        ],
        "ground_truth_definitions": {
          "rhetorical coercion": {
            "definition": "an alternative means of conceptualizing the place of rhetoric in politics",
            "context": "Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "https://aclanthology.org/2024.lrec-main.952",
        "sections": [
          "Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets (Krieger et al., 2022), which exhibit limited generalizability across diverse domains (Wessel et al., 2023). This paper builds upon the work of Wessel et al. (2023), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) (Aribandi et al., 2021a;Chen et al., 2021;Kirstein et al., 2022) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method (Spinde et al., 2022) underperforms due to its narrow task focus and does not surpass baseline outcomes. In this study, we make five main contributions: ). This enables it to outperform models based on single-task learning (STL) paradigms. 1. We present MAGPIE -the first large-scale multi-task pre-training approach for media bias detection. By pre-training on diverse bias types such as persuasive and subjective, a classifier based on MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1). 2. We introduce LBM (Large Bias Mixture), a pretraining composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias. 3. We provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks. 4. We demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset (Spinde et al., 2021c) and achieves competetive results on the Media Bias Identification Benchmark (MBIB) collection (Wessel et al., 2023). 5. We make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub: github.com/magpie-multi-task These contributions highlight the potential of MTL in improving media bias detection. Our findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results. Another key insight of our research is the value of MTL in contexts where the primary dataset is smallfoot_0 . By learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data. Overall, our research offers a multitask learning approach to media bias detection with first large-scale resources in the domain.",
          "",
          "Media bias is a complex issue (Lee et al., 2021a;Recasens et al., 2013;Raza et al., 2022) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias (Wessel et al., 2023). In their literature review, Spinde et al. (2023) provide an extensive overview of research on media bias and related subtypes of bias. Media bias detection approaches have evolved from hand-crafted features (Recasens et al., 2013;Hube and Fetahu, 2018;Spinde et al., 2021d) to neural models (Spinde et al., 2022;Chen et al., 2021;Spinde et al., 2021c;Huguet Cabot et al., 2021;Sinha and Dasgupta, 2021;Raza et al., 2022). However, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets (Wessel et al., 2023). As most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019)). We will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain. Media bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications (Recasens et al., 2013;Spinde et al., 2021b), which, so far, are not explored in relation to each other (Spinde et al., 2023). Wessel et al. (2023) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias. Their study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities. MAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks to mitigate a variety of limitations in MBIB (see Section 3.1).",
          "MTL shows significant improvements in various NLP tasks, including sentiment analysis (He et al., 2019), text summarization (Kirstein et al., 2022), and natural language understanding (Raffel et al., 2020). In MTL, a model leverages knowledge gained from one task to improve the performance of others. Aribandi et al. (2021b) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021) show that prefinetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold. As described above, media bias can be seen as a composite problem composed of various interrelated bias types (Wessel et al., 2023). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b). For instance, benchmarks such as GLUE and Super-GLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection (Wahle et al., 2023(Wahle et al., , 2022)), and semantic evaluation (Ruas et al., 2019(Ruas et al., , 2020)), thereby substantially improving performance across a range of NLU tasks (Wang et al., 2018(Wang et al., , 2019)). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues. The selection of tasks is pivotal to the efficacy of MTL. There have been several attempts to automate task selection, including learning the data selection with Bayesian optimization (Bingel and Søgaard, 2017) or estimating task relations (Ruder and Plank, 2017). The most model-agnostic approach is GradTS (Ma et al., 2021), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE. GradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task's attention. The selected tasks are trained jointly and share representations across tasks.",
          "We implement MAGPIE using pre-finetuning as introduced in (Aghajanyan et al., 2021) (See also Section 2). As such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), a large-scale task collection of bias-related datasets. We incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1). As outlined in Figure 2, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset 2 (Spinde et al., 2021c), and 2 BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model's generalizability within a single dataset the Media Bias Identification Benchmark (MBIB) collection.",
          "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias more indirectly, such as sentiment analysis or emotion classification. MAG-PIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3. First, we manually assess a list of 115 media bias-related datasets in English language, categorized into task families by Wessel et al. (2023). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks in Section 4.3, such as Aribandi et al. (2021b) proposed for general NLP tasks. We filter the collection of the 115 datasets based on the following criteriafoot_2 : • Accessibility: Datasets have to be publicly accessible. • Text granularity: We only use datasets labeled on a sentence level or its fragments (tokens)(not on, for instance, article level) • Quality of annotations: We exclude datasets with no documentation, low annotation agreement or employment of machine annotation. • Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levelsfoot_3 . We remove 5 datasets due to or unreliable source of annotations and discard 4 duplicates. Applying these criteria leaves context. originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. In order to standardize examples from various domains, we apply a unified text-cleaning procedure to each dataset. The process involves: (1) discarding sentences with fewer than 20 characters, (2) removing URL links, special characters, and whitespace, and (3) eliminating duplicates potentially created with steps (1) and (2). The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks and task families can be found in Table 4.",
          "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa (Liu et al., 2019) encoder due to its proven state-of-the-art performances across various media bias applications (Spinde et al., 2021c;Krieger et al., 2022).",
          "Pre-finetuning. To effectively harness the generalization abilities of MTL for media bias detec-tion, we adopt a pre-finetuning procedure (Aghajanyan et al., 2021). Pre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training. While incorporating MTL directly into the pre-training stage has demonstrated performance gains (Aribandi et al., 2021b), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL (Aghajanyan et al., 2021). Sharing representations. We use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads. For each task, we attach dense layers, or \"heads\", to the shared encoder. These heads are optimized individually per task while the shared encoder learns general bias representations. However, multi-task optimization presents challenges due to differing gradient directions and magnitudes (Yu et al., 2020). For instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum. On the other hand, if Task A's gradient greatly surpasses that of Task B, gradient A becomes dominant. We counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling (Yu et al., 2020). Conflicting gradients and loss scaling. In multi-task training involving n tasks, encoder parameters receive n potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad (Yu et al., 2020), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks. Therefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of n sets per update. Adopting Muppet's method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps (Aghajanyan et al., 2021).",
          "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL (Aribandi et al., 2021b;Spinde et al., 2022). We also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster. However, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM. When task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B. We aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches: 1. Head-Specific-Early-Stopping (HSES) 2. Resurrection HSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers' continuous learning, especially after they have reached an optimal state. Resurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again. This enables the task to adapt its head parameters to the latest latent representation. HSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4. However, we stress the need for further extensive analysis in Section 5.",
          "Multi-Task Learning often involves selecting wellused datasets, leading to potential selection biases. Furthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes. To automate the process of task selection, we utilize the GradTS algorithm (Ma et al., 2021). We choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability. Gradient-based Task selection. In line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrixfoot_4 for each task. Tasks are sorted by correlation between each task's matrix and BABE task's matrix. We pre-finetune m -1 models on the first k tasks from the sorted list, where k varies from 1 to m -1 and m is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal k determined by evaluation loss. For further details on the GradTS algorithm, please see Ma et al. (2021).",
          "We split the BABE dataset into train, dev and test split with 70%,15%,15% portions respectively. For both hyperparameter tuning and auxiliary task selection, we evaluate on dev split and only use test split for the final evaluation (Section 4.2). As fine-tuning transformers on small datasets often leads to inconsistent results, such as high performance variance (Dodge et al., 2020), we use a fixed random seed 321 for all runs in auxilliary task selection. For the final evaluation on the test set (Table 2) we evaluate the models using 30 random seeds and report the average performance, to minimize the influence of random weight initializations. We use values 0..29. For optimizing the models, we use a per-task batch size of 32, an AdamW optimizer, and a polynomial learning scheduler. We run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory.",
          "In this section, we present the results of our MTL approach. First, we report the set of auxiliary tasks selected by GradTS for pre-finetuning. Next, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks. We also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark. Then, we analyze the LBM taxonomy through a study on knowledge transfer between families. Lastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study.",
          "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices. We use Kendall's correlation coefficient, as suggested by Puth et al. (2015). We find a local minimum for the BABE evaluation loss when pre-finetuning on the first k = 10 most correlated tasks. The final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1. The tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection. Task Type τ (correlation) Persuasive techniques 0.73 Lexical/Informational bias 0.72 Rumour detection 0.69 Sentiment analysis 0.68 Global warming stance detection 0.68 Subjective bias 0.67 Veracity classification 0.64 Gender bias 0.64 Fake news detection 0.63 Table 1: Attention importance correlation w.r.t. the media bias task.",
          "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline. The multi-task sets are the following: • MTL:Random -Random Subset of 10 tasks • MTL:GradTS -Subset of 10 tasks selected by GradTS algorithm • MTL:All -Set of all tasks We also evaluate the model pre-finetuned on the set of all tasks on MBIB. We follow the guidelines set by Wessel et al. (2023) for the evaluation. Given that MAGPIE's pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases. Multi-Task Learning Performance. Table 2 summarizes our performance results on the BABE dataset. We observe that all of our MTL prefinetuning schemas lead to performance improvements. In particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c). While both MTL baselines -Muppet (Aghajanyan et al., 2021) and UnifiedM2 (Lee et al., 2021b) outperform singletask baseline, they underperform all of our MTL models. On MBIB benchmark, MAGPIE ranks first on 5 out of 8 tasks. However, the improvements are only marginal. The results can be found in Appendix in Table 5.  Step efficiency. In addition to the performance improvements achieved, we also assess our model training efficiency. In Figure 5, we show the F1 score on the development set for the BABE task, averaged over all 30 runs. Our findings show that Multi-Task Learning only requires 15% of the training steps used in single-task finetuning on BABE.",
          "F1 Acc loss Baseline (RoBERTa base) 80.83 (±0.69) 81.19 (±0.69) 43.6 (±3.54) DA-RoBERTa 77.83 (±1.4) 78.56 (±1.3) 47.84 (±2.97) MUPPET 80.56 (±1.3) 81.18 (±1.16) 44.19 (±4.65) UnifiedM2 81.91 (±0.91) 82.41 (±0.88) 44.86 (±3.99) MTL:Random 81.88 (±1.02) 82.28 (±0.97) 40.35 (±1.73) MTL:GradTS 82.32 (±0.79) 82.64 (±0.8) 40.96 (±2.36) MTL:All 84.1 (±1.33) 84.44 (±1.25) 39.46 (±2.41) Table 2: Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds. This result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future.",
          "In Section 3.1, following Aribandi et al. (2021b), we introduce data task families. Aribandi et al. (2021b) uses task families for selection and knowledge transfer. To assess task families' significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer. To account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families. We train tasks from the same family together and report the average change in task performance, as depicted in Figure 6. Negative knowledge transfer is prevalent across most of our task families. However, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance. Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Section 4.3. Our results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families. Conversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%. On the other hand, the Emotionality family is significantly impaired by negative transfer from other task families. The full table of transfers can be found in Figure 7. Considering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5. Task Family Transfer from Transfer to media bias -2.07% -0.94% subjective bias -1.26% 0.89% hate speech -0.87% 0.17% gender bias -1.01% -1.07% sentiment analysis 0.11% 0.72% fake news -0.13% 1.79% group bias -1.04% 0.09% emotionality 0.34% -6.56% stance detection -0.79% -1.83% Table 3: Evaluation of averaged transfer between task families.",
          "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation. We calculate the average evalua- tion loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection. The boxplot in Figure 8 shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%. However, we hypothesize that a random constellation of tasks 6 can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.",
          "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multitask learning (MTL) approach that enhances traditional models with a multi-task structure. Additionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pretraining of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias. Our study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative trans- 6 Particularly variance in task sizes and quality. fer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far. Furthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches. These findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias. While results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum. Understanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies. As media bias is emerging globally, incorporating multilingual models is a natural extension.",
          "We acknowledge the necessity for a more comprehensive analysis of the performance of the HSES and Resurrection methods to ensure robust evaluation. Given the significant computational resources required for a single multi-task training of all tasks, we allocated these resources towards robustly evaluating the model performance rather than conducting an in-depth analysis of the optimization techniques. Consequently, the presented methods may be less reliable and have limited applicability. While there are various models available, previous research suggests that RoBERTa exhibits strong performance on media bias tasks (Wessel et al., 2023;Spinde et al., 2021c). However, due to resource constraints, we were unable to explore models with different architectures and further refine our selection. As the landscape of publicly available highquality datasets for media bias is not as extensive as desired, we acknowledge our inability to capture all manifestations of media bias comprehensively. As mentioned in Section 5, a systematic and comprehensive analysis of the dataset landscape will be part of our future work. Furthermore, although analyzing media bias on a sentence level enables a detailed examination of occurring biases, certain forms of bias, such as bias resulting from the omission of information, extend beyond the linguistic aspects of individual statements. Detecting such biases may require considering an external context. We leave it to future work to investigate other levels of granularity, such as paragraph or article levels. In addition to these technical limitations, conducting a detailed analysis of agreement and label distribution in all utilized datasets will be necessary to make stronger claims about which datasets provide more reliable coverage of the overall concept of media bias. This is particularly important since media bias is a complex phenomenon that is not always easily identified during the annotationgathering process.",
          "Detecting (and as a result also highlighting) media bias instances can mitigate the negative effects of media bias, e.g., on collective decisionmaking (Baumer et al., 2015). However, media bias strongly depends on the context and individuals involved, making it a sensitive issue. Some bias forms depend on factors other than the content, e.g., a different text perception due to a reader's background. Therefore, a public classification of possible polarization and one-sidedness in the news must be performed transparently and reliably. The datasets used for training the classifier must be transparent and, ideally, constantly monitored by independent experts. To address possible implications of MAGPIE or related media bias applications cautiously, it is crucial to constantly control the classifications and datasets with recent findings on the perception of media bias, which is a main part of our ongoing and future work. To do so, we use resources such as, e.g., standardized questionnaires on media bias perception (Spinde et al., 2021a). When bias detection is balanced and transparent, it can positively affect collective opinion formation and decision-making. We see no immediate negative ethical or societal impacts of our work. However, in addition to system transparency, we want to highlight that we believe it to be required for future datasets to report in greater detail about a manipulation protection strategy when developing, training, and presenting any media bias classifier. To ensure the validity of media bias detection systems, it is essential to prevent participants, especially in public studies, from manipulating algorithms by, for example, flagging neutral content as biased. Therefore, annotations should be compared among multiple users to ensure a higher level of trustworthiness. Most of the datasets available report only limitedly about such strategies. In open or crowdsourcing scenarios, it is important to collect user characteristics and deliberately include specific content that aims to provide obvious answers but may be answered differently when users follow certain patterns. This approach helps in detecting and mitigating potential biases introduced by participants, thereby maintaining the integrity and reliability of the media bias detection process. To ensure and propose stronger standards in the future, we aim to analyze all LBM datasets with regard to potential inherent bias in future work."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "a skewed portrayal of information favoring certain group interests, which manifests in multiple facets, including political, gender, racial, and linguistic biases.",
            "context": "1. Introduction Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022).",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "",
        "reasoning": "The user did not provide any section text to analyze. Without content, it is impossible to determine whether a definition is present. Therefore, we must assume that no definition exists.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Ottinger's research on community-based air monitoring as a strategy for producing knowledge about environmental health hazards is based primarily on participant-observation in two environmental justice NGOs: Communities for a Better Environment (CBE) in Oakland, California, and the Louisiana Bucket Brigade in New Orleans, Louisiana (Ottinger 2005). As part of her ethnographic fieldwork, she devoted ten hours per week as a technical volunteer (Ottinger has a background in engineering) for each organization during two consecutive years between 2001 and 2003. At both organizations, her participation involved researching a variety of air monitoring strategies and developing tools for interpreting results from those methods. Her study is also informed by semistructured interviews of one to two hours each. She interviewed thirteen scientist-activists, community organizers, and community residents in California and more than forty activists, regulators, and petrochemical industry representatives in Louisiana. The interviews addressed organizing and community-industry relations, broadly defined, and frequently touched on issues related to ambient air monitoring techniques, with about one-third taking air monitoring as a primary theme. The case of community-friendly air monitoring involves similar issues of undone science and regulatory politics to those discussed for the chlorine controversy, but at a grassroots, community level. In communities adjacent to refineries, power plants, and other hazardous facilities, known as \"fenceline communities,\" residents suspect that facilities' emissions of toxic chemicals cause serious illnesses. However, there is a dearth of scientific research that could illuminate, in ways credible to residents, the effects of industrial emissions on community health (Tesh 2000;Allen 2003;Mayer and Overdevest 2007). The use of air sampling devices known as \"buckets\" provides one avenue for addressing issues of undone environmental health science. With the low-cost, easy-to-operate devices, fenceline community residents and allied environmental justice organizers measure concentrations of toxic chemicals in the ambient air, collecting data about residents' exposures that is necessary (though not sufficient) to understanding chemical health effects. Designed in 1994 by a California engineering firm and adapted for widespread dissemination by Oaklandbased non-profit CBE, the buckets \"grab\" samples of air over a period of minutes. By taking short samples, buckets can document chemical concentrations during periods when air quality is apparently at its worst-when a facility is flaring or has had an accident, for example-providing otherwise unavailable information about residents' exposures during pollution peaks. Both activists' strategies for air monitoring and experts' responses to activist monitoring are significantly shaped by agreed-upon procedures for collecting and analyzing air samples and interpreting their results. When measuring levels of toxic chemicals in the ambient air, regulatory agencies and chemical facilities routinely use stainless steel Suma canisters to collect samples, which are then analyzed using a method specified in the Federal Register as Federal Reference Method (FRM) TO-15. Although the canisters can be used to take shortterm samples, when regulators want to represent air quality broadly, samples are taken over a twenty-four-hour period every sixth day. Where they exist, regulatory standards for air quality form the context for interpreting the results. Louisiana, one of only two U.S. states with ambient air standards for the individual volatile organic chemicals measured by FRM TO-15, specifies eight-hour or annual averages that ambient concentrations are not to exceed; monitoring data are compared to these standards to determine whether air quality poses a potential threat to public health. 2  Specifying how air toxics data are to be collected and interpreted, these formal (e.g., FRM TO-15) and informal (e.g., the twenty-four-hour, sixth day sampling protocol) standards shape how bucket data are received by regulatory scientists and chemical industry officials. First, they act as a boundary-bridging device; that is, the standards help to render activists' scientific efforts recognizable in expert discourses about air quality and monitoring. 3  Although activists and experts collect their samples with different devices-buckets for activists, Suma canisters for experts-both strategies rely on air sampling to characterize air quality and both use FRM TO-15 to analyze the samples. The shared analytical method makes the results of individual bucket samples directly comparable to those of canister samples. Moreover, because activists use the FRM, an EPA laboratory in California was able to conduct quality assurance testing early in the bucket's development, allowing activists to refute charges that chemicals found in bucket samples were somehow an artifact of the sampling device and to claim, more generally, that the bucket was an \"EPA-approved\" monitoring method. To the extent that the standards, particularly the FRM, serve a boundary-bridging function, they help undone science get done: they allow data from an alternate method of measuring air quality, bucket monitoring, to circulate with some credibility among experts and, consequently, to address questions of pressing concern to community members but hitherto ignored by experts. Activists' monitoring with buckets has even prompted experts to undertake additional monitoring of their own. For example, in Norco, Louisiana, where resident-activists used buckets to document very high concentrations of toxic compounds in their neighborhood, Shell Chemical in 2002 began an extensive ambient air monitoring program (Swerczek 2000). 4   Simultaneously, however, standards for air monitoring serve a boundarypolicing function: the same suite of regulatory standards and routinized practices that give buckets a measure of credibility also give industrial facilities and environmental agencies a ready-made way to dismiss bucket data. Specifically, ambient air standards are typically expressed as averages over a period of hours, days, or years. 5 Bucket data, in contrast, characterizes average chemical concentrations over a period of minutes. Environmental justice activists nonetheless compare results of individual samples to the regulatory standard-asserting, for example, that a 2001 sample taken near the Orion oil refinery in New Sarpy, Louisiana, showed that \"the amount of benzene in the air that day was 29 times the legal limit\" (Louisiana Bucket Brigade 2001)-but experts vehemently reject such claims. In a 2002 interview, Jim Hazlett, part of the Air Quality Assessment division of the Louisiana Department of Environmental Quality, complained about activists' inaccurate use of bucket data: You can't really take that data and apply it to an ambient air standard . . . . So we see a headline, the citizen group over here found a, took a sample and found benzene that was 12 times the state standards. Well, it's not true. I'm sorry, but that's not what it was. In the view of Hazlett and other experts, only the average concentrations of regulated chemicals can be meaningfully compared to the standards and thus contribute to determining whether air pollution might pose a threat to human health. Ambient air standards, and the average-oriented air sampling protocols that they require, thus prove to be a mechanism for policing the boundary between activists' and experts' claims about air quality, marking experts' data as relevant and activists' data as irrelevant to the assessment of overall air quality, to the determination of regulatory compliance, and to discussions of chemical plants' long-term health effects. As boundary-policing devices, standards circumscribe activists' contributions to doing undone science. To the extent that bucket monitoring has resulted in increased enforcement activity by regulators (O'Rourke and Macey 2003) or additional ambient air monitoring by industrial facilities, the additional monitoring has been undertaken to confirm activists' results, track the causes of the chemical emissions, and fix what are assumed to be isolated malfunctions but usually not to query the possibility that routine industrial operations might pose systematic threats to community health. Even Shell's program in Norco, which collects rare data on chemical concentrations in a fenceline community, is oriented to long-term averages and thus does not shed light on the potential effects of the pollution spikes that occur with regularity as a result of flaring and other unplanned releases. As in the chlorine sunset controversy case, the example of bucket monitoring demonstrates how regulatory systems shape conflicts over undone science, even at the local level of community-based research and activism. In this instance, efforts by neighborhood activists (and other outsiders to science) to see undone science done in their own backyards illustrate the asymmetrical operation of regulatory standards and standardized practices. Air monitoring standards function as boundary-bridging devices that enable activist use of an alternative, more cost-effective method and therefore help address an aspect of environmental health science left undone by experts. However, standards also serve as boundary-policing devices. These reinforce experts' authority to define how health risks in fenceline communities should be evaluated, shutting down debates over fundamental research questions and associated methodological approaches-debates, for example, over whether average or peak concentrations of air toxics are most relevant to their determining health effects. Because it is exactly these debates that activists would, and must, provoke to shift scientific research priorities, the standards' boundary-policing aspect tends to dominate most locally organized attempts to counter undone science. However, this case also illustrates the importance of standards' boundary-bridging aspects that enable community activists to actually and forcefully enact shifts in research priorities, rather than merely advocate for alternative scientific agendas. Gibbon's research is based on ethnographic fieldwork, ongoing since 1999, that examines the social and cultural context of developments in breast cancer genetics in the United Kingdom. The larger study addresses how the knowledge and technologies associated with breast cancer genetics are put to work inside and outside clinical settings, at the interface with a culture of breast cancer activism (see Gibbon 2007). The discussion presented here draws on fieldwork conducted in a leading high-profile U.K. breast cancer research charity between 1999 and 2001 and again in 2005-2006. The fieldwork involved the analysis of promotional documents produced by the organization, participant-observation of a range of events, and more than forty-five in-depth semistructured interviews and five focus groups with the organization's fundraisers, advocates, scientists, and staff. Given the exponential growth in lay/patient and public activism in relation to breast cancer in the last twenty to thirty years (Klawiter 2004;Gibbon 2007), this would seem to be an arena where we might expect to see challenges related to undone science. In one sense, the rapid expansion in breast cancer activism has achieved much to reduce the space of undone science in breast cancer. Like AIDS activism in the 1990s, so-called breast cancer activism is often held up as an exemplary instance of successful collective lay/public/patient mobilization that has helped to raise awareness of the disease, promote a discourse of female rights, and redress gendered inequities in scientific research and health provision (e.g., Anglin 1997;Lerner 2003). It would from this perspective seem potentially to be a clear example of epistemic modernization, where research agendas may be opened up to the scrutiny of lay/patient/public communities (Hess 2007). Yet paradoxes abound in an arena where growing collective awareness of the disease also helps ensure that the management of risk and danger is the burden of individual women (Kaufert 1998;Fosket 2004;Klawiter 2004). The situation reflects what Zavestoski et al. (2004) have referred to as the \"dominant epidemiological paradigm\" of breast cancer, one that strongly informs the parameters of scientific research and medical intervention by focusing on lifestyle and/or the genetic factors of individuals and that has engendered some resistance from civil society groups. In the United States, for example, recent lobbying efforts to draw attention to alternative strategies for breast cancer have involved collaborations between specific cultures of breast cancer and broader environmental justice movements (Di Chiro 2008) in pursuit of what Brown and colleagues term a \"lab of one's own\" (2006). Nevertheless, breast cancer activism is characterized by diverse cultures, and consequently, the issue of undone science is also disjunctured and differentiated within national and across international arenas. Despite the growth of health activism around breast cancer research, environmental risk factors in breast cancer etiology remain one domain of undone science that continues to be marginalized in mainstream discourse. The particular institutional parameters that serve to sustain the space of undone science in breast cancer are illustrated by examining the predominant culture of patient and public activism in the United Kingdom. In this context, understanding how breast cancer activism operates to preserve undone science requires paying attention not only to the marginalization of environment-focused breast cancer activism (Potts 2004) but also to an institutionalized culture of cancer research, where breast cancer activism can reference and symbolize quite different activities (Gibbon 2007). Since the early part of the twentieth century, cancer research in the United Kingdom has been rooted in an institutional culture of first philanthropic donation and then charitable fundraising, helping ensure a public mandate influencing patterns of research in cancer science (see Austoker 1988). Like earlier public mobilization around the so-called wars on tuberculosis and polio, the \"war\" fought by the cancer charity establishment in the United Kingdom has proved not only a resilient cultural metaphor (Sontag 1988) but also a reflection of ongoing public support and investment in cancer research. As a result, cancer research in the United Kingdom is mostly sustained as a modernist project waged by a scientific community, focused on a cure (Löwy 1997) and supported by cancer charities that are funded significantly by public resources in the form of voluntary donations. The influences of this project on undone breast cancer science are visible within a highprofile breast cancer research charity, where narratives of involvement and identification reveal the scope of activism, the ways that this institutional culture informs the parameters of civic engagement, and how activists' engagement with research is limited to certain areas of activities. In one instance, for example, a group of women responded to the meaning of \"involvement\" in ways that mixed the morality of fundraising with campaigning work and also with moral sentiments such as \"giving something back,\" \"helping make a difference,\" or somehow \"being useful,\" as this excerpt illustrates: I was in the middle of treatment, chemotherapy, and I just happened to read-it was October-and I happened to read an article in a magazine, I think the launch of their [the charity's] £1,000 challenge. And at that point I was feeling [a] sort of a wish, a need, to put something back . . . . And I got the certificate and I got invited to the research center … there was something that drew me to it . . . . So [it] was mainly fundraising, but I could feel something could develop there. So at one point I said to one of the girls on the fundraising team, \"Can I help in a voluntary way? I've got skills I'm not using, particularly proofreading, editing, language leaflets, making things clear.\" And then it seemed to be very useful, from a \"Joe public\" point of view. And it's developed into almost like a little job; it's given me a whole new life … and I feel like I'm putting something back. And my life has value . . . . So, it's terrific. Really, it's terrific. Although often difficult to tease apart fundraising as a form of activism and the highly successful marketing strategies of the charity, narratives such as the one above suggest that lay/civic engagement in breast cancer research does little to challenge a traditional expert/lay dynamic. Instead, women became \"involved\" mostly in the pursuit of reproducing and sustaining traditional parameters of scientific expertise. Such activism has been constituted through \"heroic\" acts of fundraising, which were in turn wedded to the pursuit of basic science genetic research, collectively situated as a form of \"salvationary science\" (Gibbon 2007, 125). This continues to be a salient motif for engagement in the charity, with very few women seeing their involvement in terms of influencing a research agenda or affecting the research priorities of the charity. Although a number of women interviewed spoke of being involved in a charity in terms of \"campaigning\" or being active around the \"politics of health care,\" their narratives exhibited a general lack of interest in influencing scientific research and a strong feeling about the inappropriateness of \"stepping on the toes of the scientists.\" As two interviewees put it: I don't think any of us would push it in anyway, because we can't appreciate if you're a nonscientist. I don't … appreciate the process sufficiently to be able to direct it in a particular direction and say, \"Hey, why don't you look at this?\" I don't think laypeople can make a significant contribution to what we should study. I know that a lot of people would agree with me on that. While some interviewees observed that the whole point of being an advocate for those with breast cancer is, as one woman explained, \"You're not a scientist,\" others noted that the research undertaken by the charity was widely perceived in terms of a \"gold standard.\" Many, including those who strongly identified more as \"advocates\" rather than \"fundraisers,\" also believed that the standard of expertise might potentially be threatened or undermined by training a wider community of people affected by breast cancer to have a say in scientific research. 6Overall, interview data suggest that despite thirty years of growing activism around breast cancer and a much more open concern with implementing, developing, and identifying with advocacy, a particular institutional context continues to sustain, color, and influence the lay/ patient and public mobilization around the disease. The morality of fundraising and the faith in the expertise of scientific research expressed by these women cannot be abstracted from the institution of cancer charities in the United Kingdom. The complex and diverse nature of breast cancer activism here and elsewhere shows that what is required in understanding the dynamic space of undone science in breast cancer is a careful mapping and analysis of the nexus of interests that coalesce at particular disease/science/public interfaces (Epstein 2007;Gibbon and Novas 2007). The dense imbrication of some segments of the breast cancer movement with various institutions of scientific research in the United Kingdom means that undone science appears only to a segment of the advocacy community that has itself been historically marginalized within the larger breast cancer movement. Thus, unlike the two previous cases, which examine conflicts between industrial and government elites in conflict with social movement actors, the case of breast cancer research demonstrates conflicting notions of undone science within movements. Additionally, however, support for research into environmental etiologies of cancer may yet come from within institutional cultures of science. Postgenomic researchers have increasingly begun to explore what is described as \"gene/environment interaction,\" where the importance of a seemingly broader context of molecular interaction is becoming important (Shostak 2003). As such, researchers examining social movements must be attentive to subtle shifts around the space of undone science of breast cancer from within and outside mainstream science as different configurations of health activism interface with seemingly novel targets of scientific inquiry in contrasting national contexts. As this study shows, undone science demarcates a highly dynamic cultural space characterized by interorganizational and intraorganizational competition mediated by advances in technoscientific research and clinical practice.",
        "reasoning": "The text includes an explicit definition of the term “fenceline communities.” The passage states: “in communities adjacent to refineries, power plants, and other hazardous facilities, known as 'fenceline communities' …” This directly explains what the term refers to, satisfying the criteria of a clear definition (a term followed by an explanation of its meaning). No other section contains a similarly explicit definition with cue phrases such as “is defined as” or “means.” Therefore, the section does contain a definition.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "The main point of our analysis is not that people are better hypothesis testers than previously thought (although that may be so). Rather, the +test strategy can provide a basis for understanding the successes and failures of human hypothesis testing in a variety of situations. In this section, we apply our approach to several different hypothesis-testing situations. Each of the tasks we discuss has an extensive research literature of its own. However, there has been little cross-task generality beyond the use of the common \"confirmation bias\" label. We show how these diverse tasks can be given an integrative interpretation based on the general +test strategy. Each task has its unique requirements, and ideally, people should adapt their strategies to the characteristics of the specific task at hand. People may indeed respond appropriately to some of these characteristics under favorable conditions (when there is concrete task-specific information, light memory load, adequate time, extensive experience, etc.). We propose that, under less friendly conditions, hypothesis testers rely on a generally applicable default approach based on the +test strategy.",
        "reasoning": "The section describes how the +test strategy can be applied across various hypothesis‑testing situations, compares it to other tasks, and outlines conditions under which people may adopt it. It does not provide a clear explanation of a specific term or concept, nor does it use typical definition cues such as “is defined as,” “refers to,” or “means.” Therefore the passage does not contain an obvious definition.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "",
        "reasoning": "The user did not provide any section text to analyze. Without content, it is impossible to determine whether a definition is present. Therefore, we must assume that no definition exists.",
        "is_definition": false
      },
      {
        "paper_id": "2025.acl-long.1513",
        "sections": [
          "As online news and social networks have radically altered the media landscape, understanding the dynamics of news propagation and the narratives that news convey is more crucial than ever. Online news outlets make it possible for recurring narratives to appear, often with very different phrasings, in multiple articles and propagate with very high velocity across audiences, languages, and countries. This is especially problematic with manipulative narratives, potentially containing strong biases, mis/disinformation, propaganda, or harmful content. Such a risk is even more pervasive when it is related to divisive issues and can have large societal implications, including destabilization, conflict, hate, or incitement to violence. Various definitions of narrative can be found in the literature, depending on the context and the goal of the analysis. In this paper, we define narrative as a recurring, repetitive (across and within articles), overt or implicit claim that presents and promotes a specific interpretation or viewpoint on an ongoing (and frequently dynamic) news topic. Narratives may be embedded in thousands of articles across multiple topics, themes, languages, and news genres (e.g., news reporting or opinion). They can manifest using very different vocabularies, media frames (Entman, 2007), and tones, which makes their automatic detection and extraction very challenging. Moreover, multiple narratives can be present in the same article, or even in the same paragraph or in the same sentence. An example is shown in Fig. 1, where the narrative \"Downplaying Climate Change\" occurs in two articles from very different domains: finance vs. auto enthusiasts. Thus, the effective automatic detection, extraction, and analysis of narratives in news articles is an important challenge and remains an open problem ( §2). Such capacity is crucial for analyzing the news landscape, identifying media bias, and detecting attempts to influence readers. It can also be used to warn and educate media consumers to improve their media literacy. Large Language Models (LLMs) have made the need for such capacity even more urgent, as they are vulnerable to (Xu et al., 2024) and prone to generating manipulative narratives (Vykopal et al., 2024) with modest effort. In this work, we focus on the problem of narrative detection, treating it as multi-label paragraphlevel classification problem ( §3). The biggest challenge is the lack of annotated datasets (Nunes et al., 2024), especially for long news articles (as opposed to micro-blogs or snippets). Existing narrative classification datasets for long articles ( §2) have been annotated at the document level and assign only a single label per annotation. Narratives, however, are often conveyed by small parts of the articles, and thus it is important to pinpoint those parts, which is why we model the problem as a paragraph-level classification. We consider two news domains of current relevance ( §3.1): climate change (CC) and the military conflict between Ukraine and Russia (URW). Unlike previous work on narrative classification, where the labels are flat and coarse (Li et al., 2023) ( §2), we developed a two-level taxonomy with coarse-and fine-grained narrative labels (hereafter narratives and sub-narratives) for each topic ( §3.2). We collected news articles in four languages-English, Bulgarian, Portuguese, and Russian-related to the two domains ( §4.1), and manually annotated them at the paragraph level with labels from the corresponding taxonomies ( §4.2). We report statistics about the resulting dataset ( §4.3) and discuss its quality ( §4.4). We make the dataset, dubbed PolyNarrative, publicly available, along with experimental results ( §5) and code of several strong baselines that assign narrative labels to news articles at the paragraph or the document level; in the latter case, we use the union of the paragraph-level ground-truth labels of each document as the correct prediction. Our contributions are as follows: • We propose a new two-level hierarchical taxonomy for narrative extraction. We further develop a novel datasetfoot_0 that uses this taxonomy with news article annotations from two levels (coarse-and fine-grained), at the paragraph level (previous work was document level), using multi-label annotation (previous work was mostly single-label) across two current domains (previous work was single-domain), and four languages (previous work was for a single language). • We provide a comprehensive overview of the data acquisition and annotation process, and highlight the challenges and measures taken to address them. • We perform evaluation using several strong multi-label classification methods, and draw conclusions that can benefit future work. 2 Related Work",
          "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023;Silvano et al., 2024) and capturing narrative components in the form of semantic relationships between entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012;Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes. In an effort to synthesize various formulations of the concept, Dennison (2021) proposed a refined definition of narratives as \"selective depictions of reality across at least two points that can include one or more causal claims, and are generally generalizable and can be applied to multiple situations, as opposed to specific stories\". This formulation, although well structured from a theoretical perspective, was not sufficient to properly operationalise the concept for our intention to provide a concrete taxonomy. Denmark Punishing Farmers for Cow 'Emissions' to 'Fight Global Warming' Denmark has become the first country to force farmers to comply with the goals of the World Economic Forum's (WEF) \"Net Zero\" agenda, to supposedly \"fight global warming.\" ...",
          "Although carbon dioxide is typically blamed for causing \"climate change,\" globalists claim that methane traps about 87 times more heat on a 20-year timescale but top scientists have debunked these claims as a hoax. A recent peer-reviewed study provided conclusive scientific evidence proving that carbon dioxide (CO2) emissions in Earth's atmosphere cannot cause \"global warming.\" Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change However, not only did they find that higher levels of CO2 made no difference, but they also proved that it simply isn't possible for increases in carbon dioxide to cause temperatures to rise. Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change Nevertheless, unelected globalists at the United Nations Environment Program claim that livestock accounts for about 32% of \"human-caused methane emissions.\" ... Thus, our definition of narrative ( §1), while informed by the above general formulation, is based on previous efforts to create taxonomies and datasets for practical use (Kotseva et al., 2023;Li et al., 2023;Coan et al., 2021).",
          "",
          "To apply modern NLP methods, as highlighted by Santana et al. (2023), extraction of narratives has been approached as hierarchical event detection (Glavaš et al., 2014), as frame bias detection (Zaghouani et al., 2024), as spatiotemporal entity relationship detection (Nunes et al., 2024), and as multi-class classification (Li et al., 2023). Li et al. (2023) presented a social media dataset based on short texts from microblogs and trained a BERT-based narrative classifier detecting COVID anti-vaccine claims from and using a flat taxonomy, while Weigand et al. (2022) focused on conspiracy narrative detection by applying a binary classifier to detect the presence of conspiracy theories and applied a topic modeling segmentation without a pre-defined taxonomy. Coan et al. (2021) released an English-only dataset using a two-level taxonomy on climate change denial, annotated at the snippet level from website fragments and using a single label per snippet, while Kotseva et al. (2023) provided a multi-lingual dataset annotated at document-level using a single label per document. While datasets with individual characteristics (long-articles, hierarchical taxonomies, multilinguality) have been released before, to the best of our knowledge, there has been no other dataset that provides a multi-lingual corpus, annotated at paragraph level using a hierarchical taxonomy in a multi-label multi-class fashion.",
          "Several narrative taxonomies have been created to classify recurring argumentation in online news. Kotseva et al. (2023) created a three-level narrative taxonomy on COVID-19 and used it to classify and to analyze trends over time. Hughes et al. (2021) presented a taxonomy of common anti-vax narratives, organized on several common tropes and rhetorical strategies. Coan et al. (2021) presented a two-level taxonomy for common cases of climate change denial in short snippets. Amanatullah et al. (2023) presented a flat taxonomy of common pro-Russian narratives found in the alleged pro-Kremlin influence campaigns related to the war in Ukraine. In this work, we used the last two taxonomies as a starting point for the two domains of interest.",
          "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, \"CO 2 is beneficial\" is a sub-Narrative of the \"Climate Change is beneficial\" Narrative of the Climate Change (CC) taxonomy. A (sub-)Narrative can appear in a variety of contexts and topics, and can manifest using subtle and indirect argumentation or framing, and thus its detection can be very challenging. For example, the sub-Narrative \"Western sanctions will backfire\" could be expressed throughout a financial reporting article, or in a small paragraph at the end of an opinion article commenting on military matters. Thus, in order to better tackle this challenge, our approach focuses on highlighting specific segment(s) of each news article that expresses each (sub-)Narrative. In this sense, we consider a paragraph as a minimal segment. If a (sub-)Narrative is expressed in multiple paragraphs of an article (or the entire article), all the corresponding paragraphs need to be annotated accordingly. Moreover, one paragraph can express more than one Narrative and more than one level of Narratives (Narratives and sub-Narratives). In these scenarios, the paragraph needs to be annotated with all the applicable (sub-)Narratives. Given this formulation, the task becomes a hierarchical multi-label multi-class classification problem at the paragraph level.",
          "We selected two domains (topics) that currently receive extensive news coverage, are presented through several different perspectives, and are highly susceptible to manipulation through the creation of artificial narratives. The first domain is Climate Change (CC) and the second one is the Ukraine-Russia War (URW). In both cases, we have numerous accounts of repeated argumentation in the form of narratives. We are particularly interested in potentially manipulative narratives, and this is reflected in the choice of the (sub-)Narratives of our taxonomies. However, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily a case of mis/disinformation. This is important to highlight, as mixing legitimate with mis/disinformative claims is a frequent manipulative practice (Goel et al., 2023).",
          "To create the two taxonomies, we started from existing narrative taxonomies for URW (Amanatullah et al., 2023) and CC (Coan et al., 2021). We asked media analysts to suggest modifications based on their media monitoring experience. These modifications included additions (new (sub-)Narratives), elaborations (splitting existing (sub-)Narratives into multiple different ones), modifications (refining definitions and names) and merging (grouping into common (sub-)Narratives). We asked, when possible, to phrase each (sub-)Narrative as a con-crete claim (e.g., \"Renewable energy is dangerous\" or \"Ukraine is a puppet of the West\"). When the argumentation regarding a (sub-)Narrative was very diverse or fragmented, we gave a more general descriptive label (e.g., \"Criticism of Institutions\" or \"Speculating war outcomes\"). We ended up with 38 and 36 sub-Narratives, grouped into 11 and 10 Narratives for URW and CC, respectively. The top level of the two taxonomies (Narratives) is shown in Figure 3. The full taxonomies, including sub-Narratives and definitions, can be found in Appendix A. We note that the two taxonomies are not complete; in fact, they are not intended to be complete, rather they are representative of the practical experience of the media analysts, and different valid formulations are possible. We stress again that although our taxonomies focus on potentially manipulative narratives, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily mis/disinformation or indicates strong bias. UKRAINE-RUSSIA WAR (URW) Blaming the war on others rather than the invader Discrediting Ukraine Russia is the Victim Praise of Russia Overpraising the West Speculating war outcomes Discrediting the West, Diplomacy Negative Consequences for the West Distrust towards Media Amplifying war-related fears Hidden plots by secret schemes of powerful groups CLIMATE CHANGE (CC) Criticism of climate policies Criticism of institutions and authorities Climate change is beneficial Downplaying climate change Questioning the measurements and science Criticism of climate movement Controversy about green technologies Hidden plots by secret schemes of powerful groups Amplifying Climate Fears Green policies are geopolitical instruments ",
          "",
          "To obtain news articles for annotation, we used an in-house online news scraping and indexing tool to retrieve articles on the two selected domains in the four target languages. 2 We considered both mainstream and \"alternative\" news sources (that is, sources known to repeatedly publish content with misinformation). We also developed ad-hoc scoring methods, outlined below, to heuristically evaluate the relevance of each article to the corresponding domain. More specifically: 1. We created keyword-based queries for the two domains in all the languages, and used them to retrieve a large number of articles from the news article index of our in-house tool. For the URW domain, we included documents from 2021 to 2024. For the CC domain, the documents ranged from 2015 to 2024. 2. To assess the relevance of the resulting articles, we formulated another set of key phrases, corresponding to the (sub-)Narratives of the taxonomies (e.g., \"Ukraine is corrupt\", \"Science is debunked\") or other aspects we wished to emphasize or de-emphasize. We then used bart-large-mnlifoot_2 to perform zero-shot classification with the title and the first 300 characters of each article, resulting in a classification score per article and key phrase. 3. We used an XLM-RoBERTa-basedfoot_3 multilabel classifier, trained on the Persuasion Techniques dataset (Piskorski et al., 2023a,b), and used the approach of Nikolaidis et al. (2024) to produce Persuasiveness Score metrics per article. For the CC domain, we also used a climate change denial classifier, based on the work of Coan et al. (2021), to further filter and reduce the number of articles to review. 4. We used a linear combination of the resulting scores (relevance score per key phrase, Persuasiveness Score metrics and the score from the climate change denial classifier) to automatically rank articles from most to least likely to contain relevant (sub-)Narratives. A member of the team who was familiar with the news landscape of the language, then manually inspected the resulting set of articles, focusing on articles that seemed relevant to the defined taxonomies. Since manual inspection was timeconsuming, we used the different scores computed above to reorder the articles and focus mainly on highly ranked articles.",
          "In order to clarify the annotation process, including the meaning of key concepts such as Narrative, sub-Narrative, the precise meaning of each label of the taxonomies, and good practices, we created a document with detailed annotation guidelines. This included specific definitions and examples for each label, together with a detailed description of the annotation process to be followed. The definitions and examples of the taxonomies can be found in Appendix A. As we wanted to produce a multilingual dataset with four languages, we had multiple annotator teams, each focused on one language. We assigned two annotators per news article and a curator who handled consolidation and quality assurance before the final version. We held regular calls in which hard cases were highlighted, comparisons across annotations from different languages were made, and issues with ambiguities of the guidelines were discussed. To annotate the documents, we used the Inception annotation tool (Klie et al., 2018). Each language was assigned a coordinator who had the responsibility to sort through the collected articles, assign annotators and curators, and create annotation batches incrementally. The coordinator was also responsible for training the annotators. We instructed the annotators to read each news article paragraph by paragraph, and to annotate each of them with all the applicable Narratives and sub-Narratives before moving to the next one. When no Narrative or sub-Narrative was found, the annotator simply moved to the next paragraph. Because the sub-Narrative labels were many and difficult to memorize, we configured Inception so that for each paragraph, the annotator would first select all the applicable Narratives, and then all the applicable sub-Narratives from a menu displaying the sub-Narratives of the selected Narratives only. When a paragraph was deemed to contain a Narrative, but none of its sub-Narratives, the \"[Narrative]: Other\" label was selected at the sub-Narrative level. Similarly, the \"Other\" label was selected when no Narrative was applicable. During the annotation process, several issues were highlighted. First, the distribution of labels was very uneven and varied noticeably across languages (Table 2). This reflects the reality of the news discourse, as some Narratives are more frequent in some countries than others. After each annotation batch, the collected label statistics were used by the coordinator to optimize the selection of articles for the next batch. Second, some labels were semantically very close, which led to frequent confusion. We used frequency metrics and cura-tor feedback to discuss such cases in the regular meetings, where we reiterated and further refined the guidelines. For example, the sub-Narratives \"Ukraine is a puppet of the West\" and \"The West does not care about Ukraine, only their interests\" were frequently confused. To resolve this ambiguity, we added guidelines instructing the annotators to select the former when Ukraine was the subject and the latter when the West was.",
          "Table 1 shows statistics about the documents of the PolyNarrative dataset, broken down per language, and aggregated over both domains. We can see that, except for Russian, all languages have approximately 400 annotated training documents; the test documents are approximately 35 for each language. We aimed at articles of relatively long text with a median length of 500 words and a minimum length of 250 words. Even after the rigorous data selection process, the final label distribution after annotation is highly imbalanced (see Table 2 and Appendix B for a breakdown per language). Additionally, the distribution of labels varies noticeably from language to language, due to differences in media interest across countries, as already discussed. For example, for URW, Russian is a notable outlier in terms of distribution, with \"Praise of Russia\" being one of the most common Narratives. One note to highlight, is that after the article selection process, the proportion of mainstream and alternative news varies widely across languages and could have an impact on label distribution. Moreover, 44% and 54% of the documents in CC and URW, respectively, are assigned more than one label; for paragraphs, the corresponding percentages are 12% and 15%, respectively. This observation justifies our choice to perform multi-label annotation.",
          "Table 3 reports the inter-annotator agreement scores for Narratives (coarse labels) and sub-Narratives (fine-grained labels), per language, collectively for both domains, measured as Krippendorff's α at paragraph level using the simpledorfffoot_4 library. The agreement is under the recommended value of 0.667 but is higher than the IAA of tasks of similar complexity (Piskorski et al., 2023b(Piskorski et al., , 2024)). TRAIN Language #documents #paragraphs #sent. #char. BG 400 3,951 6,728 937,150 EN 394 3,683 8,012 1,176,513 PT 395 3,877 5,739 971,356 RU 133 583 1,630 206,124 TEST Language #documents #paragraphs #sent. #char. BG 35 325 542 70,291 EN 41 584 1,106 144,401 PT 35 330 578 86,132 RU 32 154 478 56,872 Total 1,465 13,487 24,813 3,649k Table 2: Distribution statistics for coarse-grained labels (Narratives), assigned to paragraphs and documents, in both TRAIN and TEST splits. Statistics for fine-grained (sub-Narratives) are shown in Appendix B. As one would expect, the agreement on finer labels is lower, because of their subtler differences. We noticed that the CC domain caused more confusion between the annotators than URW. In both cases, we could see that the confusion was skewed by a small set of under-agreed labels (5 for URW and 7 for CC) that achieved disagreement above 40% and 60%, respectively. If we excluded these labels, the IAA for all languages rises to 0.567 and 0.560 for the coarse and 0.452 and 0.516 for fine-grained, for CC and URW respectively. There were some sub-Narratives that were commonly confused. For example, in the URW subset \"Discrediting the West, Diplomacy: West is tired of Ukraine\", \"Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests\", and \"Discrediting Ukraine: Ukraine is a puppet of the West\". Overall, on a positive note, the majority of the disagreement between the two annotators in the sub-Narrative labels was between labels of the same Narrative (e.g. sub-Narratives under \"Discrediting the West, Diplomacy\" were frequently confused with one another) On average, out of all the individual annotator disagreements (paragraphs where the two annotators picked a different sub-Narrative), 67% was with sub-Narratives of the same Narrative. A detailed breakdown of the disagreements statistics for each label can be found in Appendix E.",
          "",
          "To provide baseline results for the new PolyNarrative dataset, we performed experiments with commonly used multi-label classification methods. We assessed the following configurations: • Label granularity: We assessed the performance in both coarse (predicting Narratives) and fine-grained (predicting sub-Narratives) settings. Additionally, we assessed how the extra information about the fine-grained predictions impacted coarse-grained predictions. • Paragraph vs. document: We assessed the performance when the models were trained and evaluated to classify paragraphs vs. entire documents. The document-level labels were produced by taking the union of the labels of all of the paragraphs contained in each document. • Prompting vs. fine-tuning: We also assessed the capability of (open-source) LLMs to annotate (sub-)Narratives without further training, given instructions similar to the guidelines given to the human annotators, comparing performance to fine-tuned models. We experimented with the following models: • XLM-RoBERTa large (Conneau et al., 2020), in two versions: -XLM-RoBERTa-coarse: trained using only the coarse labels. -XLM-RoBERTa-fine: trained using the fine-grained labels. • Llama3.1-70b (Grattafiori et al., 2024), without fine-tuning, in four versions: -Llama3.1-70b-0shot-labels: Prompted with the names of the labels of the taxonomy only, in a zero-shot fashion. -Llama3.1-70b-guidelines: Prompted with the full annotation guidelines, which include hand-picked examples for each (sub-)Narrative. -Llama3.1-70b-guidelines-labels: Prompted with the names of the labels, followed by the full annotation guidelines. -Llama3.1-70b-hierarchical: two-step hierarchical prompting, where the model is instructed to pick the coarse-grained labels first, and then given these coarsegrained predictions, decide on the finegrained label; full guidelines are also provided. When using Llama, after receiving the model's response, we performed a series of post-processing operations, where the generated output labels were filtered and normalized to remove hallucinated labels and to handle cases where the model generated labels in a slightly different form (e.g., different capitalization, punctuation errors). To extract paragraph-level annotations from Llama, we inserted extra \"[paragraph_N:]\" tags at the beginning of each paragraph, N being the number of the paragraph, and modified the prompts to instruct the LLM to output each extra tag followed by the relevant labels of the corresponding paragraph. After the call to the LLM, we recombined the outputs with the input paragraphs in a post-processing phase, using regular expressions. The exact prompts used are given in Appendix C. The XLM-RoBERTa (Conneau et al., 2020) models were trained at the token level. For each token, a multi-hot vector equal in size to the number of labels was provided as ground-truth, and each token inherited the ground-truth labels of its paragraph. At inference time, we obtained a similar predicted multi-hot vector from each token, and each label was assigned (or not) based on the majority vote of the paragraph's tokens. To bypass the 512 token limitation of XLM-R, we used a sliding window with a 50% overlap. We also report the results for a naïve stratified random baseline, where we randomly draw labels from a multinomial distribution, while respecting the frequency statistics in the training set.",
          "Since our task is multi-label multi-class classification with a large number of classes, and noticeable class imbalance, we opted to use suitable metrics that are robust in these conditions. Table 4 presents the F 1 macro and sample-averaged F 1 scores for all models in both domains (CC, URW). Sample-average F 1 score is calculated by calculating the F 1 score on each instance and subsequently averaging over all instances. We observe that the XLM-RoBERTa large classifier outperforms Llama3.1-70B in the fine-grained case both for document-and paragraph-level predictions, especially in sample F 1 , where the difference is very pronounced. The Llama3.1 prompted model seems to be more competitive in coarse-grain evaluation. Interestingly, including the full guidelines in the prompt generally achieves worse results than using the taxonomy labels only. When we add both the taxonomy labels and the guidelines, performance improves, compared to using only the guidelines, but it is unclear whether it is better to include only the taxonomy labels. Hierarchical prompting does not seem to yield competitive results. In terms of per-label performance, the weakest model performance was measured on the sub-Narratives under \"Downplaying climate change\" and \"Green policies are geopolitical instruments\" Narratives in the CC domain and \"Russia is the Victim\" and \"Blaming the war on others rather than the invader\" in the URW domain. One key observation is that models exhibited relatively lower performance for the \"[Narrative]: Other\" labels, that is, the paragraphs that contained a Narrative but none of the listed sub-Narratives, that may include a very diverse set of argumentation, coming from a set of heterogeneous sub-Narratives that our taxonomy does not cover. Performance also seemed to vary noticeably per language. In case of Russian, Llamma3.1-70b models seemed to perform rather poorly on a per-sample level. On XLM-RoBERTa, English consistently exhibited poorer performance compared to other languages, with Bulgarian and Portuguese showing different performance per domain. We provide a detailed breakdown of the performance (by label and by language) of two models (XLM-RoBERTa f ine and Llama3.1-70bguidelines-labels) on Appendix D.",
          "We present PolyNarrative, a new multi-lingual, multi-label, multi-domain dataset for extraction of narratives at the paragraph level from long news articles. The dataset comprises 1,476 articles totaling 13,625 paragraphs in 4 languages (Bulgarian, English, Portuguese, and Russian), annotated using two expert-refined Narrative hierarchical taxonomies with 38 and 36 fine-grained and 10 and 11 coarse-grained narratives for each domain. We describe in detail the data acquisition and annotation process and highlighted noteworthy issues. We present preliminary experimental results using both fine-tuned encoder (XLM-R) and prompting-based decoder (Llama 3.1) Language Models in multiple granularities (fine, coarse), levels (paragraph, document), and configurations (prompting strategies). We also highlight interesting findings regarding model performance. We hope that this dataset will catalyze new research directions in narrative detection and extraction, and stimulate the development of new methods and techniques. We highlight that the taxonomies are based on expert real-world experience and they are not meant to be perceived as complete or to capture the whole breadth of the media discourse around the two domains. We encourage the research community to propose extensions that potentially capture more diverse perspectives. We encourage the direction of developing methods that are domain-and potentially taxonomy-agnostic, able to perform narrative detection in a completely unsupervised fashion, removing the dependence on human subjectivity. We believe that a multi-domain and highly granular dataset such as this one can facilitate experimentation towards this goal. Regarding future work, we plan to experiment with ways to perform generation of new narrative labels and automatic revision of taxonomy and guidelines. One way to approach this is by analyzing the structure of narrative elements in the text relate them with narrative labels and identify new structures. We also intend to use retrieval strategies to perform narrative retrieval within large news corpora.",
          "Intended Use and Misuse Potential The main drive behind the creation of our dataset was to advance research on automated narrative classification and the detection of deceptive content across multiple languages and domains. However, given that possible risk of exploiting the dataset to boost the production of biased manipulative disinformation attempts, we advise responsible use. In this context, whoever develops a Narrative detection system is also responsible for deciding which Narratives to detect, in an ethical manner. Furthermore, this research in direction can contribute to the development of independent tools such as browser plugins or independent publicly available services that could help users contextualize the information they consume, contributing to their awareness and improving citizens' media literacy. Environmental Impact The deployment of LLMs might have a large carbon footprint, especially when training new models. In the context of the reported experiments, we did not train any new LLMs, but only used existing trained models in an in-context zero-shot scenarios, which is relatively cheap in terms of computing. Fairness The majority of the annotators, primarily researchers with linguistic background and prior annotation experience, come from the institutions of the co-authors of this manuscript. They were fairly remunerated as part of their job. The remaining part of the annotator pool consisted of (a) some students from the respective academic organizations, (b) few external experienced analysts paid at rates set by their contracting institutions, and (c) experts from a contracted a professional annotation company, who were compensated according to rates based on their country of residence.",
          "Taxonomies and Dataset Representativeness Our taxonomies were edited by experienced media analysts, active in the study of misinformation and fact-checking. As such, the taxonomies overrepresent Narratives of interest of media analysts from Western institutions. The selection should not be perceived as covering the complete discourse of the two domains, but rather what such analysts encounter in practice. The original taxonomy for URW used before the heavy revision from the analysts, was not from a peer-reviewed publication but from a technical report. The dataset presented covers two widely discussed domains around the world and a wide range of media sources. However, it should not be considered as representative of the media in any specific country, nor should it be considered as balanced in any way. Also, it is important to note that while this work focuses on narratives potentially containing mis/disinformation, the operationalized definition of Narrative can also be used to detect neutral or desirable viewpoints. Biases Although the annotators were trained and made acquainted with the specifics of the two domains of interest for our task and cross-language quality control mechanisms have been put in place in the annotation process, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset. Consequently, models trained on this dataset might exhibit certain biases. and Han Qiu. 2024. The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16259-16303, Bangkok, Thailand. Association for Computational Linguistics. Wajdi Zaghouani, Mustafa Jarrar, Nizar Habash, Houda Bouamor, Imed Zitouni, Mona Diab, Samhaa El-Beltagy, and Muhammed AbuOdeh. 2024. The FIGNEWS Shared Task on News Media Narratives. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 530-547, Bangkok, Thailand. Association for Computational Linguistics.",
          "This section introduces the taxonomies of narratives and subnarratives for the two domains. A.1 Ukraine-Russia War (URW) 1. Blaming the war on others rather than the invader: statements attributing responsibility or fault to entities other than Russia in the context of Russia's invasion of Ukraine. Example: \"The economic crisis is due to Western sanctions.\" Example: \"Ukraine's actions provoked this conflict.\" Sub-Narratives: (a) Ukraine is the aggressor: Statements that shift the responsibility of the aggression to Ukraine instead of Russia and portray Ukraine as the attacker. Example: \"Ukraine secretly provoked the war because it was harassing Donbass province citizens.\" (b) The West are the aggressors: Statements that shift the responsibility for the conflict and escalation to the Western block. Example: \"The real perpetrators were US/EU. They sabotaged Minsk II agreement only to force Russia to invade.\" 2. Discrediting Ukraine: Statements that undermine the legitimacy, actions, or intentions of Ukraine or Ukrainians as a nation. Example: \"Ukraine's government is corrupt and ineffective.\" Example: \"Ukrainian soldiers are committing atrocities.\" Example: \"Ukrainian identity does not exist\" Sub-Narratives: (a) Rewriting Ukraine's history: Statements that aim to reestablish history of Ukrainian nation in a way that discredits its reputation. Example: \"Ukraine is not a real nation, it was a fabrication to split Russia and ally with Hitler.\" (b) Discrediting Ukrainian nation and society: Statements that aggressively undermine the legitimacy and reputability of Ukrainian ethnicity and people (c) Discrediting Ukrainian military: Statements that aim to undermine the capabilities, professionalism or effectiveness of the Ukrainian armed forces. (d) Discrediting Ukrainian government and officials and policies: Statements that seek to delegitimize the Ukrainian government, its leaders, and its policies, portraying them as corrupt or incompetent. (e) Ukraine is a puppet of the West: Claims that Ukraine is controlled or heavily influenced by Western powers, particularly the United States and European Union. (f) Ukraine is a hub for criminal activities: Allegations that Ukraine is a center for illegal activities such as human trafficking, drug smuggling, or organized crime (g) Ukraine is associated with nazism: Accusations that Ukrainian society or government has ties to or sympathies with Nazi ideology, often referencing historical events or extremist groups. (h) Situation in Ukraine is hopeless: Statements that portray Ukraine as having no viable perspectives or no potential positive future. Example: \"Ukraine should just give up, it is all over debt and will be exploited by the West anyway.\" 3. Russia is the Victim: Statements that portray Russia as being unfairly targeted or victimized. Example: \"Russia is being unfairly sanctioned.\" Example: \"The West is ganging up on Russia without justification.\" Example: \"Russia is doing what every country would do (e.g. protect its interests/honour etc.)\" Sub-Narratives: (a) The West is russophobic: Statements that claim that the negative reaction to Russia's actions are because of the negative perspective of western countries instead of Russia's own actions. Example: \"Politicians in the West blame Russia for everything, instead of looking at their mistakes.\" Example: \"In Country X, they banned Tchaikovsky ballets and Chechov's plays because they cannot stand Russia and its culture.\" (b) Russia actions in Ukraine are only selfdefence: Statements that justify Russia's action solely as legitimate self-defence and not a deliberate action. Example: \"There was no other way than war to defend the Russian-speaking people in Donbass.\" (c) UA is anti-RU extremists: Statements claiming that Ukraine is comprised of extremist elements that are vehemently opposed to Russia. 4. Praise of Russia: Statements that positively highlight Russia's actions, policies, or character Example: \"Russia is leading the way in international diplomacy.\" Example: \"The Russian economy is resilient and strong.\" Example: \"Glorifying mentions of Russia's weapon systems and military might.\" Sub-Narratives: (a) Praise of Russian military might: Statements that positively highlight Russia's military institutions, equipment and scale. Example: \"Russia has far more tanks and powerful artillery that US/EU would only dream of.\" (b) Praise of Russian President Vladimir Putin: Statements that present Vladimir Putin positively, including his personal and leadership qualities. Example: \"Any country would want such a strong leader as Putin to lead the way.\" (c) Russia is a guarantor of peace and prosperity: Statements that portray Russia solely in a positive manner, emphasising their potential to provide peace and prosperity to those that cooperate. Example: \"Take a look at Africa, Russia supports countries and turns them into independent nations guided by their people's interests where Western countries colonised brutally.\" (d) Russia has international support from a number of countries and people: Statements that emphasise the popularity and acceptance of Russia in the international stage. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" (e) Russian invasion has strong national support: Statements that emphasise the popularity and acceptance of the invasion inside Russia and on Russian-speaking populations. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" 5. Overpraising the West: Statements that excessively and unduly laud or extol the virtues, accomplishments, and moral superiority of Western countries, particularly in the context of international relations and military. Sub-Narratives: (a) NATO will destroy Russia: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) and its allies are capable or already in the process of eradicating Russia. (b) The West belongs in the right side of history: Statements that portray Western nations and their actions as morally superior and aligned with progress and justice and possess moral superiority. (c) The West has the strongest international support: Statements that emphasize or claim widespread backing for Western policies and actions from the international community, potentially downplaying opposition or criticism. 6. Speculating war outcomes: Statements that predict or make assumptions about the potential results or consequences of a conflict Sub-Narratives: (a) Russian army is collapsing: Statements that suggest or claim that the Russian military is experiencing a significant decline in its effectiveness, strength, or morale. (b) Russian army will lose all the occupied territories: Speculative statements that predict or assume the potential outcomes of the conflict, specifically regarding the possibility of the Russian military losing control of all the territories it currently occupies. (c) Ukrainian army is collapsing: Statements that suggest or claim that the Ukrainian military is experiencing a significant decline in its effectiveness, strength, or morale. 7. Discrediting the West, Diplomacy: Statements that criticize the Western countries, or international diplomatic efforts. Example: \"The West is hypocritical in its foreign policy.\" Example: \"Western diplomacy has failed in resolving conflicts.\" Example: \"International organizations will not solve anything because...\" Sub-Narratives: (a) The EU is divided: Statements that present the EU as a set of divided entities and interests, usually unable to take actions. Example: \"The European Council will never vote on sanctions for Russia, since they cannot agree on even the simplest of the issues.\" (b) The West is weak: Statements presenting the West overall as a non-potent group of countries (that is not as powerful as it used to be). Example: \"The weakened West is once again impotent to act in front of the will Russia.\" (c) The West is overreacting: Statements that claim that the West and its institutions are reacting to Russia's actions in a disproportionate manner. Example: \"Putin did not invade the EU but Ukraine. Imposing harsh sanctions is not the way to deal with it, dialogue and debate is.\" (d) The West does not care about Ukraine, only about its interests: Statements that claim that the West is only interested in Ukraine for its own benefits, disregard-ing the country's fate. Example: \"The West has indebted Ukraine more than XX bln of dollars, a lucrative deal for western companies to exploit.\" Example: \"NATO's actions are endangering global security.\" (e) Diplomacy does/will not work: Statements discrediting the potential of ongoing or potential diplomatic efforts. Example: \"Diplomats are desperately trying to figure out solutions but now it's too late, they have failed and Russia is free to do whatever.\" (f) West is tired of Ukraine: Claims that Western countries, particularly the United States and European nations, are becoming fatigued or disinterested in supporting Ukraine and its efforts. 8. Negative Consequences for the West: Statements that highlight or predict adverse outcomes for Western countries and their interests. Example: \"Sanctions against Russia will backfire on Europe.\" Example: \"The West is headed for an economic downturn.\" Sub-Narratives: (a) Sanctions imposed by Western countries will backfire: Statements that catastrophize on the possible negative effects for Western sanctions of Russia. Example: \"The winter is going to be cold and with current gas prices, we are talking of societal unrest.\" (b) The conflict will increase the Ukrainian refugee flows to Europe: Statements that catastrophize on the possible refugee outflows due to the conflict. Example: \"Like we did not have refugees from the Middle East, now we will have Ukrainians stressing our housing and healthcare problems.\" 9. Distrust towards Media: Statements that question the reliability or integrity of media organizations. Example: \"Western media is spreading propaganda.\" Example: \"You can't trust what the news says about Russia.\" Sub-Narratives: (a) Western media is an instrument of propaganda: Statements that discredit the media institutions of the West and claim that they are instruments of propaganda. Example: \"... but you wouldn't hear this on a western channel, only the party line from State Department.\" (b) Ukrainian media cannot be trusted: Statements that discredit the media institutions of the Ukraine and claim that they should not be trusted for reporting on the war. Example: \"Ukraine is conducting its own propaganda using their TV channels, news and social media.\" 10. Amplifying war-related fears: Statements that evoke fear or anxiety about potential threats, dangers or reactions. Example: \"The West is pushing us towards World War III.\" Example: \"It is a matter of time before war speads on the West\" Example: \"Nuclear war is imminent\" Sub-Narratives: (a) By continuing the war we risk WWIII: Statements that warn against upsetting Russia's and its leadership, evoking fear of causing WW3. Example: \"The Western elites with their fixation on Russia are sleapwalking towards WW3\" (b) Russia will also attack other countries: Statements that claim that it is imminent that Russia will attack other countries. Example: \"... and be sure, Ukraine is the first not the last country to be invaded. Others will follow.\" (c) There is a real possibility that nuclear weapons will be employed: Statements that evoke fear or anxiety about the use of nuclear weapons. Example: \"... and if Western hypocrisy continues to provoke, Putin might be forced to press the red button. . . for good\" (d) NATO should/will directly intervene: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) ought to or will take direct military action in a conflict, potentially implying a shift in policy or strategy. 11. Hidden plots by secret schemes of powerful groups: Statements that suggest hidden plots or secretive actions by powerful groups related to the war. Example: \"There's a secret plan by the elites to control global resources.\" Example: \"The war is just a cover for something much bigger.\" A.2 Climate Change (CC) 1. Criticism of climate policies: Statements that question the effectiveness, economic impact, or motives behind climate policies.",
          "(a) Climate policies are ineffective: Statements suggesting that climate policies fail to achieve their intended environmental goals. (b) Climate policies have negative impact on the economy: Statements claiming that climate policies lead to negative economic outcomes. (c) Climate policies are only for profit: Statements that argue climate policies are driven by financial or corporate gain rather than genuine environmental concerns.",
          "Statements that challenge the competence, integrity, or intentions of various institutions and authorities in relation to climate change. Sub-Narratives: (c) Temperature increase does not have significant impact: Statements claiming that the increase in temperature is not going to have any noticeable effect in nature. (d) CO2 concentrations are too small to have an impact: Statements claiming that the concentrations of CO2 will have a negligible effect. (e) Human activities do not impact climate change: Statements that support that climate change is not caused by human activity. (f) Ice is not melting: Statements claiming that there is not melting of ice. (g) Sea levels are not rising: Statements denying that sea levels have risen (or will rise). (h) Humans and nature will adapt to the changes: Statements claiming that whatever the changes in climate humans or nature will manage to find solutions to adapt. 5. Questioning the measurements and science: Statements that raise doubts about the scientific methods, data, and consensus on climate change. Sub-Narratives: (a) Methodologies/metrics used are unreliable/faulty: Statements claiming that the scientific methodologies and metrics used to measure climate change are flawed or unreliable. (b) Data shows no temperature increase: Statements asserting that available data does not support the claim of global temperature increase. (c) Greenhouse effect/carbon dioxide do not drive climate change: Statements asserting that available data does not support the claim of global temperature increase. (d) Scientific community is unreliable: Statements discrediting scientists, the scientific community and their actions. 6. Criticism of climate movement: Statements that challenge the motives, integrity, or impact of the climate movement. Sub-Narratives: (a) Climate movement is alarmist: Statements suggesting that the climate movement exaggerates the severity of climate change for dramatic effect. (b) Climate movement is corrupt: Statements alleging that the climate movement is influenced by ulterior motives, by corruption or by unethical practices. (c) Ad hominem attacks on key activists: Statements attacking the reputation of key figures (such as scientists, activists, politicians or public figures). 7. Controversy about green technologies: Statements that express skepticism or criticism of environmentally friendly technologies. Sub-Narratives: (a) Renewable energy is dangerous: Statements claiming that renewable energy sources pose significant risks or dangers. (b) Renewable energy is unreliable: Statements asserting that renewable energy sources are not dependable for widespread adoption. (c) Renewable energy is costly: Statements asserting that renewable energy sources are too expensive, inefficient and worth adopting for widespread use. (d) Nuclear energy is not climate friendly: Statements asserting that nuclear sources are or should not be considered as good for the climate. Statements that minimize the urgency of addressing climate change by suggesting that any action taken at this point is futile or too late to make a meaningful impact. 10. Green policies are geopolitical instruments: Statements claiming that that environmental policies and initiatives are used as tools for geopolitical power and influence rather than genuine environmental concern.",
          "(a) Climate-related international relations are abusive/exploitative: Statements criticizing international relations related to climate change as exploitative or economically abusive. (b) Green activities are a form of neocolonialism: Statements suggesting that green initiatives are a way for developed countries to exert control and influence over developing nations, a modern form of colonial practices.",
          "Figures 4 and 5 report the normalized count of coarse labels per language. There are substantial differences, notably between RU and other languages for URW and between EN and other languages for CC. These differences could be due either to bias in media of each country, or be due to bias in the sampling of the articles to annotate. The reason RU is absent for CC is that it was not possible to find enough articles to annotate. In Table 5, we report on the difficulty of annotating labels. We report the proportion of each label to have a disagreement between annotators both with the Other label specifically, or with any other label. Based on the numbers, we categorise each label in a class of difficulty.",
          "In this Section we provide the different prompts used in the experiments. C.1 0shot (only taxonomy) You are an experienced analyst making labeling articles with labels specific to the war in Ukraine. Labels: {codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the above list. The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst labeling articles with labels specific [\"to the war in Ukraine\"/ \"Climate Change\"] You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below. ",
          "",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You are given the coarse label and you are asked to return the fine label. You will apply a label if the article presents at any point a narrative that is given in the annotation guidelines given below.  D Selected model performance per label and language We provide detailed breakdown of the performance of our top-performing model on fine-grained level (sub-Narratives). We provide both the detailed scores per-label (Figures 6, 7) and per-language (Figures 8,9,10,11) for the two domains (CC and URW). ",
          "We report on the detailed dissagreement statistics of the annotators per each fine-grained label, and highlight the difficulty of each label, in Table 5.    label count %conf %other mcl difficulty 1: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor 169 0.309 0.315 5 Medium 2: URW: Blaming the war on others rather than the invader: The West are the aggressors 260 0.377 0.390 7 Medium 3: URW: Discrediting Ukraine: Rewriting Ukraine's history 23 0.272 0.111 28 Medium * 4: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society 38 0.380 0.053 6 Medium * 5: URW: Discrediting Ukraine: Discrediting Ukrainian military 316 0.314 0.217 1 Medium * 6: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies 502 0.215 0.253 8 Easy 7: URW: Discrediting Ukraine: Ukraine is a puppet of the West 219 0.235 0.165 28 Easy * 8: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities 127 0.368 0.243 6 Medium * 9: URW: Discrediting Ukraine: Ukraine is associated with nazism 97 0.189 0.080 6 Easy * 10: URW: Discrediting Ukraine: Situation in Ukraine is hopeless 107 0.303 0.220 6 Medium * 11: URW: Russia is the Victim: The West is russophobic 167 0.343 0.427 2 Medium 12: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence 130 0.518 0.273 1 Hard * 13: URW: Russia is the Victim: UA is anti-RU extremists 34 0.477 0.406 11 Hard * 14: URW: Praise of Russia: Praise of Russian military might 466 0.140 0.346 5 Easy 15: URW: Praise of Russia: Praise of Russian President Vladimir Putin 100 0.157 0.348 17 Easy 16: URW: Praise of Russia: Russia is a guarantor of peace and prosperity 257 0.352 0.385 1 Medium 17: URW: Praise of Russia: Russia has international support from a number of countries and people 228 0.160 0.245 16 Easy 18: URW: Praise of Russia: Russian invasion has strong national support 20 0.333 0.250 17 Medium * 19: URW: Overpraising the West: NATO will destroy Russia 11 0.263 0.400 20 Medium 20: URW: Overpraising the West: The West belongs in the right side of history 36 0.263 0.133 28 Medium * 21: URW: Overpraising the West: The West has the strongest international support 24 0.254 0.308 7 Medium 22: URW: Speculating war outcomes: Russian army is collapsing 51 0.313 0.476 23 Medium 23: URW: Speculating war outcomes: Russian army will lose all the occupied territories 10 0.312 0.200 22 Medium * 24: URW: Speculating war outcomes: Ukrainian army is collapsing 93 0.242 0.118 5 Easy * 25: URW: Discrediting the West, Diplomacy: The EU is divided 105 0.164 0.435 28 Easy 26: URW: Discrediting the West, Diplomacy: The West is weak 145 0.301 0.267 14 Medium * 27: URW: Discrediting the West, Diplomacy: The West is overreacting 34 0.395 0.294 11 Medium * 28: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests 211 0.381 0.262 7 Medium * 29: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work 109 0.550 0.434 28 Hard * 30: URW: Discrediting the West, Diplomacy: West is tired of Ukraine 43 0.583 0.171 28 Hard * 31: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire 79 0.338 0.723 12 Medium 32: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe 12 0.000 NaN Easiest 33: URW: Distrust towards Media: Western media is an instrument of propaganda 104 0.180 0.452 11 Easy 34: URW: Distrust towards Media: Ukrainian media cannot be trusted 23 0.500 NaN 6 Hard 35: URW: Amplifying war-related fears: By continuing the war we risk WWIII 135 0.285 0.339 37 Medium 36: URW: Amplifying war-related fears: Russia will also attack other countries 133 0.211 0.270 2 Easy 37: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed 292 0.259 0.486 2 Medium 38: URW: Amplifying war-related fears: NATO should/will directly intervene 63 0.305 0.273 36 Medium * 39: CC: Criticism of climate policies: Climate policies are ineffective 74 0.407 0.174 44 Hard * 40: CC: Criticism of climate policies: Climate policies have negative impact on the economy 96 0.337 0.333 45 Medium * 41: CC: Criticism of climate policies: Climate policies are only for profit 58 0.508 0.069 68 Hard * 42: CC: Criticism of institutions and authorities: Criticism of the EU 54 0.275 NaN 45 Medium 43: CC: Criticism of institutions and authorities: Criticism of international entities 104 0.434 0.196 44 Hard * 44: CC: Criticism of institutions and authorities: Criticism of national governments 225 0.375 0.274 45 Medium * 45: CC: Criticism of institutions and authorities: Criticism of political organizations and figures 190 0.487 0.172 44 Hard * 46: CC: Climate change is beneficial: CO2 is beneficial 19 0.071 NaN 51 Easiest 47: CC: Climate change is beneficial: Temperature increase is beneficial 13 0.266 0.500 54 Medium 48: CC: Downplaying climate change: Climate cycles are natural 36 0.294 0.300 50 Medium 49: CC: Downplaying climate change: Weather suggests the trend is global cooling 13 0.406 0.308 70 Hard * 50: CC: Downplaying climate change: Temperature increase does not have significant impact 5 0.736 0.286 56 Hardest * 51: CC: Downplaying climate change: CO2 concentrations are too small to have an impact 18 0.500 0.143 59 Hard * 52: CC: Downplaying climate change: Human activities do not impact climate change 34 0.387 0.083 60 Medium * 53: CC: Downplaying climate change: Ice is not melting 18 0.117 NaN 62 Easy 54: CC: Downplaying climate change: Sea levels are not rising 2 0.500 0.500 47 Hard 55: CC: Downplaying climate change: Humans and nature will adapt to the changes 5 1.000 0.400 50 Hardest * 56: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty 51 0.333 0.118 59 Medium * 57: CC: Questioning the measurements and science: Data shows no temperature increase 10 0.562 0.333 56 Hard * 58: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change 5 0.750 1.000 Hardest 59: CC: Questioning the measurements and science: Scientific community is unreliable 45 0.540 0.185 56 Hard * 60: CC: Criticism of climate movement: Climate movement is alarmist 67 0.619 0.386 61 Hardest * 61: CC: Criticism of climate movement: Climate movement is corrupt 35 0.869 0.463 60 Hardest * 62: CC: Criticism of climate movement: Ad hominem attacks on key activists 78 0.234 0.217 60 Easy * 63: CC: Controversy about green technologies: Renewable energy is dangerous 18 0.421 0.250 65 Hard * 64: CC: Controversy about green technologies: Renewable energy is unreliable 42 0.344 0.333 45 Medium * 65: CC: Controversy about green technologies: Renewable energy is costly 27 0.512 0.100 45 Hard * 66: CC: Controversy about green technologies: Nuclear energy is not climate friendly 3 1.000 0.750 45 Hardest * 67: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites 47 0.438 0.217 68 Hard * 68: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives 74 0.548 0.094 45 Hard * 69: CC: Amplifying Climate Fears: Earth will be uninhabitable soon 51 0.432 0.188 70 Hard * 70: CC: Amplifying Climate Fears: Amplifying existing fears of global warming 864 0.124 0.477 71 Easy 71: CC: Amplifying Climate Fears: Doomsday scenarios for humans 104 0.435 0.239 70 Hard * 72: CC: Amplifying Climate Fears: Whatever we do it is already too late 18 0.264 0.111 70 Medium * 73: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative 7 0.785 0.273 68 Hardest * 74: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism 10 0.375 0.333 68 Medium * Table 5: Label difficulty, %conf is the percentage of annotation with that label that results in an inconsistency, while %other is the percentage that resulted in an inconsistency with label other. \"mcl\" is the most common label with which a specific label is confused. \"difficulty\" make assess the difficulty of the label based on %conf: Easiest (≤ .1), Easy (≤ .25), Medium (≤ .4), and Hard (≤ .4) or Hardest (> 0.6), a star indicate that there is more confusion within the labels of the taxonomy than with the Other class, meaning that it is extra difficult for the annotator"
        ],
        "ground_truth_definitions": {
          "narrative in the context of news articles": {
            "definition": "descriptions of spatiotemporal events and capturing narrative components in the form of semantic relationships between entities.",
            "context": "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity.",
            "type": "implicit"
          },
          "Opinion Mining": {
            "definition": "task to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity.",
            "context": "This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes.",
            "type": "explicit"
          },
          "sub-narrative": {
            "definition": "lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative.",
            "context": "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, “CO2 is beneficial” is a sub-Narrative of the “Climate Change is beneficial” Narrative of the Climate Change (CC) taxonomy.",
            "type": "implicit"
          },
          "Narratives": {
            "definition": "overt or implicit claims, recurring across articles and languages, promoting a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinformation.",
            "context": "We present PolyNarrative, a new multilingual dataset of news articles, annotated for narra- tives. Narratives are overt or implicit claims, recurring across articles and languages, promot- ing a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinfor- mation. We developed two-level taxonomies with coarse- and fine-grained narrative labels for two domains: (i) climate change and (ii) the military conflict between Ukraine and Russia.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "eb29476dd81aefedf2896db42f039f003a0ec5bf",
        "sections": [
          "Intense competition from large-scale growers has forced small-scaled farmers to find new niche markets for their commodities through value-added marketing. To discover the right niche market is a complicated task because demand is highly segmented among consumers who may be concerned with different attributes such as local, organic, and other specialty types. A recent survey of 500 Twin Cities consumers showed that consumers buy locally grown produce primarily because of its high quality and freshness (King, 2007). Previous studies that focused on consumers' choices at farmers' markets in other states showed that today's consumers support local farmers' markets because they expect higher quality (for example, freshness and taste) and lower prices (Brown, 2002(Brown, , 2003)). Organically grown produce is considered to be healthy and environmentally friendly because of the use of less-damaging pesticides (Magnusson et al., 2001;Thompson and Kidwell, 1998). For some consumers, premium price is the major obstacle in buying organic foods (Magnusson et al., 2001). Yet, ''locally grown'' produce is not necessarily ''organically grown'' produce. Locally grown produce can be produced using conventional production methods. Organic produce purchased by Minnesota consumers might be from California or New Zealand. Finding out why certain consumers prefer organically grown or locally grown produce and their willingness to pay (WTP) for fresh produce with different attributes is important for stakeholders such as producers and marketers in making their production and marketing decisions because it helps them figure out what type of fresh produce to grow and sell, what to emphasize in marketing efforts, and what are reasonable prices to charge. The literature on consumer preferences for and marketing segmentation of organically and locally grown fresh produce is sparse. Loureiro and Hine (2002) studied consumers' WTP for local (Colorado-grown), organic, and GMO-free potatoes. They found that the ''Colorado-grown'' attribute afforded potato producers the highest consumer acceptance and premium compared with organic and GMO-free potatoes. However, only consumers' attitudes toward potato freshness and nutrition were included in their estimation. Important consumer attitudes toward certain quality attributes such as appearance, taste, food safety, and so on (Yue et al., 2007(Yue et al., , 2008) ) were not included in their study. The traditional hypothetical survey method is preferred by most academic researchers in consumer preference studies. The hypothetical method asks individuals to choose the values they are willing to pay for certain products without asking them to exchange money and real goods. Although widely used, a question about this approach that has been aroused is whether values elicited from hypothetical experiments reflect the amounts individuals would actually pay. It is well documented that consumers are less reluctant to spend money in stated preference studies than in markets with real economic commitment. In the literature, this is known as the hypothetical bias (see, for example, Alfnes et al., 2006). Previous studies also show that there is significant hypothetical bias when consumers' WTP is elicited using pictures instead of real products in hypothetical studies in which participants do not need to pay for the products shown in the pictures (see, for example, Yue et al., 2008). Lusk and Schroeder (2004) found that there is no significant difference between marginal WTP elicited from hypothetical and nonhypothetical experiments when real products instead of pictures are used in both experiments. The objective of this study was to investigate consumers' WTP for organically grown and locally grown fresh produce and the marketing segmentation of these two types of produce. The results of this research are very important for small-scale farmers, market organizers, and sponsoring agencies in making their production and marketing decisions. This study also contributes to the literature on consumers' choice and demand segmentation between organically grown and locally grown produce. In addition, our research is different from previous studies in that we used and compared two methods to investigate consumer preference for and WTP for organically and locally grown fresh produce. We used both the traditional hypothetical experiment and the nonhypothetical experiment with real economic incentives. The use of a choice format is new in the consumer preference and WTP literature. Lusk and Schroeder (2004) developed the choice format with posted prices in different scenarios to investigate consumers' WTP for food quality attributes. Alfnes et al. (2006) used a nonhypothetical choice experiment to investigate consumers' WTP for salmon with different colors. The hypothetical experiment and nonhypothetical choice experiment with real economic incentives give data in similar formats and are complimentary to each other; one gives hypothetical WTP data and the other gives real WTP data. The use of the two methods helps eliminate the hypothetical bias that exists in traditional hypothetical experimental methods and get consumers' real WTP for fresh produce with different attributes.",
          "We tested the data from the hypothetical experiment and the nonhypothetical choice experiment for similarities and used the two data sources in a joint estimation. We tested if the use of real products instead of pictures can eliminate hypothetical bias. The details of both experiments are presented subsequently. Attributes. The fresh produce used for the study was tomatoes because they are widely grown locally and organically in Minnesota. In our experimental design, there were two qualitative attributes (organically grown and locally grown) and one quantitative attribute (price). Each qualitative attribute had two levels-yes or no. The number of levels for the quantitative factor, price, was six. The price ranged from $1.25 to $2.50 with $0.25 as intervals based on real tomato prices in different stores in the Twin Cities area when the experiments were conducted. Table 1 summarizes the levels of attributes. There were four combinations of the two qualitative attributes as shown in Table 2. In our experiment, we named the locally grown (no) and organically grown (no) as ''conventional,'' locally grown (no) and organically grown (yes) as ''organic,'' locally grown (yes) and organically grown (no) as ''local,'' and locally grown (yes) and organically grown (yes) as ''organic + local.'' The tomatoes were purchased from a farmers' market, a supermarket, and an organic farm in St. Paul, MN. Other product attributes such as tomato size, color, shape, size, firmness, and ripeness were not included in the design. However, the tomatoes were of the same type and when we made purchases, we carefully compared tomatoes' color, size, shape, ripeness, and firmness to make them be very similar to each other in these attributes. Therefore, the tomatoes' possible differences in these attributes were controlled and participants said that they were unable to distinguish between them. Scenarios. In the hypothetical and nonhypothetical experiments, participants were presented with a series of pricing scenarios and asked to choose one alternative in each pricing scenario. If none of the alternatives was of interest, they could choose ''none of these.'' To lessen the cognitive burden on participants, only two alternatives were usually included in each scenario in a choice experiment. In our experiments, each scenario had two alternatives described by conventional, organic, local, or organic plus local attributes and price. Because it was not practical to ask each participant to choose from all possible scenarios, a fractional factorial design was developed to minimize scenario number and maximize profile variation. The design was developed based on four principles: level balance (levels of an attribute occurred with equal frequency), orthogonality (the occurrences of any two levels of different attributes were uncorrelated), minimal overlap (cases in which attribute levels did not vary within a scenario were minimized), and utility balance (the probabilities of choosing alternatives within a scenario were kept as similar as possible) (Louviere et al., 2000). The fractional factorial design gave each participant sets of 12 scenarios to evaluate. The clearly dominated alternatives were eliminated. For a further discussion of factorial design, see Louviere et al. (2000). The following is an example of one of the scenarios: Please examine the tomatoes on the table. Consider a situation in which you are buying 1 lb of fresh tomatoes. From the following pairs of alternatives, please choose one alternative that you would like to purchase (you can choose ''none'' if you do not want either of them): Scenario 1-conventional $1.75; organic + local $2.25; none of these. Next to organic tomatoes, we put a USDA Certified Organic label; beside local tomatoes, we put a Minnesota Grown label; and beside locally grown, organic tomatoes, we put both the USDA Certified Organic label and the Minnesota Grown label. Hypothetical experiment. For the hypothetical experiment, participants were not asked to buy anything after they made choices. Real products were used in the hypothetical experiment. Participants were asked to answer a series of questions such as what kind of retailers they were most likely to patronize and a series of questions designed to extract information on their attitudes toward and preference for the organic and local attributes of fresh produce. Nonhypothetical experiment. For the nonhypothetical choice experiment, one of the pricing scenarios was randomly drawn as binding, i.e., the choices made in that scenario would be implemented. Participants would receive the products they chose and were required to pay the prices posted for the corresponding products in the binding scenario. The nonhypothetical choice experiment had the following five steps-Step 1: The experimental procedure was explained to participants and some information about the alternatives (the three attributes) was provided. Step 2: Participants studied the alternatives in scenarios and marked on a choice scheme which of the alternatives in each scenario they wanted to buy. Step 3: After participants completed all scenarios, each participant drew one card with a number determining his or her binding scenario. Step 4: Each participant received the fresh produce he or she chose in his or her binding scenario. Step 5: Participants went to the cashier and paid for their fresh produce. After participants made their choices, they were asked to answer the same set of questions as in the hypothetical experiment. Sampling. A booth was set up at the Minnesota State Fair to conduct the hypothetical and the nonhypothetical choice experiments in Aug. 2008. The Minnesota Sate Fair is the largest U.S. state fair in terms of average daily attendance (Minnesota State Fair, 2008). The sociodemographics of fair attendees are close to Minnesotan sociodemographics except that a slightly higher proportion of those aged between 35 and 44 years attend the Fair who live in Minnesota (Minnesota State Fair, 2008;U.S. Census Bureau, 2004). Therefore, we expected to get a representative sample of Minnesota consumers. State Fair participants were randomly selected to participate in the experiments. Each participant only participated in one type of experiment, either the hypothetical experiment or the nonhypothetical experiment. A small amount of cash or gifts worth $6 to $8 was given to participants to compensate for their time. To ensure that the sample was representative, only one family member per household was allowed to participate in the experiments. Most participants at the Minnesota State Fair came as families. When we recruited participants, we asked the primary food purchaser of the household to participate in the experiments. In total, 365 participants were surveyed with 115 participating in the nonhypothetical experiment and the remaining 250 participating in the hypothetical experiment. Some of the data were not included in the analysis because of one or more missing values. Observations of 343 participants with 110 participants from the nonhypothetical experiment and 233 from the hypothetical experiment were included in the analysis. Statistical model. Consumers' satisfaction derived from choice and consumption of dif-  (Train, 2003). We used the mixed logit model to capture all possible correlations for responses from the same participant and scenario. The statistical model was: where U nis was the utility of individual n from alternative i in scenario s; x nis were vectors of observed variables relating to alternative i and individual n; b was a vector of fixed coefficients; h was a vector of normally distributed random terms with mean zero used to capture the possible correlations; and nis was an identical and independent extreme value error term. The standard logit model is a special case of the mixed logit model where h has zero variance. The density of h was denoted by f (hjV), where V was the fixed parameter vector of the distribution. For a given h, the conditional choice probability of alternative i was a standard logit: Consequently, the unconditional choice probability, P, in the mixed logit model was the logit formula integrated overall values of h with density of h as weights: This integral should be solved analytically and was approximated through simulation (Brownstone and Train, 1999). The maximum likelihood estimation method was used to estimate coefficients with Stata 10.0 software (StataCorp, College Station, TX).",
          "The summary statistics of the sample are shown in Table 3. Approximately 84.51% of participants purchased fruits and vegetables more than once a week and only 0.28% of participants purchased fruits or vegetables less than once a month. These results, derived from the data, indicated that the sample of participants was representative of consumers who are regular fresh produce consumers. The average age of participants was 45 years old. Seventy-three percent of participants lived in urban areas and 23% of them belonged to certain environmental groups. Fourteen percent of participants had children younger than 12 years old in the household, 66% of participants were female, and the average education level was a college degree. On average, there were two to three people in a household and the average income level was $69,000. A large proportion of participants (68%) purchased most of their fresh produce from supermarkets, 27% from a roadside stand or farmers' market, 16% from high-end specialty stores, 11% from cooperatives, 9% from mass merchandisers, and 7% from discount stores. In the questionnaire, we also asked participants how important certain characteristics of locally grown and organically grown fresh produce were to their purchase decision using a Likert scale with 1 being not important and 5 being very important. The percentage of participants who considered different characteristics at various levels of importance are listed in Tables 4 and5. The characteristics were chosen based on previously published studies (Loureiro and Hine, 2002;King, 2007;Yue et al., 2007Yue et al., , 2008)). For locally grown fresh produce, 83% of participants considered freshness as ''very important'' and 72% of them rated safe to eat as ''very important.'' Forty-eight percent of participants thought lower price ''somewhat important'' and 17% of them regarded it as ''very important.'' Supporting the local econ-omy was a good reason for people to choose locally grown fresh produce as indicated by the fact that 74% of participants chose 4 or 5 on the Likert scale. Environmentally friendly was also considered to be important for 61% of participants. Carbon footprint was a concern for 54% of participants, whereas 25% of participants did not regard it as a major concern. For organically grown fresh produce, 52% of participants considered good for health to be ''very important'' and 26% of them thought it to be ''important'' (they chose 4). Forty percent of them regarded lower price as ''somewhat important'' and 42% of them chose 4 or 5, which means that these people thought that lower price was ''important'' or ''very important'' for organic fresh produce. Approximately 83% of participants thought safe to eat as ''important'' or ''very important'' for organic fresh produce and 67% of them considered environmentally friendly as ''important'' or ''very important.'' These results were consistent with previous studies showing that consumers associate organic production methods with a reduced health risk and may choose to reduce the risk from pesticide residues by switching to organically grown products (Magnusson and Cranfield, 2005;Williams and Hammitt, 2001). Recent survey evidence shows that consumers purchased organic foods because they perceived the foods to be fresh (68.3%), better for health, and a better source of nutrition (67.1%; Whole Foods Market, 2005). Furthermore, over 70.3% of the con- The numbers in parentheses were the percentages of participants who belong to the corresponding categories. x Participants were told to choose all that applied. sumers said they bought organic food or beverages to avoid pesticides. To get some idea of participants' past purchases of locally grown and organically grown produce, we asked participants ''When you buy fruits and vegetables, how often do you buy locally grown (organically grown) fresh produce when locally grown (organically grown) options are available?'' For locally grown produce, 14% of participants chose ''always,'' 40% chose ''most times,'' 38% chose ''sometimes,'' and 8% chose ''seldom'' or ''never.'' For organically grown produce, 6% chose ''always,'' 15% chose ''most times,'' 39% chose ''sometimes,'' and 40% chose ''seldom'' or ''never.'' The estimation results of the mixed logit model are shown in Table 6. The P value for the likelihood ratio test was <0.01, which indicated that the model fit the data well. Although some variables have a naturally interpretable metric, others do not, especially ordinal variables and interaction effects (McCall, 2001). Therefore, to simplify interpretation of parameters associated with the quality attributes, variables except organic, local, price, and real and the associated interaction effects among them were standardized with a mean of zero and a SD of one. The standardization was done by subtracting the respective variable's mean and dividing by its SD. We found that the P values for the correlations among the scenarios and the responses from the same participant were pretty large (>0.95). Therefore, the correlations were deemed to be highly insignificant. Price premium for local and organic attributes. The intercept and coefficients of organic, local, price, and organic*local were highly significant. The negative sign of the coefficient of price indicated that the higher the price of the product, the lower the proba-bility that participants would be likely to choose the product. The intercept divided by the coefficient of price is the consumer WTP for 1 lb of conventional tomatoes and the amount was $1.18. The coefficient of organic divided by that of price is the premium the consumer was willing to pay for 1 lb of organic tomatoes, which was $0.72. Similarly, the premium for the local attribute was $0.73. The premium for locally grown, organic tomatoes was not the sum of the two premiums, which was indicated by the highly significant negative coefficient of organic*local. Instead, the premium was $1.16 (the sum of coefficients of organic, local, and organic*local divided by the coefficient of price). Hypothetical bias. The marginal significant coefficients of real*price (P = 0.09) indicates that there might exist hypothetical bias for the WTP elicited using the hypothetical experiment compared with the nonhypothetical choice experiment. By calculation, we found that the hypothetical bias for organic attributes was 7.5% of the premium elicited using the nonhypothetical choice experiment. The hypothetical bias for the premium of the local attribute was 9% of the premium elicited using the nonhypothetical choice experiment. Relative to the $1.16 premium for the local plus organic attribute in the hypothetical experiment, participants were willing to pay a premium of $1.07 for 1 lb of locally grown, organic tomatoes in the nonhypothetical choice experiment. In this respect, our results are in line with the large literature on hypothetical bias in valuation studies. Previous studies have shown that the difference between the real and hypothetical study is not fixed with the changes in WTP (Yue et al., 2008). Some previous studies have found that hypothetical bias is quite high when pictures are used in a hypothetical experiment compared with a nonhypothetical experiment using real products (see, for example, Alfnes, 2003;Harrison and Rutström, 2005;Yue et al., 2008). In contrast, we found that hypothetical bias was not that high when real products were used in the hypothetical experiment. The results indicated that even if hypothetical bias might not be completely eliminated as found by Lusk and Schroeder (2004), it can be significantly decreased when real products were used to elicit consumer WTP for certain attributes in a hypothetical experiment. The findings have very important implications for future studies on consumers' WTP literature. Nonhypothetical experiments should be used; if the employment of a nonhypothetical experiment is not practical, real products rather than pictures of products should be used to lower any possible hypothetical bias. Sociodemographic background of consumers. We found that consumers' sociodemographic backgrounds affected their choice and WTP for conventional, organic, local, or locally grown, organic tomatoes. The interaction effect between age and organic was significant and negative, which indicated that older participants were less likely to choose organic tomatoes compared with younger participants. Older participants were more likely to purchase local tomatoes compared with younger participants, but the coefficient was not highly significant (P = 0.13). The significant and positive coefficient of income* organic meant that wealthier participants were more likely to purchase organic tomatoes. This result was in line with other studies on organic attributes of fruits and vegetables (see, for example, Yue et al., 2008). The coefficient of gender*local was significant and negative, which means that female participants were more likely to purchase locally grown tomatoes compared with male participants. In the questionnaire, we asked participants if they belonged to any groups associated with the environment. The estimation results showed that those participants who belonged to environmental groups were least likely to purchase conventional tomatoes and were most likely to purchase locally grown tomatoes followed by organic tomatoes. These results were similar to those by Yue et al. (2007), in that consumers who belonged to environmental groups were more likely to purchase organic apples. Participants' educational backgrounds affected their choice of tomatoes with different attributes. Participants with higher educational levels were more likely to purchase conventional tomatoes and less likely to purchase organic tomatoes. Participants who live in urban areas were most likely to purchase organic tomatoes followed by locally grown tomatoes and were least likely to purchase conventional tomatoes compared with participants from rural areas. One possible explanation is that consumers who live in urban areas may be less likely to grow fresh produce than those who live in rural areas. People experienced with growing products know that there is not as large a quality difference between locally produced fruits and nonlocally produced fruits compared with those who have no growing experience (Yue et al., 2007). Hartman (1997) also showed that the consumption of organic food was skewed in urban areas. We found that household size did not significantly differentiate participants in their choice of tomatoes with different attributes. However, participants with children younger than age 12 years had a higher chance of choosing locally grown tomatoes even if the coefficient was not highly significant (P value was 0.13). Also, these participants were more likely to choose organic tomatoes and less likely to choose conventional tomatoes as indicated by the positive sign of the coefficient of child*organic and the negative sign of the coefficient of child, even if the estimated coefficients were not statistically significant. Store categories. The estimation results showed that participants bought different types of tomatoes from different types of stores. Participants who purchased most fresh produce from mass merchandisers were most likely to choose conventional tomatoes, less likely to choose locally grown tomatoes, and least likely to choose organic tomatoes. Participants who purchased fresh produce mainly from cooperatives had the highest probability of purchasing organic tomatoes, followed by locally grown tomatoes, and they had the lowest probability of purchasing conventional tomatoes. Participants who bought the most fresh produce from specialty stores were more likely to purchase organic tomatoes and less likely to purchase locally grown tomatoes, but the results were not statistically significant. Those participants who often patronize supermarkets were most likely to purchase conventional tomatoes, followed by organic tomatoes, and they were least likely to purchase locally grown tomatoes. Participants who visited discount stores most often were less likely to purchase organic tomatoes. Those participants who often patronized roadside stands and farmers' markets were most likely to purchase locally grown tomatoes, less likely to purchase organic tomatoes, and least likely to choose conventional tomatoes. Marketing implications. These results have important marketing implications for fresh produce growers, wholesalers, and retailers. A large proportion of consumers consider ''freshness'' and ''safe to eat'' as ''very important'' for purchasing locally grown fresh produce, which indicates that these attributes should be stressed by local growers when they promote their products in advertisements, newsletters, and so on. As for organic fresh produce, ''good for health'' and ''safe to eat'' are their selling points and should be emphasized in promotional materials. There exists a group of consumers who view ''lower price'' as important for local and organic fresh produce, which was consistent with findings of previous studies that premium price was the major obstacle to buying organic foods for some consumers (Magnusson et al., 2001). To promote to this price-sensitive segment, emphasis on ''affordability'' by decreasing the price premium to a certain level will work to increase sales of organic and local fresh produce. The sociodemographics of consumers who choose conventional, local, or organic attributes have important implications for choices of target markets for fresh produce with differentiated attributes. Older female consumers with lower than average education levels who shop in urban markets are the right target market to promote locally grown fresh produce, whereas younger consumers with above-average income who live in urban areas are the main target market for organic produce. The average price premiums for organic, local, and organic plus local attributes of tomatoes were $0.67, $0.67, and $1.06, respectively, compared with conventional tomatoes after correction of hypothetical bias. These price premiums give growers guidelines when they decide what prices to charge for tomatoes with different attributes. Comparing prices that growers, wholesalers, and retailers actually charge with these premiums can provide these parties with information about whether their charged prices were consistent with consumers' perceptual values of these different attributes. Based on this information, the associated parties can make adjustments on their pricing strategies to attract more potential consumers. Finding out these price premiums can also help growers choose the right production methods and profitable value-added attributes to promote by comparing the premiums with the associated production and marketing costs. Interestingly, we found that the average premiums consumers were willing to pay for organic tomatoes and locally grown tomatoes were about the same. However, the profitability of the two types of produce might not be the same as a result of variation in production costs. Local growers can get a higher premium if they grow organic tomatoes and promote them as both locally grown and organically grown instead of organically grown alone in local markets. The results show that consumers actually patronize different stores to buy fresh produce with different attributes. Therefore, growers and wholesalers need to choose the right stores to sell their products, and retailers need to source the right types of produce for these parties to make profits. Mass merchandisers and supermarkets are the major places consumers purchase conventional produce, which means that conventional fresh produce growers need to choose these retail outlets as their major venues to sell their products. Cooperatives are where most consumers purchase organic produce, indicating that organic fresh produce growers might be able to sell more of their products and can charge a relatively higher price if they choose to sell their products at cooperatives. In contrast, discount stores and mass merchandisers where people expect to get low-priced products are not the major place for growers to sell organic produce. Supermarkets might be another choice for selling organic fresh produce. For locally grown fresh produce, consumers are most likely to purchase them from farmers' markets and roadside stands. These direct marketing channels are the best venues for local growers to sell their products. Cooperatives are also good venues to sell locally grown products, whereas mass merchandisers and supermarkets might not be able to gain enough profits by selling locally grown fresh produce. From the supply perspective, the availability of organic and locally grown produce in different stores also contributes to consumer patronage of these stores in purchasing fresh produce with different attributes. Fresh produce provides the greatest chances for direct sales to consumers through farmers' markets and roadside stands that are commonly used by small-scale growers. Given the fact that most local growers are small scale in Minnesota, it is not surprising that consumers are most likely to choose direct marketing channels to purchase locally grown produce. The growth of organic food has given rise to new marketing outlets in addition to traditional organic outlets such as cooperatives. Mass market groceries and chain stores entered the organic market place at a time of low prices for commodities at the farm gate. However, it has been documented that approximately one third of organic products are sold through mass market grocery stores with a disproportionately higher percentage of processed food and dairy products instead of fresh produce (Klonsky, 2000). In recent years, some mass merchandisers like Walmart started to source and advertise organic fresh produce and they could get a significant market share if they are able to keep prices low enough (Business Week, 2006) The hypothetical and nonhypothetical experiments were conducted at the Minnesota State Fair. One issue to consider is whether the results are representative of consumers in general (Mitchell and Carson, 1989). In our survey, the sample chosenthose who participated in the Minnesota State Fair-does not necessarily represent all fresh produce consumers in the United States, and in this regard, there is some population choice bias. The ''often'' variable indicated that most of participants were regular fresh produce consumers, so we expect the sample selection bias to be minor. We thus view our sample respondents as representing a population of the central Midwest instead of representing the total U.S. population. Given the potential biases, the extrapolation of our findings to other populations should be made with caution. We interpret the findings relative to the population attending local and regional fairs and markets.",
          "To assist small-and medium-sized growers who want to capitalize on growth in demand for differentiated produce, this article presents consumer WTP for the organic, locally grown, and locally grown organic attributes, the characteristics consumers perceive as important when they make purchase decisions of these differentiated fresh produce, and the various types of retail venues where consumers purchase these different products. A combination of the traditional hypothetical experimental method and nonhypothetical choice experiment was used to investigate consumer preference for valueadded fresh produce. The combination of these two methods can help eliminate any possible hypothetical bias in the hypothetical experimental method. We found that hypothetical bias existed using the hypothetical experimental method, but the bias was not high if real products were used in that survey. We elicited consumer WTP for local and organic attributes for fresh tomatoes. We found consumers perceived various characteristics as important for their purchase decision of organic and local fresh produce. Consumers' sociodemographic backgrounds affected their choice of fresh produce with different attributes and consumers patronized different types of stores to purchase conventional, organic, and local fresh produce."
        ],
        "ground_truth_definitions": {
          "hypothetical bias": {
            "definition": "the difference between what people say they will pay and what they would actually pay",
            "context": "We found that when real products were used in the hypothetical experiment, the hypothetical bias (the difference between what people say they will pay and what they would actually pay) was not high.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10462-022-10338-7",
        "sections": [
          "Recent years have shown a growing interest in the application of natural language processing techniques for extracting, summarizing, and creating new data from text. Today's digital media ecosystem generates massive streams of unstructured data, such as news articles, web pages, blog posts and tweets, thus posing a set of challenges related to their automatic understanding. Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narrative can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time. Although several attempts have been conducted in the domain of narrative extraction, some problems remain unsolved, which are not easily solved by the currently available Natural Language Processing (NLP) and Information Extraction (IE) techniques. Important challenges in this domain involve defining annotation schemes that are comprehensive enough to include the relevant features of the narrative elements, but, at the same time, not too cumbersome to avoid overloading the process of extraction; automatically extracting cohesive narratives from unstructured data with high effectiveness; detecting and characterizing storylines over single and multiple documents; identifying narrative segments within large portions of text; dealing with informal language and with different types of documents (e.g., social media and micro-blogs); coping with different human languages and with combinations of languages; defining meaningful formal representation of narratives as an intermediate step to visualization schemes; devising a standard evaluating narrative framework (made of datasets, baselines and metrics) as an entry-point for researchers interested on properly evaluating their methods; and contributing with the development of specialized linguistic resources for low-resource languages, such as narrative annotated data, to avoid creating niches. To help the research community address these challenges, different techniques can be applied depending on the type and scale of the narrative, e.g. single news articles versus whole fiction books. In this paper, we aim to survey existing approaches and techniques related to the process of extracting narratives from a text, an emergent theme of research within the field of computational linguistics and artificial intelligence. To the best of our knowledge, there is currently no survey addressing the full process of narrative extraction from textual data. Hence, collecting, organizing, documenting and describing fundamental concepts of this process is of paramount importance for different related areas, where the extraction of narratives plays an important role. Domains such as journalism (Caselli et al. 2016), finance (El-Haj 2022), health (Sheikhalishahi et al. 2019) , information access in digital libraries (Kroll et al. 2022), and other fields that require analyzing narratives over a common topic using NLP approaches, are among the main beneficiaries of this survey. Such communities are very active in the area, which makes it quite hard for a researcher to be aware of all the relevant contributions. It should be noted that in the different contexts where narratives can be extracted, there are common problems; however, there is also terminology, notation and specificities for each one. While in a financial narrative, there is a need for structure extraction (El-Haj et al. 2019) given the format in which the data are generally available, in narratives related to historical events (Lai et al. 2021), there is no such need. In this work, such particularities will not be discussed; instead, we will seek to focus on tasks that are common to narrative extraction from a general point of view. In an attempt to provide a thorough account of relevant research developed within the field of computational linguistics and artificial intelligence (AI), we conduct a survey where the different approaches are organized according to the pipeline of tasks one can find in the extraction of narratives from text. The development of this study began with the selection of fundamental articles understood as key to the subareas that make up the extraction of narratives. From these articles, we apply the snowball procedure (Wohlin 2014) to expand the pool of articles by considering relevant research published in high-quality conferences and journals in the fields of natural language processing and artificial intelligence. The remainder of this paper is organized as follows. Section 2 gives an overview of the process of extracting narratives from a text by introducing the fundamental concepts, the data acquisition and annotation effort, and the narrative extraction pipeline. Section 3 presents the initial step of narrative extraction that comprises the pre-processing and parsing of a text. Section 4 introduces research developed on the identification and extraction of narrative components, namely lexical and syntactical components. Section 5 presents the techniques found in the literature to establish the linkage between the identified narrative components. Section 6 refers to research work focused on the representation of narratives structures. Section 7 provides a snapshot of the metrics and the datasets behind the evaluation efforts. Section 8 promotes a discussion of some important aspects of narratives research including open issues. Finally, Sect. 9 concludes this paper by presenting its final remarks.",
          "In this section, we begin by presenting a detailed definition of the concept of narratives and its associated terminology. Following, we discuss important steps related to data acquisition and annotation. Finally, we describe the general pipeline behind the process of extracting narratives from textual data through the lens of a computational scope. The pipeline here introduced will define the structure of the rest of this paper.",
          "Narratives have long been studied in the field of linguistics. One of the first authors to introduce a formal definition was Adam (1992), who considers narratives to be a prototypical sequence obeying a thematic unity, regarding a chronological succession of events involving characters. The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theatrical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community. Riedl (2004) considers narratives as a cognitive tool for situated understanding, i.e., a structure designed to better understand the world around us. Motta (2005), instead, understands narratives as forms of relationships that are established due to culture, the coexistence between living beings that have interests, desires, and that are under the constraints and social conditions of hierarchy and power. As the author states, whoever narrates has some purpose in narrating, therefore no narrative is naive. The same narrative can be seen from a different point of view, i.e., from a focal point for seeing, hearing, smelling, and experiencing the accessible. Stripping the data of unnecessary information and making sure that relevant metadata is kept is also key during this process. A case in point related to narrative extracted from news is the publication date, which is relevant to determine its timeline. What follows is designing a suitable annotation scheme that is simultaneously tailored to encompass all the particularities of the target language(s), and comprehensive enough to be applied to other datasets or even be broaden. Bearing in mind, on the one hand, the diversity of annotation frameworks, and, on the other hand, the usefulness of establishing comparisons between annotated corpora from different genres in the same language, but also across languages, many proposals try to achieve this balance utilizing acknowledged standards, which have resulted in, for e.g., Ontologies of Linguistic Annotation (OLiA) (Chiarcos 2014), and ISO 24617-Language resource management-Semantic annotation framework. The decision about the annotation framework, and about the different layers of annotation, depends necessarily on a variety of factors, like the annotation purpose, the texts genre, among others (Pustejovsky et al. 2017). In the case of the narratives annotation, since it is relevant to feature participants, events, time and space, as well the relationships between them, the annotation scheme can be designed to include several intertwined semantic layers enabling temporal, referential, thematic, and spatial annotations (see for example Silvano et al. (2021)). The adequacy of the annotation tool is also of great relevance for the efficiency of different tasks, namely creation, browsing, visualization, and querying of linguistic annotations. Although one can choose to tailor a tool to the specific features of one's project, it may be labor-saving to resort to the existing ones and, if necessary, proceed with some modifications. Some of the existing annotation tools that enable annotating different markables with several attributes, as well as establishing links between those markables are the following: MMAX2 (Müller and Strube 2006), MAE and MAI Stubbs (2011), BRAT (Stenetorp et al. 2012), or ANNIS (Krause and Zeldes 2014). During the process of narrative extraction, some degree of human participation is necessary, either to verify the feasibility of the annotation scheme, to annotate linguistic aspects for which automatic models are insufficient, or to supervise the automatic annotation. Different strategies can be adopted depending, namely, on the type of annotators that the project wants or needs, with or without linguistic training, specifically: crowdsourcing (Estellés-Arolas and de Guevara 2012), class sourcing (Christopher Blackwell 2009) or gamification (Stieglitz et al. 2016). When using human annotation, calculating interannotator agreement (IAA) is crucial to ensure its reliability and accuracy. The two most used metrics in computational and corpus linguistics are: Cohen's Kappa (and its variation, Fleiss's Kapppa) and Krippendorff 's Alpha (Pustejovsky and Stubbs 2012).",
          "The study of narratives from the perspective of natural language texts can be summarized into five major stages: (1) Pre-Processing and Parsing; (2) Identification and Extraction of Narrative Components; (3) Linking Components; (4) Representation of Narratives and (5) Evaluation. Each of these tasks gives rise to the structure adopted in the rest of this survey. An overall picture is shown in Fig. 1, and a detailed description of each task can be found in the following sections.",
          "Pre-processing and parsing of a text comprise a set of lexical and syntactic tasks. Lexical activities aim to split the text into basic units, called tokens, and to normalize them into different forms; syntactical analysis identifies the grammar class of tokens, and identifies the dependency between the tokens, producing trees or chunks that relate the tokens to each other. The following subsections detail both tasks.",
          "Lexical tasks seek to standardize the input text as a way to better prepare the data content for the following steps (Sun et al. 2014). Despite being a simple procedure, they are considered to be a key factor to achieve effective results (Denny and Spirling 2018). The main lexical tasks required for the narrative extraction process are: (1) Sentence segmentation, i.e., dividing the whole text into sentences and tokens to gather the lexicon of the language (Palmer 2007); for this task, using deep learning (DL) methods Schweter and Ahmed (2019) achieved SOTA results through the use of a multi-lingual system based on three different architectures of neural networks. (2) Text tokenization, applied to break sentences into tokens, the smallest unit of a text (Vijayarani et al. 2016); and (3) Text Cleaning and Normalization, which may involve a number of optional cleaning steps, including removing numbers, punctuation marks, accent marks, or stopwords, as well as apply stemming (Jabbar et al. 2020) and lemmatization (Bergmanis and Goldwater 2018) to observe other possible discrepancies between equal or similar words. These are foundational NLP tasks; hence they will not be explored in detail in this article. More details regarding these tasks can be found in Raina and Krishnamurthy (2022).",
          "The lexical analysis precedes a more profound observation, which aims to identify the grammar class of the words and to infer the relationship between words in a sentence to help understand the meaning of a text. To go over this process, we use the text presented in Fig. 2 as a running example in the remainder of this survey. ",
          "The first step in this stage is to assign parts-of-speech tags to each word of a given text (e.g., nouns, verbs, adjectives) based on its definition and its context, which is called the Part-of-Speech (PoS) tagging task. The current state-of-the-art is settled by Bohnet et al. (2018) in several languages. In this work, the authors used recurrent neural networks with sentence-level context for initial character and word-based representations, achieving an accuracy of 97.96% on top of the Penn Treebank dataset Marcus et al. (1993). With respect to narrative texts, since their elements can be typically associated to different word classes (for instance, nouns to participants, verbs to events), POS tagging plays a fundamental role (Palshikar et al. 2019;Quaresma et al. 2019;Yu and Kim 2021). Figure 3 shows the result of applying the Stanzafoot_0 library (Qi et al. 2020) PoS tagger to the first sentence of our running example. In the figure, PROPN refers to a proper noun, PUNCT to punctuation, DET to determiner, VERB to a verb, and ADP to adposition. These tags are defined under the Universal Dependencies guidelines (de Marneffe et al. 2021).",
          "Following PoS tagging, the parsing of the text can be conducted. The chunking task (a.k.a, shallow parsing) is responsible for identifying constituent parts of the sentences (nouns, verbs, adjectives, etc.) and linking them to higher-order units that have discrete grammatical meanings (noun groups or phrases, verb groups, and others). Figure 4 illustrates the results of applying chunking on top of the first sentence of our running example through the use of the benepar library (Kitaev et al. 2019). By looking at the figure, we can observe that both \"a\" (determiner -DT), as well as \"rabbit\" (noun-NN), belong to a higher group of noun phrases (NP). Note that adpositions (tagged as \"ADP\" in Fig. 3) are now represented in Fig. 4 with the \"IN\" tag. As in other tasks, the application of deep learning approaches (Hashimoto et al. 2017;Zhai et al. 2017;Akbik et al. 2018) has brought important improvements to this particular task. The next step of the pipeline is to understand how all the words of the sentence relate to each other, which is done by a dependency parsing analysis. The objective, as referred by Jurafsky and Martin (2009), is to assign a single headword to each dependent word in the sentence through labeled arcs. The root node of the tree, that is, the head of the entire Fig. 2 Running example Fig. 3 Results of applying the stanza PoS tagging processor to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" structure, will be the main verb in the sentence (e.g., \"pulled\" in our running example). This task can enhance the performance of named entity recognition (further discussed in Sect. 4.2) models and be used to increase information extraction (de Oliveira et al. 2022). Figure 5 shows a dependency analysis of the first sentence of our running example alongside the type of relationship predicted to occur between two tokens. The results obtained stem from applying the Stanford CoreNLP Parser (Chen and Manning 2014), one of the most well-known tools in this regard. A quick look at the figure emphasizes the association between several terms, among which we highlight the relationship hierarchy between \"John\" and \"magician\". The current state-of-the-art of this task, in the English and Chinese languages, is settled by Mrini et al. (2020). The model proposed by the authors used a combination of a label attention layer with Head-driven Phrase Structure Grammar (HPSG), and pre-trained XLNet embeddings Yang et al. (2019). In a narrative context, the application of these tasks can be particular useful to identify relations between narrative components (further discussed in Sect. 4). Fig. 4 Results of applying the benepar chunking model to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Fig. 5 Results of applying the Stanford CoreNLP Parser Dependency to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\"",
          "Coreference resolution, also known as anaphora resolution, uses information about the semantic relation of coreferent expressions to infer the reference between them (Jiang and Cohn 2021). In addition to that, all the terms that reference the same real-world entity are marked (Mitkov 2014). For example, the word \"presentation\" (found in sentence 2 of our running example) refers to \"show\" (which can be found in sentence 1). Coreference resolution is essential for deep language understanding, and has shown its potential in various language processing problems (Poesio et al. 2016). Bringing it to the context of narrative extraction, some works highlight the importance of this task in the most different scenarios, such as clinical narratives (Jindal and Roth 2013a), newswire (Do et al. 2015), and also in violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System (Uppunda et al. 2021). In a general domain, several approaches have been proposed over the years (Pradhan et al. 2012;Mitkov 2014;Lee et al. 2013), but recently there has been a shift towards adopting transformers models (Ferreira Cruz et al. 2020;Kantor and Globerson 2019) to improve the connection between textual elements and, as such, enhance the coreference task. The current SOTA of coreference resolution is marked by Attree (2019) and Kirstain et al. (2021), both sticking to deep learning methods.",
          "Table 1 presents the current status of the tasks covered in this section. This includes lexical tasks (sentence segmentation, text tokenization, data cleaning, normalization, stopwords removal, stemming, lemmatization) and syntactic (PoS tagging, chunking, dependency parsing, coreference resolution). Herein, however, we only list those that have been undergoing constant improvements over the most recent years. In this table, the effectiveness of each study is shown according to the measures reported by the authors.",
          "The next step in the narrative extraction pipeline is to identify and extract the main elements that compose a narrative. This step comprises tasks like the detection and classification of events, the recognition and the classification of named entities (participants), the extraction of temporal information and of spatial data.",
          "Finding the events mentioned in the text is an essential step towards the extraction of narratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person. Xie et al. (2008) went a little bit further, and suggested describing real-world events through the use of the 5W1H interrogatives: when, where, who, what, why, and how, anchored on journalistic practices. Based on this reference, an event such as the one referenced in our running example might be depicted along with the six aspects: who-John, the magician; when-2022 (taking into account the date of writing the text); Table 1 State-of-the-art in pre-processing and parsing tasks Task Ref. Method Effectiveness Benchmark dataset Sentence segmentation Schweter and Ahmed (2019) BiLSTM 98.62 F1-score (English) Europarl Corpus Koehn (2005) 99.70 F1-score (Greek) SETimes Alperen and Tyers (2010) 99.86 F1-score (Arabic) Leipzig Corpus Goldhahn et al. (2012) Text tokenization Graën et al. (2018) Language-specific and language independent token identification rules 1% Error rate SMULTRON Treebank Volk et al. (2015) Lemmatization Bergmanis and Goldwater (2018) Standard encoder-decoder architecture + character-level sentence context 92.4 Accuracy (ambiguous words) Universal Dependency Treebank v2.0 Nivre et al. (2020) 84.30 Accuracy (unseen words) Part-of-speech tagging Bohnet et al. (2018) Recurrent network model 97.96 Accuracy Penn Treebank Marcus et al. (1993) Chunking Akbik et al. (2018) Neural character-level language modeling 96.72 F1-score Penn Treebank Marcus et al. (1993) 96.34 F1-Score Dependency parsing Mrini et al. (2020) Label attention layer + HPSG + XLNet 97.39 UAS Penn Treebank Marcus et al. (1993) 96.29 LAS Coreference resolution Attree (2019) Neural attention model 92.50 F1-Score GAP Dataset Webster et al. (2018) where -XYZ contest realized in Edinburgh; what-the magician pleased the audience during his presentation; how-taking a rabbit out of his hat; why-his presentation was better than the last one. The task of event detection has its roots in the late 90's when the Topic Detection and Tracking (TDT) project emerged (Allan et al. 1998) as the first attempt to cope with the rising of huge volumes of data. Among the entire project, the detection of new events, also known as the first story detection (FSD) (Kontostathis et al. 2004), was a subtask of the TDT project, concerned with the detection and subsequent tracking of the first and coming stories of a given news event. One way to detect events is to rely on event triggers, cues that express an event's occurrence, most often single verbs or phrasal verbs, as referred by Boroş (2018), but also nouns, noun phrases, pronouns, adverbs, and adjectives. According to Araki (2018), 95% of the ACE (Automatic Content Extraction) event triggers consist of a single token. This is the case of our running example, where the events pulled, loved, presented are triggered by verbs, and show, contest and presentation are also events represented by nouns. Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argument role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 subtypes, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example. The ACE 2005's predefined event scheme is presented in the left-hand part of the figure. The event trigger (\"pulled\"), the 3 argument roles (\"Arg-Entity\"), and 2 modifiers (\"Arg-Time\" and \"Arg-Place\") identified for the type \"Contact-Meet\" type are illustrated on the right-hand side of the figure. In this visualization, the time-reference \"this year\" is normalized to 2022 based on the document creation time (DCT). The current state-of-the-art in event extraction task for the English language is provided by Feng et al. (2018), who developed a language-independent neural network. The authors tested on top of the ACE 2005 English event detection task (Walker et al. 2006) data, achieving an F1-score of 73.40%. More detailed information about the event detection task can be found in Saeed et al. (2019) and a recent survey from Xiang and Wang (2019). In recent years, the task of monitoring streams of data and detection events has made a shift towards microblogs platforms, such as Twitter (Atefeh and Khreich 2015), a very different scenario when compared to the TDT era, when algorithms were developed to track news stories over time from traditional media like newspapers (Kalyanam et al. 2016). The detection of events also plays a fundamental role in different kinds of applications, domains, and languages, as is the case of clinical narratives (Jindal and Roth 2013b;Adams et al. 2021), and historical events (Lai et al. 2021). Despite the significant advances in the last few years, the application of event extraction techniques in the context of narrative representation has been quite limited (Metilli et al. 2019), and only recently a few works began to emerge. Metilli et al. (2019), for example, made use of numerous novel discourse and narrative features, besides common relations, such as the time at which the event takes place, or coreference events, which detects whether two mentions of events refer to the same event (Araki 2018). Aldawsari and Finlayson ( 2019) presented a supervised model to automatically identify when one event is a sub-event of another, a problem known as sub-event detection or event hierarchy construction. Another strand tackles the problem of nested event structures, a common occurrence in both open domain (not limited to a single topic or subject) and domain specific (dedicated to a particular problem representation or solution) extraction tasks (McClosky et al. 2011). For instance, a \"crime\" event can lead to an \"investigation\" event, which can lead to an \"arrest\" event (Chambers and Jurafsky 2009). The research of Metilli et al. (2019) applied a recurrent neural network model to event detection applied to biography texts obtained from Wikipedia. The model used a set of event categories (e.g., birth, conflict, marriage, and others) to depict a narrative. In the context of news stories, Zahid et al. (2019) proposed the development of heuristics to segment news according to a scheme that defines the organization and order of the events. The strategy followed in that research employed mechanisms that journalists usually exploit to compose news.",
          "Another essential category of narrative elements is the participants. They are the \"who\" of the story, relevant to the \"what\" and the \"why\". Participants, sometimes also referred to as actors, are involved in events in a varied number of ways and often correspond to entities in NLP. These, are usually identified through Named Entity Recognition (NER), a generic task aiming at seeking, locating and categorizing the entities mentioned in the text into predefined categories. Although the three most common named entities are person, organization, and location, further types such as numeric expressions (e.g., time, date, money, and percent expressions) can also be considered (Nadeau and Sekine 2007). In narrative terms, participants are more frequently restricted to the person and organization categories, though other categories such as animals may also be used. In our running example this would result in annotating \"John\" and \"audience\" as a person, and \"rabbit\" as an animal as can be observed in Fig. 7. Overall, research on named entity recognition can be categorized into four main categories: rule-based approaches, unsupervised learning approaches, feature-based supervised learning approaches, and deep-learning-based approaches (Li et al. 2022). Beyond this, other aspects, such as textual genres, entity types, and language, are also considered by research conducted on named entity identification (Goyal et al. 2018). The current stateof-the-art in named entity recognition was obtained by Wang et al. (2021) on the CoNLL 2003NER task (Tjong Kim Sang and De Meulder 2003). To automate the process of finding better concatenations of embeddings for structured prediction tasks, the authors proposed Automated Concatenation of Embeddings (ACE) based on a formulation inspired by recent progress on neural architecture search. The method achieved an F1-score of 94.60%. In narrative contexts, Lee et al. (2021) present a proposal that goes beyond the identification of entities in narratives, proposing a multi-relational graph contextualization to capture the implicit state of the participants in the story (i.e., characters' motivations, goals, and mental states). Piper et al. (2021) point out that NLP works focusing on agents have emphasized broadening the understanding of characters beyond named entities. This has been done through the concept of animacy detection, as in agents like \"the coachman\" or \"the frog\", while also distinguishing characters from other named referents (Piper et al. 2021). According to Oza and Dietz (2021)'s proposal, selecting a set of relevant entities for a story construction could be best achieved by using entity co-occurrences in retrieved text passages -especially when the relative relevance of passages is incorporated as link strength. In line with what the authors claim, their proposal is between 80% and 30% more effective than the best link-based approach.",
          "Temporal references are of the utmost importance for the understanding of the narrative and its timeline by anchoring each event or scene at a point in time. The process of identifying temporal references is commonly referred to as Temporal Tagging, and can be split into two sub-tasks: extraction and normalization (Strötgen and Gertz 2013). The former aims to correctly identify temporal expressions, and can be seen as a classification problem. The latter aims to normalize the identified temporal expressions, and can be seen as a more challenging process where different temporal expressions, carrying out the same meaning, need to be anchored at the same time-point. In this regard, three temporal taggers take the lead, Heideltime (Strötgen and Gertz 2013), SuTime (Chang and Manning 2012) and GuTime (Mani and Wilson 2000), which support the four basic types of temporal objects defined by Pustejovsky et al. (2005) in TimeML, the standard markup language for temporal annotation containing TIMEX3 tags for temporal expressions: Dates (e.g., \"December 3, 2021\"), Time (e.g., \"5:37 a.m.\"), Durations (e.g., \"four weeks\", \"several years\"), and Sets (e.g., \"every day\", \"twice a month\"). Alongside with these, other realizations of temporal expressions can be found in a text (Strötgen et al. 2012;Campos et al. 2017), which pose additional challenges. We refer to explicit (e.g., \"April 14, 2020\"), implicit (e.g., \"Christmas day 2019\"), and relative temporal expressions (e.g., \"yesterday\"), a kind of temporal expression that requires further knowledge to be normalized. A discussion of the challenges associated with each one of them can be found in Strötgen andGertz (2013, 2016). Other researchers (Jatowt et al. 2013) devised methods concerning document dating of non-timestamped documents. The rise and continuous development of temporal taggers has paved the way to the emergence of upstream research at the intersection between information extraction and several NLP tasks, where temporal expressions play an important role. For example, in the information retrieval domain, temporal information (Campos et al. 2014) can be used for temporal clustering of documents (Campos et al. 2012) or temporal ranking (Berberich et al. 2010;Campos et al. 2016); in question answering to query knowledge bases (Sun et al. 2018); in document summarization to construct timeline summaries (Campos et al. 2021); and in web archives to estimate the relevance of past news (Sato et al. 2021). At this stage of identification and extraction of narrative components, several works address the intersections between the tasks of identifying and extracting events, entities, and temporal information. Strötgen and Gertz (2012) explored event-centric aspects, considering related temporal information. In particular, they introduced the concept of event sequences, a set (or sequence) of chronological ordered events extracted from several documents. In their work, the geographic dimension played an important role by mapping event-sequences onto a map as well. In contrast, studies such as Agarwal et al. (2018) and Rijhwani and Preotiuc-Pietro (2020) explored the task of recognizing named entities considering time-aware aspects. The incipient annotation of documents to support related research has also led to the development of markup languages (e.g., TimeML (Pustejovsky et al. 2005), a formal specification language for temporal and event expressions anchored on TIMEX3 tags), annotated corpora (e.g., TimeBank corpus (Pustejovsky et al. 2006), a standard set of English news articles annotated with temporal information under the TimeML 1.2.1 (Saurı et al. 2006) guidelines) and research competitions [e.g., TempEval series (Lim et al. 2019)].",
          "The problem of extracting geographic references from texts is longstanding (Martins et al. 2008), and is highly related to the field of Geographic Information Retrieval (GIR) (Purves et al. 2018). Much of this information can be found in unstructured texts through references to places and locations (Purves et al. 2018). Many difficulties arise when attempting to comprehend geographic information in natural language or free text (e.g., under-specified and ambiguous queries) (Purves et al. 2018). Challenges related to space involve the detection and resolution of references to locations, typically, but not exclusively, in the form of place names, or more formally toponyms, from unstructured text documents (Jones and Purves 2008). In our running example, to extract the location of the main event, one can consider \"contests near Edinburgh\", which consists of three important parts, a theme (contests), a spatial relationship (near), and a location (Edinburgh). However, in the example, it is unclear whether there is only one contest in the city or multiple ones, and, if so, to which one the text is pointing to. The spatial relationship \"near\" also brings more questions than answers, making it difficult to understand whether it refers to downtown Edinburgh or some constrained space within the state of Edinburgh. Spatial relationships such as these may be both geometric (obtained using coordinate systems imposed on the real world, such as latitude and longitude) and topological (spatially related, but without a measurable distance or complete direction) (Larson 1996). The current SOTA in toponym detection and disambiguation was achieved by Wang et al. (2019) on the SemEval 2019 Task 12 (Weissenbacher et al. 2019), taking as a basis PubMed articles. In their approach, called DM_NLP, the authors made use of an ensemble of multiple neural networks. In a different approach, Yan et al. (2017) proposed a solution called augmented spatial contexts that learns vector embeddings and uses them to reason about place type similarity and relatedness. When considering space in a narrative context, other issues arise. Geographic maps, landscape paintings, and other representations of space are not always narratives, yet all narratives presuppose a world with spatial extension, even when spatial information is concealed (Ryan 2014 [Online]). Generally speaking, a narrative space is a physically existing environment in which characters live and move. This space is usually identified as a setting, that is, the general socio-historic-geographical environment in which the action takes place (Ryan 2014 [Online]), i.e., grounded by referents in the real world (e.g., the Edinburgh city in our running example) or entirely fictional (Rohan kingdom in The Lord of Rings). Aiming to identify places, some works (Bamman et al. 2019;Brooke et al. 2016) have been exploring entity recognition and toponym resolution in the context of narratives, making it possible to recognize named locations, facilities, and geopolitical entities (Edinburgh, Rohan) within this kind of text (Piper et al. 2021). Another problem is related to coreference resolution. Unlike coreference resolution of named locations, long-document coreference of common items (e.g., the house and the room), which form the narrative universe for many fictional creations, can be tough. As indicated by Piper et al. (2021), many questions can only be addressed by calculating the distance between locations described in the text: how far do Frodo and Sam go on their journey? Systems that allow better inference on spatial information within narratives could provide important insights to get the reader closer to the narrative universe.",
          "Table 2 brings a summary of the current status of the tasks covered at the identification and extraction stage discussed in this section. In this table, the effectiveness of each study is shown according to the measures reported by the authors. The type column specifies the type of task addressed in the works. All the works discussed here refer to a semantic analysis of the text. This contrasts with Table 1 where semantic tasks, but, above all, lexical and syntactic, were addressed. In our supplementary material, made available with this survey, one can find additional benchmark datasets relating to the topics covered in this section. This material has datasets available that can be used as a reference for carrying out the tasks described here. Some of these datasets are available in other languages and might help identify and extract narrative components also in these languages.",
          "After the process of identifying and extracting narrative components, it is crucial to extract the link relations between such pieces. Linking narrative components comprises the core of the extraction of narratives from the text. More than extracting separate elements from texts, establishing relations and detecting structures from them becomes essential to fully understand their meaning. In this stage, extracted pieces of information are connected, structuring the narrative at a global level. Thus, temporal and event linking, entity and  ",
          "Despite concentrating the research community's attention, predicting temporal and causal relations between events and temporal references, and investigating the evolution of an event over time as a whole (Kalyanam et al. 2016), remains a major challenge of the text understanding task (Leeuwenberg and Moens 2018) that goes beyond merely identifying temporal expressions in text documents. Ultimately, understanding a text with regards to its temporal dimension requires the following three steps: (1) identification of temporal expressions and events; (2) identification of temporal relations between them; and (3) timeline constructions. An overview of each one of them is shown in Fig. 8 for our running example. Given an input text, the system determines the temporal expressions and the events (top). Next, it assigns TimeML annotations (middle). Finally, it produces a timeline as an output (bottom). Figure 8a begins by showing the input text. By looking at it, one can easily detect \"this year\" temporal expression (normalized to 2022 assuming that the Document Creation Date-DCD-refers to November 21st, 2022) and the chronology of the events. In our example, the event pulled is carried out by a magician called John. During the show, he pulled a rabbit out of a hat. The presentation was more welcomed by the public than another previous presentation. Despite all the information collected, it is not clear whether the \"last one presented\" (event) was held during the present edition or last year's contest, or even if the \"show\" consisted of only \"pulling a rabbit out of a hat\". Ambiguity in the text might also raise doubts about whether the previous presentation refers to John or another magician. In Fig. 8b, we can observe a possible interpretation of the relation phase, where temporal links are established between the temporal expressions and the events. Finally, in Fig. 8c, we can see the generated timeline. In this visualization, one can observe a reference to the document creation time, the present time (reading time), and the ordering of the events according to the time-reference \"this year\" normalized to 2022. This apparently simple example shows how difficult it may be to understand a text in detail. In the following subsections, we present research tackling the identification of temporal expressions, events and their relations, before exploring timeline constructions. Mostafazadeh et al. (2016b), in their work, proposed a pipeline strategy for extracting events and linking them temporally, without building a timeline. The authors demonstrated that events that follow a protocol or script could be related to each other through temporal associations. These relationships are based on temporal reasoning, and provide a broad understanding of language. In another study, Roemmele and Gordon (2018) applied a neural encoder-decoder to detect events before predicting the relations between adjacent events in stories. The authors assumed that subsequent events are temporally related, and as such the extraction of events and their respective relations can be inferred. The goal is to model events that are the cause of other events and also events that are the effect of other events. Similarly, Yao and Huang (2018) deemed that the order of events in a text is the same as that of the temporal order of them in a narrative. Based on this assumption, the authors used a weakly supervised technique to propose a novel strategy to extract events and temporal relations between events in news articles, blogs, and novel books across sentences in narrative paragraphs. In another proposal, Han et al. (2019a) introduced a strategy where the extraction of events and temporal relations was jointly learned. Hence, the information about one task is employed to leverage the learning of the other. This strategy selected shared representation learning and structured prediction to solve the tasks. The results achieved were superior to the previous works. Ning et al. (2017) suggested a structured learning approach to identifying temporal relations in natural language text. Their solution was evaluated on top of the TempEval-3 data (UzZaman et al. 2013), achieving a temporal awareness [metric provided by UzZaman and Allen (2011)] of 67.2%. In the health domain, Tang et al. (2013) developed a temporal information extraction system capable of identifying events, temporal expressions, and their temporal relations found in clinical texts. In this same domain, Leeuwenberg and Moens (2020) proposed an annotation scheme for extraction of implicit and explicit temporal information for clinical reports identified events, which provides probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. A comprehensive overview of the research conducted over the years on identifying events, time references, and properly connected them is given by Derczynski (2016).",
          "",
          "The next step, after establishing the relationships between events and the temporal entities, is to arrange temporal information in such a manner that the remaining narrative components can be organized with regards to time (Leeuwenberg and Moens 2019). One important aspect of this, as stated by Leeuwenberg and Moens (2019), is the temporal reasoning task, which refers to the process of combining different temporal cues into a coherent temporal view. This is of the utmost importance as previous works have shown that the information presented in the text may not match the elapsed order (Mostafazadeh et al. 2016b). Getting to know the correct order of the events may also enable to dive deeply in the analysis of particular components. Antonucci et al. (2020) employed this notion to investigate the evolution of characters of books over time. The authors trained word embeddings related to different characters in distinct parts of literary texts (e.g., chapters). These embeddings are also called dynamic or temporal embeddings (Bamler and Mandt 2017) since, from them, it is possible to analyze relations between concepts over time. This work demonstrated that characters with strong friendships have similar behavior; therefore, such characters evolve similarly over time. Another outcome of the temporal reasoning task is building the network of events in chronological order, i.e., the text's narrative. Over the years, different researches (Bethard et al. 2007;Mani and Schiffman 2005) have been proposed with one such goal. In Leeuwenberg and Moens (2018), the authors proposed two models that predict relative timelines in linear complexity and new loss functions for the training of timeline models using TimeML-style annotations. On the sidelines, Jia et al. (2021) proposed the use of temporal information as a way to provide complex question answering related to temporal information through knowledge graphs. An overview of the temporal reasoning task, on the extraction, and on how to combine temporal cues from text into a coherent temporal view is presented in a survey of Leeuwenberg and Moens (2019).",
          "Capturing the linking between the pairs of identified entities (or participants) is an important aspect of the narrative understanding. Known as relation extraction task (Qin et al. 2021), it attempts to determine a semantic link between two named entities in a sentence. Performing this task on top of our running example would result in identifying an instance of a pulled relation between \"John\" and \"a rabbit\". Such semantic relations are usually structured in the form of < e 1 , rel, e 2 > triples, where e 1 and e 2 are named-entities, and rel is a relationship type (Batista 2016 [Online]). In our case, it could be exemplified by the triple: < John, pulled, a rabbit > . Considering that in narratives, events are often directly linked, extracting such relationships is also of paramount importance. Semantic relations also encompass objectal relations (i.e., relations between discourse entities seen as extra-linguistic concepts). These relations aim to state how two discourse entities are referentially related to one another (ISO 24617-9:2019 (E) 2000). Other NLP tasks, like Question-Answering Systems (Li et al. 2019), and the creation of Knowledge Graphs (Zhang et al. 2019b), can benefit from identifying such a relation. An example of entity relation extraction employed to build Knowledge Graphs is provided in Han et al. (2019b). In this work, the authors present OpenNRE, an open-source and expandable toolkit for implementing neural models for relation extraction. Using this tool, one can train custom models to extract structured relational facts from the plain text, which can be later used to expand a knowledge graph. The traditional relation extraction can be solved by applying one of the following approaches: (1) rule-based; (2) weakly-supervised; (3) supervised; (4) distantly supervised; and (5) unsupervised. In addition to this type of relation extraction, one can also extract semantic relationships by following Open Information Extraction approaches (OIE) (Batista 2016 [Online]), a research field that obtains domain-independent relations from a text. This task is supposed to extract all kinds of n-ary relations in the text (Xavier et al. 2015). Two methods can be applied by the Open Information approaches (1) rule-based; and (2) data-based. The former relies on hand-crafted patterns derived from PoS-tagged text or dependency parse tree rules. The latter generates patterns based on training data represented as a dependency tree or PoS-tagged text. Over the last few years, neural networks have also made their path in the particular domain of relation extraction (Qin et al. 2021), through different neural network approaches (Baldini Soares et al. 2019;Hendrickx et al. 2010;Nadgeri et al. 2021). Exploiting the narrative structure, (Tang et al. 2021) proposed a Multi-tier Knowledge Projection Network (MKPNet). The strategy was designed to leverage multi-tier discourse knowledge and present a knowledge projection paradigm for event relation extraction. According to the authors, such a paradigm can effectively leverage the commonalities between discourses and narratives for event relation extraction. This study had a focus on the projection of knowledge from discourses to narratives. (Lv et al. 2016) also proposed a strategy that takes advantage of the narrative structure. The authors applied an autoencoder to a set of features, including word embeddings, in a clinical narrative dataset, the 2010 I2B2 relation challenge (Uzuner et al. 2011). Using the same dataset, the 2010 I2B2 relation challenge, and the 2018 National NLP Clinical Challenges (N2C2) dataset (Henry et al. 2020), Wei et al. (2019) proposed two different models to extract relations from clinical narratives. The models used a language model based on transformers to extract the features, and BI-LSTM neural network attention to detect the relations. Another example comes from finances related narratives, in this context a shared task was proposed by Mariko et al. (2022a) with the intend to extract causality between existing relations. In this shared task experiments were done on top of FinCausal dataset (Mariko et al. 2022b), a dataset extracted from different 2019 financial news. The best results were achieve by a team that developed an ensemble of sequence tagging models based on the BIO scheme using the RoBERTa-Largemodel, which achieved an F1 score of 94.70 to win the FinCausal 2022 challenge. In addition to these types of connections, entities can also be also linked to other kinds of data. Knowledge databases can furnish a unique identity to entities, which can enrich the narrative with information, and aid to disambiguate the entities as well. The task of linking knowledge base-entities and narrative entities is called entity linking. In the following section, we will discuss this task.",
          "Over the years, efforts have been developed to explore relations between entities in texts (Freitas et al. 2009;Hasegawa et al. 2004). This willingness to recognize individuals in a document is a significant move towards understanding what the document is all about (Balog 2018). Truly understanding a text requires, however, linking these individuals with other pieces of information. The temporal connection between events is one way to achieve this. Another way is to relate the events of the narrative with entities from a knowledge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008). Formally, the process of linking entities can be understood as the task of recognizing (named entity recognition), disambiguating (named entity disambiguation [Eshel et al. 2017)], and linking entities (named entity linking) with unique entity identifiers from a given reference knowledge-base, such as DBpedia (Auer et al. 2007), or Yago (Suchanek et al. 2007). Given the sentence \"Bush was a former president of the USA from 1989 to 1993\", the idea is to determine that \"Bush\" refers to George H. Bush, an American Politician who served as the 41st president of the United States from 1989 to 1993, and not to \"Bush\" a British rock band formed in London, England in 1992. When connecting an entity with Wikipedia data, such as Wikidata (Vrandečić and Krötzsch 2014), this task is known as Wikification. As emphasized by Szymański and Naruszewicz (2019), when a document is Wikified, the reader can better understand it because related topics and enriched knowledge from a knowledge base are easily accessible. From a system-to-system view, the meanings of a Wikified document's core concepts and entities are conveyed by anchoring them in an encyclopedia or a structurally rich ontology. In the context of a narrative, it might mean a more knowledge-rich narrative. Figure 9 illustrates the results of applying entity linking on top of our running example. By looking at the figure, one can observe the linkage of the detected entities or events in the text with their presence on external sources, such as knowledge bases. One can get insights into who is \"John\", \"Edinburgh\" or the \"XYZ contest\". This information might be helpful not only to add more knowledge but also to disambiguate the case of entities that are associated with more than one concept. In the scope of entity linking efforts, Raiman and Raiman (2018) achieved the state-ofthe-art focusing on a cross-lingual approach, a type system on an English dataset supervised with French data. In particular, they constructed a type system and used it to constrain a neural network's outputs to respect the symbolic structure.",
          "Understanding narrative stories involves the ability to recognize events and their participants, as well as to recognize the role that a participant plays in an event, as \"'who ' did 'what' to 'whom\"\", and 'when', 'where', 'why', and 'how' (ISO 24617-4:2014 ). Generally, this can be achieved by Semantic Role Labeling (SRL). This task assigns semantic roles to the constituents of the sentence (Aher et al. 2010). Contrary to syntactic analysis Fig. 9 Overview of entity linking task result when applied to our running example (such as the ones conducted on chunking, dependency parsing, etc.), it acts on a semantic level, and is responsible for capturing predicate-argument relations, such as \"who did what to whom\"' (He et al. 2018), towards making sense of a sentence's interpretation. SRL recovers the latent predicate-argument structure of a sentence, providing representations that answer basic questions about a sentence's meaning. Figure 10 illustrates the results of applying SRL on top of the first sentence of our running example, through the use of AllenNLPfoot_1 . model, which is the implementation of Shi and Lin (2019). The relation established between the different parts of the sentence can be observed by looking at the figure. In this relation, all the parts are linked by the verb \"pulled (out)\": \"John, the magician\", \"a rabbit\" and \"of a hat\" are the core arguments, while \"at the show in Edinburgh\" and \"this year\" are modifiers, the former conveying spatial information and the latter temporal information. The current state-of-the-art of this task is settled by He et al. (2018) with an F1-score of 85.5%, using a deep learning approach based on a BiL-STM neural network to predict predicates and arguments on top of the OntoNotes benchmark (Pradhan et al. 2013). In a narrative-focused approach, Mihaylov and Frank (2019) proposed the use of linguistic annotations as a basis of a discourse-aware semantic self-attention encoder for reading comprehension on narrative texts. In this work, the authors adopted the 15 fine-grained discourse relation sense types (see section 5.5 for more information about discourse relations) from the Penn Discourse Tree Bank (PDTB). According to what is mentioned by the author, combining such an annotation scheme with self-attention yields significant improvements. Following this approach, this study's results indicate that SRL significantly improves who and when questions and that discourse relations also improve the performance on why and where questions. These results are illustrated on top of the Narra-tiveQA (Kočiský et al. 2018) reading comprehension.",
          "When reading or listening to a text, the reader/listener establishes relations of meaning between the different parts, be they clauses, sentences, or paragraphs. These relations are discourse relations (DRels)-also known as rhetorical relations or coherence relationsand are crucial to explain how discourse is organized. For that reason, they have been the basis of several frameworks, such as Rhetorical Structure Theory (RST) (MANN and Thompson 1988), Segmented Discourse Representation Theory (Asher et al. 2003).",
          "Semantic labeling result when applied to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Looking at our running example, since no connective is present, the identification of the DRel would be very challenging using automatic methods. However, taking into consideration lexical and other semantic information, as well as our world knowledge, one can infer that the two sentences are related by the DRel Result, because loving the magician's presentation is a consequence of him pulling a rabbit out of the hat. The second sentence can be divided into two arguments/ text unities related by Comparison. Figure 11 illustrates the annotation of this example using an RST markup tool (O 'Donnell 2000). In NLP, the process of uncovering the DRels between text units is called discourse parsing, a very complex task, despite a few advances over the past years. One of the most recognized discourse parsers is the one used by the news corpus Penn Discourse Tree Bank (PDTB) (Prasad et al. 2018). This end-to-end discourse parser (LIN et al. 2014) follows the same steps as PDTB human annotators: identification of the discourse connectives, arguments segmentation, labeling them Arg1 and Arg2, and recognition of the explicit DRel. When no explicit relation is extracted, the second step labels the implicit DRels. The last step consists in labeling the attribution spans (see Potter (2019) for more information about attribution relations), which are text units that reveal if the content should be attributed to the writer or to another participant. The 2021 edition of the DISRPT Shared Task (Zeldes et al. 2021) added the task of Discourse Relation Classification across RST, SDRT and PDTB, to the two existing tasks from the previous edition (Zeldes et al. 2019), Elementary Discourse Unit Segmentation and Connective Detection. The Shared Task was performed in relation to 16 datasets in 11 languages. Overall, the system with the best results was DisCoDisCo (Gessler et al. 2021) with a Transformer-based neural classifier, which was able to surpass state-of-the-art scores from 2019 DISRPT regarding the first two tasks and to obtain a solid score on the 2021 benchmark for the third task. Establishing DRels between events, such as cause, result, or temporal sequence, is of paramount importance to understand the narrative. Much has been achieved with manual annotation of datasets, which has been the basis for the development of some Discourse Parsers, namely within shared tasks (see LI et al. (2021), for a review and future trends). However, many issues need further research so that Discourse Parsing can fully contribute to narrative extraction, in particular concerning the identification of implicit DRels and the specification of the high-level discourse structures. Table 3 State-of-the-art in information linking tasks Task Ref. Method Effectiveness Benchmark dataset Temporal and event linking Ning et al. (2017) Structured Learning 67.20 Temporal Awareness TempEval-3 UzZaman et al. (2013) Named entity linking Raiman and Raiman (2018) (1) Type system, and (2) use it to constrain the outputs of a neural network to respect the symbolic structure. 94.88 micro-precision CoNLL 2003 NER task Tjong Kim Sang and De Meulder (2003) 90.85 micro-precision F1-Score Relation extraction Baldini Soares et al. (2019) BERT-based Model 89.5 F1-score SemEval-2010 Task 8 Hendrickx et al. (2010) Semantic role labeling He et al. (2018) BiLSTM 85.50 F1-score OntoNote Pradhan et al. (2013) Discourse relation parsing Gessler et al. (2021) Transformer-based neural classifier EDU segmentation on Treebanked data -91.48 F1-score (average) Connective detection on Treebanked data -91.22 F1-score (average) Relation classification on Treebanked data -61.82 F1-score (average) DISRPT 2021 Zeldes et al. (2021)",
          "Table 3 brings a summary of the current status of five of the tasks (temporal reasoning, entity relation extraction, entity linking, semantic role labeling, and discourse relation parsing) covered at the information linking stage discussed in this section. The type column specifies the type of task addressed in those works. In this table, each study's effectiveness is displayed according to the measures reported by the authors. All the discussed tasks are related to semantic analysis at this pipeline stage. In our supplementary material, made available with this survey, one can find a list if additional benchmark datasets relating to the topics covered in this section. The datasets mentioned there can be used as a reference for carrying out the tasks described here. Some of these datasets are available in languages other than English and are useful for the tasks of linking narrative components in these other languages.",
          "The representation of narratives can be categorized in two levels, the conceptual and the visual level. The first one considers that the elements in a narrative are codified as concepts that allow abstracting the meaning. This codification allows the exploration and detection of the elements of a narrative, smoothing the process of analysis of a narrative. The second level of representation is built using visual elements like lines, graphs, icons, pictures, or other graphical resources used to depict the narrative. This level of representation is more accessible for people of different degrees of general expertise. Next, we detail both levels of representation.",
          "Ontologies are a flexible and complex framework in computer science that allows building schemes to represent several kinds of concepts. They aid the multi-layered meanings of a narrative to be pictured as faithfully as possible (Ciotti 2016) since the main elements are conceptually identified. Over the years, some ontologies have been proposed as a means to representing narratives. Khan et al. (2016), for example, proposed an ontology for narratives applying it to Homer's Odyssey. Damiano and Lieto (2013), in turn, described an ontology for narratives for the hero's journey, which is a traditional archetype of the histories of the western culture. The ontology is applied in digital archives for artwork, and then to aid the browsing through the digital artifacts. Tests were made in a small dataset, and some information about artworks was obtained by reasoning. In another proposal for cultural and historic events, Meghini et al. (2021) depicted the Narrative Ontology (NOnt). The authors built an ontology that comprises some rules, like laws of physics. Then, they applied it to heritage crafts, and case studies were conducted to validate the proposed model. Although, ontology is a flexible framework, building an ontology that encompasses all the elements in a narrative is a cumbersome task. Thus, few proposals undertake this challenge.",
          "Representing and learning common sense knowledge for the interpretation of a narrative is one of the fundamental problems in the quest for a profound understanding of language (Mostafazadeh et al. 2016a). Logic-based representations, like DRT (Kamp and Reyle 1993) and Abstract Meaning Representation (AMR) (Banarescu et al. 2013), express recursive formal meaning structures that have a model-theoretic interpretation, although following different formalizations. DRT adopts a dynamic and compositional perspective, which entails determining how the meaning of a sentence can change the context, and enables a straightforward and elegant representation of not only anaphoric relations, both nominal and temporal, but also of other linguistic phenomena. Within DRT's framework, the processing of the discourse is performed one sentence at a time in Discourse Representation Structures (DRSs) in an incremental manner. Each sentence is represented in the DRSs by discourse referents, which represent entities in the discourse, always displayed at the top of the box, and by conditions, which establish a relation of identity between the discourse referents and the corresponding element of the sentence, typically displayed below the universe constituted by the discourse referents. These DRSs are recursive formal meaning structures that have a model-theoretic interpretation, and can be translated into firstorder logic (FOL). Asher (1993) and Asher et al. (2003) extended DRT with the inclusion of discourse relations and with contributions from other dynamic semantic proposals and pragmatics, naming it Segmented Discourse Representation Theory (SDRT). The aforementioned semantic representations are used by the Groningen Meaning Bank (GMB) (Basile et al. 2012;Bos et al. 2017) and the Parallel Meaning Bank (PMB) (Abzianidze et al. 2017). While PMB adopts DRT as a single semantic formalism to fully represent the different annotated linguistic phenomena, GMB goes further and enhances DRT semantics with discourse relations between DRSs, using Boxer (Bos 2015;Curran et al. 2007), a system which utilizes -calculus to generate the meaning representation of texts. Another logic-based representation, but for sentences, is the AMR (Banarescu et al. 2013). The sentence is represented as a single-rooted graph (Damonte et al. 2017). The syntax of AMRs can be defined recursively, and it is possible to specify a systematic translation to first-order logic. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The AMR Bank (Banarescu et al. 2013), based on this form of representation, is a set of English sentences paired with simple, readable semantic representations in some cases manually constructed by human annotators, but also generated in a semi-automatic manner (Gruzitis et al. 2018). It is possible to represent narratives using AMR as well. Droog-Hayes et al. (2018), for instance, employ AMR to represent Russian folktales and extract the narrative structure.",
          "Narrative visualization addresses the interplay between narrative and visualization. This includes perspectives that range from the use of visualization elements to enrich traditional narratives, to the exploration of narrative techniques in visualization-rich artefacts. In this section, we focus on the visual and structured representation of a narrative as an important step, not only as a final output of the narrative extraction pipeline, but also as a machinereadable representation that can be used as a tool for human inspection and validation of each of the pipeline steps. Broadly speaking, narrative visualization can be mapped into 7 genres (Segel and Heer 2010): magazine-style, annotated chart, partitioned poster, flow chart, comic strip, slide show, and film/video/animation. As stated by the authors, such genres vary primarily in terms of the number of frames -distinct visual scenes, multiplexed in time and/or space -that each contains, and the ordering of their visual elements. None of these genres are mutually exclusive: they can act as components and be combined to create more complex visual genres. A less explored area is the visualization of narratives themselves, a perspective of particular interest in the context of this survey, where the focus is on the representation of the extracted narrative as closely as possible to its original form. Metilli et al. (2019) developed a semi-automatic software that can import knowledge from Wikidata to allow users to construct and visualize narratives, based on a proposed ontology to annotate narratives. Baikadi et al. (2011), in turn, proposed a framework for visualizing a narrative, and introduced an environment designed to explore narrative visualization to support novice writers. Milon-Flores et al. (2019), instead, applied algorithms to extract emotions and characters involved in literary work to propose a methodology that is able to generate audiovisual summaries by the combination of emotion-based music composition and graph-based animation. The representation of narrative participants and events over time can be accomplished by projecting these elements in two-dimensional layouts. Munroe (2009) introduced the concept of a narrative chart, a visual representation that encodes narrative elements and interactions in a representation where lines flows from left (past) to right (future) representing each participant, and events are depicted using ellipsis to which participants are connected. Kim et al. (2018) presented the Story Explorer, a visualization tool to explore and communicate nonlinear narratives through the representation of story curves, which contrast story order (y-axis) with narrative order (x-axis). Story Explorer is used to analyze and discuss narrative patterns in 10 popular nonlinear movies. Less elaborate visual schemes can also be used to represent the elements from a narrative and provide an accessible understanding of it. A simpler, but powerful resource to represent the elements of narrative, their relationships, and the chronological order of them is based on the use of knowledge graphs (Ehrlinger and Wöß 2016) and their linkage to knowledge-bases (von Landesberger et al. 2011). Li et al. (2018), for instance, employed a graph structure to extract and link narrative events. Amorim et al. (2021) in their work proposed the Brat2Viz tool to build a visual representation of a knowledge graph from a DRT representation of a narrative. A useful visual representation, in this context, is also the Message Sequence Chart (MSC) (Harel and Thiagarajan 2003), a diagram created to specify systems requirements. Due to its flexibility, MSCs are also suitable to schemetize other kinds of processes. In Brat2Viz tool (Amorim et al. 2021), the authors generates a MSC from a DRT as well. Further initiatives also explore MSC for narrative visualizations (Palshikar et al. 2019;Hingmire et al. 2020). A discussion of the challenges associated with narrative visualization, and the general field of information visualization, can be found in de Ponte Figueiras (2016), and also in Tong et al. (2018). The work of Edmond and Bednarz (2021) also addresses the multidisciplinary nature of the field and outline possible trajectories based on the emphasis put on the narrative itself or the data visualization components used.",
          "With the growing maturity and understanding of the narrative extraction process, new procedures have come into play to help formalizing the narrative extraction evaluation step. In particular, shared tasks were proposed to establish a common experimental setup and overcome the lack of datasets. In this section, we discuss the evaluation carried out by studies developed in the area. Representations made of the extracted narratives need to be evaluated for their level of abstraction to the application (e.g., health care systems, games, news, etc.). The evaluation of the extracted narrative is an essential step of its understandability (Riedl and Young 2010) because it aims to observe the content's understanding by the final consumers and specialists, which can provide insights and possible improvements. However, computational evaluation presents hard challenges. In the literature, few studies evaluate narratives computationally extracted or generated since this is a subjective and application-specific task. In cases where the evaluation is considered, a manual evaluation method is usually adopted. In this case, the narrative result is delivered to specific (usually hired) people, who evaluate the generated result from pre-defined domain criteria, as it is done in Motwani et al. (2019), and give their feedback and overall perception of it. However, ways of automatically evaluating the extracted narratives are sought. Goyal et al. (2010), for example, evaluated the representation of narratives texts through plot units measuring the F-score achieved by the model. Other works (Metilli et al. 2019;Zahid et al. 2019) also assessed the tasks developed in the context of narratives in terms of this measure. Research works that use this measure have the advantage of observing a given task's performance isolated, i.e., separately. Narratives, however, need to be evaluated as a whole. Narrative Cloze Test is a common evaluation framework for script learning introduced by Chambers and Jurafsky (2008). Based on the cloze task (Taylor 1953), the narrative cloze evaluation approach consists of a sequence of narrative events in a document where one event has been removed. The evaluation is then done by predicting the missing verb and typed dependency. In the context of narratives, this approach became widely used to explore the commonsense reasoning of narratives outcome. In a similar approach, Mostafazadeh et al. (2016a) proposed the Story Cloze Test. The authors presented this measure as a generic story understanding evaluation framework that can also evaluate story generation models (e.g., by calculating the log-likelihoods assigned by the story generation model to the two ending alternatives), which does not necessarily indicate a requirement for explicit narrative knowledge learning. According to the authors, models that perform well in the Story Cloze Test reveal some deeper understanding of the story. In this work, Mostafazadeh et al. (2016a) also presented a corpus of 50k five-sentence commonsense stories (ROCStories (Mostafazadeh 2016)) developed to enable a brand new framework for evaluating story understanding -bringing a great contribution to the narratives field. Another form of evaluating narratives was proposed by Kočiský et al. (2018) in the Nar-rativeQA Reading Comprehension Challenge. In this challenge, one is presented to a dataset and a respectively set of tasks in which questions about stories must be answered. As pointed out by the authors, these tasks are designed so that successfully answering their questions requires understanding of the underlying narrative rather than relying on shallow pattern matching or salience. Approaches like these are essential to the reasoning of narratives, in the sense that they aim to provide understandable narratives to the end-user focusing on the outcome and in the causality between the events. Another aspect that should be considered when evaluating a narrative is from whose point of view the story is being told (Brahman et al. 2021). As discussed in the beginning of this survey, having access to appropriate datasets is one of the most important steps behind any evaluation procedure. In this context, we make available a summarized list of the most important datasets in several languages that suit the evaluation tasks approached in this survey. We refer the interested reader to check the the supplementary material of this paper.",
          "As exposed in the previous sections, the extraction of narratives comprehends a series of interrelated tasks related to different areas, which amount to an intricate and complicated enterprise. As such, and although much has been accomplished, different challenges need to be met. In this section, we refer to the most prominent, organizing them into general and narrative extraction-oriented.",
          "",
          "Human statements frequently contain ambiguity, error, implicit information, and other sources of complexity. Therefore, the creation of cognitive agents with human-level natural language understanding capabilities involves mimicking human cognition (McShane 2017). As narratives are composed of human declarations, the study of narratives presents a high level cognitive challenge. Some of the problems inherent to narratives are coreference resolution, for instance, when the subject is null, polysemy (i.e., the multiplicity of meanings of a word or phrase), synonymy (i.e., the expression of the same idea with different terms), ambiguity, related, not only to polysemy, but also to syntactic structure, presuppositions or sarcasm. Natural Language Understanding, a research area of NLP, can aid in the resolution of some of these issues. However, a model able to deal with the aforementioned issues, and to perform at a level proximate to human reasoning is still far from being developed.",
          "As demonstrated in this survey paper, narratives are formed by a series of links between involved participants and events, and organized according to their causality over time, which may mean that different documents that report the same event can compose a familiar narrative. However, few works found in the literature of the area explore the extraction of narratives from multiple documents. Future research should propose new methods to automatically identify, interpret, and relate the different elements of a narrative, which will likely come from various sources. This dilemma is related to the fact that current models are often centered on recurrent neural networks (which, although can represent contexts longer than other types of networks, still have limitations for this type of strategy). Working in vast environments (i.e., contexts) is closely connected to natural language and requires scaling up internal processes before larger documents can be handled.",
          "Although narratives are present in a set of diverse languages, most of the data available, however, is mainly in English. One consequence of this is that low-resource languages end up receiving far less attention. For example, while the task of temporal information extraction has drawn much attention in recent years, research efforts have mostly focused in the English language (Mirza 2016), and the development of new solutions for less known languages ends up being compromised by the lack of properly annotated datasets.",
          "Pre-processing and parsing, and the identification and extraction of narrative components, are two of the components that present the best results in the process of narrative extraction. This stems from the fact that the tasks covered by these processing stages have many applications and have been studied across several NLP tasks over the years. The remaining tasks of the narrative extraction pipeline, however, still present many challenges for further enhancements. In the following, we describe some of those challenges, highlighting future directions whenever appropriate.",
          "The annotation process is a key element when working on narrative extraction. Since a significant number of the tasks from the narrative extraction pipeline hinge on the existence of large annotated datasets, the effort of annotation is huge. One of the biggest issue is precisely the lack of manually annotated datasets available with the necessary information. Checking the manual annotations can also be troublesome because, often, the multilayer annotation is so dense that the annotator can barely unravel what was annotated. A useful solution to this is to resort to visual representations such as knowledge graphs and message sequence charters to carry out the supervising task. Nonetheless, further developments on these visual representations are needed. Another problem is related to the inter-annotator agreement. As stated before, narratives are complex and not always straightforward, which reinforces the relevance of assessing the level of agreement between annotators, both during the process of creating the annotated datasets and evaluating the annotation performed by means of automatic methods.",
          "Several challenges related to the temporal domain may be understood as possible reasons that prevent the development of more elaborated solutions in different languages. For instance, the free-text temporal expression (Strötgen and Gertz 2016), which deals with general phrases not covered by standard definitions, the resolution of implicit or relative temporal expressions, or the problem posed in normalizing temporal expressions across different time-zones, are among some of the most well-known reasons. Thus, the most promising approaches rely on cross-lingual transformer language models and cross-lingual sentence embeddings that leverage universal language commonalities.",
          "From the linguistic point of view, some texts have a stereotypical structure already well established in the area (as is the case of medical reports). The knowledge of the domain is essential to understand the structure and context of a document to improve the reasoning process. The integration of different knowledge might provide, for example, a hierarchical structure of terminology that can help to identify whether two different statements refer to the same event. For a semantic and pragmatic analysis, other computational tools, such as automated reasoning systems, are useful. These tools may help to explore how inference processes can clarify pragmatic phenomena such as conversational implications and more explicitly context-based understanding.",
          "Most text processing tools concentrate on extracting relatively simple constructs from the local lexical context and focus on the document as a unit or even smaller units such as sentences or phrases, rather than on relations of different elements within the document or even cross-document relations. Correctly inducing semantic relations, between participants, participants and events or between text spans, within a natural language story is also an open issue in narrative studies. For instance, the task of discourse parsing has still a long road to go (Morey et al. 2017), to achieve discourse relation identification beyond local dependencies.",
          "Implementing a combination of three relation extraction techniques (machine learning, heuristics, and a hybrid combination of both) may be an interesting approach to be explored in the context of narratives. As narratives result from a semantic structuring of information extracted from a text, which presents n-ary relations among themselves, the Open IE exploration might provide meaningful insights to narratives studies. As far as we know, nothing of this kind has yet been proposed.",
          "The narrative perspective, also known as the point of view, is the vantage point through which the events of a story are filtered and then transmitted to the audience. Thus, the same story might have different point of views (POV) depending on the person (narrator/ character) who narrates the story or the angle from which one looks (Al-Alami 2019; Brahman et al. 2021). Extracting narratives from different points of view using the same dataset, combining them in a coherent representation of the story or deciding on which are the most relevant are still open challenges. Considering different narrative points of view also influences the task of evaluating narratives.",
          "Without using some form of a semantic representation that offers an abstraction from the details of lexical and syntactic realizations, comprehension is arguably unattainable. Representing embedded stories -stories told inside a story-is also a complex task that remains open. Gervás (2021) proposed a simple model to represent them. Nevertheless, the author stated that there is substantial further work in the depicted model. Narrative visualization is an active area of research, mostly as a result of a strong community focused on interactive storytelling and the use of data visualization techniques to improve narrative understanding. In contrast, there is ample opportunity for further research in the visual representation of narratives themselves. Existing research is mostly focused on the representation of narrative elements such as participants and events, whereas work on commonly used narrative techniques-e.g., focalization, allegory, personification, amplification-is scarce and represents a challenging opportunity for future research. With the development of the field, research on the proposal of common visual vocabularies and patterns for recurring visual solutions also constitute a pertinent opportunity for further research. Another aspect where research opportunities exist is the study of user interaction with narratives-which degree of manipulation is useful to improve narrative understanding? Which elements and dimensions of a visual representation should be open to user interaction? Finally, evaluating the effectiveness of the visualization of a narrative typically resort to user studies involving adhoc tasks-an opportunity for the development of reference evaluation benchmarks and guidelines exists.",
          "Coming up with a framework that evaluates the narrative extraction pipeline as a whole is a crucial step for further developments. One of the difficulties refers to the process of creating a gold standard dataset, a labor intensive task that is highly dependent on the subjective interpretation of several factors, such as the emphasis put on each narrative participant, or the level of detail to include. Raters may also have differing viewpoints on annotating data or disagree about what details should be kept, contributing to a lower consensus between raters and intra-raters. On the application level, the purposes of evaluation may vary according to the context addressed. In general, the proposed systems should answer questions like: What is this narrative about? Does the system accurately extract the required elements to tell a story from it properly? How accurately and efficiently does the present system represents the narrative in terms of clarity and amount of data to the application context? The processing of narrative in large-scale or from multiple documents also bumps the issue that supervision is expensive to obtain. When considering the evaluation metrics for natural language processing, another issue is related to how well these methods match and generalize to the real complexity of human languages and how many more interesting natural language inference datasets can be generated.",
          "Narratives are an essential tool for communication, representation, and understanding information. Computational systems that are able to identify narrative elements and structures can naturally interact with human users. Such systems understand collaborative contexts as an emerging narrative and can express themselves through storytelling (Riedl 2004). This survey paper provides simultaneously an account of the study of narrative extraction and a roadmap for future research. To this end, we propose a narrative extraction pipeline, defining the key tasks involved in this process identified in the literature. By doing this, we set a common ground for further studies, highlighting the different stages of the narrative process and the most prominent approaches. During the course of this survey paper, we also pointed out extensive literature focused on extracting narratives and supporting NLP tasks. Nonetheless, and despite several recent advances, there are still important open issues demonstrating that narrative extraction is a rich and promising research area that requires multidisciplinary knowledge in the crossroads between linguistics and computation."
        ],
        "ground_truth_definitions": {
          "composition of a narrative": {
            "definition": "is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events.",
            "context": "Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narra- tive can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time.",
            "type": "implicit"
          },
          "Computational Narratology": {
            "definition": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations",
            "context": "The study of narratives from the computational perspective is carried out through a study area called Computational Narratology (Mani 2014). Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "type": "explicit"
          },
          "narrative extraction": {
            "definition": "the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "context": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources. A closely related term is Computational Narrative Understanding (Piper et al. 2021), which broadens the perspective to social, personal and cultural dimensions.",
            "type": "explicit"
          },
          "event argument": {
            "definition": "a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event mention": {
            "definition": "a sentence or expression that explains an event, including a cause and multiple arguments.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event trigger": {
            "definition": "the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "argument role": {
            "definition": "the link between an argument and the event in which it is involved.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "ontologies": {
            "definition": "formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation.",
            "context": "Another way is to relate the events of the narrative with entities from a knowl- edge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of repre- sentation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008).",
            "type": "explicit"
          },
          "discourse": {
            "definition": "The expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc.",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "explicit"
          },
          "story": {
            "definition": "the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting)",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "implicit"
          },
          "event": {
            "definition": "something significant happening at a specific time and place with consequences.",
            "context": "Finding the events mentioned in the text is an essential step towards the extraction of nar- ratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "frai-06-1225093",
        "sections": [
          "Similarly, Visual Question Answering (VQA) systems are known to learn heavy language priors (Agrawal et al., 2016). Thus, a lack of transparency can affect the decision-making process and may lead to the erosion of trust between humans and Artificial Intelligence (AI) systems. This can further jeopardize users' safety, ethics, and accountability if such a system is deployed publicly (Madsen et al., 2021). Considering the utilization of NLP in healthcare, finance, and law domains, all of which can directly affect human lives, it can be dangerous to blindly follow machine predictions without fully understanding them. For instance, a physician following a medical recommendation or an operation procedure for a patient without full knowledge of the system can do more harm than good. In addition, systems employing Machine Learning (ML), such as most current NLP methods, are prone to adversarial attacks where small, carefully crafted local perturbations can maximally alter model predictions, essentially misguiding the model to predict incorrectly but with high confidence (Finlayson et al., 2019). The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model's decision-making process of arriving at a prediction and the ability to be verified and audited. Therefore, models with explainability are interpretable and complete. In this survey, the focus is on explainability and mainly on the outcome explanation problem where Guidotti et al. (2018) describe explainability as \"the perspective of an end-user whose goal is to understand how a model arrives at its result\". In NLP, there exist various explainable techniques such as LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016), Integrated Gradients (Sundararajan et al., 2017), and ",
          "One of the emerging explainable techniques for NLP applications is rationalization (Atanasova et al., 2020). Rationalization provides explanations in natural language to justify a model's prediction. These explanations are rationales, which present the input features influencing the model's prediction. The reasoning behind the prediction could be understood simply by reading the explanation/rationale, thereby revealing the model's decision-making process. Rationalization can be an attractive technique because it is human-comprehensible and allows individuals without domain knowledge to understand how a model arrived at a prediction. It essentially allows the model to \"talk for themselves\" (Bastings et al., 2019;Luo et al., 2021). This technique is a part of a subset of explainability because it enables models to be interpretable and complete, as shown in Figure 1. Specifically, rationalization provides a local explanation since each prediction has a unique explanation rather than one for the entire model. Local explanations can be categorized into two groups: local post-hoc and local self-explaining. Danilevsky et al. (2020) present local post-hoc methods as explaining a single prediction after the model predicts and local self-explaining methods as simultaneously explaining and predicting. Rationalization in NLP was first introduced in 2007 (Zaidan et al., 2007). As described in Section 4.6, the objective was to use annotator rationales to improve task performance for text categorization. Interestingly, explainability was not the core objective. However, explainability is an advantage of rationalization because it makes the model inherently explainable even if used in the context of task improvement (Gurrapu et al., 2022). Our literature review found that rationalization can be further divided into two major groups: abstractive and extractive (El-Kassas et al., 2021). In extractive rationalization, important features or sentences from the input data are extracted as rationales to support the prediction. In contrast, abstractive rationalization is a generative task in which novel sentences are ). (B) An example of the Abstractive Rationalization approach (Rajani et al., ). generated using new words or paraphrasing existing sentences. This is typically accomplished through the use of a language model such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), T5 (Raffel et al., 2020), or GPT (Generative Pre-trained Transformer) (Radford and Narasimhan, 2018). Figure 2 demonstrate the usage of the two explanation types with examples. Recent advances in explainable NLP have led to a significant increase in rationalization research. Further, at present, the field of rationalization is disorganized. Thus, the motivations for this survey are-(a) formally define rationalization, (b) present and categorize the well-cited techniques based on NLP tasks, and (c) discuss current trends and future insights on the field. Thus, our primary goal is to provide future researchers with a comprehensive understanding of the previously scattered state of rationalization. The key contributions of this paper are as follows. 1. First literature to survey the field of rationalization in NLP.",
          "Explainable AI (XAI). 3. A comprehensive list of details on available rationalization models, XAI evaluations, datasets, and code are provided to guide future researchers. 4. Presents NLP Assurance as an important method for developing more trustworthy and reliable NLP systems.",
          "Table 1 indicates the related survey papers published in recent years. Danilevsky et al. (2020) note that previous surveys in XAI are broadly focused on AI without a specific narrow domain focus. Their work primarily focuses on surrogate-based explainability methods. NLP publications in recent years further demonstrate that this distinction is less relevant and valuable in the NLP domain because \"the same neural network can be used not only to make predictions but also to derive explanations.\" Therefore, surveying the field of Explainable AI (XAI) in NLP requires NLP-specific methods that are different from the standard XAI methods that are widely known. Thayaparan et al. (2020) survey the use of explanations specifically in Machine Reading Comprehension (MRC). The authors describe MRC papers that support explanations and provide a detailed overview of available benchmarks. Further, Madsen et al. (2021) briefly discuss rationalization and natural language explanations using a question and answer approach, CAGE (Commonsense Auto-Generated Explanations) (Rajani et al., 2019) as an example. Thus, this raises the questionhow can rationalization be generalized and applied to other tasks in the NLP domain? However, until now, no comprehensive literature review on rationalization has been available for the prominent NLP tasks. Thus, through this survey paper, we attempt to address this need.",
          "To provide clarity and distinguish terms that are typically used interchangeably in published literature, a list of definitions is provided in Table 2. These terms are used throughout the paper.",
          "The following are the inclusion-exclusion criteria for our publications collection methodology. The first known use of rationalization in NLP was in the year 2007. Our survey focuses on the domain of NLP from 2007 to early 2022. We have included peer-reviewed publications within this range that include a significant rationalization component as a method to provide explainability. We defined significance as rationalization being the main component of their research methodology and approach. We have eliminated a number of publications that are either not entirely in the NLP domain or do not contain a significant rationalization component. For identifying and selecting articles, the range of keywords and topics was limited to the following in the NLP domain: rationalization, explanation, justification, and explainable NLP. Thus, this survey includes reviews of the articles from journals, books, industry research, dissertations, and conference proceedings from commonplace AI/NLP venues such as ACL, EMNLP, NAACL, AAAI, NeurIPS, and others. Finally, these articles are categorized by important NLP tasks, as shown in Table 3. In recent years, there has been an increase in focus on explainability in NLP after a rise in deep learning techniques (Danilevsky et al., 2020). Due to this, a majority of the papers collected were from recent years (2016 and onwards), as illustrated in Figure 3. The availability of relevant articles was limited. After following the above approach, 33 articles were downselected to be the primary focus of this paper's discussion. Instead of providing a broad yet surface-level understanding of the work, we focus on demonstrating in-depth the most important approaches and TABLE Three related survey papers on explainability and interpretability in NLP. No. Survey title Year Papers reviewed 1 A Survey on the state of Explainable AI for Natural Language Processing (Danilevsky et al., 2020) 2020 50 2 A survey on explainability in machine reading comprehension (Thayaparan et al., 2020) 2020 69 3 Post-hoc interpretability for neural NLP: a survey (Madsen et al., 2021) 2021 27 TABLE List of common terms that are used interchangeably in published literature.",
          "",
          "A \"machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans\" (Guidotti et al., 2018).",
          "Interpretability \"aims at developing tools to understand and investigate the behavior of an AI system\" (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
          "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
          "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model's output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
          "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain. progress made in each NLP task. Overall, we selected six articles in multiple NLP domains, five on Machine Reading Comprehension and Sentiment Analysis, four on Text Classification, Fact-Checking and Commonsense Reasoning, and three on Natural Languages Inference, and two articles on Neural Machine Translation (NMT). .",
          "In this section, we discuss relevant papers and their rationalization techniques categorized by the NLP tasks listed in Table 3. Tables with important information on the papers for each subsection are presented at the beginning. TABLE Details on seven NLP tasks and their definitions which are surveyed in this paper.",
          "",
          "Enabling a model to answer questions regarding a given context (Baradaran et al., 2022).",
          "Going beyond pattern recognition to make inferences using world knowledge (Apperly, 2011;Sap et al., 2020).",
          "Determining if a hypothesis entails or contradicts a premise (MacCartney, 2009).",
          "Classifying if a claim is either true or false based on evidence (Vargo et al., 2018;Gurrapu et al., 2022).",
          "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
          "Categorizing textual data by automatically assigning labels (Minaee et al., 2021).",
          "Translating languages using deep neural networks (Sutskever et al., 2014).",
          "MRC enables a model to answer questions regarding a given context (Baradaran et al., 2022). For this reason, it also frequently referred to as Question Answering (QA) Systems. As shown in Table 4, for MRC applications, we found five recent articles from which three articles provide novel datasets (Ling et al., 2017;Mihaylov et al., 2018;Xie et al., 2020) and the remaining articles Sharp et al. (2017) and Lakhotia et al. (2021) each propose a new MRC framework. The first article, published in 2018, presented a new questionanswering dataset based on the open book exam environment for elementary-level science-OpenBookQA (Mihaylov et al., 2018). This dataset consists of two components-(i) Questions (Q): a set of 5,958 multiple choice questions and (ii) Facts (F): a set of 1,326 diverse facts about elementary level science. This dataset was further tested for evaluating the performance of existing QA systems and then compared with the human performance. The results indicated that human performance was close to 92%, but many existing QA systems showed poor performance close to the random guessing baseline of 25%. Additionally, the authors found that simple neural networks achieved an accuracy of about 50%, but it is still not close to the human performance, about 92%. Recently an extension of the WorldTree project (Jansen, 2018), i.e., WorldTree V2 (Xie et al., 2020), is presented. The main goal of this project is to generate a science domain explanation with a supporting semistructured knowledge base. The WorldTree project is a part of TABLE Selected Machine Reading Comprehension papers. References Name Year Explanation Models XAI Metric Dataset Code Sharp et al. (2017) -2017 Extractive TF-IDF, FFNN -AI2 science, Aristo Mini -Ling et al. (2017) -2017 Extractive LSTM, Seq2Seq -AQuA Mihaylov et al. (2018) OpenBookQA 2018 Abstractive BiLSTM Max-out -OpenBookQA, Xie et al. (2020) WorldTree V2 2018 Abstractive TF-IDF, BERT -WorldTree V2 Lakhotia et al. (2021) FiD-Ex 2021 Extractive T5, BERT-to-BERT -Natural Questions - explainable question answering tasks that provide answers to natural language questions and their human-readable explanations for why the answers are correct. Xie et al. (2020) notes that most multi-hop inference models could not demonstrate combining more than two or three facts to perform inference. However, here the authors merge, on average, six facts from a semi-structured knowledge base of 9216 facts. Thus, this resulted in the WorldTree V2 corpus for standardized science questions. This corpus consists of 5100 detailed explanations to support training and instrumenting multi-hop inference question answering systems. Lakhotia et al. (2021) demonstrates a new MRC framework called FiD-Ex (Extractive Fusion-in-Decoder). It has been noted that seq2seq (Sequence to Sequence) models work well at generating explanations and predictions together. However, these models require a large-labeled dataset for training and bring a host of challenges such as fabricating explanations for incorrect predictions, the difficulty of adapting to long input documents, etc. Thus, to tackle these challenges, the Fid-Ex framework includes sentence markers to encourage extractive explanations and intermediate fine-tuning for improving fewshot performance on open-domain QA datasets. This new framework is tested on ERASER (Evaluating Rationales And Simple English Reasoning) datasets and their benchmarks for evaluations (DeYoung et al., 2020). This experiment concludes that FiD-Ex significantly improves upon prior work on the explanation metrics and task accuracy on supervised and few-shot settings. Sharp et al. (2017) proposes a new neural network architecture that re-ranks answer justifications as an intermediate step in answer selection. This new approach alternates between a max pooling layer and a shallow neural network (with ten nodes, glorot uniform initializations, tanh activation, and L2 regularization of 0.1) for providing a justification. This approach contains three components: 1) retrieval component, which retrieves a pool of candidates' answer justification, 2) extractor, which extracts the features and 3) scores, which perform the scoring of the answer candidate based on the pool of justifications. The authors used 8th-grade science questions provided by Allen Institute for Artificial Intelligence (AI2) for evaluations. The training set includes 2,500 questions with four options, and the test set consists of 800 publicly released questions. Further, a pool of candidate justifications corpora containing 700k sentences from StudyStack and 25k sentences from Quizlet is used. The top 50 sentences were retrieved as a set of candidate justification. For model tuning, the authors used five-fold cross-validation, and during testing, the model architecture and hyperparameters were frozen. The authors compared results using two baselines: IR baseline and IR++. They concluded that this new approach showed better accuracy and justification quality while maintaining near state-of-the-art performance for the answer selection task. Ling et al. (2017) presented a dataset and an approach that provides answer rationales, sequences of natural language, and human-readable mathematical expressions for solving algebraic word problems. The authors proposed a sequence-to-sequence model which generates a sequence of instructions and provides the rationales after selecting the answer. For this purpose, a twolayer LSTM (Long Short-Term Memory) with a hidden size of 200 and word embedding with a size of 200 is utilized. Further, the authors also built a dataset containing 100,000 problems in which each question is decomposed into four parts -two inputs and two outputs. This new dataset is used for generating rationales for math problems and for understanding the quality of rationales as well as the ability to obtain a correct answer. Further, the authors used an attention-based sequence to sequence model as a baseline and compared results based on average sentence level perplexity and BLEU-4 (Bilingual Evaluation Understudy). The authors noted that this new approach could outperform the existing neural models in the ability to solve problems and the fluency of the generated rationales.",
          "Commonsense knowledge helps humans navigate everyday situations. Similarly, commonsense reasoning in NLP is the ability for a model to go beyond pattern recognition and use world knowledge to make inferences (Apperly, 2011;Sap et al., 2020). As shown in Table 5 on commonsense reasoning, we found four articles, and all of them provide unique solutions that contribute to the development of commonsense reasoning frameworks. Sap et al. (2020) demonstrates a solution for commonsense reasoning using LSTM encoder and decoder. The main goal was to convert the actions of an autonomous agent into natural language using neural machine translation. For this purpose, the authors built a corpus of thoughts of people as they complete tasks in the Frogger game which are then stored as states and actions. In the next step, LSTM encoder and decoder are used to translate actions as well as states into natural language. Lastly, the authors used the BLEU score to calculate sentence similarity and assessed the accuracy for selecting the best rationale. The authors also conducted a survey to evaluate the rationales based on human satisfaction. The Frogger experiment is concluded with Encoder-Decoder framework outperforming the baselines and demonstrates that the use of game theory approaches for generating rationales is a promising technique (Chang et al., 2019(Chang et al., , 2020;;Yu et al., 2019;Li et al., 2022). Further, it is noted that deep learning model performance is poor when used in tasks that require commonsense reasoning due to limitations with available datasets. To tackle this problem, Rajani et al. (2019) developed the Commonsense Auto-Generated Explanations (CAGE) framework for generating explanations for Commonsense Question Answering (CQA). The authors also created a new dataset-Common Sense Explanations (CoS-E)by collecting human explanations for commonsense reasoning and highlighting annotations. From this paper, the authors concluded that CAGE could be effectively used with pre-trained language models to increase commonsense reasoning performance. Recently, Majumder et al. (2021) and Tang et al. ( 2021) presented novel solutions for commonsense reasoning. Majumder et al. (2021) focused on the Natural Language Expiations (NLEs), which are more detailed than Extractive rationales but fall short in terms of commonsense knowledge. In this solution, the authors proposed a self-rationalizing framework RExC (Rationales, Explanations, and Commonsense). RExC first extracts rationales that act as features for the prediction then expands the extractive rationales using commonsense resources. In the last step, RExC selects the best suitable commonsense knowledge for generating NLEs and a final prediction. The authors tested RExC on five tasksthree natural language understanding tasks and two vision language understanding tasks. Overall, the results indicated improvement in the quality of extractive rationales and NLEs that bridges the gap between task performance and explainability. On the other hand, Tang et al. (2021) focused on Visual Commonsense Reasoning (VCR). They focused on a problem when a question with a corresponding input image is given to the system, and it attempts to predict an answer with a rationale statement as the justification. To explore this, author presented a multi-model approach by combining Computer Vision (CV) and NLP. Their approach leverages BERT and ResNet50 (Residual neural network) as the feature representation layer and BiLSTM (Bidirectional LSTM) and Attention for the multimodal feature fusion layer. These layers are then concatenated into an LSTM network for the encoder layer before passing into the classifier for the prediction layer. This was tested on the benchmark VCR dataset and it indicated significant improvements over existing methods and it also provided a more interpretable intuition into visual commonsense reasoning.",
          "Natural Language Inference (NLI) task helps with identifying a natural language hypothesis from a natural language premise (MacCartney, 2009). For this application, as shown in Table 6, we found three articles. The first article presents a new dataset-e-SNL (explanation-augmented Stanford Natural Language Inference) (Camburu et al., 2018)-and the other two articles discuss approaches that can improve NLI. Camburu et al. (2018) extended the Stanford NLI (SNLI) (Bowman et al., 2015a) dataset by providing human-annotated explanations for the entailment relations. This new dataset-e-SNLI-is used in a series of classification experiments involving LSTM-based networks for understanding its usefulness for providing human-interpretable full-sentence explanations. The authors also evaluated these explanations as an additional training signal for improving sentence representation and transfer capabilities of out-of-domain NLI datasets. Thus, from these experiments, the authors conclude that e-SNLI can be used for various goals mentioned above and also be utilized for improving models as well as asserting their trust. Another issue with NLI is the faithfulness of the generated explanations, tackled by Kumar and Talukdar (2020) and Wiegreffe et al. (2021). Kumar and Talukdar (2020) mentioned that existing methods do not provide a solution for understanding correlations of the explanations with the model's decision-making and this can affect the faithfulness of the generated explanations. Considering this problem, the authors proposed and presented a new framework -NILE (Natural language Inference over Labelspecific Explanations). The NILE framework can generate natural language explanations for each possible decision and process these TABLE Selected commonsense reasoning papers. References Name Year Explanation Models XAI metric Dataset Code Ehsan et al. (2018) -2018 Extractive LSTM, Seq2Seq ---Rajani et al. (2019) CAGE 2019 Abstractive GPT, BERT -CoS-E, CommonsenseQA Majumder et al. (2021) RExC 2021 Extractive Transformer -ComVE, e-SNLI, COSe, e-SNLI-VE, VCR -Tang et al. (2021) DMVCR 2021 Extractive LSTM, BERT -VCR TABLE Selected Natural Language Inference papers. References Name Year Explanation Models XAI metric Dataset Code Camburu et al. (2018) e-SNLI 2018 Abstractive BiLSTM, Seq2Seq -e-SNLI Kumar and Talukdar (2020) NILE 2020 Abstractive GPT-2, RoBERTa -e-SNLI Wiegreffe et al. (2021) -2020 Abstractive T5 -CoS-E, SNLI explanations to produce a final decision for the classification problems. To test this approach, the authors used two datasets -SNLI and e-SNLI -and compared NILE with baseline and other existing approaches based on explanation accuracy, in-domain evolution sets (SNLI), and on out-of-domain examples (train on SNLI and test on MNLI) (Williams et al., 2018). Based on the first 100 SNLI test samples, the results indicated that NILE variants are comparable with the ETPA (Explain Then Predict Attention) baseline, and NILE explanations generalize significantly better on out-of-domain examples. For out-of-domain examples (MNLI ), results showed that the percentage of correct explanations in the subset of correct label predictions was significantly better for all the NILE variants. Thus, the authors concluded that NILE is an effective approach for accurately providing both labels and explanations. Further, Kumar and Talukdar (2020) also focused on the need for faithfulness for denoting the model's decision-making process by investigating abstractive rationales. The author proposed two measurements -robustness equivalence and feature importance agreement -to investigate the association of the labels and predicted rationales, which are required for a faithful explanation. This investigation was performed on CommonsenseQA (Talmor et al., 2019) and SNLI dataset using T5based models (Narang et al., 2020). The results indicated that stateof-the-art T5-based join models demonstrate desirable properties and potential for producing faithful abstractive rationales. .",
          "Fact-checking has become a popular application of NLP in recent years given its impact on assisting with misinformation and a majority of the work has been with claim verification (Vargo et al., 2018;Gurrapu et al., 2022). Based on a paper published in 2016, there are 113 active fact-checking groups and 90 of which were established after 2010 (Graves and Cherubini, 2016). This indicates the growth of the fact-checking application. Considering the scope of this literature review, as shown in Table 7, we found four articles on fact-checking. Two of the studies in this section present novel datasets, and the remaining two provide new techniques to improve fact-checking. In 2017, a large dataset for the fact-checking community called LIAR (Wang, 2017) was introduced, including POLITIFACT data. Most works on this data were focused on using the claim and its speaker-related metadata to classify whether a verdict is true or false. The evidence-an integral part of any fact-checking processwas not part of the LIAR and was overlooked. Thus, in Alhindi et al. (2018) extended the LIAR dataset to LIAR-plus by including the evidence/justification. The authors treated the justification as a rationale for supporting and explaining the verdict. Further, they used Feature-based Machine Learning models (Logistic Regression and Support Vector Machine) and deep learning models (Bi-Directional Long Short-term Memory (BiLSTM) and Parallel-BiLSTM) for binary classification tasks to test the data. The results demonstrated a significant performance improvement in using the justification in conjunction with the claims and metadata. Further, Hanselowski et al. (2019) introduced a new corpus for training machine learning models for automated fact-checking. This new corpus is based on different sources (blogs, social media, news, etc.) and includes two granularity levels-the sources of the evidence and the stance of the evidence toward the claim-for claim identification. Authors then used this corpus to perform stance detection, evidence extraction and claim validation experiments. In these experiments, a combination of LSTMs, baseline NN, pretrained models have been used, and their results are compared based on precision, recall, and F1 macro. The results indicated that fact-checking using heterogeneous data is challenging to classify claims correctly. Further, the author claims that the fact-checking problem defined by this new corpus is more difficult compared to other datasets and needs more elaborate approaches to achieve higher performance. It has been noted that the fact-checking systems need appropriate explainability for the verdicts they predict. The justifications that are human-written can help to support and provide context for the verdicts, but they are tedious, unscalable, and expensive to produce (Atanasova et al., 2020). Considering this issue, Atanasova et al. (2020) proposed that the creation of the justifications needs to be automated to utilize them in a large-scale fact-checking system. The authors presented a novel method that automatically generates the justification from the claim's context and jointly models with veracity prediction. Further, TABLE Selected fact-checking papers. References Name Year Explanation Models XAI metric Dataset Code Alhindi et al. (2018) LIAR-PLUS 2018 Extractive SVM, BiLSTM -LIAR-PLUS Hanselowski et al. (2019) -2019 Extractive BERT -FEVER Atanasova et al. (2020) -2020 Extractive DistilBERT -LIAR-PLUS -Rana et al. (2022) RERRFACT 2022 Extractive RoBERTa, BioBERT -SCIFACT this new method is then tested on the LIAR dataset (Wang, 2017) for generating veracity explanations. The results indicated that this new method could combine predictions with veracity explanations, and manual evaluations reflected the improvement in the coverage and quality of the explanations. Another important domain in which fact-checking is useful is Science. Researching and providing substantial evidence to support or refute a scientific claim is not a straightforward task. It has been seen that scientific claim verification requires in-depth domain expertise along with tedious manual labor from experts to evaluate the credibility of a scientific claim. Considering this problem, Rana et al. (2022) proposed a new framework called RERRFACT (Reduced Evidence Retrieval Stage Representation) for classifying scientific claims by retrieving relevant abstracts and training a rationaleselection model. RERRFACT includes a two-step stance prediction that differentiates non-relevant rationales then identifies a claim's supporting and refuting rationales. This framework was tested on the SCI-FACT dataset (Wadden et al., 2020) and performed competitively against other language model benchmarks on the dataset leaderboard. .",
          "Sentiment Analysis is a subset of the text classification field (Minaee et al., 2021). It focuses specifically on the \"computational study of people's opinions, sentiments, emotions, appraisals, and attitudes toward entities such as products, services, organizations, individuals, issues, events, topics and their attributes\" (Zhang et al., 2018). The use of rationales to support sentiment analysis models in NLP is widely used compared to other NLP tasks. For this task, as shown in Table 8, we identified five papers in this field. In 2016, Lei et al. (2016) pioneered rationalization in sentiment analysis by proposing a problem: \"prediction without justification has limited applicability\". To make NLP outcomes more transparent, the authors propose an approach to extract input text which serves as justifications or rationales for a prediction. These are fragments from the input text which themselves are sufficient to make the same prediction. Their implementation approach includes a generator and an encoder architecture. The generator determines which can be potential candidates for a rationale from the input text. Those candidates are fed into the encoder to determine the prediction and the rationales are not provided during training. They employ an RCNN (Region-based Convolutional Neural Network) and an LSTM architecture and when compared with each other the RCNN performed better. The experiment was conducted on the BeerAdvocate dataset. The paper's approach outperforms attention-based baseline models. They also demonstrate their approach on a Q&A retrieval task indicating that leveraging rationales for sentiment analysis tasks is very beneficial. Similarly, Du et al. (2019) claim that explainability alone is not sufficient for a DNN (Deep Neural Network) to be viewed as credible unless the explanations align with established domain knowledge. In essence, only the correct evidences are to be used by the networks to justify predictions. In this paper, the authors define credible DNNs as models that provide explanations consistent with established knowledge. Their strategy is to use domain knowledge to improve DNNs credibility. The authors explore a specific type of domain knowledge called a rationale which are the salient features of the data. They propose an approach called CREX (Credible Explanation), which regularizes DNNs to use the appropriate evidence when making a decision for improved credibility and generalization capability. During training, instances are coupled with expert rationales and the DNN model is required to generate local explanations that conform to the rationales. They demonstrate it on three types of DNNs (CNN, LSTM, and self-attention model) and various datasets for testing. Results show that the CREX approach allows DNNs to look at the correct evidences rather than the specific bias in training dataset. Interestingly, they point that incorporating human knowledge does not always improve neural network performance unless the knowledge is very high quality. Many papers published in the rationalization field indicate that a machine learning system learning with human provided explanations or \"rationales\" can improve its predictive accuracy (Zaidan et al., 2007). Strout et al. (2019) claim that this work hasn't been connected to the XAI field where machines attempt to explain their reasoning to humans. The authors attempt to show in their paper that rationales can improve machine explanations as evaluated by human judges. Although automated evaluation works, Strout et al. (2019) believe that since the explanations are for users, therefore humans should directly evaluate them. The experiment is done by using the movie reviews dataset and by having a supervised and an unsupervised CNN model for a text classification task. They use attention mechanism and treat the rationales as supervision in one of the CNN models. Results indicate that a supervised model trained on human rationales outperforms the unsupervised on predictions. The unsupervised is the model where the rationales/explanations are learned without any human annotations. The selective rationalization mechanism is commonly used in complex neural networks which consist of two componentsrationale generator and a predictor. This approach has a problem of model interlocking which arises when the predictor overfits to the features selected by the generator. To tackle this problem this paper proposes a new framework A2R which introduces TABLE Selected sentiment analysis papers. References Name Year Explanation Models XAI metric Dataset Code Lei et al. (2016) -2016 Extractive LSTM, RCNN -BeerAdvocate, AskUbuntu Du et al. (2019) CREX 2019 Extractive CNN, LSTM -BeerAdvocate, MovieReview -Strout et al. (2019) -2019 Extractive RA-CNN, AT-CNN -MovieReview -Yu et al. (2021) A2R 2021 Extractive BiGRU -BeerAdvocate, MovieReview Antognini and Faltings (2021) ConRAT 2021 Extractive CNN, BiGRU -AmazonReviews, BeerAdvocate a third component for soft attention into the architecture (Yu et al., 2021). The authors have used BeerAdvocate and MovieReview for understanding the effectiveness of the framework. The authors compared results from A2R with the original rationalization technique RNP (Rationalizing Neural Predictions) along with 3PLAYER, HARD-KUMA and BERT-RNP. For implementation authors have used bidirectional Gated Recurrent Units (GRU) in the generators and the predictors. Furthermore, they performed two synthetic experiments using BeerAdvocate dataset by deliberately inducing interlocking dynamics and then they performed experiments in real-world setting with BeerAdvocate and MovieReview. From the results they made two conclusions-(1) A2R showed consistent performance compared to other baselines on both the experiments, ( 2) A2R helps to promote trust and interpretable AI. In the future, the authors would like to improve A2R framework for generating casually corrected rationales to overcome the lack of inherent interpretability in the rationalization models. Existing methods in rationalization compute an overall selection of input features without any specificity and this does not provide a complete explanation to support a prediction. Antognini and Faltings (2021) introduce ConRAT (Conceptbased RATionalizer), a self-interpretable model which is inspired by human decision-making where key concepts are focused using the attention mechanism. The authors use the BeerReviews dataset to not only predict the review sentiment but also predict the rationales for key concepts in the review such as Mouthfeel, Aroma, Appearance, Taste, and Overall. ConRAT is divided into three subodels, a Concept Generator which finds the concepts in the review, a Concept Selector that determines the presence or absence of a concept, and a Predictor for final review predictions. ConRAT outperforms state-of-the-art methods while using only the overall sentiment label. However, Antognini et al. (2021) have further demonstrated that attention mechanism usage can contribute to a tradeoff between noisy rationales and a decrease in prediction accuracy.",
          "Text classification, also commonly known as text categorization, is the process of assigning labels or tags to textual data such as sentences, queries, paragraphs, and documents (Minaee et al., 2021). Classifying text and extracting insights can lead to a richer understanding of the data but due to their unstructured nature, it is challenging and tedious. NLP techniques in text classification enable automatic annotation and labeling of data to make it easier to obtain those deeper insights of the data. For this task, as shown in Table 9, we identified four papers in this sfield. Traditionally, rationales provide well-defined kinds of data to nudge the model on why a prediction is the way it is given the data. Moreover, they require little additional effort for annotators and yield a better predictive model. When classifying documents, it is beneficial to obtain sentence-level supervision in addition to document-level supervision when training new classifications systems (Zhang et al., 2016). Previous work relied on linear models such as SVMs (Support Vector Machines), therefore, Zhang et al. (2016) propose a novel CNN model for text classification that exploit associated rationales of documents. Their work claims to be the \"first to incorporate rationales into neural models for text classification\". The authors propose a sentence-level CNN to estimate the probability that a sentence in a given document can be a rationale. They demonstrate that their technique outperforms baselines and CNN variants on five classification datasets. Their experimentation task uses Movie Reviews and the Risk of Bias (RoB) datasets. On the movie review dataset, their technique performs with a 90.43% accuracy with the RA-CNN (Recurrent Attention Convolutional Neural Network) model and similar strong results are also indicated on the RoB datasets. It seems intuitive that more data or information can lead to better decision-making by the neural networks. Zaidan et al. (2007) propose a new framework to improve performance for supervised machine learning by using richer \"kinds\" of data. Their approach is called the \"annotator rationales\" technique and it is to leverage a training dataset with annotated rationales. The rationales highlight the evidence supporting the prediction. Zaidan et al. (2007) test their approach on text categorization tasks, specifically, sentiment classification of movie reviews and they claim that these rationales enable the machine to learn why the prediction is the way it is. Rationales help the model learn the signal from the noise. ML algorithms face the \"credit-assigment problem\" which means that many features in the data (X) could have affected the predicted result (Y). Rationales provide a \"shortcut\" to simplifying this problem since they provide hints on which features of X were important. Zaidan et al. (2007) used a discriminative SVM for experimentation and the results indicate that this technique significantly improves results for the sentiment classification and they hypothesize that leveraging rationales might be more useful than providing more training examples. Recently, rationales have been a popular method in NLP to provide interpretability in the form of extracted subsets of texts. It is common to have spurious patterns and co-varying aspects in the TABLE Selected text classification papers. References Name Year Explanation Models XAI Metric Dataset Code Zaidan et al. (2007) -2007 Extractive SVM -MovieReview -Zhang et al. (2016) -2016 Extractive SVM, RA-CNN -Risk of Bias -Liu et al. (2019b) GEF 2019 Extractive CNN, LSTM -PCMag Reviews, Skytrax User Reviews Plyler et al. (2022) CDA 2021 Extractive RL, RNN -TripAdvisor Reviews, RateBeer dataset due to which rationale selectors do not capture the desired relationship between input text and target labels. Considering this problem this paper proposes CDA (Counterfactual Data Augmentation) framework to aid rational models trained with Maximum Mutual Information (MMI) criteria (Plyler et al., 2022). CDA consists of transforms-for rational and classificationsbecause of their effectiveness over RNNs in NLP. The authors used TripAdvisor.com and RateBeer datasets for testing CDA with three baselines -MMI, FDA (Factual Data Augmentation), and ANT (simple substitution using antonyms). The results of the rational models were compared using precision and the accuracy of the classifier is reported based on the development set. From the results, authors concluded that the models trained using the CDA framework learn higher quality rationales and it doesn't need human intervention. In the future, the authors would like to explore more on counterfactual predictors and on CDA framework that could connect with other rationalization strategies. Similarly, Liu et al. (2019b) proposed a novel Generative Explanation Framework (GEF) for classification problems that can generate fine-grained explanations. The motivation behind this explanation framework is to provide human-readable explanations without ignoring finegrained information such as textual explanations for the label. For understanding the accuracy of explanations, the authors conducted experiments on two datasets-PCMag and Skytrax User Reviewswhich were processed by the Stanford Tokenizer. Further, the authors used Encoder-Predictor architecture in which they used Conditional Variational Autoencoder (CVAR) as a base model for text explanations and Long Short-Term Memory (LSTM) for numerical explanations. The experimental results indicated that after combining base models with GEF the performance of the base model was enhanced along with improving the quality of explanations. Further, the authors also used human evaluation for evaluating the explainability of the generated text explanations. The authors noted that for 57.62% of the tested items GEF provided better or equal explanations compared with the basic model.",
          "With the advent of deep learning, Neural Machine Translation (NMT) became the successor to traditional translation methods such as Rule-based or Phrase-Based Statistical Machine Translation (PBSMT) (Yang et al., 2020). NMT models leverage Deep Neural Networks architecture to train the model end-to-end to improve translation quality and only require a fraction of the storage memory needed by PBSMT models (Sutskever et al., 2014). The use of explanations to support NMT model's prediction is relatively new, however, there has been some pioneering work to provide more explainability. For this task, as shown in Table 10, we identified two relevant papers in this area. Quality Estimation (QE) models perform well at analyzing the overall quality of translated sentences. However, determining translation errors is still a difficult task such as identifying which words are incorrect due to the limited amounts available training data. The authors explore the idea that since QE models depend on translation errors to predict the quality, using explanations or rationales extracted from these models can be used to better detect translation errors (Fomicheva et al., 2021). They propose a novel semi-supervised technique for word-level QE and demonstrate the QE task as a new benchmark for evaluating feature attribution (the interpretability of model explanations to humans). Instead of natural language explanations, their technique employs various feature attribution methods such as LIME, Integrated Gradients, Information Bottleneck, causal, and Attention. It was shown that explanations are useful and help improve model performance and provide better explainability. Deep learning models are black-boxes because they involve a large number of parameters and complex architectures which makes them uninterpretable. Considering this problem and to bring interpretability in deep learning models (Alvarez-Melis and Jaakkola, 2017) propose a model-agnostic method for providing explanations. The explanations provided by this method consist of sets of inputs and output tokens that are causally related in the black-box model. Further, these causal relations are inferred by performing perturbations on the inputs from the black-box models, generating a graph of tokens, and then solving a partitioning problem to select the most relevant components. To test the methods via runtime complexity analysis with respect to the number of LM (Linear Model) forward/backward passes. Finally for performance standard dataset-specific metrics accuracy (for SST and CoS-E) and macro-averaged F1 (for Movies) are used. The results indicated that UNIREX allows effective trade-off between performance, faithfulness, and plausibility to identify better rationale extractions. Further, authors also mention UNIREX trained rationale extractors can generalize to unseen datasets and tasks. With many ML systems demonstrating performance beyond that of human across many applications, the field of XAI is advancing techniques to improve transparency and interpretability. Das and Chernova (2020) explores XAI in the context of a question previously unexplored in ML and XAI communities: \"Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human?\". The authors investigate this question through the game of Chess where computational game engines performance surpass the average player. They present an automated technique for generating rationales for utilitybased computational methods called the Rationale-Generating Algorithm. They evaluate this with a user study against two baselines and their findings show that the machine generated rationales can lead to significant improvement in human task performance. They demonstrate that rationales can not only be used to explain the system's actions but also instruct the user to improve their performance. Sharma et al. (2020) explores an of rationalization in the mental health support field and understanding empathy. Empathy is important for mental health support and with the rise of text-based internet platforms, it becomes crucial to understanding empathy in only communication. The paper presents a computational approach to understanding empathy by developing a corpus of 10,000 pairs of posts and responses with supporting rationales as evidence. They use a multi-task RoBERTabased bi-encoder model to identify empathy in conversations and extract rationales for predictions. Their results demonstrate that their approach can effectively identify empathic conversations. To improve interpretability for NLP tasks, recent rationalization techniques include Explain-then-Predict models. In this technique, an extractive explanation from the input text is generated and then a prediction is generated. However, these models do not use the rationales appropriately and consider the task input as simply a signal to learn and extract rationales. Zhang et al. (2021) propose a novel technique to prevent this problem with their approach called ExPred where they leverage mult-task learning on the explanation phase and embed a prediction network on the extracted explanations to improve task performance. They experiment with three datasets [Movie Reviews, FEVER (Fact Extraction and VERification), MultiRC] and conclude that their model significantly outperforms existing methods.",
          "This section compares extractive and abstractive rationalization techniques. It can be observed from Figure 4 that there is more interest and focus on extractive rationalization techniques compared to abstractive. There are multiple reasons for this, and the progress in the Automatic Text Summarization (ATS) domain can help explain.",
          "In most extractive rationalization approaches, generating a rationale is similar to text summarization. These rationales contain the salient features of the input text, which users need to understand as the most influenced features of the model's prediction. Next, two steps are implemented while performing the task-(i) irrelevant information is ignored, and (ii) most crucial information is selected based on a scoring algorithm. This approach is a common foundation of summarization techniques. In extractive summarization, meaningful sentences are extracted to form a summary of the original text while still retaining the overall subject matter (El-Kassas et al., 2021). The critical difference with rationalization is that it is able to justify a neural network's prediction with evidence. In a way, extractive rationalization uses extractive summarization's fundamentals and takes it further. It frames the task as can we rationalize the output prediction where rationalize means to understand the prediction process and reason with supporting evidence. This introduces an interdependent relationship between the rationale and the prediction. This process is close to how humans rationalize with a sequence of reasons to justify a decision. This can be implemented in the NLP process to make models more explainable. As interest in ATS systems grew in the past few decades, researchers have mainly focused on extractive summarization due to its simplicity, and reliability (El-Kassas et al., 2021). The abstractive summarization needed reliable natural language generation; thus, it was in its infancy from the 2000s to the early 2010s. Therefore, an increasing body of knowledge on extractive techniques is available, which researchers interested in rationalization could leverage and build on. This intuition behind extractive summarization paves the way for extractive rationalization. The stark difference between extractive and abstractive in Figure 3 is expected and reasonable, and the fields of summarization and rationalization follow similar paths. However, summarization approaches should purely be used for inspiration -following the identical methods for rationalization would be insufficient, and it does not provide reliable model explainability. Chan et al. (2021) notes that for appropriate explainability, the desiderata for the rationale is that-(i) it must reflect the model's reasoning process (faithfulness), (ii) be convincing to the nonexpert (plausibility), and (iii) the rationale extraction should not hurt task performance. Thus, there is more work than simply extracting sentences as rationales. Moreover, extractive rationalization is insufficient because extracted sentences themselves are insufficient to provide full explainability. Humans do not fully understand without context and a coherent and logical explanation.",
          "The extensive research in extractive summarization reached its maturity, has peaked in terms of performance, and now the progress is stagnated (Gupta and Gupta, 2019). Recent advances in deep learning and the advent of the Transformer architecture in 2017 have led to more reliable and influential language models (Vaswani et al., 2017;Devlin et al., 2019). In 2019, Liu and Lapata (2019)  Abstractive techniques allowed novel words and phrases to be generated instead of extracting spans from the input. Due to these advances, the research focuses gradually shifted from extractive to abstractive summarization. It is expected that rationalization will follow a similar trend. Abstractive rationalization is still relatively new, with limited research available. However, there have been promising and pioneering approaches such as Rajani et al. (2019) and Kumar and Talukdar (2020). Almost every paper discussed with an abstractive rationalization technique in Section 4 leveraged some implementation of the Transformer architecture, such as BERT, GPT-2 (Radford et al., 2019), and T5, amongst others. BERT was the most frequently used language model. When BERT was released in 2018, it achieved SOTA results on many NLP tasks and surpassed human performance on tasks such as question answering and commonsense reasoning (Devlin et al., 2019). It made a giant leap in terms of performance compared to other language models of its time. This led to wide adoption and variations of BERT for the tasks where the Transformer-based model was required. Recently introduced models such as BART (Lewis et al., 2020), GPT-3 (Brown et al., 2020), and T5 demonstrate promising results and surpass BERT in some tasks. This is due to language models growing exponentially, and they continue to improve and perform incredibly well at natural language generation (Sanh et al., 2019). For example, in some cases, text produced by GPT-3 is almost on par if not better, than human-written text. This enables more opportunities for research in abstractive rationalization, which is needed. By leveraging SOTA language models, explanations can become more comprehensive and convincing when illustrating a model's decision-making process. As mentioned in Section 2, it is almost as if the models are \"talking for themselves\". We believe that significant progress can be made in rationalization by focusing more on improving abstractive techniques.",
          "In this section, we discuss insights from the literature reviewed, challenges, and potential future directions to propel progress on rationalization. Most importantly, we introduce a new XAI subfield called Rational AI.",
          "In Section 1, we have seen the need for explainability and the available methods in NLP. The numerical methods, such as SHAP values (Lundberg and Lee, 2017b) or Attention scores, visualization methods, such as LIME (Ribeiro et al., 2016), and saliency heatmaps, all require specialized domain knowledge to understand. At the same time, with increasing interactions with NLP-based systems, the nonexpert also deserves to know and understand how these black-box systems work because it has some degree of influence on their lives. This is formally called the right to an explanation, a right to receive an explanation for an algorithm's output (Miller, 2019b). A classic example is a bank system with an NLP model that automatically denies a loan application. In this situation, providing the loan applicant with SHAP values or saliency heatmaps to justify the bank's algorithms is not very meaningful. Thus, explainability methods are truly explainable and helpful if the nonexpert can understand them (Mittelstadt et al., 2019). We introduce Rational AI (RAI) as a potential solution.",
          "Rationalization techniques come the closest to this goal because they are built on natural language explanations (NLEs). NLEs are intuitive and human comprehensible because they are simply descriptive text. The textual information can be easily understood and translated into other languages if needed. Across all of the NLP tasks discussed in Section 4, we have seen the benefits of NLEs and the accessibility it provides to the nonexpert. We believe there is a critical need to focus on explainability techniques with NLEs. Considering these factors, we propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI. Further, rationality should not be confused or used interchangeably with the general AI term of a rational agent Russell and Norvig (2002). These are distinct topics with similar names. In this survey, RAI and rationality are purely considered in the context of model explainability. We also have not seen any usage or previous definitions of RAI within this context. We compare rationality to the other fields shown in Figure 5. Models with interpretability are interpretable, while those with explainability are interpretable and complete, as described in Section 1. Models with rationality are interpretable and complete and can rationalize their behavior through an NLE. The explainability methods described earlier in this subsection explain, but they do not justify in a way that is accessible and comprehensible to the nonexpert. In recent years, language models have become powerful and incredibly good at language generation tasks, but we have yet to see their full potential. As they continue to grow exponentially, we predict this is the beginning of explainability techniques using NLEs. The intuition behind RAI is that rationalization is one such technique, and many are yet to be developed. This calls for a more organized field to improve research focus and the need for RAI to exist.",
          "Although RAI arises from the need for better explainability for NLP tasks, it is potentially applicable in general AI and other fields in AI. Other fields, such as Computer Vision, Speech, and Robotics, could leverage rationalization methods to improve their model explainability. For example, rationalization in Computer Vision can help explain through an NLE which visual features contributed the most to an image classifier prediction in place of complex explainable techniques (Sundararajan et al., 2017;Tjoa and Guan, 2021). Many promising opportunities exist for researchers to apply rationalization in other disciplines.",
          "We have seen that rationalization is a relatively new technique, and with it, various challenges exist. In this subsection, we share challenges and potential solutions to improve the current state.",
          "No standard statistical evaluations exist currently for rationalization. There is a wide variety of metrics that are in use, such as Mean Squared Error (Lei et al., 2016), Accuracy (Zaidan et al., 2007;Du et al., 2019;Rajani et al., 2019), F1 Score (Alhindi et al., 2018;Rana et al., 2022), ANOVA (Analysis of variance) (Das and Chernova, 2020), and Precision (Plyler et al., 2022). We have observed that the most preferred statistical metric is accuracy. It is reasonable for evaluation metrics to be task-dependent and focused on the prediction. However, those alone are insufficient because the accuracy of the NLE also needs to be considered. For example, if the task prediction had high accuracy, but the NLE was unclear and incomprehensible, then it is not helpful. Metrics such as the BLEU (BiLingual Evaluation Understudy) score by Papineni et al. (2002) and the ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation) score by Lin (2004) exist for evaluating open-ended machine-generated texts. However, we have seen limited use in the literature review, such as Camburu et al. (2018). The scores work by comparing the generated text with a set of ground-truth reference texts, and often these are human-written references. These scores are helpful, especially for abstractive rationalization, where explanations can be open-ended; however, they come with limitations since the evaluation is effectively token-level matching. Since an NLE is the typical outcome of systems with rationalization, adopting a standard evaluation metric can help improve research progress. Consistent evaluations also make it easier to compare different experiments and approaches.",
          "The availability and the need for more diversity of appropriate datasets is also a problem hindering progress. Availability: Data collection is an expensive and timeconsuming task. It is possible to repurpose existing datasets, but modifying them requires manual human labor. Thus, researchers often build their datasets for a specific task they are working on. Camburu et al. (2018) developed the e-SNLI dataset by modifying the SNLI dataset from Bowman et al. (2015b). Camburu et al. TABLE Human-Centered Evaluations (HCE) are performed in out of papers surveyed in this review. Year References Method name Venue NLP domain HCE performed? 2007 Zaidan et al. (2007) -NAACL Text Classification No 2016 Ribeiro et al. (2016) LIME NAACL Multiple Domain Yes 2016 Lei et al. (2016) -EMNLP Sentiment Analysis No 2016 Zhang et al. (2016) -EMNLP Text Classification No 2017 Sharp et al. (2017) -CoNLL Machine Reading Comprehension No 2017 Ling et al. (2017) -ACL Machine Reading Comprehension No 2017 Alvarez-Melis and Jaakkola (2017) SOCRAT EMNLP Neural Machine Translation Yes 2018 Ehsan et al. (2018) -AIES Commonsense Reasoning Yes 2018 Alhindi et al. (2018) LIAR-PLUS FEVER Fact-Checking No 2018 Mihaylov et al. (2018) OpenBookQA EMNLP Machine Reading Comprehension Yes 2018 Xie et al. (2020) WorldTree V2 LREC Machine Reading Comprehension Yes 2018 Camburu et al. (2018) e-SNLI NeurlPS Natural Language Inference No 2019 Rajani et al. (2019) CAGE ACL Commonsense Reasoning Yes 2019 Hanselowski et al. (2019) -CoNLL Fact-Checking No 2019 Du et al. (2019) CREX ICDM Sentiment Analysis No 2019 Strout et al. (2019) -BlackboxNLP Sentiment Analysis Yes 2019 Liu et al. (2019b) GEF ACL Text Classification Yes 2020 Atanasova et al. (2020) -ACL Fact-Checking Yes 2020 Lakhotia et al. (2021) ERASER ACL Multiple Domain Yes 2020 Das and Chernova (2020) RGA IUI Multiple Domain Yes 2020 Sharma et al. (2020) EPITOME EMNLP Multiple Domain No 2020 Kumar and Talukdar (2020) NILE ACL Natural Language Inference Yes 2020 Wiegreffe et al. (2021) -EMNLP Natural Language Inference No 2021 Majumder et al. (2021) RExC ICLR Commonsense Reasoning Yes 2021 Tang et al. (2021) DMVCR DaWaK Commonsense Reasoning No 2021 Tang et al. (2021) FiD-Ex EMNLP Machine Reading Comprehension No 2021 Chan et al. (2021) UNIREX ICML Multiple Domain No 2021 Zhang et al. (2021) EXPred WSDM Multiple Domain No 2021 Fomicheva et al. (2021) -ACL Neural Machine Translation No 2021 Yu et al. (2021) A2R NeurlPS Sentiment Analysis Yes 2021 Antognini and Faltings (2021) ConRAT ACL Sentiment Analysis Yes 2021 Plyler et al. (2022) CDA NeurlPS Text Classification No 2022 Atanasova et al. (2020) RERRFACT AAAI Fact-Checking No (2018) achieved promising results on their task, demonstrating how their dataset can enable a wide range of new research directions by altering and repurposing existing datasets. Diversity: Without enough datasets, new research in rationalization will be limited. Researchers will be constrained to the existing datasets to make new progress. This trend is evident in the literature reviewed in MRC and Sentiment Analysis compared to NMT. In MRC, the datasets are very diverse. In sentiment analysis, most papers rely on either the BeerAdvocate (McAuley et al., 2012) or MovieReviews (Maas et al., 2011) datasets to perform their experiments. In both domains, we discovered five publications each. For a domain such as NMT, progress seems limited, and we found only two publications. The lack of appropriate rationalization datasets for NMT tasks is a possible reason for this. As we observed in our literature review, there is a direct relationship between dataset availability and the progress made. More work in creating new datasets for rationalization can help improve diversity and the progress of certain domains lagging behind, such as NMT. New datasets across all domains, in general, will increase the interest and work in rationalization because researchers will have more flexibility in designing new techniques and experimenting with a wide variety of data. Stamper et al. (2010) has organized the largest repository of learning science datasets called DataShop, and it led to improvements in research progress. Similarly, an organized central repository for rationalization supporting datasets can be beneficial. Without a centralized model evaluation and development system, reproducibility and accessibility will remain low.",
          "NLP has direct applications in many disciplines. For example, MRC and commonsense reasoning are helpful in the education discipline. Our literature review indicates using Q&A tools and commonsense injection to generate explanations for educational needs (Mihaylov et al., 2018;Li et al., 2019). Further, NLP has also been used to enhance human task performance, as we saw in Das and Chernova (2020), and to provide support for mental health (Sharma et al., 2020). Additionally, fact-checking is another application, and it is crucial in social media, fake news detection, and law (Alhindi et al., 2018). It has become common to interact with these systems, and they may have a significant influence on all aspects of our society. Due to this, the European Union recently passed a regulation that requires algorithms to provide explanations that can significantly affect users based on their userlevel predictions (Doshi-Velez and Kim, 2017).",
          "The explanations provided by the NLP systems must provide enough information to the user to help them understand its decision-making process (Putnam and Conati, 2019). Considering these aspects, the human-machine partnership is essential for evaluating and generating accurate explanations. This calls for better methods to evaluate the explanations generated. The field of HCE addresses this problem, and Sperrle et al. (2021) defines it as a \"field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios.\" In this literature survey, we found 15 out of 33 papers in which HCE is performed, and a summary is provided in Table 12. Sperrle et al. (2021) shares the increasing trend of HCE since 2017 compared to the previous years. While conducting this literature survey, this trend was not observed in the rationalization domain. Overall, we found that HEC is incorporated in most of the papers on Machine Reading Comprehension (2 out of 5), Commonsense Reasoning (3 out of 4), Fact-Checking (1 out of 4), Natural Language Inference (2 out of 5), Neural Machine Translation (1 out of 2), Sentiment Analysis (3 out of 5) and Multiple Domain (3 out 6). From our observations, researchers give more attention to performance while evaluating AI algorithms and ignore human factors such as usability, user intentions, and user experience. Thus, along with the accuracy of AI algorithms, it is also essential to focus on the interpretability and reliability of the explanations generated by AI algorithms. The articles in which HCE is used are primarily performed via crowdsourcing using Amazon Mechanical Turk, and the focus is on user-based evaluations or annotations. This pattern necessitates conducting expert evaluations to understand users' needs better because it can help improve trust in AI algorithms. HCE is a subset of the Human-Computer Interaction (HCI) field, which is integrated with the AI paradigm after the algorithmic performance evaluations as shown in Figure 6. This integration can be regarded as human-centered AI, and Riedl (2019) claims this as an AI/ML perspective that intelligent systems are part of a more extensive system that also includes human stakeholders. The literature (Hwang and Salvendy, 2010) on usability, and user experience testing demonstrated three widely used methods to perform HCE -Think Aloud (TA), Heuristic Evaluation (HE), and Cognitive Walkthrough (CW). The TA method is a standard method and can be more effective considering the evaluations of explanations in the NLP domain. In the TA method, evaluators are asked to \"think aloud\" about their experience while an experimenter observes them and listens to their thoughts (Fox, 2015). This way, an HCE method can be used in the final step to understand usability, user intentions, and user experience. This may lead to a better understanding of the interpretability and reliability of the explanations generated by rationalization. Therefore, in addition to statistical evaluation techniques, we strongly encourage researchers to integrate HCE as part of their evaluations.",
          "It is critical to perform rigorous testing and validation of NLP systems at all stages before their deployment. For example, it should be ensured that the data is unbiased, models are interpretable, and the process of arriving at the outcome is explainable to a nonexpert. In the last step of this process, it would be beneficial to use RAI techniques. Integrating rationalization with human-centered evaluations and elements of NLP Assurance can invoke human-AI trust and safety with the systems -with the recent rise of chatbots such as ChatGPT, the need for more rigorous validation is more important than any other time. This process may also transform black-box systems into white-box systems and make NLP models more comprehensible and accessible for nonexpert users."
        ],
        "ground_truth_definitions": {
          "abstractive rationalization": {
            "definition": "generating a novel explanation",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "extractive rationalization": {
            "definition": "extracting text fragments from the input",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Interpretability": {
            "definition": "Interpretability aims at developing tools to understand and investigate the behavior of an AI system.",
            "context": "Interpretability “aims at developing tools to understand and investigate the behavior of an AI system” (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to “explain or to present in understandable terms to a human” what the AI system is performing.",
            "type": "implicit"
          },
          "Rationalization": {
            "definition": "justifies a model’s output by providing a natural language explanation.",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Black-box Model": {
            "definition": "A machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans",
            "context": "Given in a Table",
            "type": "explicit"
          },
          "Explainability": {
            "definition": "the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction.",
            "context": "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
            "type": "explicit"
          },
          "interpretable": {
            "definition": "i.e., allowing humans to understand the internals of a model",
            "context": "The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models.",
            "type": "explicit"
          },
          "NLP Assurance": {
            "definition": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users.",
            "context": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain.",
            "type": "explicit"
          },
          "Completeness": {
            "definition": "The ability to accurately describe the operations of a system that allows humans to anticipate its behavior better.",
            "context": "However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model’s decision-making process of arriving at a prediction and the ability to be verified and audited.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "304b9778edf07175e6964dfa3f37cacec6006197",
        "sections": [
          "Airports are symbols of mobility. They have become emblematic of our post-modern world. Yet in the early days of airports, there was no such thing as the rivers of passengers that flow through the sculpted steel and glass façades of contemporary terminal buildings. The airport consisted of a few tents and an airfield, remnants of air travel's military beginnings. Airports now cater for the millions of people who flow in, out and within countries each year. The huge mile long terminal structures are carefully built to facilitate the mobility of passengers, baggage, and cargo to their destination. To ensure the security of the country the flows are entering, while at the same time to protect the very means of their travel, these flows must be watched and controlled. The airport is well and truly a space under surveillance. And yet, the airport's role in surveillance has gone largely unnoticed by surveillance studies, reflecting the invisibility of these transient 'non-places' within the social sciences (see Crang, 2002;Gottdeiner, 2001;Rosler, 1998;Augé, 1995). Nonetheless, since the TWA flight crash in 1996 and the more recent September 11th attacks, countries such as the United States are pushing through increased measures to improve airport security and safety, launching airports into the public consciousness. Security itself has become an all-encompassing term, spreading into the current vocabulary. Widespread gun crime at schools has highlighted the 'airport-style security' measures of metal detectors and baggage checks that are now common place (BBC News Online, 1999). This paper explores several aspects of airport surveillance. In themselves, airports offer interesting case studies of how mobility may become watched and controlled. The heavy government presence in these spaces, the possibility for the invasion of privacy and the disregard of human rights must be questioned. We may also learn useful lessons at airports. For as recently discussed by Lyon (2003) airports may be seen as \"possible microcosms of wider societal surveillance trends\" (20) signposting possible surveillance developments in other spaces and places. In this paper, I shall therefore explore a number of the most recent surveillance techniques for monitoring and controlling passengers at the airport. These practices are argued to closely match designations of a 'panoptic sort' (Gandy, 1993) or surveillant sorting (Lyon, 2002b) from the surveillance studies literature. I discuss the implication of these techniques in terms of privacy issues and discrimination. In-turn, the second section to this paper examines the surveillance of non-human mobilities at airports. Informed by an actor network theory (ANT) approach and recent turns towards the agency of 'objects' and 'things', the paper provides a brief over-view of the surveillance of baggage, possessions and aircraft -indicative of a lack of research in this area and a trend towards the surveillance of non-humans. Firstly, let me contextualise this discussion within current turns towards mobility and security. I examine the theorisation of mobility within surveillance studies and trends towards surveillant sorting, before moving on to illustrate the increasing need for an ANT approach towards the subjects of surveillance.",
          "Recent progress towards the understanding of movement within surveillance studies probably reflects the current interest in mobility from disciplines such as sociology (Urry, 2002;2000), geography (Cresswell, 2001;Thrift, 1995) and cultural studies (Morris, 1988). This is no-doubt indicative of the increasingly mobile world in which we live, and must also owe its momentum to the popular fluid and mobile thought of philosophers such as Gilles Deleuze, Felix Guattari, Ian Chambers and Paul Virilio. This new mobility paradigm has moved beyond static idealizations of society towards theories that are marked by terms such as nomadism, displacement, speed and movement. Mobility has become as central as the tenets of society, space and power that fill much of our thinking. In many cases, mobility has often been understood to be an escape from a social order and as an act of resistance (Cresswell, 1993). De Certeau's (1984) tactics and Deleuze and Guattari's (1988) nomad-power transgress the fixity of the state. Gypsies and travellers cross physical boundaries and so become recognized as 'out of place' by authorities and local residents (Sibley, 1995). In many cases, physical mobility is often seen to signify the metaphorical movement from, and transgression of social norms. Mobility is therefore often viewed in terms of risk to the safe and static containers of space, territory and social order. Bauman's (1993) stranger and vagabond provide the perfect figures of risk to security as spatial and social boundaries are crossed. This is not to argue that mobility is always taken as deviant. Obviously, tourists and business travellers do not necessarily contradict social norms. Taking a relational (Cresswell, 1996) view of mobility allows us to see movement merely injected with meaning in different socio-spatial circumstances -it means different things, to different people, in different places and at different times. Therefore, where a tourist may be welcomed into a country a migrant may not be. However, the spaces at which mobility may be channelled provide the most obvious points for these social differentiations. Henceforth, it is at the borders, at airports -where movement and distinct spatial boundaries coexist -where undesirable mobilities may be distinguished from the desirable. This is increasingly achieved by surveillance. Surveillance is one method used to control mobility. As the growth of information and communication technologies allow data packets to be transferred between databases and other surveillant technologies, surveillance itself has become more mobile (Lyon, 2002b). For Lyon andothers (2002a, Phillips andCurry, 2002) however, surveillance may also struggle with mobility by stasis. What Lyon describes as the 'phenetic fix' characterizes surveillance techniques that attempt to capture the essence of movement -taking a snapshot of movement, bodies and identities. Information abstracted from moving bodies may be appropriated and fitted into neat categories and definitions. In this paper, I want to examine a particular form of the 'phenetic fix' labelled 'surveillant sorting' that is probably most illustrative of airport and border surveillance. Surveillant sorting is often used to describe the formation of those categories and profiles discussed above. Gandy's (1993) term the panoptic sort being another terminology. Issues surrounding sorting emphasise the possibility for discrimination -treating some better than others. Surveillant sorting is also able to distinguish the mobile. For example, studies have illustrated that the transfer of data packets may become shaped and prioritised, some given quicker speed and access than others (see: Graham and Wood, 2003). Mobile sorting may also work materially. Cars travelling through the Canadian-US border (Sparke, forthcoming) are put into categories of 'good' and 'bad' by the NEXUS system, the good are able to pass through quickly, where 'bad guys' are forced to wait in long queues for questioning. At airports, surveillance, rather than examination, is becoming the key method with which to distinguish passengers (Salter, 2003). In this paper, I examine a number of issues surrounding these procedures. And yet, I also want to explore a form of sorting that has gone on within the wider study of surveillance itself and which airports may be used to highlight. By this, I mean that there seems to be a tendency to limit our understanding of mobility to humans -to sort the human from the non-human. This paper could examine the mobility of passengers passing through airports and the associated methods of surveillance and control. However, to do this would be to ignore the orbiting movement of non-humans that are also subjected to the airport's controlling gaze. It is in this respect that we may turn to Actor Network Theory (ANT), the work of Bruno Latour, John Law and the recent veering of the social sciences towards the role of objects (Pels et al., 2002), things and their mobility (de Laet, 2000). These approaches are somewhat absent from surveillance studies as noted by Wood in a past issue of this journal (Wood, 2003). Wood observes Kirstie Ball's paper (2002) to be one of the only contributions to such a debate. Ball's approach pinpoints the elements of surveillance systems that \"are all socio-technically perpetuated through interactions of artefacts, technologies and people\" (586). Therefore, whilst humans and non-humans may form networks of the elements of surveillance, little emphasis has been placed upon these aspects of the subjects of surveillance. Examining how surveillance deals with mobility need not be limited to the mobile human, but extended to include things, and indeed their amalgamation with people (Haraway, 1991;Urry, 2002). Such a shift in focus is particularly pertinent when we turn to the increasing concentration upon the means of terrorism by governments and borders -shifting surveillance away from the perpetrators. Carter (2001) states that governments need better capabilities for the detection and prevention of terrorist activities. He argues that: This involves surveillance of persons and motives -a delicate matter -but also surveillance of potential means of destruction such as crop dusters, germ cultures and pilot instruction. Surveillance of means raises far fewer civil liberties issues than does surveillance of persons, and it might be much more effective. (15) The surveillance of the means of destruction is a much more attractive option for security. Governments take the view that a bag will not mind if it is x-rayed, CT scanned, and rummaged through. Examining the subjects of surveillance can enable our better understanding of the purpose, workings, and impacts of these systems. This approach is discussed in the final section. Firstly, it is useful to situate these practices within the development of air-travel, the growth of airports, and the rising need for airport security.",
          "The first airport resembling anything that we know today was built in 1922 at Königsberg in East Prussia (Zukowsky, 1996). Königsberg's terminal was a considerable advance from the windswept military landing fields and beaches. The new terminals became bridges for the transition between land and flight. Airports captured the imagination representing a symbol of progress and technological achievement in modernity. Indeed, the driving ambition of Hitler's Nazi regime was manifest in the rebuilding of Berlin's Templehof airport -the new airport reborn from the ashes of the failing past government and decaying economic system (Braun, 1995). From these early beginnings, airport space was a segregated and exclusionary one, being inhabited only by military personnel or the wealthy. Even before the First World War, aviation became an impossible dream fulfilled. Higher society frequented airfields to watch the displays of macho pilots and their flying machines (Wohl, 1994). Flights became social and cultural gatherings for the upper classes, excluding the lower. Soon, the development of the vacation and package holidays saw the growth in trips to Europe; to Spain and Portugal and the beginnings of mass transatlantic travel (Löfgren, 1999b). International air transport became less costly and connected to many destinations. Wide-body jets meant that 200 to 500 people could be carried by one flight eliminating the extortionate costs involved. The further deregulation of the American and British airlines saw airlines multiply, stimulating the competitiveness of seat prices, and also a greater choice of destination for many travellers. These changes also meant radical developments in the complexity of running airports as airlines rapidly grew and folded. Although, the growth of mass travel was still unable to fully democratize this space. VIP lounges developed for business and frequent flyers to enjoy the quiet luxury of a comfortable seat, the amenities of television, and most recently a shower or sauna. The progression of the airport culminated into the enormous multi-terminal, multi-runways sites that see millions pass through each year. They soon became airport-cities employing the work force of urban populations. Manuel Castells (1996) describes airports as a 'space of flows', nodes on the global networks of mobility. But the terminals are also built for a carefully controlled internal mobility. David Pascoe argues that the terminal was \"predicated on confluence, the precisely timed pumping of passengers into and out of [space]\", airlines then \"constructed specially designed terminals to control and direct the masses of passengers…\" (2001: 226). For as passengers increased so did the potential dangers and security risks. This was due partly to the function of airports as borders, but also to the nature of international terrorism that has developed over the past three decades. Perhaps Paul Virilio (1997) is correct where he writes that the border is now at the centre of our cities. Airports act as border zones to the vertical vectors of mobility that cross national and state boundaries. For this, Airports must function in a similar way to the borders that police the boundaries of our countries, regulating the movement of people that enter and leave. For Virilio, \"the doors to the city are its tollbooths and its custom posts are dams, filtering the fluidity of the masses, the penetrating power of the migrating hordes\" (1986: 7). Airports must therefore function as a screening or filter for the threats to a nation, but airports also become screens for quite different threats. Since the 1970's terrorism has grown, looking towards the aviation industry as a target. On the 22nd of July 1968, an El Al flight destined for Tel Aviv was hijacked. The hijacking was a bold political statement where passengers were to be traded for Palestinian terrorists imprisoned in Israel. By attacking the Israeli airline they were effectively attacking the Israeli state (Hoffman, 1998). In addition to this, the intense media coverage given to the hijacking saw the event publicized all over the world. Aviation became the perfect target for terror, which since 1968 has endured a rapid rise of these attacks. Planes were spaces that could be controlled easily; the fear of crashing subduing any passenger resistance. Airports also offered limited surveillance of the throngs of people that were travelling. The likelihood of a successful hijack has even been calculated, at 76% it is an obvious incentive for terrorists (Merari, 1998). Due to the increasing risks from terrorism and illegal immigration, airports had to find a way to take control of the airport space. Developments of airport security and surveillance were initially felt through actual changes to the space -to the architecture of the airport. John Zukowsky notes that, \"Intensified security measures changed the planning of airports, deliberately cutting up the open flow of space \" (1996: 15). The development of the sterile lounge concept (Wallis, 1998) saw baggage and security checks made before entrance to the gate. This removed the chance of threatening objects reaching the plane by person. Security checks could then be completed in a purpose built room far away from the gate to reduce queues disruptive to passengers. These techniques are still regarded as the preferred method, evident in the centralized security 'choke points' in most terminals (NRC, 1999). Surveillance has become therefore, one of the primary means of ensuring that airports are made safe and secure. 'Threat vectors' (NRC, 1999) that is, the paths by which threats may enter the airport and find their way to a plane are identified and monitored. In the next two sections, a number of these approaches and their implications are discussed.",
          "Monitoring passengers equally, proved to be far from the truth of running an international airport (Jenkins, 1998). Airport authorities needed a way of putting passengers under surveillance without having to examine every passenger rigorously. For this, methods of passenger screening and profiling have been developed to effectively sort the most probable threat to security from other passengers.",
          "The most recently publicized and perhaps most worrying form of surveillant sorting at airports has come through a surveillance technique known as profiling. Profiling is the ability for information or data about an individual to be built up. People may be sorted into profiles of particular consumer groups. The psychological profiles crime investigators use is an obvious example. Profiles are then used to predict a person's likely behaviour or the likely characteristics a criminal may embody. Profiling also usually relies upon vast quantities of information gathered about someone that are then stored and shared. In light of the growth of international terrorism, a new form of profiling has been developed called Computer Assisted Passenger Pre-Screening (CAPPS) in the United States. It is possible to discuss here some of the issues surrounding profiling and its potential impacts. Profiling was introduced in the US on the recommendations of the Al Gore led White House Commission for Aviation Safety and Security following the TWA flight crash of 1996. The Commission found that, \"passengers could be separated into a very large majority who present little or no risk, and a small minority who merit additional attention\" (1997). The rationale behind profiling is then to concentrate upon the minority -those likely to be a threat, rather than the majority, which requires much greater resources. The recommendations were based upon the practices of the airline El Al who used profiling techniques to fit the descriptions and likely behaviour of terrorists to passengers. CAPPS is reported to work by matching likely terrorist behaviour with present airline flight information. A flagged 'selectee' will then be subjected to personal checking of possessions and perhaps an interview and questioning. The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\" (1997). Problems are also clear over the secrecy of these profiles and the categories passengers become inadvertently pushed into. The lack of control passengers have over these profiles has led to concerns that innocent travellers may be unable to rid themselves of the 'selectee' status of their sorting. The repercussions of September 11th have seen the reorganisation of US airport security under the Transport Security Administration, itself under the newly formed Department for Homeland Security. Bolstered by this restructuring of governance, President Bush's developing and renamed Terrorist Information Awareness Program and rumours of the testing of CAPPS II have caused concern throughout privacy awareness groups. Posted to the US federal register on January 15th of this year, the TSA introduced a new system of records and amendment of the Privacy Act known as \"Aviation Security Screening Records\" (DOT, 2003) designed to facilitate the CAPPS II system. The proposal was open to comment and provoked a massive response from individuals and privacy awareness groups. The prime concern behind this system is again the discrimination and segregation of passengers, and also the sharing of passenger information between multiple government agencies. However, not only the sharing of Personal Name Records (PNR) are at stake (see Bennett, 1999), but the vague details of the report does not count out the possibility that more detailed banking records, tax histories and other sources of information may become easily passed between airports and other state departments. Fortunately, in Europe, different rules apply. In March 2003, the European Parliament moved to reverse an agreement made between the European Commission and the United States that would have forced PNR records to be transferred from European airlines to US airports. Another version of CAPPS II was posted to the Federal Register on August 1st with several revisions made upon the January version. At present, however, the majority of concerns lie in the current testing of CAPPS II, particularly with regard to the JetBlue controversy. The popular airline JetBlue illegally transferred a large number of passengers' records to an agency working for the US government (Singel, 2003). The information was to be used for the testing of passenger profiling on internal flights. Some of this information became public and was published to the Internet. A similar scheme has also been argued to be taking place through Galileo -the computer reservation system (CRS) that runs a large part of air travel reservation in the US.",
          "Other forms of surveillant sorting are occurring through biometric technologies. Biometry is the measurement of the body. Biometrics effectively treats the 'body as text'; identifying specific body parts such as the iris, face and palm signatures to identify the individual (see Ploeg, 1999Ploeg, , 2002)). The use of biometrics extends far beyond air travel, forensic evidence such as fingerprints are commonly used to identify suspects. Nevertheless, biometrics has also become popular at certain US and European airports. Biometrics work by containing detailed records of a passenger's body information (Delio, 2003), the most popular being retina patterns, but this may also include finger prints, DNA matching and face recognition (Agre, 2001) to name but a few. This information and passengers' identities are then stored on cards or a central database to be compared. Biometric systems are usually referred to as authentication systems, where a positive match to the stored information authenticates the identity (Clarke, 2003). However, other biometric systems, referred to as identification systems, compare the captured body data to large amounts of records kept on other databases. Here, passengers are not proving their identity; rather, airport authorities are identifying them. Biometric systems are of obvious concern to proponents of privacy rights, the critique and defence of biometrics coming from several different philosophical perspectives of technology and human agency (see van der Ploeg, 2003). One particular view is to critique biometrics for their treatment of passengers as objects. Much like a piece of baggage to be identified by a bar code, passengers' bodies offer similar bar codes to be read by authorities. Objections have also been raised over the penetration of bodies as biometric systems scan passenger identifiers. Others m ay analyse biometrics from the perspective of how the information is stored, particularly over the ease by which data may be shared with other 3rd parties. The possibility for data to be hacked and misused by external sources is another issue. Irma van der Ploeg (2003) discusses problems over the security of the American immigration service INSPASS authentication scheme at Los Angeles airport. Recorded hand geometry data was designed to be stored only on the card carried by the passenger. However, van der Ploeg illustrates that the card, if lost, would be quickly replaced by INSPASS thereby revealing the storage, somewhere, of this supposedly private information. But biometrics also has several surveillant sorting implications. For example, we can look to the effects upon the movement of passengers. Schiphol airport, Amsterdam has a fully working biometric system developed by the airport and the Dutch immigration service. The 'Privium' scheme is an 'authentication' scheme that, for 90 euros, allows enrolled passengers to bypass busy queues and check in delays. Passengers are then sorted into those enrolled by the scheme and those not. The logic behind these systems is similar to profiling in that it then gives more time for additional security measures to be placed upon those not enrolled in the biometric scheme. The systems therefore virtually sort passengers' information but also work to materially sort passenger's mobility within the terminal. The business 'kinetic elites' (Graham and Marvin, 2001;Andreu, 1998) may pass through to the VIP lounge at speed. And yet, the average traveller, forced to park long distances from the terminal, is excluded from member's club lounges and has to endure waiting in lengthy 'check-in' and security queues.",
          "Finally, the sorting of passengers has also begun with the introduction of what can be known as digital surveillance, or most commonly algorithmic surveillance (see Norris, 2002;Graham and Wood, 2003). Of course, CCTV has become one of the predominant modes of surveillance within airports as well as within cities, shops and indeed on my own university campus. The ability for CCTV to pick up deviant and threatening behaviour has been well documented, as well as its use in identifying suspects. Within airports, its continued and extended application has become a clear priority of airport security concerns. In the UK, the John Wheeler (2002) report on airport security suggested that valuable improvements have been made in this area. A technology now widely used at airports in the United States is a system called Exit Sentry developed by Cernium. Exit Sentry is able to monitor the direction of movement of passengers walking through the exit corridor of secured areas of an airport terminal such as arrivals. A passenger walking the wrong way, trying to enter the secured area through the exit corridor is warned with a flashing light. If the suspect then persists, a siren alerts security staff, and a recording of the suspects' movements is made. Systems such as Exit Sentry are exemplary of recent algorithmic surveillance technologies that in real time analyze CCTV footage of spaces. These systems understand the differences in movement of individual passengers, and may filter out static background information. Threats are not identified by a particular property of an object; rather, particular movements are inscribed with meanings of what is an allowed movement and what is considered suspicious and deviant. The uses for such technologies have been suggested to recognize the movements of car thieves and even people contemplating suicide at quiet train platforms (Norris, 2002). Indeed, research in progress at Southampton University has developed an approach called gait recognition that may be able to identify the identity of individuals by their distinctive walking styles. CCTV algorithmic surveillance effectively sorts and differentiates between mobility, in Exit Sentry's case, that is accepted (non-threatening) and that is unacceptable (threatening). The meanings are not essential to the mobility picked up by the cameras; rather the algorithms are imbued with these meanings. Sorting objects I now move beyond my discussion of airport surveillant sorting, towards a sorting of a different kind. Here I want to discuss the non-humans that have become placed under surveillance at the airport. As I have discussed airports are obviously places of human mobility. And yet, objects and things also intersect these spaces. Baggage flows through the baggage systems miraculously arriving at our destination. We carry on duty-free, cigarettes, and alcohol. But we are only aware of a tiny spectrum of the surveillance systems that place objects under this scrutiny. Initially, the sorting of objects and things at airports has occurred through the physical arrangement of airport spaces. For instance at the re-building of Templehof, the architect Ernst Sagebiel made the innovation of separating passengers, goods and baggage onto different levels of the airport (Braun, 1995). In addition, freight could arrive by an underground train opposite to the subway from which passengers arrived. This form of sorting is still used today, as automated baggage handling systems operate behind the holes in which our luggage disappears.",
          "The monitoring of our luggage at airports is probably one of the most visible methods of surveillance. Contaminating or illegal goods must be stopped. Airports are also nervous over the possibility of explosives and other weapons that may be used for terrorist activities. The identification of these objects is therefore one of the prime duties of airport security and immigration control as they concentrate upon 'threat vectors' (NRC, 1999). These objects may be identified and put under surveillance in a number of ways. If we first take hand luggage, the classic examples are probably the phase induction (PI) metal detection systems (Jenkins, 2002). Memorable scenes in films depict passengers unable to pass through these archways due to a belt or keys. At this point in a person's journey, hand luggage is usually x-rayed. X-ray machines are able to look inside a person's belongings, where operators search for suspicious looking objects, these include explosives, arms, and organic material. Although these x-ray systems, according to FAA research, could also be used upon passengers (NRC, 1996). X-ray operators could easily identify passengers carrying concealed weapons, without having to resort to personal body searches. Objects such as metals and sharp objects become a threat when held by a passenger. They have the potential to become a risk to the plane or the airport. The agency of a threat becomes possible because of the stable network of actors such as a knife, a passenger and indeed a plane. However, objects may take the shape of a threat independently of a person. For example, timer device explosives may be set in advance or barometric sensor devices respond to changes in pressure. Much like Latour's (1999) example of a sleeping policeman (speed bump), where the agency of a policeman to deter speeding becomes deferred onto the speed bump, here, the terrorists will or agency is given to the bomb. It is this deferred relationship that has led airports to implement baggage reconciliation: Positive Passenger Bag Matching (PPBM) recommended by the White House Commission (1997) in the United States. These systems are designed for the possibility that terrorists are not willing to blow themselves up when detonating a bomb. The terrorist would then check their bags onto a flight without actually boarding the plane. The PPBM system automatically flags up the bag that has been put onto the flight without an owner, the bag may then be pulled from the flight. Such a system relies upon the ability of the airport to know where a passenger's luggage is, through most recently, radio frequency (RFID) tagging and even individual trays that a bag is placed onto. Baggage handling systems most commonly use bar code technology however, so that baggage can be read to determine the identity of its owner and the flight it should be on. Surveillance technology has necessarily become more intense in an attempt to secure aviation from these objects. It is also much easier for airports to scan checked baggage than to actually interrogate a human subject. And yet, similarly to passenger screening, airports have not had the staff or equipment to check every person's bag. As such, the scanning of baggage has been linked to the CAPPS profiling systems so that a 'selectees' luggage is put under increased surveillance and identification for threats. For example, the Explosive Detection Systems (EDS) were initially used to scan 'selectee' passengers' bags for traces of explosives. Here, a 'selectee's' baggage is that scanned by the EDS machines. Since the events of 9/11 efforts have been made to increase the installation of EDS machines to ensure every passenger's bag is scanned, this has culminated in the International Civil Aviation Authority (ICAO) recommending that 100% hold scanning be made by 2006. Other systems use the computer tomography (CT) scanners used in hospitals. A slice or tomography of a bag can then be used to calculate the mass and density of materials that are then matched against explosive and hazardous materials.",
          "Perhaps the object we are most conscious of at airports is the passport. Although, considering the passport's importance to international travel there has been very little written about it in the social sciences (for an exception, see: Torpey, 2000;Caplan and Torpey, 2000;O'Bryne, 2001;Salter, 2003). And yet, the passport is one of the primary tools of states for the surveillance of their population's movements. For Torpey (2000), passports provide the means to govern a population's movement, 'penetrating' the individual to 'embrace' populations. Passports are used to identify a person, making a person legible to the state who may then enforce their authority over movement. Passports are also symbols of nation-states and our allegiance to them (O'Byrne, 2001). Paul Fussell's (1982) Abroad typically articulates the standardization that passports impose upon the traveller: where do people belong, where do they live, what is their hair colour, eye colour. All these aspects of our selves that we continually negotiate and question become squashed into the tight categories of the passport. For Löfgren (1999a): \"as a traveller you now had to live up to your passport identity to be able to prove your identity\" (19). This is nowhere more obvious than at the airport. Identification is then one of the primary means of airport surveillance; the display of the passport marks a 'reading' of the individual that occurs at multiple times during their journey. From supplying the ticket at check in, to security, to boarding the plane with your boarding pass and then landing and undergoing immigration control in the destination country, the passport must be displayed many times in the airport. It has become integral to the ritual of international travel. Airport staff and workers must also supply, not their passport, but a similar identification card to access areas of the airport. Indeed, airport vehicles must also be identifiable, clearly bearing company insignias and paint designs. In terms of mobility this is vital, passports and identity cards govern where a person, airport worker or object may go. Passports are a marker of our identity. Identities are read and given mobile and spatial limits by states and airports as they regulate mobility.",
          "We can also look outside the terminal to airspace for examples of non-human surveillance. Airspaces are territorial units but they also act as highways for the traffic of aircraft, these spaces must also become monitored and controlled for the purposes of security and safety. At the dawn of civil aviation, the organization of plane movement was described as \"decidedly sketchy in nature\" (quoted in Wegg, 1995: 115). The pilot had to rely upon skill and sight, and had no knowledge of local weather conditions. In order for the aviation industry to provide efficient and safe travel, it was essential that more could be known about environmental conditions. The first known aviation weather station in the UK was introduced at Croydon airport in 1921 (Wegg, 1995). This surveillance of weather and the general environment is perhaps an increasingly common trend for surveillance. Brazil's System for the Vigilance of the Amazon (SIVAM) developed by Raytheon provides a huge environmental monitoring system to show signs of illegal drug trafficking; mining and logging that have previously gone undetected in the rain forest. Still, the monitoring of weather is obviously not the only concern of air traffic control. As was quickly found in the 1920's crashes were most likely to occur around airports, where planes in the air and on land found it difficult to avoid each other. The need to orchestrate these movements proved paramount to safety. For this, Air Traffic Control (ATC) developed at airports, reinforcing the idea of an airspace that could be vigilantly monitored and controlled for the purposes of both safety and security. Again, at Croydon airport, the new terminal built in 1928 featured an ATC. The tower, or \"chart house\" scanned for incoming aircraft and maintained radiotelephony (RT) communications with planes. The towers therefore monitored all the positions of incoming and taking off aircraft. Un-identified or enemy planes could also be monitored from an ATC. More sophisticated radar technologies have since developed, the air traffic control radar beacon system (ATCRBS) being one. After takeoff, aircraft turn on their transponders that send signals recognized by equipment at air traffic control that may then monitor not merely the position of an aircraft but also the unique identity of the flight can be gained from the transponder signal. Air traffic control, can then manage and organize the airspace for the most efficient and safe flow of aircraft. A similar system has also developed on the ground at airports so that 'aircraft incursions' -planes wandering onto runways without permission -may be avoided. This has usually been completed using a pair of binoculars; however, problems arise during periods of low visibility in bad weather and particularly fog. Airport surface detection equipment (ASDE) is a radar system able to locate and monitor planes movement, combined with tower automated ground surveillance system (TAGS) aircraft and indeed, any other airport vehicle may also be identified on the ground surface (Wells, 1996). Although, it must be noted that newer airspace surveillance is moving considerably away from the control tower as power is given back to the planes. Instead, decentralised forms of surveillance are becoming evident in the form of intelligent on-board systems that allow each aircraft, and airport land vehicles to monitor each other's position.",
          "This paper has in part attempted to question the broad brushstrokes of 'mobility' and 'flow' that are often used to describe airports. Airports act to monitor these flows and filter out threats, sorting and categorizing mobile bodies. Whilst this may happen spatially in-terms of access to specific places of the airport, it also occurs in terms of movement. As I have shown, people may become discriminately separated into categories that also affect their passage and speed through the airport. These systems have been shown to be increasingly troubling in their invasion of privacy and the possibilities for expansive data sharing. In-turn this paper has examined a conceptual sorting within surveillance studies, using the airport to recognise how non-humans may become monitored and controlled -to explore the surveillance of not only human but also non-human mobility. This approach has been argued to be of particular significance given that governments are increasingly turning to non-human surveillance (Carter, 2001). Now the focus is on the means of terrorism. Objects such as explosives have become actants in the eyes of airport security, where barometric or timer explosives pose threats independent to the whereabouts of their makers. Indeed, the surveillance of weather and the location of the actual aircraft is also key to the safety concerns of flight. The mobility of non-humans therefore, pose new challenges for airport surveillance in a climate where 'terror', 'bomb' and 'explosives' are words on every passenger's minds. At its most basic level, I think an ANT approach can help scholars to acknowledge this monitoring of non-humans. Then we can go further than the mere description of these techniques. ANT's implication for surveillance studies is not the simple mobilisation of supposedly inaminate, non-sentient or nonhuman things that may be monitored. We can do more than merely \"follow the actors\" (Latour, 1993; see also : Hitchings, 2003). In particular, ANT may be used to give greater insight into the relationships or networks of objects that surveillance systems examine. But this exercise may seem somewhat pointless if the relationships between humans and nonhumans are not understood. Indeed, investigating the monitoring of a machine in a factory may seem interesting, but is this important for surveillance studies? Does this even fit our definition of what surveillance is? In my opinion, for now, our starting point must be human. This paper has separated its treatment of humans and non-humans in an attempt to illustrate a tendency for scholars to delimit their focus to the study of the surveillance of humans and its implications. It has not been my purpose to forget the complexity of relationships between people and things, the cyborg 'machinic ensembles' of the plane or car that transport us to our destination (Thrift, 1995;Lyon, 2001). Whilst the separations I have made have been deliberately artificial for ease of illustration, any examination of the surveillance of things, only really becomes important if we are to appreciate the impacts they may have upon the human and indeed the human associations with these things. Certainly, it seems as though the surveillance of non-humans, in the examples I have explored, are used precisely because of the difficulties posed by monitoring the human. Many of us do not wish to have a bar-code printed on us, or to be radio tagged. Therefore, non-humans are monitored instead of humans. In-turn non-human monitoring may also be used as indicators of human activity -as evidence. It is far easier to trace illegal weapons and explosives than to read the minds of passengers to test for the intent of their actions. In other cases, particularly remote monitoring, the material effects of human activity are easily seen by satellite photography. Illegal logging may therefore become recognised on vast areas of the Amazon by its initial impacts, as opposed to the individual actions and movements of a person. Of course, the network metaphor may not always fit. At airports, the relationship of passenger and baggage only really resembles a traditional network when passengers' are in contact with their luggage -when their relationship is static. More fluidic metaphors can be invoked to better describe the often shifting relationships of passenger to bag -a 'mutable mobile'. Furthermore, Law and Mol's (2001) article suggests that the metaphor of fire is useful to describe the oftenflickering presence of actors within a network. This metaphor can be seen in the instance of a passenger not boarding a flight and leaving their baggage behind. As discussed, baggage reconciliation ensures that this separation is not complete given the linkage created between passenger and bag. If a passenger fails to get on their flight, their 'absent presence' flickers into significance to ensure the removal of their luggage from the plane. At the same time, do we also limit our examination to humans, objects and things? Activists may argue that the surveillance of animals should be questioned. At airports in particular, animals are routinely transported illegally. But animals are in-fact still key to the surveillance and security operations at the airport. Animals are not only a subject of surveillance but are part of the surveillance system itself. Sniffer dogs or K9 teams can be trained as vapour detectors to monitor illegal goods such as narcotics, and explosive devices (Clutterbuck, 1994). These practices are not simply completed by people or animals but succeed because of their mutual relationship (Saunders, 1999; see also Haraway, 1991;Wolch, 2002). Questioning these relationships is long overdue in our fixation upon the high-tech. We must ask how do these interactions take place? What implications do they have? However, this is not to suggest that ANT is a magic recipe for success. There are obviously many shortcomings illustrated by critiques that query the pretension to inclusion -the universal enfranchisement of ANT (Lee and Brown, 1994), its traditional ignorance of spatiality (Hetherington and Law, 2000) and indeed the shortcomings of the network metaphor as illustrated above. That said we should not shy away from the many positives. Examining the networks of the subjects of surveillance sheds light upon the relationality of humans and things. An exploration of these networks exposes how an amalgamation of people and things may become illegitimate, worthy of surveillance. For example, a terrorist 'agency' or threat can be understood as the achievement of a person carrying scissors on a plane. Examining surveillance through this lens may reveal much about the very workings of surveillance systems, the norms and assumptions that frame them, and the implications for those they monitor. The control of the airport is in my view, a suitable starting point for such a discussion."
        ],
        "ground_truth_definitions": {
          "racial bias": {
            "definition": "biases being held towards ethnicity, and national origin - a person becoming sorted based on personal prejudice",
            "context": "The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\"(1997).",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "05bfced33d92944b7a0672490c371342d28ee076",
        "sections": [
          "Individual preferences over immigration policy are an essential input into any complete model of immigration policymaking. To understand both the policies implemented and the accompanying political conflict, we need to know who supports more or less-restrictionist policies and why. Preferences surely depend on a host of considerations including political ideology, ethnic and racial identity, and expectations about the economic impact of new immigrants. Among economic considerations, the anticipated effect of immigration on wages is likely to play a key role, as current factor income is a major determinant of individual economic welfare. Because current factor income depends primarily on individual skill levels, there may be a significant link from skills to wages to immigration-policy preferences. Different economic models, however, make contrasting predictions about the nature of this link. In the \"multi-cone\" Heckscher-Ohlin model of international trade, immigrants sometimes have no impact on native wages. \"Factor-proportions analysis,\" a framework often used by labor economists researching immigration, predicts that immigrants pressure the wages of similarlyskilled natives nationwide. \"Area analysis,\" an alternative framework in the labor literature, predicts that immigrants pressure the wages of similarly-skilled natives who reside in gateway communities where immigrants settle. In short, there is theoretical uncertainty about the wagesmediated link between skills and preferences in addition to the empirical uncertainty regarding whether individuals consider labor-market competition when evaluating immigration policy. 1In this paper we provide new evidence on the determinants of individual immigration-policy preferences and on what these preferences imply about how economies absorb immigrants. We use a direct measure of these preferences obtained from the 1992 National Election Studies (NES) survey (Miller 1993), an extensive survey of current political opinions based on an individual-level stratified random sample of the U.S. population. Our direct measure is responses of U.S. citizens to a question asking about the number of immigrants U.S. policy should permit. Building on the NES survey, we construct an individual-level data set identifying both stated immigration-policy preferences and potential immigration exposure through several channels. We then evaluate how these preferences vary with individual characteristics that alternative theories predict might matter. We have two main empirical results. First, less-skilled workers are significantly more likely to prefer limiting immigrant inflows into the United States. This result is robust to several different econometric specifications which account for determinants of policy preferences other than skills. Our finding suggests that over time horizons relevant to individuals when evaluating immigration policy, individuals think the U.S. economy absorbs immigrant inflows at least partly by changing wages. Further, they form policy opinions in accord with their interests as labor-force participants. These preferences are consistent with a multi-cone Heckscher Ohlin trade model and with a factor-proportions-analysis labor model. Second, we find no evidence that less-skilled workers in high-immigration communities are especially anti-immigrationist. If anything, our evidence suggests attenuation of the skills-preferences correlation in high-immigration communities. These preferences are inconsistent with an area-analysis labor model. There are five additional sections to this paper. Section 2 relates our work to the politicaleconomy literature on immigration. Section 3 presents alternative economic models of immigration-policy preferences. The following section discusses the data and our model specifications. Section 5 presents the empirical results, while Section 6 concludes.",
          "Previous research on the determinants of immigration policy in receiving countries has emphasized the variation in immigration politics across countries and over time (Joppke 1998, Kessler 1998, Perotti 1998, Money 1997, Freeman 1992and 1995). There is general agreement that systematic differences in policy outcomes across countries depend on varying political institutions, divergent national histories of settlement and colonialism, and the different effects of a changing international context. Moreover, it seems clear that even within countries the character of immigration politics changes over time. For example, a country's interest groups can dominate the policymaking process during some periods while in other periods partisan electoral competition is central. In contrast to this observed variation across time and space, very little research has focused on the distribution of individual preferences over immigration policy. Who supports free movement? Who advocates further restrictions? We contend that only once these questions about preferences have been answered adequately can a convincing account of cross-country and overtime variation in policymaking be constructed. Accounts of individual preferences can usefully be divided into economic and non-economic determinants. Non-economic factors include individual beliefs about civil rights and expectations regarding the cultural impact of immigrants. The civil-rights dimension of immigration-policy preferences has both a non-discrimination aspect as well as a more straightforward free movement of persons element. Individual policy preferences are also likely to depend both on the degree to which individuals think immigrants change native culture and on the desirability of those changes. Economic determinants are generally hypothesized to be a function of the aggregate costs and benefits of immigration, the fiscal impact on the public sector, and the impact of immigrants on native labor-market returns. This last consideration is arguably the most critical economic factor influencing individual policy preferences, and it is often the most controversial factor as well. Consequently, it is the main issue addressed in this paper. 2In previous work, Goldin (1994) and Timmer and Williamson (1996) present historical evidence on the potential impact of labor-market outcomes on immigration policy. Goldin (1994) finds that House Representatives in 1915 were more likely to vote in favor of a literacy test to restrict immigrant inflows the lower were wage increases from 1907 to 1915 in the Representatives' district cities. Goldin interprets this as indirect evidence that immigrants' pressure on native wages contributed to tighter immigration restrictions. Pooling five countries from 1860 to 1930, Timmer and Williamson (1996) find that more-restrictionist immigration policies were significantly correlated with lower unskilled wages relative to average per capita income. They interpret this correlation as evidence that countries with more unequal income distributions tended to restrict immigration to maintain the relative income position of the less-skilled. 3   In contrast to the policy focus of Goldin (1994) and Timmer and Williamson (1996), Citrin, et al (1997) use individual-level survey data to study the immigration-policy preferences of a crosssection of U.S. citizens. Controlling for a wide range of factors that potentially shape preferences, they conclude \"that personal economic circumstances play little role in opinion formation\" (p. 858). Specifically, they find that labor-market competition does not influence preferences. Using information from a national poll, Espenshade and Hempstead (1996) find some mixed evidence that less-educated and lower-family-income individuals are more likely to support immigration restrictions. They interpret this evidence as suggesting that people care about immigration's labormarket impacts on wages, employment, and work conditions. All these studies provide valuable information on the economic determinants of immigrationpolicy preferences and political behavior. However, our work improves upon them in at least three important ways. First, our study uses a direct measure of individual immigration-policy preferences. Some studies cited above infer from observed political actions or policy outcomes something about immigration-policy preferences. These indirect preference measures face the important limitation of being endogenous outcomes of the interaction between immigration-policy (and possibly other, e.g., foreign-policy) preferences and domestic political institutions. Policy preferences and institutions together determine policy actions, so the mapping from preferences to actions and outcomes is not unambiguous. Scheve and Slaughter (1998) discuss this point further. Second, our study draws heavily on the trade and labor-economics literature on immigration to test properly for the economic determinants of immigration preferences. We test three alternative models of how immigration affects the economic welfare of natives. In contrast, none of the related studies explicitly lays out any models of immigration. Instead, they all simply assume that 3 Hanson and Spilimbergo (1998) analyze the impact of economic conditions in the United States and Mexico on a different aspect of immigration policy: border enforcement and apprehensions. They find that the Mexican (i.e., not U.S.) purchasing power of U.S. nominal wages is strongly correlated with border apprehensions of illegal Mexican immigrants. immigration hurts natives via lower wages, unemployment, and other adverse outcomes. Many important issues have not been explored, such as whether immigration preferences are systematically different in gateway communities. Third, our study uses measures of individual economic exposure to immigration that follow closely from economic theory. This issue applies most strongly to Citrin, et al (1997) and Espenshade and Hempstead (1996). Empirical labor economists commonly measure skills via educational attainment or occupation classification; our empirical work below uses both these measures. 4 In contrast, Citrin, et al (1997) interpret educational attainment as a \"demographic variable\" rather than an \"economic factor.\" Although this choice has some justification in previous studies on the relationship between education and tolerance, we will demonstrate that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Citrin, et al (1997) measure skills with income and with eight dichotomous occupation variables. Only four of the eight cover working individuals, and these --\"white collar,\" \"pink collar,\" \"low threat blue collar,\" and \"high threat blue collar\" --are never defined or justified with reference to economic theory or evidence. Espenshade and Hempstead (1996) use dichotomous variables for educational attainment and family --not individual --income, with all specifications using both types of variables. Overall, these earlier studies use questionable skill measures, and they do not report specifications with single measures only nor do they test the joint significance of all skill measures together. These uncertainties regarding measurement and specification suggest the need for further analysis.",
          "To make the connection between individual economic interests and immigration-policy preferences we focus on how immigration affects individual factor incomes. Different economic models make contrasting predictions about the nature of the link from immigration to factor 4 For example, in the recent research on the rising U.S. skill premium the two most commonly used measures of the skill premium have been the relative wage between college graduates and high-school graduates and the relative wage between non-production workers and production workers (in manufacturing only). See Katz and Murphy (1992) or Lawrence and Slaughter (1993), for example. Berman, et al (1994) document for the United States that employment trends for this jobclassification measure track quite closely employment trends measured by the white-collar/blue-collar job classification-which in turn closely reflects the college/high-school classification. incomes to policy preferences. In this section we briefly summarize three alternative models: the multi-cone Heckscher-Ohlin trade model, the factor-proportions-analysis model, and the areaanalysis model. Across all three models we make two important assumptions. First, we assume that current factor income is a major determinant of people's economic well-being. Second, we assume that U.S. citizens think that current immigrant inflows increase the relative supply of less-skilled workers. As will be seen below, this assumption about the skill-mix-effects of immigrants is not explicitly stated in the NES question about immigration preferences. But this assumption clearly reflects the facts about U.S. immigration in recent decades. Borjas, et al (1997, p. 6) report that \"on average, immigrants have fewer years of schooling than natives-a difference that has grown over the past two decades, as the mean years of schooling of the immigration population increased less rapidly than the mean years of schooling of natives. As a result, the immigrant contribution to the supply of skills has become increasingly concentrated in the lower educational categories.\" Thus, we are assuming NES respondents are generally aware of U.S. immigration composition. 5   Given these two assumptions, we think that the economic determinants of an individual's immigration-policy preferences depend on how an immigration-induced shift in the U.S. relative endowment towards less-skilled workers affects that individual's factor income. To maintain focus on equilibrium wage determination, in all models we assume that wages are sufficiently flexible to ensure full employment. This allows us to abstract from unemployment, both equilibrium and frictional, though unemployment will be considered in our empirical work. To maintain focus on different skill groups, in all models we assume just two factors of production, skilled labor and unskilled labor. This keeps our analysis as simple as possible. 6 5 This skills gap between immigrants and natives does not address other interesting facts about the distribution of skills among immigrants. For example, Borjas, et al (1997. p. 7) show that the skill distribution of U.S. immigration has been somewhat bimodal at both the high-skill and low-skill ends of the distribution. 6 In the political economy literature, some researchers analyze the theory of economic determinants of immigration-policy preferences. Benhabib (1996) considers a one-good model in which natives have different endowments of capital. Kessler (1998) focuses on how trade and immigration affect native factor returns in standard trade models. Bilal, et al (1998) consider the case of a three-factor, two-household, two-country world.",
          "The multi-cone Heckscher-Ohlin (HO) trade model usually makes two key assumptions. First, there is one national labor market for each factor. Thanks to sufficient mobility of natives (and immigrants upon arrival), there are no geographically segmented \"local\" labor markets. The second key assumption is there are more tradable products (i.e., sectors) than primary factors of production, with products differentiated by their factor intensities. Multiple products are essential for establishing many fundamental trade-theory results, such as comparative advantage. With these assumptions, in equilibrium a country chooses (via the decentralized optimization of firms) the \"output mix\" that maximizes national income subject to the constraints of world product prices, national factor supplies, and national technology. This output mix consists of both which products actually get produced --i.e., the country's \"cone of diversification\" --and the quantities of production. In turn, this output mix helps determine the country's national factor prices. The general intuition is that each produced sector has a world price and some technology parameters that both help determine national wages. In the standard case where the country makes at least as many products as the number of primary factors, national wages are completely determined by the world prices and technology parameters of the produced sectors. Wages do not depend on national endowments or on the prices and technology of the non-produced sectors. 7   Immigration's wage effects depend both on the initial product mix and on the size of the immigration shock. Consider the standard case where the initial output mix is sufficiently diversified that wages depend only on world prices and technology. In this case, \"sufficiently small\" shocks have no wage effects. The country completely absorbs immigrants by changing its output mix as predicted by the Rybczynski Theorem: the same products are produced, but output increases (decreases) in the unskill-intensive (skill-intensive) sectors. Wages do not change 7 In the algebra of the Heckscher-Ohlin model, wages are determined by the set of \"zero-profit conditions.\" Each zero-profit condition is an equation setting a sector's world price equal to its domestic average cost, which in turn depends on domestic production technology and domestic wages. Algebraically, wages are the unknown endogenous variables and prices and technology are the known exogenous variables. In the standard case there at least as many equations as unknowns, so these equations alone determine wages. National endowments do not matter; nor do prices and technology in the idle sectors (which do not have binding zero-profit conditions because the world price is less than domestic cost --thus the national decision not to produce these sectors). In the alternative case with fewer produced sectors than primary factors, there are fewer equations than unknowns. Here, endowments matter because prices and technology are not sufficient to set wages. because the set of products does not change. This insensitivity of national wages to national factor supplies Leamer and Levinsohn (1995) call the Factor-Price-Insensitivity (FPI) Theorem. 8   With \"sufficiently large\" immigration shocks, however, national wages do change. Large enough shocks lead the country to make a different set of products. Different products entail different world prices and technology parameters influencing national wages --and thus different wages. Overall, a country absorbs large immigration shocks by changing both its output mix and its wages. In the literature on U.S. immigration, Hanson and Slaughter (1999) present evidence of immigration-related output-mix effects among U.S. states. Figure 1 displays the national labor market for the case of an HO world with three products. The distinguishing feature is the shape of relative labor demand. It has two perfectly elastic portions, each of which corresponds to a range of endowments for which FPI holds. The national output mix varies along the demand schedule. A different set of two products is made on each elastic part; accordingly, different relative wages prevail on each elastic part. On the downwardsloping portions the country makes only one product. Along these portions output-mix changes are not possible, so immigrants must price themselves into employment by changing wages. Point E o designates the initial labor-market equilibrium, with relative labor supply RS o and relative wages (w s /w u ) o . Two immigration shocks are shown. The \"sufficiently small\" immigration shock shifts RS o to RS'. Relative wages do not change, as immigrants trigger Rybczynski output-mix effects. The \"sufficiently large\" shock shifts RS o to RS\". The country now produces a new set of products. As a result the unskilled wage falls relative to the skilled wage (to (w s /w u )\"), and with fixed product prices this relative-wage decline will be a real-wage decline as well. 9 8 These two theorems follow closely from the Factor-Price-Equalization (FPE) Theorem, first demonstrated formally by Samuelson (1948). With additional assumptions about cross-country similarities (such as identical tastes and production technology), the FPE theorem predicts not only that national wages are determined by world prices and technology only but that national wage levels equal foreign wage levels. Also, note that factor-price insensitivity assumes that the country is sufficiently small in the world economy that changes in its relative-output mix do not change world product prices. If world prices do change than so, too, do domestic wages as predicted by the Stolper-Samuelson Theorem. 9 Three detailed comments on Figure 1. First, the relative-supply schedule is vertical under the assumption that all workers are sufficiently willing to work that they price themselves into employment regardless of the going relative wage. If workers make some explicit labor-leisure trade-off then the relative-supply schedule slopes upward but is not perfectly vertical. Second, along the national demand schedule the country's output mix progresses according to sector factor intensities. The likely output mixes are as follows. Along the leftmost branch of RD the country makes only the most unskilled-labor-intensive product. Along the first flat it makes this product and the \"middle\" intensity product, switching to only the middle product along the middle downward-sloping branch. The country picks up the most skilled-laborintensive product as well along the second flat; finally, along the rightmost branch it makes only the skilled-labor- The HO model has different predictions about link between skills and immigration-policy preferences. If individuals think FPI holds then there should be no link from skills to preferences. In this case people evaluate immigration based on other considerations. If individuals think that immigration triggers both output-mix and wage effects then unskilled (skilled) workers nationwide should prefer policies which lower (raise) immigration inflows.",
          "Like the HO model, this model also assumes a national labor market. The fundamental difference between the two is this model assumes a single aggregate output sector. Under this assumption there can be no output-mix changes to help absorb immigrants. Accordingly, any immigration inflow affects national wages by the same logic described above. Lower relative wages for unskilled workers induces firms to hire relatively more of these workers. The greater the immigrant inflow, the greater the resultant wage changes. In the labor literature, studies using this framework include Borjas, et al (1996Borjas, et al ( , 1997)). These studies calculate immigration-induced shifts in national factor proportions and then infer the resulting national wage changes. Figure 2 displays the national labor market for the factor-proportions-analysis world. Here the relative-labor-demand schedule slopes downward everywhere, with no infinitely-elastic portions where FPI holds. Initial relative labor supply is again given by the schedule RS o , with initial equilibrium again at E o and (w s /w u ) o . Immigration shifts the supply schedule back to RS', and the national skill premium rises to (w s /w u )'. Again, for fixed product prices real wages change, too. This model makes a single prediction about the link from skills to immigration-policy preferences: unskilled (skilled) workers nationwide should prefer policies to lower (raise) immigration inflows. This prediction can also come from the HO model without FPI. Accordingly, evidence of a link between skills and preferences is consistent with both models.",
          "Like the previous model, the area-analysis model also assumes a single output sector. The fundamental difference between the two is this model assumes distinct, geographically segmented intensive product. Finally, note that underlying the downward-sloping portions of RD is the assumption of flexible production technologies with substitutability among factors. With Leontief technology these portions would be vertical. labor markets within a country. This assumption is likely untrue in the very long run, but it may be true over shorter time horizons thanks to frictions such as information and transportation costs that people (both natives and immigrants upon arrival) must incur to move. \"Local\" labor markets are usually defined by states or metropolitan areas (many of which cross state boundaries). Each local labor market has its own equilibrium wages determined by local supply and local demand. If there is literally no mobility among local labor markets, immigrants' wage effects are concentrated entirely in the \"gateway\" communities where they arrive: immigration lowers (raises) wages for the unskilled (skilled). In contrast, in a national labor market immigrants' wage pressures spread beyond gateway communities. Natives can leave gateway communities when immigrants arrive; immigrants can move on to other communities; or natives can choose not to enter gateway communities as planned pre-immigration. In cases between these two extremes, immigrants affect wages everywhere but to a greater extent in gateway labor markets. The areastudies framework has guided a number of empirical studies of immigration. Studies such as Card (1990), Altonji and Card (1991), LaLonde and Topel (1991), and Goldin (1994) have tested for correlations between immigrant flows into local labor markets and local native wages. Graphically, the area-analysis model also looks like Figure 2 --but with the key difference that now this figure represents local, not national, conditions. Here, immigration shifts only the local relative supply of labor and thus depresses only local unskilled wages. Given this, the areaanalysis model predicts the following: unskilled (skilled) workers in gateway communities should prefer policies to lower (raise) immigration inflows. What about workers in non-gateway communities? With no geographic labor mobility over time horizons relevant to individuals when evaluating immigration policy, there should be no correlation between these workers' skills and their preferences. More generally, with some labor mobility workers in non-gateway communities should have qualitatively similar preferences but the skills-preferences link should be stronger among gateway workers. Less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (less-restrictionist) immigration policies than lessskilled (more-skilled) workers in non-gateway communities.",
          "",
          "We measure immigration-policy preferences by responses to the following question asked in the 1992 NES survey. \"Do you think the number of immigrants from foreign countries who are permitted to come to the United States to live should be increased a little, increased a lot, decreased a little, decreased a lot, or left the same as it is now?\" This question requires respondents to reveal their general position on the proper direction for U.S. immigration policy. To apply our theory framework to this question, we assume that respondents think that U.S. immigrant inflows increase the relative supply of less-skilled workers. As we discussed, this assumption clearly reflects the facts about U.S. immigration in recent decades. We constructed the variable Immigration Opinion by coding responses 5 for those individuals responding \"decreased a lot\" down to 1 for those responding \"increased a lot.\" Thus, higher levels of Immigration Opinion indicate preferences for more-restrictive policy. 10   Our theoretical framework hypothesizes that immigration policy can affect individuals' factor income according to their skill levels. To test whether skills are a key determinant of immigrationpolicy preferences, for each individual we construct two commonly used skill measures. First, respondents were asked to report their occupations coded according to the three-digit 1980 Census Occupation Code classification. From the U.S. Department of Labor (1992) we obtained the 1992 U.S. average weekly wage for each three-digit occupation. Under the assumption that the average market returns for a given occupation are determined primarily by the skills required for that occupation, these average wages, called Occupational Wage, measure respondents' skill levels. As a second skill measure, the NES survey also records the years of education completed by each respondent, Education Years. Educational attainment is another commonly used measure of skills, so we use it as an alternative skills variable. 10 The 1992 NES survey asked other questions about immigration-related topics which we do not analyze. For example, respondents were asked whether they think Asians or Hispanics \"take jobs away from people already here\". We do not focus on this question because it does not explicitly address immigration policy. Moreover, its responses cannot clearly distinguish among our three competing economic models. All our models assume full employment, so no natives could have jobs permanently \"taken away\" from immigrants. Moreover, our models are silent on the dynamics of adjustment. All three models could have immigrants \"taking\" jobs from natives during adjustment to a new full-employment equilibrium. As discussed earlier, Citrin, et al (1997) interpret educational attainment as a demographic variable rather than a skills variable. Below we present strong evidence that education measures labor-market skills once other considerations such as gender and political ideology are controlled for. Also, our mapping of occupation categories into average occupation wages captures skills across occupations much more accurately than the poorly defined occupation categorical variables in Citrin, et al (1997). In addition to skill measures, we need measures of where respondents live combined with information about gateway communities. For each respondent the NES reports the county, state, and (where appropriate) metropolitan statistical area (MSA) of residence. We combine this information with immigration data to construct several alternative measures of residence in a highimmigration area. First, we defined local labor markets two ways: by a combination of MSAs and counties, and by states. In our MSA/county definition each MSA (with all its constituent cities and counties) is a separate labor market; for individuals living outside an MSA the labor market is the county of residence. Following the extensive use of MSAs in area-analysis studies and Bartel's (1989) finding that immigrants arrive mostly into cities, we prefer the MSA/county definition but try states for robustness. Second, for each definition of local labor markets we try three different definitions of a high-immigration labor market: 5%, 10%, and 20% shares of immigrants in the local population. These immigration and labor-force data are from the 1990 decennial census as reported by the U.S. Bureau of the Census (1994). Altogether, for each of our six primary measures we construct a dichotomous variable, High Immigration MSA, equal to one for residents in high-immigration labor markets. In the tables we report results for our preferred measure, the MSA/county -10% definition. Alternative measures are discussed in the robustness checks. 11   We also constructed several measures of non-economic determinants of preferences. Following previous work in the political-economy literature, we include the following measures in our baseline analysis: gender; age; race; ethnicity; personal immigrant status; party identification; 11 In 1990 immigrants accounted for 7.9% of the overall U.S. population. Accordingly, our 5% cutoff might seem too low, but for completeness we tried it anyway. Also, the 1990 Census MSA data are organized by 1990 MSA definitions, but the 1992 NES survey locates individuals by 1980 MSA definitions. Using unpublished information on 1980-1990 MSA changes obtained from Census officials, we corrected discrepancies as best we could. and political ideology. Gender is a dichotomous variable equal to one for females. Age is a continuous variable. For race we construct the dichotomous variable Black, equal to one if the respondent is African-American. For ethnicity we construct the dichotomous variable Hispanic, equal to one if the individual self-identifies with a Hispanic ethnic group. Immigrant is a dichotomous variable equal to one if the respondent or his/her parents were immigrants into the United States. Party Identification is a categorical variable ranging from one for \"strong Democrat\" to seven for \"strong Republican.\" Finally, Ideology is a categorical variable ranging from one for \"extremely liberal\" to seven for \"extremely conservative.\" In addition to these variables, for certain specifications we included additional regressors which we discuss below in the robustness checks.",
          "Upon constructing the variables described in Section 4.1 and combining them into one individual-level data set, we observed that there was a significant amount of missing data. In the NES survey some individuals did not report either occupation or educational attainment; for these respondents we could not construct skill measures. Missing data also existed for some of our noneconomic determinants of immigration-policy preferences. Across the range of models which we estimated, when we simply dropped observations with any missing data we generally lost between 40% and 45% of the total observations. This standard approach for dealing with missing values, known as \"listwise deletion,\" can create two major problems. One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem. We also had little reason to think our data were missing at random, so we worried about biased inferences (see King, et al (1998a) for a detailed discussion). Alternatives to listwise-deletion for dealing with missing data have been developed in recent years. The most general and extensively researched approach is \"multiple imputation\" (King, et al (1998a), Schafer (1997), Little and Rubin (1987), Rubin (1987)). Multiple imputation makes a much weaker assumption than list-wise deletion about the process generating the missing data. mentioned above, these variances account for both the ordinary within-sample variation and the between-sample variation due to missing data. See King et al (1998a) and Schafer (1997) for a complete description of these variances. Table 1 reports the summary statistics of our immigration-opinion measure and explanatory variables calculated by pooling together all 10 of the imputed data sets. The \"average\" value for Immigration Opinion was about 3.6, between the responses \"left the same as it is now\" and \"decreased a little.\" Also, 23.5% of respondents lived in an MSA/county with an immigrant concentration of at least 10%. 13",
          "Our empirical work aims to test how skills and other factors affect the probability that an individual supports a certain level of legal immigration. The level of immigration preferred by a respondent could theoretically take on any value, but we do not observe this level. We observe only whether or not the respondent chose one of five ordered categories. Because we have no strong reason to think ex ante that these five ordered categories are separated by equal intervals, a linear regression model might produce biased estimates. The more appropriate model for this situation is an ordered probit which estimates not only a set of effect parameters but also an additional set of parameters representing the unobserved thresholds between categories. In all our specifications we estimate an ordered probit model where the expected mean of the unobserved preferred immigration level is hypothesized to be a linear function of the respondent's skills, a vector of demographic identifiers, political orientation, and (perhaps) the immigration concentration in the respondent's community. The key hypothesis we want to evaluate is whether more-skilled individuals are less likely to support restrictionist immigration policies as predicted in 13 The exact breakdown of all responses to Immigration Opinion is as follows: 58 \"increased a lot\" (2.3% of the total sample, 2485); 116 \"increased a little\" (4.7%); 937 \"left the same\" (37.7%); 552 \"decreased a little\" (22.2%); and 505 \"decreased a lot\" (20.3%). In addition we imputed responses for the 87 people (3.5%) responding \"don't know / no answer\" and the 230 people (9.3%) not asked the question because of survey design (all results reported in the paper are robust to excluding these 230 observations from the analysis). Among our other High Immigration Area measures, 43.7% of respondents lived in MSA/county with immigrants accounting for at least 5% of the population, while 8.5% of respondents lived in an MSA/county with immigrants accounting for at least 20% of the population. Finally, we note that the summary statistics in our data are similar to those obtained from the 1992 Merged Outgoing Rotation Groups of the Current Population Survey (CPS). For example, in the 1992 CPS 52.2% of the sample was female, 11.5% was black, and the average age was 43.3. the multi-cone Heckscher Ohlin trade model and in the factor-proportions-analysis model. Accordingly, in our baseline specifications we regress stated immigration-policy preferences on skills, demographic identifiers, and political orientation. In a second set of specifications we also include a dummy variable indicating whether or not the respondent lives in a high-immigration area and an interaction term between this indicator and the respondent's skills. With these second specifications we can test whether the skills-immigration correlation is strongest in highimmigration labor markets as predicted in the area-analysis model.",
          "",
          "Our initial specifications, estimated on the entire sample, allow us to test the HO and factorproportions-analysis models. Table 2 presents the results, where in Model 1 we measure skills with Occupational Wage and in Model 2 we use Education Years. The key message of Table 2 is that by either measure, skill levels are significantly correlated with Immigration Opinion at at least the 99% level. Less-skilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This skills-preferences link holds conditional on a large set of plausible noneconomic determinants of Immigration Opinion. Among these other regressors Gender, Age, Hispanic, and Party Identification are insignificantly different from zero. Black and Immigrant are significantly negative: blacks, and the group of immigrants plus children of immigrants, prefer less-restrictionist immigration policy. Ideology is significantly positive: more-conservative people prefer more-restrictionist immigration policy. We note that these non-skill estimates are similar to those found by Citrin, et al (1997) and Espenshade and Hempstead (1995). 14The actual coefficient estimates in Table 2 identify the qualitative effect on Immigration Opinion of skills and our other regressors. However, these coefficients do not answer our key substantive question of how changes in skill levels affect the probability that an individual supports immigration restrictions. To answer this question we used the estimates of Models 1 and 2 to conduct simulations calculating the effect on immigration preferences of changing skills while holding the other variables constant at their sample means. Our simulation procedure works as follows. Recognizing that the parameters are estimated with uncertainty, we drew 1000 simulated sets of parameters from their sampling distribution defined as a multivariate normal distribution with mean equal to the maximum likelihood parameter estimates and variance equal to the variance-covariance matrix of these estimates. For each of the 1000 simulated sets of coefficients we then calculated two probabilities. Setting all variables equal to their sample means, we first calculated the estimated probability of supporting immigration restrictions, i.e., the probability of supporting a reduction in immigration by either \"a lot\" or \"a little.\" We then calculated the estimated probability of supporting immigration restrictions when our skills measure is increased to its sample maximum while holding fixed all other regressors at their means. The difference between these two estimated probabilities is the estimated difference in the probability of supporting immigration restrictions between an individual with average skills and someone with \"maximum\" skills. We calculated this difference 1000 times, and then to show the distribution of this difference we calculated its mean, its standard error, and a 90% confidence interval around the mean. Table 3 reports the results of this simulation for our two models. Increasing Occupational Wage from its mean to its maximum ($513 per week to $1138 per week), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.086 on average. This estimated change has a standard error of 0.031 and a 90% confidence interval of (-0.138, -0.036). The results for Education Years are similar. Increasing Education Years from its mean to its maximum (about 12.9 years to 17 years), holding fixed all other regressors at their means, reduces the probability of supporting immigration restrictions by 0.126 on average. This estimated change has a standard error of 0.029 and a 90% confidence interval of (-0.174, -0.081). Both cases give the same result: higher skills are strongly and significantly correlated with lower probabilities of supporting immigration restrictions. 15  Citrin, et al (1997) assume that Occupational Wage and Education Years do not measure labormarket skills. For example, Education Years might indicate tolerance or civic awareness. To test this possibility, we split our sample between those in the labor force and those not in the labor force and then reestimated Models 1 and 2 on each subsample. If Occupation Wage and Education Years measure labor-market skills, then the correlation between these regressors and Immigration Opinion should hold only among labor-force participants. If these regressors measure non-labormarket considerations, then their explanatory power should not vary across the two subsamples. Table 4 reports the results. For the labor-force subsample both Occupation Wage and Education Years are strongly significant --in fact, these coefficient estimates are larger than the full-sample estimates from Table 2. For the not-in-labor-force subsample the coefficient estimates are much smaller (in absolute value) and are not significant. We interpret these results as strong evidence that Occupation Wage and Education Years measure labor-market skills. 16   The result that skills correlate with immigration-policy preferences is inconsistent with an HO model in which immigration is completely absorbed by Rybczynski output-mix effects. It is consistent both with the factor-proportions-analysis model and with an HO model in which immigration affects both wages and output mix. By pooling all regions of the country in Tables 2   through 4, however, we have not yet tested the area-analysis model. To do this we modify our initial specifications by adding the regressor High Immigration MSA and its interaction with skills. If preferences are consistent with the area-analysis model, then less-skilled (more-skilled) workers in gateway communities should have stronger preferences for more-restrictionist (lessrestrictionist) immigration policies than less-skilled (more-skilled) workers in non-gateway communities. These preferences imply a positive coefficient on High Immigration MSA and a negative coefficient on its interaction with skills. 17 16 We defined the subset of labor-force participants as those individuals reporting they were either employed or \"temporarily\" unemployed but seeking work. This subsample is 64.9% of the total sample, close to the 1992 aggregate labor-force participation rate of 66.6%. The reported occupation for those not in the labor force is their most-recent job. Also, we obtained the same results qualitatively from an alternative specification of our skills test in which we pooled the full sample and interacted skills with a dichotomous variable for labor-force status participation. The split-sample test is more general in that it does not constrain the non-skill regressors to have the same coefficient for both labor-force groups. 17 The positive coefficient on High Immigration MSA would indicate that low-skilled people in high-immigration areas prefer more-restrictive policies relative to low-skilled people living elsewhere. Combined with this positive coefficient, Table 5 presents the results for this specification, where Model 3 uses Occupational Wage and Model 4 Education Years. The results for all the non-skill regressors are qualitatively the same as before. Our skill measures are still negatively correlated with preferences at at least the 95% level. But in neither case is High Immigration MSA significantly positive or its interaction with skills significantly negative. In fact, for Education Years we obtain the exact opposite coefficients on both regressors at about the 95% significance level. In unreported specifications we tested this specification using our other five definitions of High Immigration MSA and/or splitting the sample as in Table 4. In almost every case the interaction term's coefficient was positive but not significant; in no case did the interaction term ever have a significantly negative coefficient or High Immigration MSA a significantly positive one. Overall, people living in high-immigration areas do not have a stronger correlation between skills and immigration-policy preferences than people living elsewhere. If anything, the skills-preferences link may be attenuated in high-immigration areas. In any case, we conclude that this link is inconsistent with the area-analysis model. 18",
          "We checked the robustness of the empirical results by trying other measures of our important regressors. For skills we tried three dichotomous variables of educational attainment (high-school dropouts, high-school graduates, and some college--the omitted group being college and beyond) to look for any non-linearities in how skills affect preferences. 19 We discovered no clear nonlinearities: the relative coefficients on the dichotomous measures seemed consistent with an overall linear effect. For skills we also tried the respondents' reported 1991 annual income, and obtained the negative interaction term would indicate that high-skilled people in high-immigration areas prefer less-restrictive policies relative to high-skilled people living elsewhere. 18 Although the attenuation was only marginally significant in a few regressions, we explored further what might cause it. One possibility is that more-skilled people in gateway communities worry about higher tax liabilities caused by an immigration-induced rise in demand for public services. If this were true, the skills regressor would be conflating two separate effects: the wage effect and the tax effect. To test this hypothesis we added \"fiscal\" regressors (home ownership; annual family income; and individual responses to the question of whether immigrants \"cause higher taxes due to more demands for more public services\") to our specification to control for individual tax liability. If the tax hypothesis were true then the skills-preferences attenuation would disappear in specifications which include the fiscal regressors. This did not happen, however. An alternative explanation is that people in high-immigration communities worry less about wage effects than people elsewhere because they have more direct experience of the output-mix effects of the HO model. Unfortunately, we know of no good way to test this idea in our data. 19 Among those answering the Education Years question there were 466 high-school drop-outs, 812 high-school graduates, 572 people with some college, and 570 people with a college degree or higher. qualitatively similar results to those for Occupation Wage and Education Years. 20 In addition to the six measures of High Immigration Area discussed earlier, we also tried a dichotomous measure of residence in one of the \"big six\" immigrant states of California, Florida, Illinois, New Jersey, New York, and Texas. Borjas, et al (1997) report that in 1960 60% of all U.S. immigrants lived in these six states and that by 1990 that share had risen to 75%. Borjas, et al (1996) report that in 1992 60% of all U.S. legal immigrants came into California or New York alone; another 20% entered the other four gateway states. With this measure we again found no evidence of preferences consistent with the area-analysis model. We also checked the robustness of our results by including other regressors. One was union membership: union members preferred more-restrictionist immigration policy, an effect that was statistically significant in some specifications. Two other regressors were retrospective evaluations of the national economy and retrospective evaluations of personal finances. Both retrospective measures tended to have the expected sign --those with gloomier retrospections preferred morerestrictionist immigration policy --but were always insignificant. Finally, we included state unemployment rates, another geography-varying regressor in addition to High Immigration MSA, to control in the cross-section for any business-cycle effect on immigration-policy preferences. This regressor was always insignificant, however.",
          "In this paper we have provided new evidence on the determinants of individual immigrationpolicy preferences and on what these preferences imply about how economies absorb immigrants. In particular, we documented a robust link between labor-market skills and preferences: lessskilled (more-skilled) people prefer more-restrictionist (less-restrictionist) immigration policy. This link strongly supports the contention that people's position in the labor force influences their policy opinions. It is consistent both with the factor-proportions-analysis model and with a Heckscher-Ohlin multi-cone model. We found no evidence that this skills-preferences link is stronger in high-immigration labor markets--if anything, the link may be attenuated in these areas. This finding is inconsistent with the area-analysis model. These results are important for constructing empirically useful models of the political economy of immigration policymaking in receiving states. In particular, the link between skills and immigration-policy preferences suggests the potential for immigration politics to be connected to the mainstream redistributive politics over which political parties often contest elections. In addition, our findings shed further light both on how individuals form preferences over international economic policies and what these preferences imply for the domestic politics of countries with significant flows of goods, capital, and people across their borders. The skills cleavage over immigration policy reinforces our earlier finding of a strong relationship between individual skill levels and support for trade protection in the United States (Scheve and Slaughter, 1998). Taken together, these two studies suggest that skill levels play an important role in shaping political divisions in the electorate over international economic policies.  Notes: Skilled labor is subscripted \"s\" and unskilled labor \"u\". The RS schedule is relative supply and the RD schedule is relative demand. For the factor-proportions-analysis model this picture represents the single national labor market; for the area-analysis model this picture represents each separate local labor market.   Notes: Using the estimates from Model 1 and 2, we simulated the consequences of changing each skill measure from its mean to its maximum on the probability of supporting immigration restrictions. The mean effect is reported first, with the standard error of this estimate in parentheses followed by a 90% confidence interval. Notes: These results are estimates of ordered-probit coefficients based on the listwise-deletion data set. In Model 1 there are 1380 observations; in Model 2 1475 observations. In both models the dependent variable is individual opinions about whether U.S. policy should increase, decrease, or keep the same the annual number of legal immigrants. This variable is defined such that higher (lower) values indicate more-restrictive (lessrestrictive) policy preferences. The regressors Tau 1 through Tau 4 are the estimated cut points."
        ],
        "ground_truth_definitions": {
          "observational bias": {
            "definition": "Observed data differs systematically from the unobserved data",
            "context": "One is inefficiency caused by throwing away information relevant to the statistical inferences being made. Furthermore, inferences from listwise-deletion estimation can be biased if the observed data differs systematically from the unobserved data. In our case inefficiency was clearly a problem.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "2d1201d181da2166ccbf1e922634b8b74a58f141",
        "sections": [
          "The Power of Political Rhetoric RONALD R. KREBS University of Minnesota, Minneapolis, USA PATRICK THADDEUS JACKSON American University, Washington, DC, USA While scholars of International Relations and comparative politics have usually treated rhetoric as epiphenomenal, one strand of constructivism has recently returned rhetoric to the heart of political analysis, especially through the mechanism of persuasion. We too maintain that rhetoric is central to political processes and outcomes, but we argue that persuasion is theoretically and methodologically problematic. We aver that rhetoric's role may be more usefully conceptualized in the context of coercion, and we advance a stylized model that illustrates how rhetorical coercion operates, explains why it works, and identifies key scope conditions. We subsequently illustrate our model's relevance through a detailed examination of a 'hard' case. This article's agenda is twofold. First, it advises scholars in these fields to avoid focusing on unanswerable questions about actors' motives and to examine instead what actors say, in what contexts, and to what audiences. Second, it lays the groundwork for a 'coercive constructivism', complementing the liberal version so prevalent today. KEY WORDS ♦ coercion ♦ constructivism ♦ discourse ♦ Habermas ♦ Israel ♦ military service ♦ persuasion ♦ rhetoric Talk is often thought to belong to the realm of diplomacy, war to the realm of action. Yet, during the 2003 Iraq War, the Bush administration was nearly as preoccupied with how the combat was portrayed as with the combat itself. Its foreign policy team invariably spoke of coalition forces rather than American forces, the war to liberate Iraq rather than the invasion of Iraq, Saddam's death squads rather than Saddam's fedayeen. Rhetoric is central to politics, even when politics takes the form of war. Yet rhetoric is curiously not central to much scholarship in comparative politics and International Relations. Politics is typically marked by rhetorical competition, but our theoretical frameworks are generally hard-pressed to make sense of its dynamics and outcomes. Whereas the dominant materialist tradition treats rhetoric as epiphenomenal, we argue, following recent constructivist work, that the rhetorical interplay itself provides leverage in explaining outcomes. We are less comfortable, however, with the argument advanced by some constructivists that political actors deploy resonant rhetorical forms and thereby persuade their interlocutors of the correctness of their preferred course of action. Although persuasion undoubtedly does occur in the political arena, it is also rare. Moreover, such mechanisms rest on a strong specification of the subjective motivations of individuals and thus are methodologically intractable. Recent mainstream constructivist research has prompted a refreshing debate on and advanced our understanding of political deliberation and argumentation. By focusing on internalized norms as the driving forces of behavior and policy, however, such research has ultimately diverted attention from the dynamics of rhetoric. Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics. 1  Employing an expansive definition of rhetoric that includes all speech actswhether they are oral or written, whether they take place under conditions approximating Habermas' 'ideal speech situation' or not -we argue that rhetorical maneuver can prove critical to success in political contests even when one's opponents have not internalized the promoted values. While claimants may deploy arguments in the hope that they will eventually persuade, their more immediate task is, through skillful framing, to leave their opponents without access to the rhetorical materials needed to craft a socially sustainable rebuttal. Rhetorical coercion occurs when this strategy proves successful: when the claimant's opponents have been talked into a corner, compelled to endorse a stance they would otherwise reject. Most narrowly conceived, this article suggests and explores a language-focused mechanism of political influence complementing recent scholarship, but it also has a broader agenda. At the level of methodology, it calls on mainstream scholarship to avoid centering causal accounts on unanswerable questions about actors' true motives and to focus instead on what actors say, in what contexts, and to what audiences. At the level of substantive theory, it joins European Journal of International Relations 13(1) other recent work in returning rhetoric to the heart of political analysis. But rhetoric is often both the medium and the locus of political contest, and thus focusing on its role in the context of deliberation can take us only so far. Rhetoric matters, we argue, when human beings power as well as when they puzzle. This article represents a step away from constructivism with a liberal flavor, focused on the transformation of values, toward constructivism with coercive characteristics, focused on the exercise of power. This article proceeds in four main sections. First, we critically review existing approaches to rhetoric in empirical political science, examining in particular the limitations of (liberal or deliberative) constructivist approaches premised on persuasion. Second, we set forth our generalizable mechanism of rhetorical coercion and explicate both its logic and its limits. Third, we illustrate this mechanism's operation by exploring in detail a single 'hard' case: the relatively successful efforts of Druze Arabs in Israel to garner greater citizenship rights by framing them as the just deserts of their military sacrifice. This outcome is not entirely explicable without close attention to the dynamics of competing rhetorical claims, and our model of rhetorical coercion helps make sense of this puzzling and important case. Fourth, we briefly discuss the particular possibilities and limits of rhetorical coercion within international politics.",
          "Most scholars of international and comparative politics disparage rhetoric as epiphenomenal. The very phrase 'mere rhetoric' captures the view that what counts is not the language people use but the material power resources upon which they can draw. This perspective is shared by 'realist' (and especially rationalist) writers, and it accords with a well-established understanding of political power. Disenchanted with narrow materialist approaches, some have in recent years sought to harness the power of ideas. Many have invoked beliefs, ideas, and culture to supply inputs and to serve as coordination devices in games with multiple equilibria (Goldstein and Keohane, 1993;Laitin, 1998). Political psychologists have long argued that ideas influence how actors interpret evidence and sift through information (Jervis, 1976). Students of collective action have suggested that principled commitments can motivate participants and render them unusually insensitive to the costs of protest (Cohen, 1985). However, scholars with an ideational bent have typically relegated rhetoric to the margins. For 'idealists', words matter only insofar as they reflect actors' true beliefs, with private statements seen as more revealing than public pronouncements. Public rhetoric is of causal consequence only from the top down, as leaders of states and social movements deploy resonant language in an effort to mobilize support (Edelman, 1964;Snow et al., 1986;Snow and Krebs and Jackson: Twisting Tongues and Twisting Arms Benford, 1988). While devotees of 'interests' and 'ideas' vigorously debate the relative causal power of their favored variables, scholars in both traditions have little use for public rhetorical contestation. But to treat rhetoric as epiphenomenal is to render much of politics puzzling and to do violence to politics as lived by its participants. Rhetoric is certainly a weapon of the weak, but those holding the reins of power can and must deploy it as well. The acquisition and maintenance of rule ultimately hinge as much on legitimacy as on physical coercion, and such legitimacy can be established only through rhetorical action (Weber, 1968). While political contestants unquestionably further their agenda by exploiting material resources, they generally at the same time 'frame' their political activity, explaining the purposes to which their material power is put (Gamson, 1992). Such contests among state leaders and societal activists should be at the center of the study of politics. Rationalists and political psychologists might both protest that they have done precisely this. Rationalist scholars have pointed out that talk is not always cheap. State leaders who renege on their public rhetorical commitments may bear substantial domestic and international costs, giving them incentives both to tell the truth and to strategically manipulate audience costs (Fearon, 1994;Sartori, 2002). Though important, this insight flattens rhetoric into a purely informational tool, a way of efficiently revealing whether one is a high-cost or low-cost actor. As such, it cannot shed light on the framing competitions that often lie at the heart of politics. Political psychologists working within the area of political communication have demonstrated that speakers who succeed in defining the relevant considerations can shape mass opinion and ultimately policy outcomes. 2 But nearly all framing experiments have abstracted far from reality in exposing subjects to just a single issue frame (Druckman, 2001). Even the exceptions have failed to interrogate fully the dynamics of framing. Recent studies have concluded that the framing effect disappears when targets are exposed to competing frames from equally credible sources and that targets then respond in accord with supposedly invariant preferences (Druckman, 2004;Sniderman and Theriault, 2004). By this account, the intense framing contests endemic to politics are peripheral in that their only effect (given the caveat of equal credibility) is to render frame competition inconsequential, and baffling in that actors continue to expend copious resources on their rhetorical efforts. Moreover, this research program has ironically diverted attention from rhetorical interplay and instead directed it towards attributes of the target (e.g. political knowledge, awareness, expertise) and the source (e.g. credibility, trustworthiness, expert status). Studies of 'message effects' are comparatively rare. 3 Our debt to the psychological literature on framing is clear, but we wish to bring rhetoric itself back into the heart of the analysis.",
          "Most approaches fail to accord public rhetoric per se causal status, but one major exception has been the vibrant constructivist literature in international and comparative politics (Checkel, 1998;Finnemore and Sikkink, 2001). A central problem for constructivists has been explaining how and why new norms emerge and why actors might obey norms despite contrary material pressures. 4 Among the universe of mechanisms through which political actors develop 'shared understandings', and arguably occupying pride of place, is persuasion: 'normative claims ', Finnemore (1996: 141) asserts, 'become powerful and prevail by being persuasive' (see also Payne, 2001;Johnston, 2001). The targets of persuasive rhetorical moves do not grudgingly comply, but rather sincerely internalize new beliefs and consequently adopt new identities and preferences. Through persuasion, 'agent action becomes social structure, ideas become norms, and the subjective becomes the intersubjective' (Finnemore and Sikkink, 1998: 914). Through persuasive rhetoric, national and transnational social movements prompt not merely compliance with but consent to existing norms, and norm entrepreneurs need not resort to coercion to effect change. Persuasion -and public rhetoric as its medium -have emerged as the coins of the constructivist realm (see, among many others, Checkel, 2001;Crawford, 2002;Keck and Sikkink, 1998;Risse et al., 1999;Risse, 2000). Though sometimes only implicitly, these scholars have often drawn on Jürgen Habermas' logic of 'communicative action' to explain the power of rhetoric (Johnson, 1993;Lynch, 1999Lynch, , 2002;;Müller, 2001;Risse, 2000). 5  Habermas acknowledges that political actors often engage in narrowly goaldirected (teleological) action, but what makes his political vision distinctive is its affirmation of the possibility and prevalence of rational dialogue, of openminded deliberation in the quest for truth. Every legitimate use of language, Habermas argues, is premised on mutual understandings, on 'idealizations that transcend any particular language game', and these 'give rise to the perspective of an agreement that is open to criticism on the basis of validity claims' (Habermas, 1990: 199). For Habermas, politics (at least in its ideal form) is less about contest than consensus, less about powering than puzzling, and deliberative exchange consequently takes center stage (Habermas, 1984). Ideally, actors leave power and rank at the door, and they seek to persuade others and are themselves open to persuasion. Experimental studies of deliberation in fact conclude that the key to sustaining reflective discussion lies in the willingness of participants to keep an open mind (Barabas, 2004). We are very much in sympathy with this emphasis on public rhetorical interchange, but we are skeptical as to the utility of privileging persuasion as a causal mechanism. As Habermas himself recognizes, the unconstrained dialogue of 'communicative action' is not an empirically accurate portrait of politics, for power and rank are omnipresent in the political sphere. Actors do not leave their identities at the door when entering into dialogue, and they do not employ language unadulterated by earlier political contestation. Discourse is therefore always structured -and not by reason alone. Rules of exclusion and employment dictate what arguments can be proffered, under what conditions, and by whom (Foucault, 1972). Insofar as empirically oriented constructivists invoke 'communicative action' to explain real-world processes, it is fair to ask whether rational deliberation characterizes actual political debate. Only on the rare occasions when it does can Habermasian discourse ethics yield empirical insight into political contestation. It would, therefore, seem more helpful to adopt a theoretical framework that explicates the power of rhetoric even when politics is not truth-seeking and truth-generating, and a conception of rhetoric that includes all speech, no matter how interlaced with power relations. We are also skeptical that analysts can overcome the methodological hurdles standing in the way of demonstrating that persuasion has occurred. Such an explanation presumes that one can pin down the 'real motives' driving individual choice, so that one can distinguish beliefs sincerely embraced from those adopted for strategic or other reasons (Payne, 2001: 39-41). But conclusive proof of persuasion is elusive, for it requires unmediated access to people's minds (Wittgenstein, 1953: §150-5). Did Gorbachev accede to the reunification of Germany because he had been persuaded by the trump card of self-determination (Risse, 2000) or because he had calculated that the net costs of resisting the United States were prohibitive? Did Europeans abandon their overseas colonies because they had been persuaded that formal empire was illegitimate (Crawford, 2002) or because they had calculated that the net expected costs of retaining the colonies were excessive? It is impossible to say based on the available evidence, nor is it clear what evidence could even in principle clinch the case. 6 Relying on statements delivered in private settings does not escape the problem, since these articulations may be as strategic as their public counterparts (Scott, 1990). Many of these questions have received attention elsewhere. Scholars of transnational activism have drawn attention to the 'mobilization of shame', and they too have concluded that ways of framing claims cannot be fabricated out of whole cloth (Crawford, 2002;Price, 1998Price, , 2003;;Risse and Sikkink, 1999). Elements of William Riker's suggestive work on 'heresthetics ' (1996) are apropos, as is Frank Schimmelfenig's research on 'rhetorical action ' (2004). We seek to build on and systematize these important contributions, but we also depart from them. First, while many constructivists recognize that the targets of activist efforts may comply at first for strategic reasons, they claim that lasting normative change requires internalization. 7 This formulation is problematic because it relies on incompatible microfoundations in stage one (instrumental adaptation) and stage two (internalization). On methodological grounds, we prefer a mechanism of political influence highlighting strategic European Journal of International Relations 13(1) action throughout the entire episode. We agree with Weber that consistency, not completeness, should be the hallmark of sound work in the social sciences (Weber, 1949). Further, we are reluctant to characterize any normative change as 'lasting' in the sense of forestalling normative contestation over the long run. We proceed from the premise that norms are inherently subject to challenge and that the rhetorical arrangements sustaining norms are never fully stabilized and are at best relatively stable. Rhetorical contestation is thus always possible in theory, although it may not always be manifest in practice. Consequently, we focus on specific bounded episodes of contestation (McAdam et al., 2001: 29-30). Finally, existing accounts focusing on the strategic uses of public language -both constructivist (e.g. Risse, 2000) and rationalist (e.g. Riker, 1996)continue to be crafted around assertions about the 'interests' served by various rhetorical deployments and the 'motives' driving both speakers and targets. We seek to place the mechanism of rhetorical coercion on firmer methodological footing and, more broadly, to demonstrate the viability of a non-purposive social science. 8 Our skepticism regarding explanations rooted in the identification of motives is not novel. Hans Morgenthau, for one, declared the search for statesmen's motives 'futile' because 'motives are the most illusive of psychological data' (Morgenthau, 1993(Morgenthau, [1948]]: 5). Scholars in the rationalchoice tradition have offered similar observations (Frieden, 1999). Our 'solution', however, is more controversial. We are not prepared to follow our rational-choice colleagues down the path of assuming and deducing preferences. Rather, we take a less-traveled road and argue that social scientists should not begin by specifying the intentions of actors. Let us be clear: we are not arguing that actors do not possess motives, that those motives do not shape actors' behavior, or that such behavior is irrelevant to political outcomes. But we do aver that purposive accounts are analytically less useful than models in which preferences need not be specified or ranked. We therefore seek to minimize the place of motives as driving forces in our accounts of political processes and outcomes. We do not recommend this course lightly, for it flies in the face of deeply held epistemological commitments among social scientists in general and political scientists in particular. Purging empirical narratives of the language of subjective motivation is consequently difficult indeed. 9 Moreover, we recognize that all methodological choices are problematic, since the modeling of complex social processes requires abstracting from reality. Yet while this path is less traveled, it is by no means virgin ground. Many social scientists have distanced themselves from subjectivist understandings of culture as a coherent system of beliefs that might be ascertained through sophisticated survey techniques. We, like they, conceptualize culture as the always contested and often contradictory intersubjective semiotic practices through which social actors seek to generate meaning (Ross, 1997;Wedeen, 2002). Public semiotic codes constrain social actors regardless of whether they truly 'believe' in these codes' content, and thus the question of 'actual' belief is of little relevance (Weber, 1968;Jackson, 2002). Relational sociologists, most notably Charles Tilly, have maintained that individualist approaches commit the sin of reductionism: even if we did know people's motives, that would not help us explain social processes and outcomes. We follow these scholars in avoiding individual consciousness as a causal factor and in shifting 'the emphasis from consciousness to conversation, from action to interaction, from selves to sociabilities' (Tilly, 1998: 400; see also Emirbayer, 1997;and Shotter, 1993). These foundational theoretical moves serve as the point of departure for our model of rhetorical coercion. 10 We cannot observe directly what people think, but we can observe what they say and how they respond to claims and counter-claims. In our view, it does not matter whether actors believe what they say, whether they are motivated by crass material interests or sincere commitment. What is important is that they can be rhetorically maneuvered into a corner, trapped into publicly endorsing positions they may, or may not, find anathema. Rhetoric affects political outcomes even when all actors are cynical operators with little interest in genuine deliberation. The resolution of political issues through public debate need not imply any significant level of intersubjective consensus. Thinking about public rhetoric in this fashion avoids the crudeness of vulgar materialism, the reductionism of ideational approaches, and the heroic optimism of persuasion. Our perspective, which one might term 'coercive constructivism', both relates to and is different from existing approaches. Unlike both materialist and ideational accounts, we argue that language has a real causal impact on political outcomes. Unlike ideational approaches and liberal constructivism, we are skeptical that politics is more often characterized by puzzling than powering; we are equally skeptical that actors can transcend coercion and participate in fully rational deliberation. 11",
          "Rhetoric lies at the heart of politics. But many accounts of politics fail to accord it much weight, and many models fail to appreciate its explanatory leverage. This section seeks to explain how and why skillful rhetorical maneuvering can underpin a successful political campaign -not by persuading one's opponents of the rectitude of one's stance, but by denying them the rhetorical materials out of which to craft a socially sustainable rebuttal. Rhetorical coercion is a political strategy that seeks to twist arms by twisting tongues. This section proceeds by clarifying what rhetorical coercion is, how it operates, and the conditions under which it can be effective.",
          "We begin with a stylized account of an episode of rhetorical contestation. Seeking to effect change in some policy or ongoing course of action, a claimant (C) directs an argument toward the opposition (O) in view of a public (P). For the purposes of the model, it does not matter whether O has exclusive control over the actions at issue (if O is, for example, the government) or whether C and O share responsibility (if they are, for example, political parties in a legislative assembly). All that matters is that O's accession or resistance is critical to the outcome. Any argument that C puts forward contains two analytically separable parts: a frame (or set of terms) that characterizes the issue at hand and a set of implications that C suggests follows from that frame. For example, C might be a group of students protesting a war; their argument, advanced against the national government O through speeches and signs at a rally, portrays the war both to O and to P as unjust and unnecessary (frame) and calls for the end of hostilities and the withdrawal of troops (implications). In responding to C, O may accept or reject either or both the frame and the implications of C's argument. These alternatives yield four different potential outcomes of this episode of political contestation, reflected in Table 1. In case 1, O accepts both the frame and the implications of C's argument, and policy changes accordingly. 12 For C, this outcome represents an unmitigated triumph. Case 2 is more mixed. In case 2, O accepts the proposed implications and changes the policy, but it rejects C's reasoning. An example would be if an environmental movement (C) urged the government (O) to restrict logging for ecological reasons, and the government agreed to restrict logging but justified the policy shift in economic terms. Though the movement would have won the substantive battle, it might reasonably perceive its victory as less than complete; the government might in the future expand logging rights on economic grounds as easily as it had earlier restricted them on that very basis. It is possible that the environmentalists' rhetorical tactics drove the government's concessions, but determining that would require Krebs and Jackson: Twisting Tongues and  access to the authorities' true motives -something we earlier argued one can never ascertain. Both case 1 and case 2 mark (at least temporary) terminuses; contestation might resume at a later stage, but these cells represent relatively stable outcomes in the short to medium run. In contrast, cases 3 and 4 are cases of continued contestation. In case 3, O accepts the terms of C's arguments, but rejects the implications that C draws. For example, a lobbying group (C) may argue that high taxes are an unfair burden on the citizenry and should be reduced through an across-the-board reduction in tax rates; a political party (O) may agree that high taxes are problematic but suggest instead targeted tax cuts. As this relatively narrow policy debate occurs within a common issue frame, it might be termed an implication contest. In case 4, the two parties disagree about the very terms of debate as well as the policies that follow, and their rhetorical efforts consequently focus on advancing their preferred issue frame in the hope that their political opponents will accept it (along with the concomitant implications). Without a common frame bounding the debate, such rhetorical interchange -a framing contestis far more fluid, wide-ranging, and fundamental than in an implication contest. Politics is replete with such framing contests. Advocates of bilingual instruction invoke diversity and inter-cultural respect, while critics charge that instruction in anything other than the dominant language would threaten national ideals and students' eventual success in the marketplace. Opponents of abortion call the fetus an unborn child, portray the act as murder, and label themselves defenders of the right to life; supporters of abortion rights depict the fetus as an insentient being, portray the act as a medical procedure, and dub themselves the defenders of a woman's right to choose. Engaged in either an implication contest or a framing contest, C seeks through its claims and counter-claims to alter the rhetorical environment within which political battle is waged. While C might ideally prefer to persuade O, it must design a political strategy that can attain its desired ends even if persuasion proves impossible. The public plays a crucial role: both C and O must craft their appeals with an eye to some audience which sits in judgment of their rhetorical moves. If C can shift the rhetorical basis on which O justifies its stance to P, C can limit the range of O's possible responses and policy options, transcending the erstwhile implication or framing contest. Put simply, rhetorical coercion has taken place when this strategy proves successful -when O, regardless of its private beliefs, can no longer sustain its public opposition.",
          "Rhetorical contestation consists of parties attempting to maneuver each other onto more favorable rhetorical terrain and thereby to close off routes of European Journal of International Relations 13(1) acceptable rebuttal. Rhetorical coercion is successful when C's rhetorical moves deprive O of materials out of which to craft a reply that falls within the bounds of what P would accept. In the end, O finds itself, against its better judgment, endorsing (or at least acquiescing in) C's stance regardless of whether O has been persuaded or believes the words it utters. The alternatives -enduring punishment at the hands of P or investing significant resources in creating new terms of debate -would be prohibitively costly and time-consuming. In our model of rhetorical coercion, neither the motives nor the sincerity of the parties is particularly relevant. 13  C can succeed in rhetorically coercing O because of two key facts of social life. First, political actors can rarely take tangible steps or advance policy positions without justifying those stances and behaviors -in short, without framing. Politics may entail coercion or distribution, but at the same time it involves the struggle over meanings. Meanings, however, cannot be imposed unilaterally or through the exercise of material power alone. They are, by their very nature, intersubjective (Laffey and Weldes, 1997), and the effort to forge shared meaning implicates some audience in the process. The debate between C and O does not transpire in a vacuum: both are continually striving to legitimate their positions in P's eyes (Perelman, 1982). Second, speakers may not say just anything they would like in the public arena: rhetoric is not infinitely elastic but is structured. 14 Every community of discourse shares a number of topoi, or rhetorical commonplaces, that both enable and constrain speakers' rhetorical possibilities. These commonplaces are not 'fully predetermined, already decided distinctions', but weakly shared notions that can be 'expressed or formulated in different ways in different, concrete circumstances' (Shotter, 1993: 170-1). While C and O are free to weave together these commonplaces in creative ways, they are not free to deploy utterly alien formulations in the course of contestation: such arguments would fall, almost literally, on deaf ears. The available space for rhetorical contestation is, therefore, locally bounded, and the parties to a contentious episode cannot introduce entirely novel arguments. They must configure their appeals utilizing rhetorical tools drawn from a chest that is, in the short term, effectively fixed (Swidler, 1986). Rhetorical innovation, while possible and even inevitable in the long run, is far less likely in the short run for three reasons. First, while structures of discourse are never fully hegemonic and are continually being reworked, coherent political action would be impossible if rhetorical universes were in a state of continuous deep flux. Relative rhetorical stabilities must emerge to permit the construction of political strategies, and thus, at any given time, 'the terrain of dominant discourse is the only plausible arena of struggle' (Scott, 1990: 102). Second, imagining, formulating, and disseminating a brand-new commonplace requires an unusual level of commitment on the part of a claimant in terms of material resources, time, and effort. Deploying existing commonplaces -which have already been imagined, formulated, and disseminated -is far less costly (McAdam et al., 2001: 47-50). The process of transforming an unusual rhetorical form into a commonplace is necessarily lengthy and uncertain, but political actors normally possess far shorter time horizons. Arguments can prove powerful only when the commonplaces on which they draw are already present in the rhetorical field, which is shaped both by the unintended consequences of prior episodes of contestation and/or by campaigns undertaken in advance with the express purpose of reconfiguring the rhetorical terrain. Third, and related, novel rhetorical resources are likely to be drowned out by existing arguments and frames. Dense networks of communication carry and regularly reinforce established commonplaces, overwhelming proffered alternatives. Rhetorical innovations consequently tend to arise at the margins, where communication networks are less dense. Overcoming such barriers is possible in principle, but it is in reality so improbable that C and O must, in a given contentious episode, play within the rhetorical arena that presents itself to them. And that arena privileges particular frames and places others beyond the pale, regardless of the material resources at the speaker's disposal. If C and O were the only two actors in our stylized story, they might introduce novel arguments costlessly and endlessly, and rhetorical coercion would then be impossible. Their rhetorical maneuvers are constrained, however, because rhetorical contestation is public in nature (Bennett, 1980: 805-6). The continual competition for public legitimation ensures that P occupies a critical position, setting limits to the policy stances that the contestants can in practice advance. The relevant audience sets the contours of rhetorical contestation, and stepping beyond or reshaping them requires an investment not attractive or even feasible in the short run. This focus on the public's role constitutes a point of intersection with scholars in the rational-choice tradition who have ascribed great importance to 'audience costs' (Fearon, 1994). In such models, the actors and the audience have clear and consistent preferences that precede the contentious episode; public pronouncements serve to signal the content and/or intensity of those preferences. By contrast, our model does not begin by identifying and ordering the parties' desires, and the outcome of a particular episode does not depend on these preferences' content or ranking. Instead, the importance of the public P lies in the imperative for appeal frames to draw on rhetorical commonplaces present in the public's everyday deliberations (Shotter, 1993: 65-9; see also Bennett, 1980). Our model presumes not that European Journal of International Relations 13(1) P has a well-thought-out position in advance on the issue being contested, but rather that there is only a limited set of arguments that P would, in principle, find minimally acceptable. 15  In sum, one argument 'wins' not because its grounds are 'valid' in the sense of satisfying the demands of universal reason or because it accords with the audience's prior normative commitments or material interests, but because its grounds are socially sustainable -because the audience deems certain rhetorical deployments acceptable and others impermissible. One cannot, however, fashion strict covering laws regarding the audience's response. Students of conversation and argumentation from across several disciplines (anthropology, communication, sociology, philosophy, psychology) have argued that context is unusually crucial to their field of study (Billig, 1996;Gumperz, 1982;Perelman, 1982;Sawyer, 2001;Shotter, 1993;Toulmin, 1958). Classifying structures of discourse, reconstructing the rhetorical possibilities, exploring the rhetorical interplay: these critical challenges require one to formulate arguments limited in time and space and limited by the arena of rhetorical competition. Thus, for example, we do not ask below what sorts of appeals are most effective in general at compelling authorities to grant oppressed minorities first-class citizenship. Rather, through an interrogation of the Israeli case, we suggest that a specific claim (framed around military sacrifice and the equation of rights and obligations) is more likely to be successful in a particular discursive context (when talk about citizenship is narrowly republican). Such an explanation is modest, yet powerful, and potentially generalizable.",
          "The logic of rhetorical coercion suggests that two factors are most important in explaining when the model will have explanatory value. First, is P a relevant party to C's claims-making? Recall that C's power is rooted in its credible threat, implicit or explicit, to bring P in on its side; C's ability to do so deprives O of the option of refusing to respond to or even acknowledge C's claims. If C cannot issue its threat credibly, then O can safely ignore C's claims, no matter how they are framed. We can readily imagine conditions under which P would not be relevant to the interaction between the protagonists in our stylized story. It may be that P simply cannot hear C -perhaps because C is being actively repressed; perhaps because C lacks sufficient resources to publicize its message; or perhaps because C's claims regard alleged misdeeds that transpire far from the public eye. 16 Moreover, in some social contexts, decisions need not be justified. 17 The prevalence of justificatory behavior in the political arena suggests, however, that we are correct to maintain that the scope of the model's application is quite large.",
          "Second, how limited is the universe of rhetorical commonplaces? We have argued that actors must, in any particular claims-making episode, forge their appeals by drawing upon existing commonplaces that represent the boundaries of legitimate framing. When these boundaries are loose, numerous rhetorical strands are available, ensuring that O can almost always put forward a meaningful rebuttal to C. In such circumstances, the terms of debate are hardly constraining. When these boundaries are relatively restricted, rhetorical coercion is more easily achieved, as O has less rhetorical 'wiggle room' -that is, fewer socially plausible ways of rebutting C's claim. We recognize that verbal appeals are multivocal, that advocates and opponents of particular policies may advance their arguments employing similar terms. But public language, in our view, is never infinitely flexible, and even so capacious a rhetorical formulation as that of 'rights' in the American context disciplines the speaker. Such disciplining is more common and thus rhetorical coercion is more likely when speakers have fewer commonplaces on which to draw. 18",
          "One of the most venerable norms in the West is that linking military service and citizenship. 'Whoever puts himself in the service of the state', observed Otto Hintze, 'must logically and fairly be granted the regular rights of citizenship' (Hintze, 1975: 211), and the call to the colors is widely considered the supreme example of state service (Janowitz, 1976). Thus the leaders of those relegated to second-class citizenship have often counseled in wartime against draft evasion and have even urged voluntary enlistment. Afterwards they have shrewdly contrasted their people's record of loyalty and sacrifice to the reality of entrenched political and social inequity. For instance, African-Americans volunteered in droves for the Union Army in the US Civil War and for the American Expeditionary Force in World War I, wrapping their postwar demands in the bloody flag. The rhetoric of military sacrifice has at times proven effective. Mobilized veterans have most obviously exploited their status to gain both political rights and distributional benefits. Oppressed and marginalized ethnic, racial, and religious groups have, based upon their collective sacrifice, also advanced claims for citizenship rights, albeit with varying degrees of success. African-Americans, for example, have repeatedly found this path to citizenship blocked. But their lack of postwar progress is hardly surprising given the high stakes of citizenship battles. The demands of subordinate groups for first-class citizenship challenge the core of the existing social order, and the opposition to meaningful change is consequently often fierce. It is not African-Americans' repeated failures that require explanation as much as the successes of others -such as American women after World War I, Japanese-Americans European Journal of International Relations 13(1) after World War II, and Sikhs in post-independence India -in deploying the rhetoric of sacrifice to win improvements in their status. The opponents of reform typically occupy powerful positions in political, social, and economic structures, and few would expect them to give way before words alone. The politics of citizenship is thus a 'hard case' for the power of rhetoric, for material interests and resources would appear to provide a ready explanation for which groups' citizenship claims are granted and which are denied. We will argue, however, that the particular rhetorical configurations advanced by Israel's various Arab minorities help explain these groups' differential rates of success in overcoming the barriers to first-class citizenship in this 'ethnic democracy' (Smooha, 1997). Drawing on their record of military service, Druze Arabs argued that equal obligations demand equal rights. Jewish politicians found themselves unable to craft a culturally meaningful rebuttal to Druze demands, and they were compelled to concede Druze claims. In contrast, Christian and Muslim Arabs, who have never been drafted into the Israel Defense Forces (IDF), insisted that Israel abide by its public commitment to liberal democracy. While this frame also drew on Israeli commonplaces, it permitted Jewish politicians to justify discrimination against Christians and Muslims by arguing that those who do not perform civic duties cannot lay claim to equivalent public rights and benefits. Although the Druze were poorer than their fellow Arabs and smaller in number, for many years they were more effective in placing their concerns on the national agenda and in garnering a favorable response from the authorities. Consequently, material factors cannot account for the success of the Druze relative to other Arabs, and Jewish politicians' continual resistance to Druze demands suggests that persuasion was not at work. Israel's relationship with its Arab minorities nicely illustrates the power and limits of rhetorical coercion. 19   Background Israel's Arab population -today well over a million strongis divided by religion, with the Muslims, the Christians, and the Druze constituting the three largest groups. Possessing basic political rights, the entire Arab community has enjoyed great socioeconomic progress since Israel's founding in 1948, particularly relative to Palestinians in the West Bank and the Gaza Strip and to Arabs throughout the Middle East. Yet they have also been openly labeled a fifth column, and they have, often without regard to religious affiliation, been the victims of variegated forms of discrimination on the part of the state and the majority Jewish population. The new Israeli government subjected Arabs to military rule, conducted pervasive surveillance within the Arab communities, limited Arabs' capacity for travel and an independent economy, expropriated the bulk of Arab-controlled land, excluded Arabs from the giant labor Krebs and Jackson: Twisting Tongues and Twisting Arms federation known as the Histadrut, and so on. Jewish leaders typically invoked the language of security to justify these policies, but its broad definition encompassed ideological, economic, and even partisan interests as well as security more narrowly conceived. Much overt discrimination came to an end by the late 1960s, but substantial discrimination persisted in more subtle forms (Kretzmer, 1988;Lustick, 1980;Shafir and Peled, 2002). Starting from this common baseline, however, the paths of Israel's Arab minorities have diverged. While all continue today to endure second-class status relative to Jewish Israelis, the Druze have made greater headway than the larger and wealthier Christian and Muslim communities. Since 1956 the Druze have been able to deploy a set of rhetorical commonplaces unavailable to their Christian and Muslim neighbors. That year, male Druze became subject to the military draft, reversing a policy of exemption that had been applied to all Arabs since the passage of Israel's universal conscription law in 1950 (Peled, 1998). Some have speculated plausibly that the Israeli authorities hoped that Druze service in the IDF would provide proof of Israeli liberalism and thereby neutralize Arab nationalists and satisfy Western critics (Firro, 1999;Lustick, 1980), but even full declassification of the relevant documents would not definitively establish the government's motives. We argue that, regardless of who wanted to draft the Druze and why, conscription helped the Druze prosper in Israeli politics.",
          "In the mid-1960s, the Druze began to mobilize, and they regularly sought a public forum in which to air their grievances, which they consistently framed around their people's military service and the equation of rights and obligations. In 1967 Druze activists circulated widely an appeal insisting that 'since we have done our duties … and see in the lack of equality … an infringement and denial of our rights, we urgently demand correction of this wrong' (Firro, 1999: 187). Surveying the history of Druze claims-making, one scholar noted that, among both Jews and the Druze, 'injustices to individual Druzes are usually attacked as ingratitude toward a man who was willing to shed his blood for his country, but now his country turns against him' (Ben Dor, 1979: 134-5). Even in recent years, the Druze have continued to gravitate to this rhetorical mode. During a series of public protests in the 1990s, mayors of Druze villages and towns 'usually denounce [ed] the Israeli government for its broken promises, and stress[ed] the price paid by the community in the form of hundreds of Druze soldiers killed during their service in the Israeli army' (Yiftachel and Segal, 1998: 487). Even the minority of Druze who have objected to the draft have accepted these basic terms. As a Communist Druze Knesset representative argued in the early 1990s, 'If this is a blood covenant, then give us the feeling that we are living in our state, in our homeland, with full rights like all citizens . . . And if there is to be no equality European Journal of International Relations 13(1) of rights, then do not demand equality of obligations, and allow Druze to choose -to serve or not to serve in the IDF' (Zohar, 1991). In constructing their claims frame, the Druze have drawn on rhetorical commonplaces deeply rooted in Israel's rhetorical traditions. Although participation in the pre-state Jewish community, the yishuv, was necessarily voluntary, it prized individual and group commitment to the public good, defined as the historical mission of the Jewish people to rebuild the land of Israel and epitomized by the kibbutz movement. The new state preserved this emphasis on civic commitment as the basis for membership, grafting a powerful statist ideology onto the earlier discourse (Liebman and Don Yihya, 1983). In an early Knesset debate, Prime Minister David Ben Gurion insisted that women too be subject to the draft, bluntly declaring, 'There are no rights without obligations. And above all is the obligation of security, the obligation of defending our existence, our freedom, our independence, and our growth' (Divre Ha-Knesset, 1950: 3:537). Even dissidents have protested within the terms of this dominant definition of citizenship: as one conscientious objector from the Lebanon War put it, 'military service, paying taxes, and obeying the law . . . That is what makes you a citizen and makes you eligible to enjoy the defense and the fruits that [the state] equally distributes' (Helman, 2000: 324). 20  This civic republican tradition has long sat uncomfortably alongside an ascriptive component in the definition of the Israeli polity. The 1948 Declaration of Independence proclaimed the new country 'the Jewish state' even as it promised that Israel would promote the welfare and protect the basic freedoms of all its inhabitants. The country's formal name, the State of Israel, suggests an abiding commitment to pluralistic secularism, but its historical-religious title, the Land of Israel, remains in popular use (Kimmerling, 1985). Some have concluded that 'Jewish ethnicity is a necessary condition for membership in the political community, while the contribution to the process of Jewish national redemption is a measure of one's civic virtue' (Peled, 1992: 435). Yet this ignores the potential contradictions between these discourses. Such tensions are muted when only Jewish citizens are drafted, for then Israeli leaders can publicly embrace civic republicanism while preserving ethnoreligious priorities. But Druze claims-making framed around collective military sacrifice challenged the coherence of Israel's 'ethnorepublican' citizenship. Confronted with such a challenge, Jewish Israeli leaders had two alternatives: either they could acknowledge their exclusive definition of the political community, or they could open its doors. While the Druze squeezed concessions out of Jewish politicians beginning in the late 1960s, there is no evidence that their appeals were persuasive: both the limited and incremental nature of Israeli concessions as well as the grudging fashion in which they were offered are not consistent with a narrative centered Krebs and Jackson: Twisting Tongues and Twisting Arms around persuasion. Rather, our model of rhetorical coercion can help explain why the Druze made headway in this ethnic democracy. Faced with Druze claims, Jewish leaders could have launched a framing contest by (a) admitting openly the discriminatory nature of the state. Alternatively, they could have conceded the Druze frame but challenged its implications by (b) claiming that the Druze were not relatively disadvantaged or (c) arguing that the Druze community's problems were of its own making. The first alternative frame was unappealing -not just because of the likely international repercussions, but also because it would have contradicted both Israel's self-proclaimed status as the sole democracy in the Middle East and its dominant way of speaking about citizenship. The second put forward a claim widely known to be false, and the third possible response, while perhaps plausible with regard to questions of economic development, could not justify administrative segregation, which was, whether cleverly or fortunately, among Druze activists' first targets. There was, however, a fourth option: Jewish politicians could have refused to reply -as long as relevant audiences were unaware of the Druze's plight. The Druze's capacity to engage in rhetorical coercion was consequently dependent on garnering attention from the Hebrew-language media. As a general rule, the Hebrew press rarely covered the Arab communities, for its readership was almost entirely Jewish. But the Druze arguments were resonant with republican rhetorical practice, producing a narrative that Jewish publics comprehended and embraced. As a result, when the Druze made noise, the press amplified it, compensating for the Druze's lack of political clout and ensuring a hearing in the halls of power (Asa'ad, 2000;Toledano, 2000). In short, the Druze trapped Jewish leaders in a rhetorical cul-de-sac in which an implication or framing contest was unsustainable. They maneuvered their Jewish opponents onto a rhetorical playing field on which the Druze could not lose, for no rebuttal would have been acceptable to key audiences, both domestic and international. Wary of calling punishment upon themselves, Jewish leaders had little choice but to concede. Consequently, 'when the Druze demanded something, the ears were much more open' (Toledano, 2000). Despite the Druze community's electoral insignificance, cases of alleged discrimination against individual Druze received attention even in the Knesset (Ben Dor, 1979: 134-6). The Druze demand in the late 1960s for full membership in the Labor Party was irrefutable: as a prominent Labor functionary wrote in a major newspaper, 'How can we explain to that Druze … that he is good enough to endanger his life for the state but that he is not good enough to be a member of the Labor Party?' (Lin, 1970). Confident that their rhetorical resources have bequeathed disproportionate influence, the Druze have resisted an alliance with their numerous fellow Arabs, with whom they share many grievances. Even those Druze otherwise critical of Israel's policies have often supported continued conscription so that European Journal of International Relations 13(1) the military sacrifice frame would remain available. Lacking abundant political resources, the Druze adopted a strategy aimed at rhetorical coercion, and they demonstrated that it could work. Jewish politicians were compelled to say things they (likely) did not believe and ultimately even to follow through on those promises. Like their Druze counterparts, Christian and Muslim Arabs drew on Israeli commonplaces, arguing that Israel needed to eliminate discriminatory measures to live up to its democratic promise. Yet, in comparison to the Druze, they have encountered far greater public resistance, for Jewish politicians had a rhetorical escape hatch. Arab rights, they often argued, were not sacrosanct, for those who did not fulfill their civic obligations (that is, military service) deserved less protection than those who had made the supreme sacrifice. In seeking to convince a wavering Knesset member to support the military administration in 1962, Ben Gurion protested, 'I have recognized all these years that rights are dependent on obligations . . . I do not understand why I may limit the freedom and steal the time of young Jewish men and women when they are in the army, but why it is forbidden to limit much less the freedom of those who do not serve in the army.' 21 That same Labor functionary who advocated offering the Druze membership in the party justified the exclusion of other Arabs on similar grounds: 'I asked them [close Arab friends] how they could sit in the party central committee … next to a bereaved father or mother, next to a party member who had just returned from the front, next to a member who stands ready to leave the next day for reserve service, while they still do not fulfill this obligation' (Lin, 1970). Faced with claims framed in liberal terms, Jewish leaders had a response at the ready, evading the rhetorical dead-end into which Christians and Muslims might have hoped to maneuver them. For decades, this rhetorical move has frustrated Arab efforts to attain first-class citizenship. As one Arab activist recently complained, 'They are trying to link our rights as citizens to performing military service. That's not how a real democracy works' (quoted in Schechter, 2003). Christian and Muslim claimants have long remained mired in a framing contest, unable to persuade or rhetorically coerce Jewish politicians. 22   Alternative Explanations How else might one explain the relative success of the Druze in attaining increments of Israeli effective citizenship? The most intuitive answer would attribute this outcome to the distribution of material resources. But the Druze's potential voter base has been tiny, their per capita income has remained relatively low, they have historically eschewed violent confrontation with the authorities, and they have often had difficulty creating and sustaining effective lobbying organizations (Firro, 1999). By all standard measures of political power, the Druze should have been less successful than other Arabs, not more so.",
          "A second alternative would invert the first. Precisely because the Druze were so small, so weak, and so moderate, Israel could countenance their inclusion in the IDF and grant them some modicum of effective citizenship without threatening the state's Jewish identity. Had the Druze's population been as large as Israel's Christian or Muslim communities, concessions would have been unthinkable. The Druze's small size made them an attractive target for cooptation (Lustick, 1980). But if this argument were correct, presumably the state should have flung its doors wide open, for the costs would have been negligible and the propaganda advantages substantial. In fact, however, while Jewish decision-makers readily offered rhetorical concessions to the Druze, they resisted implementation: 'To say that if the ears were open, things were done -there is a difference' (Toledano, 2000). The Druze made tangible progress, but they fought hard for those increments of meaningful citizenship. Nor did the Druze act like a model coopted minority. They were not content with petty favors, but rather became deeply dissatisfied with their status and continually presented Jewish leaders with difficult and uncomfortable choices. A third alternative would posit that Israel's conquest of the West Bank and Gaza Strip in 1967 and the addition of a large, politicized Palestinian population gave Jewish politicians incentives to grant Druze citizens' demands so as to prevent the Palestinization of the Druze community. Presumably, however, they faced far greater incentives to accommodate the much larger Christian and Muslim citizen populations, whose members had not yet openly allied with the Palestinian national cause. Nevertheless, the Israeli establishment remained highly resistant to Christian and Muslim claims-making. This argument thus has difficulty accounting for the divergent patterns in Israel's relations with its various Arab communities after 1967 -a difficulty not faced by the mechanism of rhetorical coercion. A fourth alternative account would attribute the Druze anomaly to their predilection for taqiyya, or the art of concealment. 23 Long viewed as heretics by their Muslim neighbors, the Druze have historically sought to shield themselves from persecution by hiding their religious observances. With regard to politics, this ultra-pragmatic doctrine prescribes neutrality when possible and 'bandwagoning' -allying with the likely victor -when necessary (Layish, 1985). One would thus have expected the Druze to avoid challenging stronger parties, but in fact the Druze displayed a rebellious streak during Ottoman and European colonial domination of the region. Despite a long history of communal tension, they made common cause with their fellow Arabs in the struggle for Lebanese autonomy early in the 20th century. And, frustrated that one foreign ruler had simply replaced another, their leadership initiated a nationalist revolt against the French in the mid-1920s -with disastrous consequences (Tauber, 1993a(Tauber, , 1993b;;Firro, 1999). The Druze were not proto-nationalists, but they were at times willing to set aside intra-Arab differences, compelling one to ask why they European Journal of International Relations 13(1) did not cast their lot with other Arabs in Israel in the decades after 1948. Taqiyya is obviously of little help in answering this question. Equally important, it lacks analytical bite. The doctrine is so flexible that it is less an explanation for Druze behavior than a tidy description of it.",
          "The Druze's success in compelling Jewish Israeli politicians to grant them a modicum of first-class citizenship illustrates nicely the operation of rhetorical coercion. Although this single case study cannot of course prove that the mechanism is generalizable, we view rhetorical coercion as among the set of 'widely operating explanatory mechanisms that combine differently and therefore produce different outcomes in one setting or another ' (McAdam et al., 2001: 13-14). The Druze case suggests the applicability of rhetorical coercion to domestic politics, but is the mechanism operative in the international arena? Extending the earlier theoretical discussion leads to the conclusion that rhetorical coercion is more likely as social ties in political communities, domestic and even international, grow increasingly extensive and dense. On the whole, however, rhetorical coercion will operate less effectively and less frequently in international settings. Key factors highlighted by the model are often present in international relations: state leaders speak to international audiences, they are sensitive to those audiences' reactions, and justificatory speech acts are prevalent. Indeed, while not systematically elaborating the mechanism's dynamics, others have shown rhetorical coercion to be of critical explanatory importance in such divergent contexts as Cold War crisis bargaining (Bially Mattern, 2004), intra-Arab alliance dynamics (Barnett, 1998), eastward enlargement of the European Union and NATO (Fierke and Wiener, 1999;Schimmelfenig, 2004), and Soviet premier Mikhail Gorbachev's proposals for nuclear disarmament (Evangelista, 2001). Yet even these examples suggest the limits of rhetorical coercion in international politics. Rhetorical coercion, like other forms of political communication, is premised on a political community that shares at least some understandings of the boundaries of acceptable discourse. The more tightly linked the community, the greater the possibilities for rhetorical coercion. Thus it is not surprising to find this mechanism primarily operative among regional groupings with relatively strong requirements for legitimation. The Soviet Union had long legitimated its leadership of the communist bloc by casting itself as the true force for global peace, and its routine calls for nuclear disarmament sustained that claim; Soviet generals could not publicly dispute Gorbachev's proposal without simultaneously undercutting Soviet authority among communist regimes. Similarly, Egypt had, under Nasser, positioned itself as the leader of the Arab world Krebs and Jackson: Twisting Tongues and Twisting Arms based on a pan-Arab discourse that represented Israel as its inherently opposed Other; Nasser reluctantly accepted the risk of war with Israel in 1967 because deviating from the norms of Arab nationalist discourse would have imperiled Egypt's status as the spokesman for Arab nationalism and might have jeopardized his regime's stability. Finally, the European Union, the most tightly bound regional arrangement on the globe today, could not have denied membership to consolidated democracies in Eastern Europe without contradicting the community's legitimating discourse which had intertwined the European concept with liberal democracy; to have vetoed or even vocally opposed eastward enlargement would have challenged the basis for community and threatened individual members' credibility. Such dynamics thus depend on the strength and density of the ties binding the community's members, and such ties vary substantially across the international system. 24 Where such ties are strong and dense -that is, within such regional groupings of states, where international anarchy (as even realists might concede) is most attenuated -rhetorical coercion remains distinctly possible. The cosmopolitan dream of a 'global public' remains today just that, although there are emerging global standards of legitimacy in several arenas and increasingly robust mechanisms through which norm violators might be held accountable (Grant and Keohane, 2005). Where such ties are weak and sparse -as in relations across such regional groupings and as in the many arenas lacking shared bases for legitimation -rhetorical coercion will be far more difficult. As the metaphor of the two-level game implies, political elites in such circumstances are likely to be engaged with and responsive to multiple and different audiences simultaneously, and these audiences are unlikely to share a single conception of what rhetorical moves are acceptable and appropriate. The more complex the social environment, the greater the opportunities for actors to evade the consequences of rhetorical interchange, and thus the more imposing the obstacles to rhetorical coercion. Where such ties have never existed or have been ruptured -perhaps as a consequence of the limits of communications technologies or perhaps as a result of enduring interstate rivalry -communication (including rhetorical coercion) is impossible, and diplomacy is a dialogue of the deaf. As a consequence, it is not the imposing capacity of foreign publics, let alone some elusive global public, to sanction state action that makes rhetorical coercion potentially successful on the international scene. When rhetorical coercion is operative internationally, the critical punishing agent is more often a domestic audience prepared to exact costs for violations of standards of rhetorical appropriateness. While foreigners have disapproved of American unilateralism in recent years, it is not clear that this opposition has driven their governments' policies (Brooks and Wohlforth, 2005), and while anti-Americanism has unquestionably been prevalent, its political consequences European Journal of International Relations 13(1) appear to have often been exaggerated (Keohane and Katzenstein, 2006). The George W. Bush administration could dismiss foreign opposition to the Iraq War, particularly because cooperation on matters of joint concern such as trade and terrorism (narrowly construed) continued seemingly unabated. But the American public's increasing disenchantment with the war has been a far greater cause for concern for the administration, as it is this public that might be directly brought into a political contest over the war -a political contest within which rhetorical coercion is far more likely to operate. 25",
          "Machiavelli famously advised that 'a ruler need not have all the positive qualities …but he must seem to have them… He must seem, to those who listen to him and watch him, entirely pious, truthful, reliable, sympathetic, and religious' (Machiavelli, 1994: 55). Realists, not inaccurately, read Machiavelli as warning against allowing moral considerations to impede actions necessary for the pursuit of political power. But, as coercive constructivists, we would add that these lines imply that social identity has an effect on social and political outcomes, regardless of whether or not the actor internalizes the components constituting this identity. A ruler pursuing policies incapable of public justification would find her path strewn with numerous practical obstacles. This lack of rhetorical resources might even compel her to alter her course of action. In accord with other constructivists, we aver that social norms and social identities matter. 26 In accord with political realists, we argue that whether or not a social actor has internalized a particular set of normative principles is not causally relevant. In accord with the broader cultural turn in the social sciences, we combine these positions by focusing on public language, on how normative principles are deployed in public debate, and by exploring the causal impact this deployment can have on what people say and consequently on what they do -not on the impossibly elusive question of what they believe. Our model of rhetorical coercion can helpfully capture the dynamics of framing contests, when there is controversy about how an issue should be characterized, and of implication contests, when narrower (yet no less important) battles are waged over the consequences of some accepted framing. Our analysis draws attention to two key conditions affecting the applicability of the model. First, rhetorical coercion can occur only when the public is a party to the debate or when the claimant can credibly threaten to involve the audience. Second, not all structures of discourse are equally conducive to rhetorical coercion. This mechanism is more likely to operate when these structures are relatively restricted (and constraining) than when they are relatively loose (and permissive). Furthermore, our model cannot illuminate the process of rhetorical contest in two circumstances: first, when an opponent accepts a claim without argument, and, second, when an opponent accepts the implications of an argument but not its terms. We contend, however, that much social and political life lies within our model's scope. 27  In short, this article seeks to invigorate the study of political rhetoricbut in a fashion relatively unfamiliar to recent debates in international and comparative politics. We argue that rhetorical contestation shapes policy outcomes and that the approaches most common in these subfields miss a significant portion of political and social reality. We further suggest that it is possible to make causal claims without trespassing into the murky waters of subjective motivation and without relying on problematic mechanisms like persuasion. Many IR constructivists would be comfortable with the first of these claims, many realists and students of political culture with the second, few from either camp with their combination. This article has sought to demonstrate that their conjunction is both logically sustainable and potentially productive.",
          "material interests' problematic and have instead embraced a linguistic focus that is not dissimilar to our own. 5. Among those drawing on social psychology are Checkel (2001); Crawford (2002: 26-7);and Johnston (2001). Others have remained agnostic as to how persuasion operates (Finnemore and Sikkink, 1998). We limit ourselves here to discussion of the Habermasian version of the persuasion argument, since in our judgment it is the most clearly articulated mechanism. For an alternative treatment of Habermas in international relations, see Mitzen (2005). 6. On the lack of evidence of changed minds in Crawford (2002), see Dessler and Owen (2005: 602). 7. We are grateful to Kathryn Sikkink for helpful discussion on this point. 8. For treatments closer to our own, see, among others, Barnett (1998); Bially Mattern (2004); Fierke (1998); Goddard (2006). 9. Such language even slips on occasion into our own case study, for the conventions of historical writing insist on subjects who possess agency, articulate goals, and pursue motives. The Foucauldian alternative -interrogating the central discursive formations -might help us escape this trap, but it would come at the cost of the narrative form, which better captures the tensions and surprising reversals of actual political contest. Such traces of motivational language are consequently inevitable, but they should not cloud or draw attention from the rhetorical dynamics themselves. We are grateful to an anonymous reviewer for pressing us on this point. 10. The distinction between persuasion and coercion as modes of political influence is by no means original, but our conceptualization of rhetorical contestation as potentially coercive is more unusual. Scholars, especially those working within a psychological framework, sometimes treat persuasion as synonymous with all attitude change, citing even nuclear deterrence as an example. In contrast, we define coercion as the imposition of constraint on choice, whereas normative persuasion seeks ideally to bypass choice. Both coercion and persuasion may lead the target to assent, but the coerced target merely complies, while the persuaded target consents. Put differently, persuasion speaks to change in preferences, while coercion seeks to alter the social environment confronting an actor while leaving preferences untouched. 11. For a related discussion of 'realist constructivism', see Jackson and Nexon (2004). 12. Note that 'acceptance' here does not imply agreement, but refers only to O's public reaction to C's argument. 13. While we make no strong assumptions regarding the content of the actors' motives, the model does rely on the thin, almost trivial, assumption that actors are fundamentally strategic -that they pursue policies that they believe will further their goals (McAdam et al., 2001). 14. For related arguments, see Cruz (2000); Spillman (1995); Steinberg (1999). 15. In this stylized model, we presume that both actors must legitimate their stance in the eyes of the same audience, but the reality of both international and domestic politics is often more complicated. Speakers in both arenas may be playing to entirely different audiences or may be competing to define which audience is in fact relevant. The dynamics of such contestation are not well addressed by this paper and will be taken up in future articles. We are grateful to anonymous reviewers for raising this question. 16. This is not infrequently the case when O is a government whose commands are implemented by a secretive bureaucracy. A potential weakness of rhetorical coercion is its implication that rhetorical concessions translate smoothly into policy change. O may renege on its commitments, particularly during implementation when there may be little need for public justification. While such breakdowns are likely, we maintain that violations will prove difficult to sustain should C hold O's feet to the public fire in a new episode of rhetorical contestation. 17. Superiors often do not explain their logic to subordinates, and exasperated parents often tell children, 'Just do as I say!' In such circumstances, rhetorical contestation is not likely to be effective. We are grateful to Robert Jervis for pointing this out. 18. For more on the logic of rhetorical coercion, see Jackson (2006); Krebs (2006). 19. For a more extensive treatment of these cases, see Krebs (2006: Chs 3-5). 20. Military service occupies so prominent a site in the Israeli cultural complex that 'civic virtue has been constructed in terms of and identified with military virtue' (Helman, 2000: 320). 21. Ben Gurion to M.K. Yizhar, 12 February 1962, State Archives (Jerusalem, Israel), Office of the Prime Minister Files, C 6304/1086. 22. Christians and Muslims could not control the rhetorical playing field, but they nevertheless made significant gains in the mid-1980s thanks to their raw political power. After the 1977 breakdown of Labor Zionism's stranglehold over the political system, the electoral competition tightened, and the Arab population was actively wooed. Candidates from across the political spectrum stumped for the first time in Arab villages, and the major parties committed themselves to improving the Arabs' lot. 23. A different version of the cultural argument, which Druze and Jews alike have relished, argues that the two peoples are natural allies as fellow minorities in the Muslim Middle East. But the assertion of a natural Jewish-Druze affinity is propagandistic rhetoric that has, since the founding of Israel, served the interests of both Israeli officials and Druze leaders. Druze religious texts are typically dismissive of other religions, and while particularly disparaging of Islam -the dominant regional religious tradition responsible for their oppression -they are hardly admiring of Jews. See Parsons (2001: 70-6); and Dana (2003: 43-51). 24. As this formulation suggests, we believe that network analysis provides a more useful set of tools for grasping such variation than do metaphors regarding, for example, the 'thickness' of the 'normative environment' (Barnett, 1998: 39). For an insightful application of network logic to imperial dynamics, see Nexon and Wright (forthcoming). 25. Consider also the examples discussed earlier. That audiences in the communist bloc outside the USSR might have questioned Soviet leadership or that Arab elites might have challenged Egypt's did matter, but primarily indirectly. Had the Soviet generals or Nasser anticipated that domestic audiences would respond to the prospective loss of regional or ideological authority with collective indifference, they would have perceived themselves to be operating with a far freer hand. That domestic audiences were sensitive to their country's standing in the relevant community of nations, and were believed to be prepared to act on that basis, was crucial in these cases. 26. The language of identity has not figured explicitly in this article, but it is our view that rhetoric and identity are inseparable. Identity is not the mental property of an individual, nor can collective identity be apprehended by aggregating the attitudes of the constituent individuals. Rather, identities are necessarily relational, and thus they are forged through the process of their articulation. Political communities are defined by a common rhetorical lexicon and coalesce around particular rhetorical configurations. 27. Our model cannot, however, capture the long-term processes through which new commonplaces enter the rhetorical space."
        ],
        "ground_truth_definitions": {
          "rhetorical coercion": {
            "definition": "an alternative means of conceptualizing the place of rhetoric in politics",
            "context": "Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "https://aclanthology.org/2024.lrec-main.952",
        "sections": [
          "Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets (Krieger et al., 2022), which exhibit limited generalizability across diverse domains (Wessel et al., 2023). This paper builds upon the work of Wessel et al. (2023), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) (Aribandi et al., 2021a;Chen et al., 2021;Kirstein et al., 2022) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method (Spinde et al., 2022) underperforms due to its narrow task focus and does not surpass baseline outcomes. In this study, we make five main contributions: ). This enables it to outperform models based on single-task learning (STL) paradigms. 1. We present MAGPIE -the first large-scale multi-task pre-training approach for media bias detection. By pre-training on diverse bias types such as persuasive and subjective, a classifier based on MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1). 2. We introduce LBM (Large Bias Mixture), a pretraining composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias. 3. We provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks. 4. We demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset (Spinde et al., 2021c) and achieves competetive results on the Media Bias Identification Benchmark (MBIB) collection (Wessel et al., 2023). 5. We make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub: github.com/magpie-multi-task These contributions highlight the potential of MTL in improving media bias detection. Our findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results. Another key insight of our research is the value of MTL in contexts where the primary dataset is smallfoot_0 . By learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data. Overall, our research offers a multitask learning approach to media bias detection with first large-scale resources in the domain.",
          "",
          "Media bias is a complex issue (Lee et al., 2021a;Recasens et al., 2013;Raza et al., 2022) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias (Wessel et al., 2023). In their literature review, Spinde et al. (2023) provide an extensive overview of research on media bias and related subtypes of bias. Media bias detection approaches have evolved from hand-crafted features (Recasens et al., 2013;Hube and Fetahu, 2018;Spinde et al., 2021d) to neural models (Spinde et al., 2022;Chen et al., 2021;Spinde et al., 2021c;Huguet Cabot et al., 2021;Sinha and Dasgupta, 2021;Raza et al., 2022). However, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets (Wessel et al., 2023). As most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019)). We will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain. Media bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications (Recasens et al., 2013;Spinde et al., 2021b), which, so far, are not explored in relation to each other (Spinde et al., 2023). Wessel et al. (2023) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias. Their study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities. MAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks to mitigate a variety of limitations in MBIB (see Section 3.1).",
          "MTL shows significant improvements in various NLP tasks, including sentiment analysis (He et al., 2019), text summarization (Kirstein et al., 2022), and natural language understanding (Raffel et al., 2020). In MTL, a model leverages knowledge gained from one task to improve the performance of others. Aribandi et al. (2021b) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021) show that prefinetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold. As described above, media bias can be seen as a composite problem composed of various interrelated bias types (Wessel et al., 2023). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b). For instance, benchmarks such as GLUE and Super-GLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection (Wahle et al., 2023(Wahle et al., , 2022)), and semantic evaluation (Ruas et al., 2019(Ruas et al., , 2020)), thereby substantially improving performance across a range of NLU tasks (Wang et al., 2018(Wang et al., , 2019)). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues. The selection of tasks is pivotal to the efficacy of MTL. There have been several attempts to automate task selection, including learning the data selection with Bayesian optimization (Bingel and Søgaard, 2017) or estimating task relations (Ruder and Plank, 2017). The most model-agnostic approach is GradTS (Ma et al., 2021), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE. GradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task's attention. The selected tasks are trained jointly and share representations across tasks.",
          "We implement MAGPIE using pre-finetuning as introduced in (Aghajanyan et al., 2021) (See also Section 2). As such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), a large-scale task collection of bias-related datasets. We incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1). As outlined in Figure 2, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset 2 (Spinde et al., 2021c), and 2 BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model's generalizability within a single dataset the Media Bias Identification Benchmark (MBIB) collection.",
          "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias more indirectly, such as sentiment analysis or emotion classification. MAG-PIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3. First, we manually assess a list of 115 media bias-related datasets in English language, categorized into task families by Wessel et al. (2023). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks in Section 4.3, such as Aribandi et al. (2021b) proposed for general NLP tasks. We filter the collection of the 115 datasets based on the following criteriafoot_2 : • Accessibility: Datasets have to be publicly accessible. • Text granularity: We only use datasets labeled on a sentence level or its fragments (tokens)(not on, for instance, article level) • Quality of annotations: We exclude datasets with no documentation, low annotation agreement or employment of machine annotation. • Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levelsfoot_3 . We remove 5 datasets due to or unreliable source of annotations and discard 4 duplicates. Applying these criteria leaves context. originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. In order to standardize examples from various domains, we apply a unified text-cleaning procedure to each dataset. The process involves: (1) discarding sentences with fewer than 20 characters, (2) removing URL links, special characters, and whitespace, and (3) eliminating duplicates potentially created with steps (1) and (2). The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks and task families can be found in Table 4.",
          "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa (Liu et al., 2019) encoder due to its proven state-of-the-art performances across various media bias applications (Spinde et al., 2021c;Krieger et al., 2022).",
          "Pre-finetuning. To effectively harness the generalization abilities of MTL for media bias detec-tion, we adopt a pre-finetuning procedure (Aghajanyan et al., 2021). Pre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training. While incorporating MTL directly into the pre-training stage has demonstrated performance gains (Aribandi et al., 2021b), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL (Aghajanyan et al., 2021). Sharing representations. We use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads. For each task, we attach dense layers, or \"heads\", to the shared encoder. These heads are optimized individually per task while the shared encoder learns general bias representations. However, multi-task optimization presents challenges due to differing gradient directions and magnitudes (Yu et al., 2020). For instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum. On the other hand, if Task A's gradient greatly surpasses that of Task B, gradient A becomes dominant. We counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling (Yu et al., 2020). Conflicting gradients and loss scaling. In multi-task training involving n tasks, encoder parameters receive n potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad (Yu et al., 2020), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks. Therefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of n sets per update. Adopting Muppet's method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps (Aghajanyan et al., 2021).",
          "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL (Aribandi et al., 2021b;Spinde et al., 2022). We also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster. However, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM. When task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B. We aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches: 1. Head-Specific-Early-Stopping (HSES) 2. Resurrection HSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers' continuous learning, especially after they have reached an optimal state. Resurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again. This enables the task to adapt its head parameters to the latest latent representation. HSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4. However, we stress the need for further extensive analysis in Section 5.",
          "Multi-Task Learning often involves selecting wellused datasets, leading to potential selection biases. Furthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes. To automate the process of task selection, we utilize the GradTS algorithm (Ma et al., 2021). We choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability. Gradient-based Task selection. In line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrixfoot_4 for each task. Tasks are sorted by correlation between each task's matrix and BABE task's matrix. We pre-finetune m -1 models on the first k tasks from the sorted list, where k varies from 1 to m -1 and m is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal k determined by evaluation loss. For further details on the GradTS algorithm, please see Ma et al. (2021).",
          "We split the BABE dataset into train, dev and test split with 70%,15%,15% portions respectively. For both hyperparameter tuning and auxiliary task selection, we evaluate on dev split and only use test split for the final evaluation (Section 4.2). As fine-tuning transformers on small datasets often leads to inconsistent results, such as high performance variance (Dodge et al., 2020), we use a fixed random seed 321 for all runs in auxilliary task selection. For the final evaluation on the test set (Table 2) we evaluate the models using 30 random seeds and report the average performance, to minimize the influence of random weight initializations. We use values 0..29. For optimizing the models, we use a per-task batch size of 32, an AdamW optimizer, and a polynomial learning scheduler. We run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory.",
          "In this section, we present the results of our MTL approach. First, we report the set of auxiliary tasks selected by GradTS for pre-finetuning. Next, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks. We also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark. Then, we analyze the LBM taxonomy through a study on knowledge transfer between families. Lastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study.",
          "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices. We use Kendall's correlation coefficient, as suggested by Puth et al. (2015). We find a local minimum for the BABE evaluation loss when pre-finetuning on the first k = 10 most correlated tasks. The final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1. The tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection. Task Type τ (correlation) Persuasive techniques 0.73 Lexical/Informational bias 0.72 Rumour detection 0.69 Sentiment analysis 0.68 Global warming stance detection 0.68 Subjective bias 0.67 Veracity classification 0.64 Gender bias 0.64 Fake news detection 0.63 Table 1: Attention importance correlation w.r.t. the media bias task.",
          "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline. The multi-task sets are the following: • MTL:Random -Random Subset of 10 tasks • MTL:GradTS -Subset of 10 tasks selected by GradTS algorithm • MTL:All -Set of all tasks We also evaluate the model pre-finetuned on the set of all tasks on MBIB. We follow the guidelines set by Wessel et al. (2023) for the evaluation. Given that MAGPIE's pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases. Multi-Task Learning Performance. Table 2 summarizes our performance results on the BABE dataset. We observe that all of our MTL prefinetuning schemas lead to performance improvements. In particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c). While both MTL baselines -Muppet (Aghajanyan et al., 2021) and UnifiedM2 (Lee et al., 2021b) outperform singletask baseline, they underperform all of our MTL models. On MBIB benchmark, MAGPIE ranks first on 5 out of 8 tasks. However, the improvements are only marginal. The results can be found in Appendix in Table 5.  Step efficiency. In addition to the performance improvements achieved, we also assess our model training efficiency. In Figure 5, we show the F1 score on the development set for the BABE task, averaged over all 30 runs. Our findings show that Multi-Task Learning only requires 15% of the training steps used in single-task finetuning on BABE.",
          "F1 Acc loss Baseline (RoBERTa base) 80.83 (±0.69) 81.19 (±0.69) 43.6 (±3.54) DA-RoBERTa 77.83 (±1.4) 78.56 (±1.3) 47.84 (±2.97) MUPPET 80.56 (±1.3) 81.18 (±1.16) 44.19 (±4.65) UnifiedM2 81.91 (±0.91) 82.41 (±0.88) 44.86 (±3.99) MTL:Random 81.88 (±1.02) 82.28 (±0.97) 40.35 (±1.73) MTL:GradTS 82.32 (±0.79) 82.64 (±0.8) 40.96 (±2.36) MTL:All 84.1 (±1.33) 84.44 (±1.25) 39.46 (±2.41) Table 2: Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds. This result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future.",
          "In Section 3.1, following Aribandi et al. (2021b), we introduce data task families. Aribandi et al. (2021b) uses task families for selection and knowledge transfer. To assess task families' significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer. To account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families. We train tasks from the same family together and report the average change in task performance, as depicted in Figure 6. Negative knowledge transfer is prevalent across most of our task families. However, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance. Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Section 4.3. Our results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families. Conversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%. On the other hand, the Emotionality family is significantly impaired by negative transfer from other task families. The full table of transfers can be found in Figure 7. Considering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5. Task Family Transfer from Transfer to media bias -2.07% -0.94% subjective bias -1.26% 0.89% hate speech -0.87% 0.17% gender bias -1.01% -1.07% sentiment analysis 0.11% 0.72% fake news -0.13% 1.79% group bias -1.04% 0.09% emotionality 0.34% -6.56% stance detection -0.79% -1.83% Table 3: Evaluation of averaged transfer between task families.",
          "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation. We calculate the average evalua- tion loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection. The boxplot in Figure 8 shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%. However, we hypothesize that a random constellation of tasks 6 can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.",
          "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multitask learning (MTL) approach that enhances traditional models with a multi-task structure. Additionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pretraining of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias. Our study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative trans- 6 Particularly variance in task sizes and quality. fer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far. Furthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches. These findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias. While results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum. Understanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies. As media bias is emerging globally, incorporating multilingual models is a natural extension.",
          "We acknowledge the necessity for a more comprehensive analysis of the performance of the HSES and Resurrection methods to ensure robust evaluation. Given the significant computational resources required for a single multi-task training of all tasks, we allocated these resources towards robustly evaluating the model performance rather than conducting an in-depth analysis of the optimization techniques. Consequently, the presented methods may be less reliable and have limited applicability. While there are various models available, previous research suggests that RoBERTa exhibits strong performance on media bias tasks (Wessel et al., 2023;Spinde et al., 2021c). However, due to resource constraints, we were unable to explore models with different architectures and further refine our selection. As the landscape of publicly available highquality datasets for media bias is not as extensive as desired, we acknowledge our inability to capture all manifestations of media bias comprehensively. As mentioned in Section 5, a systematic and comprehensive analysis of the dataset landscape will be part of our future work. Furthermore, although analyzing media bias on a sentence level enables a detailed examination of occurring biases, certain forms of bias, such as bias resulting from the omission of information, extend beyond the linguistic aspects of individual statements. Detecting such biases may require considering an external context. We leave it to future work to investigate other levels of granularity, such as paragraph or article levels. In addition to these technical limitations, conducting a detailed analysis of agreement and label distribution in all utilized datasets will be necessary to make stronger claims about which datasets provide more reliable coverage of the overall concept of media bias. This is particularly important since media bias is a complex phenomenon that is not always easily identified during the annotationgathering process.",
          "Detecting (and as a result also highlighting) media bias instances can mitigate the negative effects of media bias, e.g., on collective decisionmaking (Baumer et al., 2015). However, media bias strongly depends on the context and individuals involved, making it a sensitive issue. Some bias forms depend on factors other than the content, e.g., a different text perception due to a reader's background. Therefore, a public classification of possible polarization and one-sidedness in the news must be performed transparently and reliably. The datasets used for training the classifier must be transparent and, ideally, constantly monitored by independent experts. To address possible implications of MAGPIE or related media bias applications cautiously, it is crucial to constantly control the classifications and datasets with recent findings on the perception of media bias, which is a main part of our ongoing and future work. To do so, we use resources such as, e.g., standardized questionnaires on media bias perception (Spinde et al., 2021a). When bias detection is balanced and transparent, it can positively affect collective opinion formation and decision-making. We see no immediate negative ethical or societal impacts of our work. However, in addition to system transparency, we want to highlight that we believe it to be required for future datasets to report in greater detail about a manipulation protection strategy when developing, training, and presenting any media bias classifier. To ensure the validity of media bias detection systems, it is essential to prevent participants, especially in public studies, from manipulating algorithms by, for example, flagging neutral content as biased. Therefore, annotations should be compared among multiple users to ensure a higher level of trustworthiness. Most of the datasets available report only limitedly about such strategies. In open or crowdsourcing scenarios, it is important to collect user characteristics and deliberately include specific content that aims to provide obvious answers but may be answered differently when users follow certain patterns. This approach helps in detecting and mitigating potential biases introduced by participants, thereby maintaining the integrity and reliability of the media bias detection process. To ensure and propose stronger standards in the future, we aim to analyze all LBM datasets with regard to potential inherent bias in future work."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "a skewed portrayal of information favoring certain group interests, which manifests in multiple facets, including political, gender, racial, and linguistic biases.",
            "context": "1. Introduction Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "24589ef86242ff182e7bae495b94e7c11931f20c",
        "sections": [
          "Whitehouse: Listening to Birds in the Anthropocene / 55 The post-war Great Acceleration had its symbol in the loss of something meaningful and beautiful, but also common enough to have once been taken for granted. Listening to birds, Carson implied, was no longer something conceivable as straightforward or inevitable. Carson exemplified this shift by quoting an Alabama resident writing about the effects of a massive spraying programme to control fire ants: Our place has been a veritable bird sanctuary for over half a century. Last July we all remarked, \"There are more birds than ever.\" Then, suddenly, in the second week of August, they all disappeared … There was not a sound of the song of a bird. It was eerie, terrifying. What was man doing to our perfect and beautiful world? Finally, five months later a blue jay appeared and a wren. 8 Life in such a perfect world was thus accompanied by the presence of birds, a presence made most readily and delightfully manifest in their songs. When once familiar companions no longer accompanied local residents, the shock was unexpected and unnerving. The Anthropocene is still a new and largely academic term but one that relates to concerns that have been more widely felt, particularly since they were so clearly articulated by Carson. First, there is the concern that everyday experiences of birds and other wildlife are under threat from everyday human activities. Second, there is a belief that local changes and local activities can have global causes and effects. Finally, there is the concern that while things might seem okay in the present, nothing can be taken for granted about the future-not even the near future and what now seems commonplace. The Anthropocene is thus both a term that relates to real and observable changes in the local worlds people perceive around them and to semiotic elaborations on those perceptions that draw together local and global, human and non-human, present and future, into anxiety-laden narratives. There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"9 Anxious semiotics is central to listening to birds in the Anthropocene not only directly in the face of loss but in the tensions of daily experiences that might seem, on the face of it, to be positive. Fluctuations and changes in what can be heard lead back to possible human causes, perhaps even our own activities. Anxious semiotics can even emerge in response to an increase in some species, perhaps spreading in the wake of anthropogenic changes, or to birds singing earlier in the season, perhaps because of climate change. Anxiety points towards potential, as well as actual, loss. It also points towards concerns about human culpability for change, both locally and more generally. Conservationists monitor the fortunes of birds and their habitats assiduously, enjoying the support of much of the public as they do so. But growing desires to attend to and to care for birds and their ecology seem to be continually outstripped by our capacity to disrupt and endanger. The environmentalism spawned by Silent Spring has constantly drawn attention to the ways that ecosystems, both local and global, are affected by human activity. What prospers and what disappears are causally bound together with human actions in ways that are sometimes readily apparent and sometimes barely perceptible. I explore the associations that people perceive as emerging between bird sounds and environmental changes. Following the semiotic approach of Kohn,10 I argue that the symbolic and moral connotations of listening to birds in the Anthropocene follow from their iconic and indexical grounding in places, producing an anxious semiotics in which even positive associations can have portentous or uncertain implications. My argument progresses from recent claims made by Bernie Krause about the evolution of soundscapes and their disruption by humans to a series of narratives contributed to the Listening to Birds project: an anthropological study of people's relations to birds through sound. Finally, I invite the reader to listen to four recordings and consider the sounds they hear as a means of sensing life in the Anthropocene.",
          "Harmony and balance was a state that Carson considered to be under threat in the modern world. She began Silent Spring with the nostalgic statement, \"There was once a town in the heart of America where all life seemed to live in harmony with its surroundings.\"11 This harmony was not just something that could be seen or measured but that could also be heard in the varied mix of sounds in the environment. This point finds elaboration in the work of musician and scientist Bernie Krause, most notably in his book The Great Animal Orchestra. 12 Krause's argument is that places have evolved their own acoustic ecology and that in a natural environment the sounds will tend to fit together rather than disrupting or competing with one another. He begins The Great Animal Orchestra by speculatively describing the soundscape of the American plains, 16,000 years ago and prior to the arrival of humans in North America. An array of now extinct mammals, along with more familiar birds, insects and amphibians, fill the air with their sounds. But despite this complex mix of sounds, each animal is able to make itself heard. Inspired by Murray Schafer's soundscape approach, Krause argues that this is because each species has a sound that occupies a distinct bandwidth and so can exist in harmony with the other sounds that tend to occur around it, be they from other species or from physical factors such as running water. \"This is,\" he writes, \"the tuning of the great animal orchestra.\" 13 This tuning can be rendered visible through the use of spectrograms, which translate sound into a graph plotting time against frequency. As Mundy has pointed out, spectrograms have played a significant role in creating an \"image of evolution,\" but she argues that, more typically, this takes the form of a stable, visible object that is readily comparable and can thus be more easily incorporated into larger taxonomies.14 In Krause's argument, each spectrogram is an image of evolution because it is assumed that the sound of each species is indexically related to the other sounds in the same soundscape, i.e. there is a causal relation between the sounds in an environment and the \"bandwidth\" occupied by the sounds made by each species. This set of inter-relating sounds occupying a distinct sonic niche has emerged, according to Krause, through evolutionary processes. The nesting of these sounds is rendered as a visible object in the spectrogram. Krause divides up sound into three categories. First is the geophony, sounds made by the physical environment such as waves, wind or rain. Second is the biophony, which consists of sounds made by animals, plants and other organisms. Finally is the anthrophony or humangenerated sound. While this classification is ostensibly based on the origins of the sounds rather than what they actually sound like, Krause emphasises the evolutionary emergence of the different types of sound, arguing that: The sounds of geophony were the first sounds on Earth-and this element of the soundscape is the context in which animal voices, and even important aspects of human sonic culture, emerged. 15 So here we have a sense of the sounds of the world developing in relation through the geological eras and into the present day. The different types of sound that Krause outlines might themselves be regarded as sonic epochs in which a particular category of sound emerged or tended to predominate. As such, Krause is implying that we have now entered the epoch of anthrophony and that human sounds are drowning out the biophony and geophony in many parts of the world. The Anthropocene has also ushered in a new kind of anthrophony, with the sounds of industry, machinery, combustion engines and electronic amplification being a rather different and more disruptive type of human-induced sounds than those that would have predominated in the pre-industrial era. As such, I would argue that one can differentiate between pre-and post-Anthropocene anthrophony, the former tending to integrate more closely with other sounds and the latter often disrupting or dominating them. Indeed, Krause's main concern is not with the effects of anthrophony in general but the effects of what he calls electromechanical sounds. Problematic sounds don't so much originate in human bodies themselves but in Anthropocene technologies used by humans. Humans, Krause argues, are not just having an effect on the biophony by drowning out the sounds of other species with their own but also by disrupting ecosystems. Here his concerns are clearly aligned with those of Carson. He is troubled by the loss of sounds but also by the disruption to the harmonious sonic ecosystem. It is not simply a spring of silence that is worrying but one of disharmony. Humans have effects on other sounds not just through their own sounds but through their effects on habitats. These changes reduce the quantity of sound but also cause disruption to the relative harmony of the soundscape. When a biome is compromised, spectrograms will lose both density and diversity, along with the clear bandwidth discrimination among voices that is otherwise visible in nonstressed-habitat graphic displays. Biophonies from stressed, endangered, or altered biomes tend to show little organisational structure. 16 This emphasis on considering sounds within the context of the environments in which they emerge is a point sometimes forgotten in the analysis of bird sounds by scientists and other scholars, who take recorded sounds as discrete, decontextualized objects. Biophonic sounds are thus indexical of the conditions within which organisms live and have evolved. Krause also defines noise in relational terms: I think of noise as an acoustic event that clashes with expectation-loud heavy metal music in an intimate restaurant is noise … A straight-piping motorcycle gunning its way through the delicate landscape of Yosemite Valley shatters the numinous experience for both visitors and animals.17  Noise is thus a conflict that emerges from perceived disjunctions of one set of sounds and another, or between one aspect of the world as experienced and another. It is dissonant rather than resonant, and Krause's argument is that dissonance and noise have increased greatly to the detriment of the biophony and to human well-being. As sounds are the products of activities, this also points to increasing dissonance between human activities more generally and the activities of other species in our environment. These conclusions lead Krause to lament that, \"In order to hear the wild biophonic world, we need to get to places free from human noise.\" 18 The world as it should sound, for Krause, is not just pre-Anthropocene, like the rural idyll Carson draws on, but pre-human. It is a world not of companions but of strangers. How then should one listen to birds in the Anthropocene-in a world in which humans are not just present but dominant and noisy and in which our activities are often dissonant with those of other species? What signs are people listening for and how are they listening? The Anxious Semiotics of Humans and Other Species Krause's work provides support for what many have long realised: that the sounds of birds and other animals are importantly sounds in place. Sound-making happens in relation to other sounds and to the acoustic conditions. For birds, sound-making is also place-making; it is an act of territorialising space, of making relations with other birds and continually re-weaving the context of their lives. In order to do this, as Krause argues, birds make sound in relation to other sounds and both their lives and their evolution enable them to be responsive to signs within their environment. This emphasis on sound and communication as emerging relationally and across species boundaries can be aligned with the recent proliferation of \"more-than-human\" approaches in the social sciences. 19 These new ways of thinking about life have created fresh impetus in the environmental humanities and social sciences, but few authors in this field have explicitly addressed how to analyse the sorts of meanings and communication that emerge in more-than-human relations. A recent exception has been Eduardo Kohn, who has argued that the grounding for how humans and non-humans perceive and understand their world is similar across species. 20 Drawing on the varied works of Peirce, Bateson and von Uexküll, Kohn argues that the semiosis of life is grounded in iconic and indexical signs. The tendency of anthropology and other social sciences has been to emphasise symbolic signs, particularly language, as the primary means of communication. The apparent absence of symbolic communication in non-humans has fuelled an emphasis on human communicative exceptionalism that appears to forge a gulf in human and non-human semiotics. Kohn argues that symbols should instead be seen as nested within a semiotic field of icons and indexes from which their meaning emerges. In this respect, the ways that humans find meaning in the world is not so different to the ways that other organisms do: they are all grounded in the iconic and indexical. 21 Birds perceive the representations of other organisms, including those of their own kind, and their sound-making emerges in association while, for humans, the symbolic only \"works\" because it is emergent from the same sorts of associations. As Kohn puts it, \"The semiosis of life is iconic and indexical. Symbolic reference ... is an emergent dynamic that is nested within this broader semiosis of life from which it stems and on which it depends.\" 22 What follows from this grounded and emergent concept of the symbolic is morality. \"The moral is also distinctively human, because to think morally and to act ethically requires symbolic reference. It requires the ability to momentarily distance ourselves from the world and our actions in it to reflect on our possible modes of future conduct … This distancing is achieved through symbolic reference.\" 23 While I am more hesitant than Kohn to exclude ethical consciousness from non-humans, I agree that both the symbolic and the ethical emerge from the iconic and indexical semiosis of being-in-the-world 24 and that the symbolic also provides the potential for a conceptual, and usually partial, distancing from this immersion. Being-in-the-world is, for humans at least, double-edged because it entails a constant oscillation along a spectrum of mindfulness 25 that runs between engagement and detachment (see also the work on scientists and naturalists by Candea, MacDonald, and Ellis), 26 between Dwelling and Skill (London: Routledge, 2000); S. Eben Kirksey and Stefan Helmreich, \"The Emergence of Multispecies Ethnography,\" Cultural Anthropology 25, no. 4 (2010): 545-576. 20  Kohn,How Forests Think. 21 Ibid.,[38][39] Ibid., 55. 23 Ibid., 133. 24 By \"being-in-the-world\" I wish to convey a relational sense of life that is derived from thinking of the organism-in-its-environment as indivisible and emergent. \"Being\" cannot be separated from \"the world.\" the real and the symbolic, between is and ought. Focusing on this oscillation is, I argue, essential to understanding how people listen to birds in the Anthropocene. In order to clarify my argument, I first need to describe the semiotics of being-in-theworld that I have in mind. Developing Kohn's argument, I bring his semiotic approach into conversation with Tim Ingold's claim that life is best realised as a meshwork. By doing so, I aim to add some flesh to Kohn's semiotics and give a sense of how meanings emerge from the entanglements of the meshwork. Ingold introduces his understanding of meshwork thus: Every… trail [of movement or growth] discloses a relation. But the relation is not between one thing and another-between the organism 'here' and the environment \"there.\" It is rather a trail along which life is lived. Neither beginning here and ending there, nor vice versa, the trail winds through or amidst like the root of a plant or a stream between its banks. Each such trail is but one strand in a tissue of trails that together comprise the texture of the lifeworld. This texture is what I mean when I speak of organisms being constituted within a relational field. It is a field not of interconnected points but of interwoven lines; not a network but a meshwork. 27 As such, any organism is a tangle of interconnecting life lines and thus is emergent from these relations and associations. \"Organisms and persons … are not so much nodes in a network as knots in a tissue of knots, whose constituent strands, as they become tied up with other strands, in other knots, comprise the meshwork.\" 28 For Ingold then, life is characterised by movement and constant birth; it is a process out of which forms emerge but are never fixed. A question that arises from such a view of life is how, in such a fluid and entangled world, can any phenomenon be knowable enough to be dealt with or represented? How can living beings make a good enough \"guess\" at what is emerging to keep on living in a form that is iconic of what it has been before? It is in answer to these questions that Kohn's employment of Peirce's semiotics can be helpful. According to Kohn, Peirce \"strove to situate his entire philosophical project … within a special kind of realism that could encompass actual existence within a broader framework that would account for its relationship to spontaneity, growth, and the life of signs in human and nonhuman worlds.\" 29 This \"broader framework\" might be better thought of as a meshwork-a way of thinking about the world as emergent, fluid and entangled that chimes with both Kohn's and Peirce's ontological claims. The lines, knots and texture of the meshwork are themselves semiotic. Iconic forms are continually emerging in the continuous rebirth of the world; materialised indexical associations are what \"things\" are made of. The meshwork is semiosis made flesh. Kohn develops these themes by drawing on Peirce's concepts of \"firstness,\" \"secondness\" and \"thirdness.\" Firstness is that which is spontaneous and ephemeral-a glimpse of possibilities that are as yet unrealised and unconnected. Kohn describes the tantalising whiff of cinnamon from a freshly opened passion fruit as an example of firstness. Secondness is more startling and sometimes shocking; secondness can be unpredictable or a jolt to habits. For Kohn, a crashing palm tree exemplifies secondness, as a sudden disruption in the meshwork that might realign its relations and draw attention to troubling indexical causation. Thirdness, finally, is more general, regular and habitual. It can take the form of behaviour but can also be seen in terms of certain general tendencies of life such as selforganisation and resistance to entropy. As Kohn argues, \"This tendency is what makes the world potentially predictable and what makes life as a semiotic process, which is ultimately inferential, possible. For it is only because the world has some semblance of regularity that it can be represented. Signs are habits about habits.\"30 Thirdness is the sort of regularity that Krause hears represented in a relatively uncompromised ecosystem in which each bird responds to the regularities of other sounds and acoustic conditions, and it is essential to semiosis more generally. All signs contain aspects of firstness, secondness and thirdness but tend to amplify one of these more than the others. They are all relational. Humans and other beings can sometimes be shocked (secondness) by what is encountered, but being shocked draws deeper attention to habits and regularities. We know the world and our movements within its meshwork differently through shock, and listening to birds in the Anthropocene is habitually shocking. In examining perceptual practices in the meshwork, Ingold writes not of shock but instead marks a distinction between astonishment and surprise. 31 Astonishment, he argues, is a response to a world that is open and continually being remade. In this, even the mundane can be astonishing. Surprise is a response to a world conceived of as closed and predictable, which thus creates the possibility for surprising deviations. Astonishment involves \"riding the crest of the world's continued birth\"32 whereas surprise involves grasping the world \"within a grid of concepts and categories.\" 33 The former Ingold associates with animism and the latter with the sciences. However, I do not see the two sorts of response as being wholly inimical. Kohn's argument that the symbolic realm, in which the world might be conceived of as closed, predictable and thus potentially surprising, is itself emergent from an openness to the world, in which entanglement, continuous birth and astonishment are endemic. Moving too far into the closed, symbolic world can be pathological. As Kohn points out when describing his own temporary breakdown, symbolic semiosis that is divorced from an iconic and indexical grounding can easily induce panic: Panic and its dissipation reveal these properties of symbolic semiosis. They point both to the real dangers of unfettered symbolic thought and to how such thought can be regrounded. Watching birds regrounded my thoughts, and by extension my emerging self, by recreating the semiotic environment in which symbolic reference is itself nested. Through the artifice of my binoculars I became indexically aligned with a bird, thanks to the fact that I was able to appreciate its image now coming into sharp focus right there in front of me. 34 In his account, Kohn describes a movement from being deeply alienated and ungrounded in his semiosis while travelling on a bus to the dissipation of this panic when he takes time to look at a tanager while out walking. This move marked a 're-astonishment' with the world, while the panic was a shock that revealed more clearly his regular habits of being. The move was not so much between engagement and detachment but between immersion and alienation. This is what can happen when symbolic thoughts drift free from their anchoring in the indexical and iconic conditions of being-in-the-world. As Candea argues in relation to his work with scientists studying meerkats, detachment is not the opposite of engagement but a particular manifestation of engagement, one that enables scientists to understand the world of the meerkats more deeply and sensitively. 35 It is, in fact, a manifestation of the oscillation I described earlier between the real grounding of being-in-the-world and the symbolic and ethical. The surprise of a scientist is not so much a product of disengagement but of the oscillation towards an emergent symbolic semiosis in which the world is conceptualised as a closed model. However, in producing such a model the scientist needs to be fully engaged with a world of complex movement and constant form generation. They need to have consciously dwelt in the meshwork in order to build the model. In order to be surprised, one first needs to be astonished. The shocking surprises of the Anthropocene are most readily perceived by those who find the world constantly astonishing. My reason for discussing these relational semiotic approaches is to emphasise that the symbolic and ethical associations and interpretations of listening to birds in the Anthropocene are grounded in an experience of being-in-the-world; they are not simply cultural models that are imposed onto what is perceived. Any such models or symbolic associations are emergent from being-in-the-world rather than prior to it. Likewise moral ideas are emergent from the iconic and indexical grounding of the meshwork. Following Ingold and Kohn I argue that it is not possible to have symbolic and moral ideas without first being instantiated in the world in a meshwork of relations and associations that are at once both material and semiotic. As such, the ways that humans listen to their surroundings are not fundamentally different in their grounding to the ways that birds listen to theirs. In the narratives that follow a range of moral and symbolic ideas are represented, but these draw on the indexical associations of place and temporality. While the approach that I have outlined above is more broadly about how humans and other species perceive the world, there are some particularities to listening to birds in the Anthropocene that require elaboration. I argue that the semiotic context of the Anthropocene is peculiarly anxious because, as I pointed out earlier, it emphasises our separation from the rest of life just at the moment we connect with it. As Aldo Leopold knew, \"One of the penalties of an ecological education is that one lives alone in a world of wounds.\" 36 The Anthropocene world is increasingly not a world of reassuring thirdness and regularity but of a shocking secondness-such as Carson wrote about in Silent Spring-in which the causes of deleterious change seem to lead us back to our own actions. As the indexical and iconic grounding of our being-in-the-world shifts, those wounds become more apparent to those that are the least alienated from other forms of life. The moral and symbolic fallout is the anxious semiotics of the Anthropocene.",
          "I derive three principles from the above discussion that together provide insights into the particularities of listening to birds in the Anthropocene: 1. The semiotic grounding of perception is an indexical and iconic meshwork, both for birds and for humans. Although the meshwork is constantly emerging, there is still potential for regularities (thirdness) that are responded to by organisms in their actions and that provide a basis for the continuance of their lives. 2. The symbolic emerges from this grounding and provides potential for an oscillation between different kinds of mindfulness e.g. engagement and detachment, real and ideal etc. Human perception often involves this kind of oscillation. 3. The semiotic grounding of the Anthropocene is particularly unstable and the causes of this instability can often be traced to human action. The perception of instability and uncertainty thus provokes a distinctly anxious semiotics that can shock and unsettle senses of place and time, as well as provoking moral disquiet. To add some flesh to the bones of these principles, I turn now to the Listening to Birds project, an anthropological study of people's relations with birds through sound. Through this research I received many stories from people narrating their own experiences of listening. 37 Most came from respondents in the UK but many were also contributed from other parts of the Englishspeaking world, including Australia and New Zealand. I take these narratives as a means by which people highlight what sounds they notice, why those sounds are significant and how they respond to changes in what they hear. They also illustrate broader moral and symbolic dimensions of people's relations with their environment and the role that the sound-making of birds plays in this. What I find most striking about many of the stories is that they concern listening to birds in familiar places around where people live. They are about birds that live alongside people and inhabit their gardens and towns. These are not stories of distant soundscapes in remote and wild places but of companion species that make their own places through sound in the places that people make. What also becomes clear in these narratives is the importance to people of a sense of resonance with birds and with their environment more generally. By this I mean a kind of iconic resemblance or indexical association that arises between human activities and those of birds, as represented in their sound-making. The activities of birds are widely understood to resonate with the rhythms of time and season and, following from this understanding, many 37 These were received via the project website, which invited contributions of people's experiences of listening to birds. The website was widely publicised when the project was covered by the BBC News website. respondents expressed feelings of well-being in response to hearing birds. This seems most apparent when people's lives also resonate with these rhythms,38 as Rob from Droitwich wrote: As a farmer I'm up well before dawn at this time of year but even now there are birds singing in the morning. I heard my first dunnock yesterday. They have already started pairing up and the little owls have been amusing us with their bright chirrups every evening and morning. They certainly brighten up the dark mornings and evenings. As we move into spring I'm lucky enough to hear the dawn chorus every day and it does give me a real lift. Lambing time can be magical when you see new-born lambs taking their first steps to a sometimes deafening suite of blackcaps, robins, chiffchaffs, willow warblers and the rest. If you can't appreciate that then there's something seriously wrong with you. When people like Rob attend to the activities of birds as they go about their lives they are given, as he puts it, \"a real lift.\" This lift, I argue, stems from a sense of resonance that comes from this sympathetic attention to the activities of other beings around us. The sort of attunement of activities that Rob describes resonates, but it also requires that he be sensitive to the lives of those around him. Rob is, according to his account, astonished on a daily basis by listening to birds. His engagement with his surroundings enables him to notice the daily and seasonal changes and regularities. The signs of new and burgeoning life that he perceives serve to exemplify the stability of the place where he lives and the life he leads in it. This resonant grounding even draws Rob into making a moral point about the need for others to appreciate such an experience, perhaps reflecting an anxiety that many do not. His sensing of place and time and of his own resonance with birds is grounded in his being-in-the-world but also draws forth this generalised reflection. As Ingold argues, resonance is analogous to the rhythmic interplay of musicians, who may be playing different sounds but who are following the patterns of sounds around them in ways that are complementary and harmonious. 39 As Krause implies, resonance is also integral to the listening and sound-making of birds and other animals, as they make themselves heard through resonating with the acoustics of their own worlds. The apparent regularity of home, as signalled to Rob by the sound-making of birds, is important in forming many people's sense of the temporality of days and seasons. Rachel Carson clearly understood this connection and its power. Indeed, her book might better have been called Silenced Spring. Spring as a period on the calendar might not be silent without the birds, but for many it would scarcely have been spring either. Bird song does more than simply herald the spring; in an important sense it is spring. As Carson realised, the Anthropocene was making home seem less stable and unproblematic. Many narratives describe the effects of this instability in how home sounds through its birds. A respondent from northern England wrote: I grew up in a suburban area of a northern industrial town and was used to seeing lapwings flitting away across the fields, pee-witting as they did so. I haven't seen a lapwing anywhere near the area for years now and it saddens me to think that my daughters won't get that simple joy of seeing and hearing something otherworldly on their doorstep. Whitehouse: Listening to Birds in the Anthropocene / 65 In this example, an integral sight and sound of the rural-urban fringe has disappeared, probably because of the changes wrought by modern agriculture and our attempts to produce more food more cheaply. The silencing of lapwings is felt anxiously by a father who wants his daughters to experience some continuity with his own sense of place. Again, the grounding of sights and sounds comes to have symbolic connotations of home and of this respondent's own biography, but the differences between his own experience and that of his daughters provokes shock. Other narratives describe new birds that have moved in, like the Asian ring-necked parakeets that now add colour and sometimes discordant sound to the south-eastern suburbs of England. A respondent from Surrey writes: Over the last year or so I started hearing strange squawking in my garden. Normally the bird songs are of the usual garden variety, so this was interesting. It turns out that we have ring-necked parakeets living in the area and it was them creating the squawks and screeches. Now I listen out for them as it's lovely to see the bright green parakeets flying around and \"terrorizing\" the local woodpigeons. The ordinary suburban lives of the English Home Counties are enlivened by these exotic birds, terrorising the woodpigeons and contrasting with bird songs \"of the usual garden variety.\" Sometimes the apparent stability and quietude of suburbia needs something to shake it up; the shock of the new serves to emphasise familiar regularities. But whether these stories concern disappearing species or incoming exotics they speak of the sorts of connections with place that people make through bird sounds and of the acoustic ecologies of places. This grounding leads on to moral pronouncements about how people would like places and times to sound and of the sort of resonance they seek. In this final example, a respondent from Cumbria articulates memories of the place where he grew up before raising concerns over the increasing dissonance between farming and the activities of the birds, a dissonance that has led to the silencing of one particular bird: I grew up in Ireland and lived about three miles from Foxford, County Mayo. In the summer, and especially in the late evenings and at night, one could hear the corncrake call-lots of them-some near, and others far away. The call told me that summer was at last here. I would dearly love to hear that sound again or for my children and grandchildren to hear it too. Sadly the corncrake went into decline because of silage making-farmers cutting grass while the corncrake and other wild birds are nesting on the ground. I would love to see a comeback. The resonance that my respondents described in relation to the sound-making of birds is, I argue, an emergent ethic that draws lives and activities together. It is integral to how they experienced their surroundings but also to how they idealised experience and how they wanted their relations with a place to be. The desire for corncrakes to make comeback in County Mayo derives from this sort of idealisation of how a place should be experienced, and this ethical concern is thrown into sharp relief by their disappearance since the advent of Anthropocene agricultural intensification. This desire for resonance is not just about how things sound but about fitting in with the activities of birds in a way that is sometimes iconic and sometimes complementary. Listening to birds, for my respondents, is not simply about apprehending the sound but about trying to understand how sounds relate to their own activities and the places and temporalities they inhabit. When people move to new areas or other countries they often find the different bird sounds they encounter to be unsettling, to precipitate a sense that they don't belong and that their activities no longer resonate with the right kind of bird sounds. Even without the abruptness of moving to a new area, changes in local bird populations and the loss of species and their sounds can foster a sense of anxiety and loss. But silence, as Carson and Krause both realised, is more unsettling still. My respondents are, I think, revelling in perceiving the activities of other species as they become manifested in sound. Life makes sound and movement and their lack can easily come to be understood as a kind of death. The silence and stillness of birds killed by pesticides or motor vehicles, or habitats depleted by logging or drainage, promote an anxious semiotics of death and loss typical of the Anthropocene, when change and loss is readily attributed to disruptive human actions, sometimes perhaps even the listener's own. Silence and discordance are symptomatic of the Anthropocene.",
          "To explore these issues further, I now invite the reader to listen to four sound recordings of birds that were made in four different countries. These span a range of situations from urban gardens, to islands, to a celebrated rural location for birds, to a rainforest remnant where an enigmatic bird lingers on the edge of extinction. I offer a description of my own listening as a starting point for the reader and as a guide to what is being heard. These are not intended as exhaustive interpretations but to facilitate closer listening. I aim to assist the reader in understanding why these places sound the way they do and to encourage them to listen more closely to what they hear as they go about their lives more generally. The anxious semiotics that is never far away only emerges through active listening. Finally, I add as a proviso that listening to these recordings is not the same experience as \"being there.\" Listening is not simply a process in which sound is heard but is a whole bodily experience of being in place in which sound is a focal point. Recordings may \"transport\" one somewhere but they do not furnish the entire semiotic context and sense of being-in-the-world. Capertee Valley, New South Wales: June 2013 40 I spent a week writing a draft of this paper in the, to me, unfamiliar surroundings of the Australian bush. The Capertee Valley is a renowned area for birds a couple of hours from Sydney: a mix of woodland, fields, hills and valley. Certainly there is abundant life here: a constant twitter of fairy-wrens, thornbills and red-browed finches; the chirrups of a Jacky winter; a garrulous cry from a kookaburra or a currawong; the eerie calls of Australian magpies and ravens; the distant squawks of a flock of cockatoos and the agitated bleating of masked lapwings. What do these sounds tell me about the place where I am? At times it seems chaotic, with a whole range of disparate and similar sounds competing with one another. But I am, unusually, in a place where birds, together with a few insects and amphibians, are almost all I can hear. If this place sounds of anything it sounds of its birds. It is far from silent, or even quiet, but does seem to have what Krause calls tranquility. 41 Silence, he argues, is not a desirable state but quite disorienting. In silence, as Carson probably realised, it is hard to find a sense of place. Tranquility, on the other hand, is a state between noise and silence in which a kind of endorphin-rich serenity is attained. The only significant disruption to the sounds of birds comes when a plane flies high overhead. I likely flew over this area on my own journey to Australia and so the sound of aircraft is, in part, enabling me to hear this place. But air travel also seems to be contributing to climate change and the increasingly severe summer heat that regularly leads to devastating fires in this part of Australia. The valley is, to use Haraway's phrase, \"full of bumptious life\" but the strains of Anthropocene anxieties and the ironies of global travel still encroach. 42 Mata da Balbina, Brazil: August 2008 43 The Atlantic forest of eastern Brazil has fragmented enormously over the last century. Many of its bird species have disappeared or become extremely scarce and localised. Some are so rare they are almost like ghosts. One such bird is Stresemann's bristlefront, a thrush-sized bird with a long tail and mass of bristles around the base of its bill from which it takes its name. Until the mid-1990s this was a species only known from two museum specimens. It seemed like a shadowy myth of the past but was then discovered fleetingly in the coastal forests before disappearing as soon as it had appeared. Then in the early 2000s it was found again in a fragment of hilltop forest called Mata da Balbina close to the border of the Bahia and Minas Gerais provinces. Here the small population lingered and I was able to visit in 2008, together with another British birder. We arrived at the forest knowing little about the bird and our knowledge of its singing came from a single online recording made the previous year. The field guides that we used had no description of the sound at all, presumably because it was unknown when the guides were produced. After some time searching, a liquid, cascading song rose up from the thick undergrowth. It was soon close enough to make a recording, which we carefully played back, quietly and at long intervals, to try to entice the bird to show itself. 44 The bird we could hear in the forest sounded slightly different to the online recording because it added in an alternating ending to the song. Curiously, it seemed to respond more enthusiastically to the recording we had downloaded rather than the playback of its own song. It overlapped with the recording almost as if in a duet. The lack of scientific information on the singing of Stresemann's bristlefront rather liberated my thinking about what I was listening to; the singing had no \"facts\" to conform to. Almost an hour passed in the midst of the dense tangle of branches before we saw a female creep nervously across the forest floor, its singing encircling us enigmatically. It was a 41 Krause, The Great Animal Orchestra, 216. 42 Donna Haraway, The Companion Species Manifesto: Dogs, People and Significant Otherness (Chicago: Prickly Paradigm Press, 2003), 32. 43 See audio clip #2 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 44 The ethical issues surrounding playback of bird sounds are complex. For further discussion see David Sibley, \"The Proper Use of Playback in Birding,\" accessed 27 February 2015 http://www.sibleyguides.com/2011/04/the-proper-use-of-playback-in-birding/. beautiful but disconcerting experience; I wondered if the recording I made of its song might outlive the species itself. Stewart Island, New Zealand: November 2008 45 The small harbour of Oban is named after a port in the west of Scotland but this Oban is the main settlement on Stewart Island, an hour or two's boat ride from the southern tip of New Zealand's South Island. It's only my second day in the country, after a lengthy flight from the UK. Jet-lagged and weary, I had already had the strange experience of finding myself on the other side of the world, switching from early winter to early summer as I did so. And yet the birds were much the same as at home in Britain, thanks to a range of familiar garden birds that had been brought to New Zealand by acclimatisation societies in the late 19 th century. The impoverished avifauna of the main islands is swamped by these settler birds, most of which are well-regarded songbirds back in Europe. The sense of similarity to home is oddly disorienting. I am, in part, shocked by the familiarity of what I hear. I arrived by boat in Oban and began to hear some of the \"real\" New Zealand birds: the voluptuous warbles of tuis and the raucous gabble of kakas. These exotic sounds contrasted with the more prosaic chatter of house sparrows and the descending chirruping of chaffinches. This mix of the recognisable sounds of home and the new sounds of the Antipodes jarred, both in terms of familiarity and in the quality of sound. A duck flying over was again familiar, although most ducks in the area were a hybrid mix of introduced European mallards and native grey ducks. An oystercatcher and a gull were both familiar sounding too, much like those that careen about the coastlines of Scotland, but these were subtly different native species rather than introduced birds. Human history and the sounds of birds are inextricably bound together in New Zealand. The relatively short period of human settlement has shaped the avifauna and ecology in dramatic and readily perceptible ways. As much as the presence of the birds I hear, I sense the absence of those that have disappeared since Europeans arrived: bush wren, piopio, huia and many others. In Krause's terms it sounded like an ecosystem that has yet to become sonically integrated, with disparate types of sound, meant for different acoustic and ecological conditions, in uneasy coexistence. Aberdeen, Scotland: March 2014 46 A bright morning, just as winter is turning to spring. The winter has been the wettest on record, perhaps an effect of global warming; the switch to mild, dry weather seems to encourage a rush of activity in the botanical gardens. I'm reminded of the hubbub of sounds from the Capertee Valley: a rush of twittering finches, tits, dunnocks, treecreepers, goldcrests and robins. A great spotted woodpecker taps loudly at a trunk and, being near the coast, the wail of a herring gull cuts through the air. While the gardens seem like a green oasis in the city, the 45 See audio clip #3 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 46 See audio clip #4 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with an Olympus LS11. Whitehouse: Listening to Birds in the Anthropocene / 69 distant thrum of traffic is ever-present and soon the stream of birdsong is engulfed by a passing helicopter heading out to serve the offshore oil rigs. These garden and suburban sounds seem homely enough and the vibrancy of early spring exudes from the tumble of sounds. The clashes and contrasts are most obvious with the rumble of traffic, both aerial and terrestrial. I wonder if the birds are adapting their soundings to the ambient conditions, just as great tits are purported to have done in noisy urban areas. 47 Is the loud exuberance of singing a response to the noisy environment as much as the onset of spring warmth? The above descriptions reveal something about the sounds on the recordings and the context of their making. They also reveal a lot about how I listen to birds and how my own Anthropocene anxieties inform how I do this. My listening practices are somewhat specialised, given that listening to birds has been a central part of my life, but there are certain aspects of my own process of listening that I consider to be exemplary of how many people listen to birds in the Anthropocene. Much of what might be deemed distinctive about my own practices derives from a fine-grained knowledge of what kinds of birds I am hearing. Although in some situations I was not entirely familiar with the local avifauna, I could, with a little practice, put a name to many of the sounds. This was a result of careful listening and comparison to recordings but also through visual identification, facilitated by field guides illustrated with systematically arranged iconic images and descriptive texts. 48 But, while I was doing this to a rather precise degree, the naming of what is being heard is common to any narrative of listening. Even if the listener only thinks of the sound as being a bird then they are entering into the symbolic process of relating a perceptual encounter to a linguistic marker. This process of naming enters the experience into other sets of relations. Some named sounds might be redolent of place and time; others might be out of place or exotic. Some will be familiar and expected; others could be strange and unexpected. Like many people, I am astonished by the liveliness of the birds I listen to, but I can also be shocked, both by the strange case of a ghostly bird clinging tenuously to existence and by the familiarity of the birds I hear in another hemisphere. When people name aspects of what they directly encounter, they enter those phenomena into a whole range of other narratives, relations and reflections, which are in turn re-entered into the way one listens to one's surroundings. In the Anthropocene those new relations often lead us back to ourselves and the actions of humans, leading us into listening for discordance, disruption and absence. Even in hearing familiar and comforting sounds there is an anxiety in knowing that these can shift abruptly, or are competing with ever encroaching noise and habitat destruction.",
          "The more we care about our world and the more we pay attention to it, the worse things seem to get. This is a powerful anxiety for many in the Anthropocene. Engaging with and reflecting upon the world is essential to our own sense of well-being, but it brings with it the realisation of our own destructiveness. The more we listen to birds the more we notice the loss of birds from pesticides, the destruction of habitat, the encroaching dominance of Anthrophonic sounds, the sounds that are out of place and the ecosystems that are dissonant. There is nothing new about the way that the semiotics of our listening oscillates between our being-inthe-world and our reflection upon it. The semiotics of the Anthropocene, however, destabilises the groundings of our perceptions and draws our reflections anxiously towards our own disruptiveness. Silence and loss hang heavy in the Anthropocene, but this does not mean that our ongoing relations with birds and their sound-making are not fruitful or that we should desist attending to them to avoid the inevitable unease. These lives are ones that people often hope to resonate with, that is, to attend to them in a way that is ethical, that is aesthetic. As Donna Haraway puts it: \"All ethical relating, within or between species, is knit from the silk-strong thread of ongoing alertness to otherness-in-relation.\" 49 The \"ongoing alertness\" to the sounds of birds that my respondents told me about is just this kind of ethical relating, but it is a relating that is importantly grounded in the same kinds of semiotic processes through which birds listen to their own world. I argue that listening to birds in the Anthropocene should not encourage a separation of human activities from those of birds but should instead ground the development of relations of companionship. It elicits not simply a narrative of encroaching loss and the ever present threat that humans pose to non-humans, but one of enskillment, of how we learn to listen to birds and to the rest of our world and how we learn to make ourselves, however uneasily, at home in it. Andrew Whitehouse is an anthropologist and birder teaching at the University of Aberdeen, Scotland. His work has explored conservation conflicts in western Scotland, landscape and anthropology and, more recently, people's relations with birds through sound. He is co-editor of the volume Landscapes Beyond Land: Routes, Aesthetics, Narratives published by Berghahn in 2012."
        ],
        "ground_truth_definitions": {
          "anxious semiotics": {
            "definition": "Sense of uncertainty and concern over potential human culpability in loss.",
            "context": "There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Peña, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Peña's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fernández, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fernández, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fernández, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
