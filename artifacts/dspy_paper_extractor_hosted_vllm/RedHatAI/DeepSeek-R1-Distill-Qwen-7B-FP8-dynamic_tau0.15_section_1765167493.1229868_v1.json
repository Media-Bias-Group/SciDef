{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "hypotheses based on a careful mapping of psychological profiles of over 2,300 American Twitter users linked to behavioral sharing data and sentiment analyses of more than 500,000 news story headlines. The findings contradict the ignorance perspective but provide some support for the disruption perspective and strong support for the partisan polarization perspective. Thus, individuals who report hating their political opponents are the most likely to share political fake news and selectively share content that is useful for derogating these opponents. Overall, our findings show that fake news sharing is fueled by the same psychological motivations that drive other forms of partisan behavior, including sharing partisan news from traditional and credible news sources. With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018). Political observers have linked the spread of fake news to recent key political events, like the \"Brexit\" vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016). Accordingly, media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017). In light of these sizable investments, it is surprising how little we know about why people share fake news on social media. Studies suggest the spread of false information cannot be attributed only to bots or \"sock puppet\" accounts (Guess, Nagler andTucker 2019, Vosoughi, Roy andAral 2018). But researchers have largely failed to conduct adequate tests of why ordinary citizens take part in the distribution of fake news (Wagner and Boczkowski 2019). This is unfortunate because potential solutions to the spread of fake news rest on psychological assumptions about the root causes of the problem. For example, fact-checking should reliably work only if citizens actually want to share true information but need help to weed out falsehoods. Presently, we do not know whether this portrayal of citizens' motivations is correct. As summarized in a recent study, \"most existing studies rely solely on publicly available social media data and there is little research that seeks to understand people's motivations for (...) sharing news\" (Chadwick, Vaccari and O'Loughlin 2018, 4258). In this study, we ask: What are the individual-level psychological motivations behind fake news sharing? The first major challenge in addressing this research question is data unavailability (King andPersily 2019, Stier et al. 2019). Data on citizens' personality and political profiles are rarely available in combination with behavioral data on their sharing activities on social media (for notable exceptions, see Guess, Nagler and Tucker 2019, Guess, Nyhan and Reifler 2020, Grinberg et al. 2019). Here, we circumvent these data limitations by linking unique behavioral data on news sharing activity and sentiment analyses of more than 500,000 news story headlines to individual-level survey data with detailed psychological measures for a large sample of American Twitter users. This approach allows us to conduct a comprehensive study of competing psychological theories of fake news sharing. The second challenge is that multiple psychological motivations could potentially influence the sharing of false and factually correct news; these motivations may differ across individuals and across news content. Building on theories about information sharing in general, we first outline these potential psychological motivations before applying them to our research topic: the sharing of political fake news on social media, a domain characterized by intense conflict. 1Then, we deduce testable implications about relevant individual differences and differences in political news content.",
        "reasoning": "The section discusses the motivations behind fake news sharing on social media, including psychological perspectives like ignorance, disruption, and partisan polarization. It defines fake news as fabricated information that resembles news media content in form but not intent. The study highlights that individuals who hate their political opponents are more likely to share political fake news. It also mentions various methods used to detect and combat fake news, such as fact-checking services and algorithms. The section does not explicitly define terms beyond 'fake news' and 'political fake news', so no additional terms were extracted.",
        "extracted_terms": [
          {
            "term": "fake news",
            "definition": "The definition found in the text is: \"fake news can be defined as 'fabricated information that mimics news media content in form but not in organizational process or intent' (Lazer et al. 2018).",
            "context": "The verbatim sentences surrounding the definition are: \"While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018).\"",
            "type": "explicit"
          },
          {
            "term": "political fake news",
            "definition": "The definition found in the text is: \"Political observers have linked the spread of fake news to recent key political events, like the 'Brexit' vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016).\"",
            "context": "The verbatim sentences surrounding the definition are: \"Media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017).\"",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The section discusses racial/ethnic disparities in food insecurity, emphasizing the need to address both interpersonal and structural racism. Key terms include \"racial/ethnic disparities,\" \"need to move beyond,\" \"exposures,\" \"discrimination and structural racism,\" \"racial discrimination,\" \"disparate treatment,\" \"disparate impact,\" \"racial discrimination,\" \"structural racism,\" \"interpersonal discrimination,\" and \"systemic racism.\" Each term is defined within the context provided, focusing on their roles in perpetuating food insecurity among people of color.",
        "extracted_terms": [
          {
            "term": "racial/ethnic disparities",
            "definition": "unequal treatment of persons or groups on the basis of their race or ethnicity",
            "context": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade",
            "type": "explicit"
          },
          {
            "term": "need to move beyond",
            "definition": "the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites",
            "context": "There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes.",
            "type": "explicit"
          },
          {
            "term": "exposures",
            "definition": "exposures that may impact people of color differently from whites",
            "context": "The need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites",
            "type": "explicit"
          },
          {
            "term": "discrimination and structural racism",
            "definition": "the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems",
            "context": "Structural racism refers to 'the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)'",
            "type": "explicit"
          },
          {
            "term": "racial discrimination",
            "definition": "unequal treatment of persons or groups on the basis of their race or ethnicity",
            "context": "The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity.",
            "type": "explicit"
          },
          {
            "term": "disparate treatment",
            "definition": "differential treatment on the basis of race that disadvantages a racial group",
            "context": "The later component, 'disparate impact', also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites.",
            "type": "explicit"
          },
          {
            "term": "disparate impact",
            "definition": "treatment on the basis of inadequately justified factors other than race that disadvantages a racial group",
            "context": "The later component, 'disparate impact', also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites.",
            "type": "implicit"
          },
          {
            "term": "racial discrimination",
            "definition": "differential treatment on the basis of race that disadvantages a racial group",
            "context": "A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders.",
            "type": "implicit"
          },
          {
            "term": "structural racism",
            "definition": "the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems",
            "context": "Structural racism refers to 'the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)'",
            "type": "explicit"
          },
          {
            "term": "interpersonal discrimination",
            "definition": "differential treatment on the basis of race that disadvantages a racial group",
            "context": "While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
            "type": "implicit"
          },
          {
            "term": "systemic racism",
            "definition": "rooted within systems",
            "context": "Structural racism refers to 'the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems'",
            "type": "implicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "Using personalised PageRank algorithm Pr = σ MPr + (1 -σ )v, M is a transition matrix, Pr is a vector where each element represents the score assigned to the corresponding node.",
        "reasoning": "The section does not provide explicit definitions for the terms \"M\", \"Pr\", \"σ\", or \"v\". These terms are standard in the context of the PageRank algorithm and are not defined within the provided content.",
        "extracted_terms": []
      },
      {
        "augmented": true,
        "section": "We model the impact of media bias on voting in the presence of a new media source. Voting depends on both ideological preferences and perceived politician quality. We define a news media outlet to be biased if it systematically slants its broadcast of politician quality to favor either the Republican or the Democratic candidate. We consider both voters who rationally update from news reports and voters who are subject to non-rational persuasion. Setup. In each period t, the media reports about a different pair of politicians, one Republican and one Democrat. These can be candidates running against each other in an election, such as Bush and Gore, or non-elected officials, such as Rove and McAuliffe. While the politi-cal affiliation of the politicians is known, the ability is unknown and can be guessed only using the media reports. The true differential ability of the period-t Republican politician relative to the period-t Democratic politician is θ t , which is i.i.d. and distributed normally with mean 0 and precision γ θ : θ t ∼ N (0, 1/γ θ ) . The media observes the politician ability θ t and broadcasts ψ t = θ t + β, where β is the time-invariant degree of media bias, drawn from the distribution N (β 0 , 1/γ β ). A media source with positive β provides a more favorable broadcast of the Republican politician, and the converse for a media source with negative β. Since the focus of the model is on voter behavior and not on media strategy, we simplify matters by assuming a reduced-form 'news production function', instead of deriving an optimal bias policy from the utility maximization of the media. The voters learn about the bias β over time from the sequence of media reports ψ t . After observing the media reports on all the candidates from period 1 to period T, voters cast their ballots in period T for the candidates that the media covered in period T . The voting is based upon the estimated quality of the period-T candidates, as well as on ideological preference. A voter casts a ballot for the Republican candidate in period T if b θ T + α > 0 where b θ T is the voter estimate of the differential ability of the Republican politician in period T and α is the ideological preference for the Republican candidate (α can be positive or negative). Ideological preference α is heterogeneous, with a continuum of voters, c.d.f. F (α), and p.d.f. f (α) > 0 for all α. (Voters are instead homogeneous in their updating about β and θ) Signal extraction. The voter faces a signal extraction problem. Upon observing a media signal ψ t , the voter makes inferences with respect to the politician quality θ t and the degree of media bias β. A positive signal ψ t could be due to an able Republican candidate, or to a right-wing media source. After observing T news reports with average report ψ T = P T t=1 ψ t /T , given the normal distribution of the signals, the voters estimate the bias to be The estimated bias is a convex combination of the mean bias, β 0 , and the observed average report ψ T , with weights given by the precision of the prior γ β and the precision of the observed signal T γ θ . The more signals (higher T ) the voter observes, the more weight the voter puts on the news reports. Using this estimate of media bias, rational voters estimate the differential quality of the period-T Republican candidate θT . They subtract the estimated bias βT from the report ψ T , and combine it in a precision-weighted average with the prior about θ T , that is, zero (Republican and Democratic politicians are on average of the same quality). Hence where W, the precision of ψ T -βT , equals (γ So far, we have considered fully rational updating by voters. We now consider a simple reduced-form model of voters subject to persuasion. As in DeMarzo et. al. (2003), we interpret persuasion as incomplete filtering of bias. Voters subject to λ-persuasion believe that the report ψ t equals θ t + (1λ) β, even though in reality the report ψ T equals θ t + β. The parameter λ ∈ [0, 1] measures the degree of persuasion, with λ = 0 nesting the standard model, and λ = 1 indicating extreme persuasion. These voters systematically underweight the importance of media bias in influencing news reports. This is consistent with the experimental findings in Cain et al. (2005) in which subjects did not take the bias of the information-provider sufficiently into account, even though the bias was common-knowledge. For simplicity, we assume that the degree of persuasion λ affects neither the estimate of media bias βT nor the precision W of ψ T -(1λ) βT ; therefore, β and W are not indexed by λ. 5 When estimating θT , voters subject to λ-persuasion subtract only (1λ) βT from ψ T to form the estimate θλ T : In Proposition 1, we summarize the effect of media bias β on the estimated ability of the Republican politician θλ T , conditional on the realized politician quality θ t , t = 1, ..., T . We consider the case of voters subject to λ-persuasion, since the standard case is embedded for λ = 0. Proposition 1. For any finite T, (i) an increase in media bias β increases the estimated quality of the Republican politician, b θ λ T : ∂( b θ λ T )/∂β > 0, and (ii) this increase is higher the more voters are subject to λ-persuasion: ∂ 2 ( b θ λ T )/∂β∂λ > 0. (iii) In the limit as T → ∞, the effect of media bias is positive in the case of persuasion (λ > 0) and zero otherwise: The intuition for Proposition 1 is straightforward. There are two effects of media bias β on expected politician quality θλ T . First, a more right-wing media (higher β) issues a more positive signal ψ T on the T -th Republican politician, which leads to a higher perceived θλ T . Second, a higher β is associated with a higher average of past signals ψ T , and therefore to a higher perceived media bias βT , leading to a lower perceived θλ T . For finite T, the first, direct effect dominates the second, indirect effect, and hence media bias has an impact on beliefs: . Moreover, since the degree of persuasion, λ, decreases the second effect, media bias has a larger impact on beliefs for higher λ (Proposition 1.(ii)). As T → ∞, the estimated media bias βT converges to the true bias β. Since eventually the voters become fully aware of the degree of bias, the media bias has no effect on the expected perceived quality as long as voters are rational (λ = 0). The same would be true if the bias β were known from the start. Voters that suffer from persuasion (λ > 0), instead, are affected by media bias β even after an infinite number of media reports (Proposition 1.(iii)). Finally, we analyze the impact of media bias on voting, which takes place in period T . A voter subject to λ-persuasion casts a ballot for the period-T Republican candidate if α + b θ λ T > 0. Then, the share of votes for the Republican candidate is , an increasing function of the estimated (differential) ability b θ λ T of the Republican candidate. Proposition 2 summarizes the effect of media bias β on the vote share 1 -F (-b θ λ T ). Proposition 2. (i) For any finite T, an increase in media bias β increases the Republican vote share In the limit as T → ∞, the effect of media bias is positive in the case of persuasion (λ > 0) and is zero otherwise. The impact of media bias on voting follows in a straightforward manner from its impact on beliefs. A new media source can temporarily move voters in the direction of its bias even if voters are rational (Proposition 2.(i)); in the long run, however, only voters subject to persuasion remain affected (Proposition 2.(ii)). Proposition 1.(ii) does not generalize to voting without restrictions on the shape of the distribution of voters F . While a higher λ increases the impact of β on the estimated politician quality b θ λ T , it does not necessarily increase the impact on the vote share 1 -F (-b θ λ T ) if the elasticity of voting with respect to beliefs, captured in f, is lower for individuals with higher λ. The empirical predictions of the model depend on the length of the experience of the audience with Fox News by 2000. Since Fox News was started in 1996, by the year 2000 its audience had the chance to experience its coverage of several politicians, from Clinton and Dole, to Ted Kennedy and John McCain, to Gore and Bush. The viewers were also exposed to Fox News' reporting on non-elected political figures such as Ken Starr or Jesse Jackson. To the extent that each pair of politicians is an observation t, therefore, one reasonable assumption is that by the 2000 elections the audience had a precise estimate of the Fox News bias (case T → ∞). In this case, only voters subject to persuasion should be affected by the exposure to Fox News. Fox News should not impact rational voters. A different interpretation holds that, instead, four years are too short a period for the audience to form a clear opinion of the Fox News bias, especially since some of the audience listened to it for only a year or two before 2000. In this case, Fox News should affect voting on Bush and Gore also for rational voters. The impact should diminish over time, between 2000 and the subsequent elections, and should be stronger if voters suffer from persuasion.",
        "reasoning": "The section discusses media bias and its impact on voting behavior. It defines media bias as the systematic slanting of broadcast content to favor one party, and explains how this bias influences media reports, voter preferences, and the perceived quality of politicians. The analysis includes how voters update their beliefs based on media reports and their ideological preferences, and how this affects voting outcomes over time.",
        "extracted_terms": [
          {
            "term": "media bias",
            "definition": "Systematic slanting of broadcast content to favor one party.",
            "context": "We define a news media outlet to be biased if it systematically slants its broadcast of politician quality to favor either the Republican or the Democratic candidate.",
            "type": "explicit"
          },
          {
            "term": "voting",
            "definition": "Voting depends on both ideological preferences and perceived politician quality.",
            "context": "Voting is based upon the estimated quality of the period-T candidates, as well on ideological preference.",
            "type": "explicit"
          },
          {
            "term": "media reports",
            "definition": "These are the content broadcasted by media outlets about politicians.",
            "context": "While the politi-cal affiliation of the politicians is known, the ability is unknown and can be guessed only using the media reports.",
            "type": "explicit"
          },
          {
            "term": "politician quality",
            "definition": "The inherent ability of the candidates.",
            "context": "The true differential ability of the period-t Republican politician relative to the period-t Democratic politician is θ t , which is i.i.d. and distributed normally with mean 0 and precision γ θ .",
            "type": "explicit"
          },
          {
            "term": "ideological preference",
            "definition": "Heterogeneous preferences for the candidates.",
            "context": "Ideological preference α is heterogeneous, with a continuum of voters, c.d.f. F (α), and p.d.f. f (α) > 0 for all α.",
            "type": "explicit"
          },
          {
            "term": "report",
            "definition": "The media signal ψ t = θ t + β.",
            "context": "A media source with positive β provides a more favorable broadcast of the Republican politician, and the converse for a media source with negative β.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10462-022-10338-7",
        "sections": [
          "Recent years have shown a growing interest in the application of natural language processing techniques for extracting, summarizing, and creating new data from text. Today's digital media ecosystem generates massive streams of unstructured data, such as news articles, web pages, blog posts and tweets, thus posing a set of challenges related to their automatic understanding. Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narrative can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time. Although several attempts have been conducted in the domain of narrative extraction, some problems remain unsolved, which are not easily solved by the currently available Natural Language Processing (NLP) and Information Extraction (IE) techniques. Important challenges in this domain involve defining annotation schemes that are comprehensive enough to include the relevant features of the narrative elements, but, at the same time, not too cumbersome to avoid overloading the process of extraction; automatically extracting cohesive narratives from unstructured data with high effectiveness; detecting and characterizing storylines over single and multiple documents; identifying narrative segments within large portions of text; dealing with informal language and with different types of documents (e.g., social media and micro-blogs); coping with different human languages and with combinations of languages; defining meaningful formal representation of narratives as an intermediate step to visualization schemes; devising a standard evaluating narrative framework (made of datasets, baselines and metrics) as an entry-point for researchers interested on properly evaluating their methods; and contributing with the development of specialized linguistic resources for low-resource languages, such as narrative annotated data, to avoid creating niches. To help the research community address these challenges, different techniques can be applied depending on the type and scale of the narrative, e.g. single news articles versus whole fiction books. In this paper, we aim to survey existing approaches and techniques related to the process of extracting narratives from a text, an emergent theme of research within the field of computational linguistics and artificial intelligence. To the best of our knowledge, there is currently no survey addressing the full process of narrative extraction from textual data. Hence, collecting, organizing, documenting and describing fundamental concepts of this process is of paramount importance for different related areas, where the extraction of narratives plays an important role. Domains such as journalism (Caselli et al. 2016), finance (El-Haj 2022), health (Sheikhalishahi et al. 2019) , information access in digital libraries (Kroll et al. 2022), and other fields that require analyzing narratives over a common topic using NLP approaches, are among the main beneficiaries of this survey. Such communities are very active in the area, which makes it quite hard for a researcher to be aware of all the relevant contributions. It should be noted that in the different contexts where narratives can be extracted, there are common problems; however, there is also terminology, notation and specificities for each one. While in a financial narrative, there is a need for structure extraction (El-Haj et al. 2019) given the format in which the data are generally available, in narratives related to historical events (Lai et al. 2021), there is no such need. In this work, such particularities will not be discussed; instead, we will seek to focus on tasks that are common to narrative extraction from a general point of view. In an attempt to provide a thorough account of relevant research developed within the field of computational linguistics and artificial intelligence (AI), we conduct a survey where the different approaches are organized according to the pipeline of tasks one can find in the extraction of narratives from text. The development of this study began with the selection of fundamental articles understood as key to the subareas that make up the extraction of narratives. From these articles, we apply the snowball procedure (Wohlin 2014) to expand the pool of articles by considering relevant research published in high-quality conferences and journals in the fields of natural language processing and artificial intelligence. The remainder of this paper is organized as follows. Section 2 gives an overview of the process of extracting narratives from a text by introducing the fundamental concepts, the data acquisition and annotation effort, and the narrative extraction pipeline. Section 3 presents the initial step of narrative extraction that comprises the pre-processing and parsing of a text. Section 4 introduces research developed on the identification and extraction of narrative components, namely lexical and syntactical components. Section 5 presents the techniques found in the literature to establish the linkage between the identified narrative components. Section 6 refers to research work focused on the representation of narratives structures. Section 7 provides a snapshot of the metrics and the datasets behind the evaluation efforts. Section 8 promotes a discussion of some important aspects of narratives research including open issues. Finally, Sect. 9 concludes this paper by presenting its final remarks.",
          "In this section, we begin by presenting a detailed definition of the concept of narratives and its associated terminology. Following, we discuss important steps related to data acquisition and annotation. Finally, we describe the general pipeline behind the process of extracting narratives from textual data through the lens of a computational scope. The pipeline here introduced will define the structure of the rest of this paper.",
          "Narratives have long been studied in the field of linguistics. One of the first authors to introduce a formal definition was Adam (1992), who considers narratives to be a prototypical sequence obeying a thematic unity, regarding a chronological succession of events involving characters. The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theatrical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community. Riedl (2004) considers narratives as a cognitive tool for situated understanding, i.e., a structure designed to better understand the world around us. Motta (2005), instead, understands narratives as forms of relationships that are established due to culture, the coexistence between living beings that have interests, desires, and that are under the constraints and social conditions of hierarchy and power. As the author states, whoever narrates has some purpose in narrating, therefore no narrative is naive. The same narrative can be seen from a different point of view, i.e., from a focal point for seeing, hearing, smelling, and experiencing the accessible. Stripping the data of unnecessary information and making sure that relevant metadata is kept is also key during this process. A case in point related to narrative extracted from news is the publication date, which is relevant to determine its timeline. What follows is designing a suitable annotation scheme that is simultaneously tailored to encompass all the particularities of the target language(s), and comprehensive enough to be applied to other datasets or even be broaden. Bearing in mind, on the one hand, the diversity of annotation frameworks, and, on the other hand, the usefulness of establishing comparisons between annotated corpora from different genres in the same language, but also across languages, many proposals try to achieve this balance utilizing acknowledged standards, which have resulted in, for e.g., Ontologies of Linguistic Annotation (OLiA) (Chiarcos 2014), and ISO 24617-Language resource management-Semantic annotation framework. The decision about the annotation framework, and about the different layers of annotation, depends necessarily on a variety of factors, like the annotation purpose, the texts genre, among others (Pustejovsky et al. 2017). In the case of the narratives annotation, since it is relevant to feature participants, events, time and space, as well the relationships between them, the annotation scheme can be designed to include several intertwined semantic layers enabling temporal, referential, thematic, and spatial annotations (see for example Silvano et al. (2021)). The adequacy of the annotation tool is also of great relevance for the efficiency of different tasks, namely creation, browsing, visualization, and querying of linguistic annotations. Although one can choose to tailor a tool to the specific features of one's project, it may be labor-saving to resort to the existing ones and, if necessary, proceed with some modifications. Some of the existing annotation tools that enable annotating different markables with several attributes, as well as establishing links between those markables are the following: MMAX2 (Müller and Strube 2006), MAE and MAI Stubbs (2011), BRAT (Stenetorp et al. 2012), or ANNIS (Krause and Zeldes 2014). During the process of narrative extraction, some degree of human participation is necessary, either to verify the feasibility of the annotation scheme, to annotate linguistic aspects for which automatic models are insufficient, or to supervise the automatic annotation. Different strategies can be adopted depending, namely, on the type of annotators that the project wants or needs, with or without linguistic training, specifically: crowdsourcing (Estellés-Arolas and de Guevara 2012), class sourcing (Christopher Blackwell 2009) or gamification (Stieglitz et al. 2016). When using human annotation, calculating interannotator agreement (IAA) is crucial to ensure its reliability and accuracy. The two most used metrics in computational and corpus linguistics are: Cohen's Kappa (and its variation, Fleiss's Kapppa) and Krippendorff 's Alpha (Pustejovsky and Stubbs 2012).",
          "The study of narratives from the perspective of natural language texts can be summarized into five major stages: (1) Pre-Processing and Parsing; (2) Identification and Extraction of Narrative Components; (3) Linking Components; (4) Representation of Narratives and (5) Evaluation. Each of these tasks gives rise to the structure adopted in the rest of this survey. An overall picture is shown in Fig. 1, and a detailed description of each task can be found in the following sections.",
          "Pre-processing and parsing of a text comprise a set of lexical and syntactic tasks. Lexical activities aim to split the text into basic units, called tokens, and to normalize them into different forms; syntactical analysis identifies the grammar class of tokens, and identifies the dependency between the tokens, producing trees or chunks that relate the tokens to each other. The following subsections detail both tasks.",
          "Lexical tasks seek to standardize the input text as a way to better prepare the data content for the following steps (Sun et al. 2014). Despite being a simple procedure, they are considered to be a key factor to achieve effective results (Denny and Spirling 2018). The main lexical tasks required for the narrative extraction process are: (1) Sentence segmentation, i.e., dividing the whole text into sentences and tokens to gather the lexicon of the language (Palmer 2007); for this task, using deep learning (DL) methods Schweter and Ahmed (2019) achieved SOTA results through the use of a multi-lingual system based on three different architectures of neural networks. (2) Text tokenization, applied to break sentences into tokens, the smallest unit of a text (Vijayarani et al. 2016); and (3) Text Cleaning and Normalization, which may involve a number of optional cleaning steps, including removing numbers, punctuation marks, accent marks, or stopwords, as well as apply stemming (Jabbar et al. 2020) and lemmatization (Bergmanis and Goldwater 2018) to observe other possible discrepancies between equal or similar words. These are foundational NLP tasks; hence they will not be explored in detail in this article. More details regarding these tasks can be found in Raina and Krishnamurthy (2022).",
          "The lexical analysis precedes a more profound observation, which aims to identify the grammar class of the words and to infer the relationship between words in a sentence to help understand the meaning of a text. To go over this process, we use the text presented in Fig. 2 as a running example in the remainder of this survey. ",
          "The first step in this stage is to assign parts-of-speech tags to each word of a given text (e.g., nouns, verbs, adjectives) based on its definition and its context, which is called the Part-of-Speech (PoS) tagging task. The current state-of-the-art is settled by Bohnet et al. (2018) in several languages. In this work, the authors used recurrent neural networks with sentence-level context for initial character and word-based representations, achieving an accuracy of 97.96% on top of the Penn Treebank dataset Marcus et al. (1993). With respect to narrative texts, since their elements can be typically associated to different word classes (for instance, nouns to participants, verbs to events), POS tagging plays a fundamental role (Palshikar et al. 2019;Quaresma et al. 2019;Yu and Kim 2021). Figure 3 shows the result of applying the Stanzafoot_0 library (Qi et al. 2020) PoS tagger to the first sentence of our running example. In the figure, PROPN refers to a proper noun, PUNCT to punctuation, DET to determiner, VERB to a verb, and ADP to adposition. These tags are defined under the Universal Dependencies guidelines (de Marneffe et al. 2021).",
          "Following PoS tagging, the parsing of the text can be conducted. The chunking task (a.k.a, shallow parsing) is responsible for identifying constituent parts of the sentences (nouns, verbs, adjectives, etc.) and linking them to higher-order units that have discrete grammatical meanings (noun groups or phrases, verb groups, and others). Figure 4 illustrates the results of applying chunking on top of the first sentence of our running example through the use of the benepar library (Kitaev et al. 2019). By looking at the figure, we can observe that both \"a\" (determiner -DT), as well as \"rabbit\" (noun-NN), belong to a higher group of noun phrases (NP). Note that adpositions (tagged as \"ADP\" in Fig. 3) are now represented in Fig. 4 with the \"IN\" tag. As in other tasks, the application of deep learning approaches (Hashimoto et al. 2017;Zhai et al. 2017;Akbik et al. 2018) has brought important improvements to this particular task. The next step of the pipeline is to understand how all the words of the sentence relate to each other, which is done by a dependency parsing analysis. The objective, as referred by Jurafsky and Martin (2009), is to assign a single headword to each dependent word in the sentence through labeled arcs. The root node of the tree, that is, the head of the entire Fig. 2 Running example Fig. 3 Results of applying the stanza PoS tagging processor to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" structure, will be the main verb in the sentence (e.g., \"pulled\" in our running example). This task can enhance the performance of named entity recognition (further discussed in Sect. 4.2) models and be used to increase information extraction (de Oliveira et al. 2022). Figure 5 shows a dependency analysis of the first sentence of our running example alongside the type of relationship predicted to occur between two tokens. The results obtained stem from applying the Stanford CoreNLP Parser (Chen and Manning 2014), one of the most well-known tools in this regard. A quick look at the figure emphasizes the association between several terms, among which we highlight the relationship hierarchy between \"John\" and \"magician\". The current state-of-the-art of this task, in the English and Chinese languages, is settled by Mrini et al. (2020). The model proposed by the authors used a combination of a label attention layer with Head-driven Phrase Structure Grammar (HPSG), and pre-trained XLNet embeddings Yang et al. (2019). In a narrative context, the application of these tasks can be particular useful to identify relations between narrative components (further discussed in Sect. 4). Fig. 4 Results of applying the benepar chunking model to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Fig. 5 Results of applying the Stanford CoreNLP Parser Dependency to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\"",
          "Coreference resolution, also known as anaphora resolution, uses information about the semantic relation of coreferent expressions to infer the reference between them (Jiang and Cohn 2021). In addition to that, all the terms that reference the same real-world entity are marked (Mitkov 2014). For example, the word \"presentation\" (found in sentence 2 of our running example) refers to \"show\" (which can be found in sentence 1). Coreference resolution is essential for deep language understanding, and has shown its potential in various language processing problems (Poesio et al. 2016). Bringing it to the context of narrative extraction, some works highlight the importance of this task in the most different scenarios, such as clinical narratives (Jindal and Roth 2013a), newswire (Do et al. 2015), and also in violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System (Uppunda et al. 2021). In a general domain, several approaches have been proposed over the years (Pradhan et al. 2012;Mitkov 2014;Lee et al. 2013), but recently there has been a shift towards adopting transformers models (Ferreira Cruz et al. 2020;Kantor and Globerson 2019) to improve the connection between textual elements and, as such, enhance the coreference task. The current SOTA of coreference resolution is marked by Attree (2019) and Kirstain et al. (2021), both sticking to deep learning methods.",
          "Table 1 presents the current status of the tasks covered in this section. This includes lexical tasks (sentence segmentation, text tokenization, data cleaning, normalization, stopwords removal, stemming, lemmatization) and syntactic (PoS tagging, chunking, dependency parsing, coreference resolution). Herein, however, we only list those that have been undergoing constant improvements over the most recent years. In this table, the effectiveness of each study is shown according to the measures reported by the authors.",
          "The next step in the narrative extraction pipeline is to identify and extract the main elements that compose a narrative. This step comprises tasks like the detection and classification of events, the recognition and the classification of named entities (participants), the extraction of temporal information and of spatial data.",
          "Finding the events mentioned in the text is an essential step towards the extraction of narratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person. Xie et al. (2008) went a little bit further, and suggested describing real-world events through the use of the 5W1H interrogatives: when, where, who, what, why, and how, anchored on journalistic practices. Based on this reference, an event such as the one referenced in our running example might be depicted along with the six aspects: who-John, the magician; when-2022 (taking into account the date of writing the text); Table 1 State-of-the-art in pre-processing and parsing tasks Task Ref. Method Effectiveness Benchmark dataset Sentence segmentation Schweter and Ahmed (2019) BiLSTM 98.62 F1-score (English) Europarl Corpus Koehn (2005) 99.70 F1-score (Greek) SETimes Alperen and Tyers (2010) 99.86 F1-score (Arabic) Leipzig Corpus Goldhahn et al. (2012) Text tokenization Graën et al. (2018) Language-specific and language independent token identification rules 1% Error rate SMULTRON Treebank Volk et al. (2015) Lemmatization Bergmanis and Goldwater (2018) Standard encoder-decoder architecture + character-level sentence context 92.4 Accuracy (ambiguous words) Universal Dependency Treebank v2.0 Nivre et al. (2020) 84.30 Accuracy (unseen words) Part-of-speech tagging Bohnet et al. (2018) Recurrent network model 97.96 Accuracy Penn Treebank Marcus et al. (1993) Chunking Akbik et al. (2018) Neural character-level language modeling 96.72 F1-score Penn Treebank Marcus et al. (1993) 96.34 F1-Score Dependency parsing Mrini et al. (2020) Label attention layer + HPSG + XLNet 97.39 UAS Penn Treebank Marcus et al. (1993) 96.29 LAS Coreference resolution Attree (2019) Neural attention model 92.50 F1-Score GAP Dataset Webster et al. (2018) where -XYZ contest realized in Edinburgh; what-the magician pleased the audience during his presentation; how-taking a rabbit out of his hat; why-his presentation was better than the last one. The task of event detection has its roots in the late 90's when the Topic Detection and Tracking (TDT) project emerged (Allan et al. 1998) as the first attempt to cope with the rising of huge volumes of data. Among the entire project, the detection of new events, also known as the first story detection (FSD) (Kontostathis et al. 2004), was a subtask of the TDT project, concerned with the detection and subsequent tracking of the first and coming stories of a given news event. One way to detect events is to rely on event triggers, cues that express an event's occurrence, most often single verbs or phrasal verbs, as referred by Boroş (2018), but also nouns, noun phrases, pronouns, adverbs, and adjectives. According to Araki (2018), 95% of the ACE (Automatic Content Extraction) event triggers consist of a single token. This is the case of our running example, where the events pulled, loved, presented are triggered by verbs, and show, contest and presentation are also events represented by nouns. Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argument role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 subtypes, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example. The ACE 2005's predefined event scheme is presented in the left-hand part of the figure. The event trigger (\"pulled\"), the 3 argument roles (\"Arg-Entity\"), and 2 modifiers (\"Arg-Time\" and \"Arg-Place\") identified for the type \"Contact-Meet\" type are illustrated on the right-hand side of the figure. In this visualization, the time-reference \"this year\" is normalized to 2022 based on the document creation time (DCT). The current state-of-the-art in event extraction task for the English language is provided by Feng et al. (2018), who developed a language-independent neural network. The authors tested on top of the ACE 2005 English event detection task (Walker et al. 2006) data, achieving an F1-score of 73.40%. More detailed information about the event detection task can be found in Saeed et al. (2019) and a recent survey from Xiang and Wang (2019). In recent years, the task of monitoring streams of data and detection events has made a shift towards microblogs platforms, such as Twitter (Atefeh and Khreich 2015), a very different scenario when compared to the TDT era, when algorithms were developed to track news stories over time from traditional media like newspapers (Kalyanam et al. 2016). The detection of events also plays a fundamental role in different kinds of applications, domains, and languages, as is the case of clinical narratives (Jindal and Roth 2013b;Adams et al. 2021), and historical events (Lai et al. 2021). Despite the significant advances in the last few years, the application of event extraction techniques in the context of narrative representation has been quite limited (Metilli et al. 2019), and only recently a few works began to emerge. Metilli et al. (2019), for example, made use of numerous novel discourse and narrative features, besides common relations, such as the time at which the event takes place, or coreference events, which detects whether two mentions of events refer to the same event (Araki 2018). Aldawsari and Finlayson ( 2019) presented a supervised model to automatically identify when one event is a sub-event of another, a problem known as sub-event detection or event hierarchy construction. Another strand tackles the problem of nested event structures, a common occurrence in both open domain (not limited to a single topic or subject) and domain specific (dedicated to a particular problem representation or solution) extraction tasks (McClosky et al. 2011). For instance, a \"crime\" event can lead to an \"investigation\" event, which can lead to an \"arrest\" event (Chambers and Jurafsky 2009). The research of Metilli et al. (2019) applied a recurrent neural network model to event detection applied to biography texts obtained from Wikipedia. The model used a set of event categories (e.g., birth, conflict, marriage, and others) to depict a narrative. In the context of news stories, Zahid et al. (2019) proposed the development of heuristics to segment news according to a scheme that defines the organization and order of the events. The strategy followed in that research employed mechanisms that journalists usually exploit to compose news.",
          "Another essential category of narrative elements is the participants. They are the \"who\" of the story, relevant to the \"what\" and the \"why\". Participants, sometimes also referred to as actors, are involved in events in a varied number of ways and often correspond to entities in NLP. These, are usually identified through Named Entity Recognition (NER), a generic task aiming at seeking, locating and categorizing the entities mentioned in the text into predefined categories. Although the three most common named entities are person, organization, and location, further types such as numeric expressions (e.g., time, date, money, and percent expressions) can also be considered (Nadeau and Sekine 2007). In narrative terms, participants are more frequently restricted to the person and organization categories, though other categories such as animals may also be used. In our running example this would result in annotating \"John\" and \"audience\" as a person, and \"rabbit\" as an animal as can be observed in Fig. 7. Overall, research on named entity recognition can be categorized into four main categories: rule-based approaches, unsupervised learning approaches, feature-based supervised learning approaches, and deep-learning-based approaches (Li et al. 2022). Beyond this, other aspects, such as textual genres, entity types, and language, are also considered by research conducted on named entity identification (Goyal et al. 2018). The current stateof-the-art in named entity recognition was obtained by Wang et al. (2021) on the CoNLL 2003NER task (Tjong Kim Sang and De Meulder 2003). To automate the process of finding better concatenations of embeddings for structured prediction tasks, the authors proposed Automated Concatenation of Embeddings (ACE) based on a formulation inspired by recent progress on neural architecture search. The method achieved an F1-score of 94.60%. In narrative contexts, Lee et al. (2021) present a proposal that goes beyond the identification of entities in narratives, proposing a multi-relational graph contextualization to capture the implicit state of the participants in the story (i.e., characters' motivations, goals, and mental states). Piper et al. (2021) point out that NLP works focusing on agents have emphasized broadening the understanding of characters beyond named entities. This has been done through the concept of animacy detection, as in agents like \"the coachman\" or \"the frog\", while also distinguishing characters from other named referents (Piper et al. 2021). According to Oza and Dietz (2021)'s proposal, selecting a set of relevant entities for a story construction could be best achieved by using entity co-occurrences in retrieved text passages -especially when the relative relevance of passages is incorporated as link strength. In line with what the authors claim, their proposal is between 80% and 30% more effective than the best link-based approach.",
          "Temporal references are of the utmost importance for the understanding of the narrative and its timeline by anchoring each event or scene at a point in time. The process of identifying temporal references is commonly referred to as Temporal Tagging, and can be split into two sub-tasks: extraction and normalization (Strötgen and Gertz 2013). The former aims to correctly identify temporal expressions, and can be seen as a classification problem. The latter aims to normalize the identified temporal expressions, and can be seen as a more challenging process where different temporal expressions, carrying out the same meaning, need to be anchored at the same time-point. In this regard, three temporal taggers take the lead, Heideltime (Strötgen and Gertz 2013), SuTime (Chang and Manning 2012) and GuTime (Mani and Wilson 2000), which support the four basic types of temporal objects defined by Pustejovsky et al. (2005) in TimeML, the standard markup language for temporal annotation containing TIMEX3 tags for temporal expressions: Dates (e.g., \"December 3, 2021\"), Time (e.g., \"5:37 a.m.\"), Durations (e.g., \"four weeks\", \"several years\"), and Sets (e.g., \"every day\", \"twice a month\"). Alongside with these, other realizations of temporal expressions can be found in a text (Strötgen et al. 2012;Campos et al. 2017), which pose additional challenges. We refer to explicit (e.g., \"April 14, 2020\"), implicit (e.g., \"Christmas day 2019\"), and relative temporal expressions (e.g., \"yesterday\"), a kind of temporal expression that requires further knowledge to be normalized. A discussion of the challenges associated with each one of them can be found in Strötgen andGertz (2013, 2016). Other researchers (Jatowt et al. 2013) devised methods concerning document dating of non-timestamped documents. The rise and continuous development of temporal taggers has paved the way to the emergence of upstream research at the intersection between information extraction and several NLP tasks, where temporal expressions play an important role. For example, in the information retrieval domain, temporal information (Campos et al. 2014) can be used for temporal clustering of documents (Campos et al. 2012) or temporal ranking (Berberich et al. 2010;Campos et al. 2016); in question answering to query knowledge bases (Sun et al. 2018); in document summarization to construct timeline summaries (Campos et al. 2021); and in web archives to estimate the relevance of past news (Sato et al. 2021). At this stage of identification and extraction of narrative components, several works address the intersections between the tasks of identifying and extracting events, entities, and temporal information. Strötgen and Gertz (2012) explored event-centric aspects, considering related temporal information. In particular, they introduced the concept of event sequences, a set (or sequence) of chronological ordered events extracted from several documents. In their work, the geographic dimension played an important role by mapping event-sequences onto a map as well. In contrast, studies such as Agarwal et al. (2018) and Rijhwani and Preotiuc-Pietro (2020) explored the task of recognizing named entities considering time-aware aspects. The incipient annotation of documents to support related research has also led to the development of markup languages (e.g., TimeML (Pustejovsky et al. 2005), a formal specification language for temporal and event expressions anchored on TIMEX3 tags), annotated corpora (e.g., TimeBank corpus (Pustejovsky et al. 2006), a standard set of English news articles annotated with temporal information under the TimeML 1.2.1 (Saurı et al. 2006) guidelines) and research competitions [e.g., TempEval series (Lim et al. 2019)].",
          "The problem of extracting geographic references from texts is longstanding (Martins et al. 2008), and is highly related to the field of Geographic Information Retrieval (GIR) (Purves et al. 2018). Much of this information can be found in unstructured texts through references to places and locations (Purves et al. 2018). Many difficulties arise when attempting to comprehend geographic information in natural language or free text (e.g., under-specified and ambiguous queries) (Purves et al. 2018). Challenges related to space involve the detection and resolution of references to locations, typically, but not exclusively, in the form of place names, or more formally toponyms, from unstructured text documents (Jones and Purves 2008). In our running example, to extract the location of the main event, one can consider \"contests near Edinburgh\", which consists of three important parts, a theme (contests), a spatial relationship (near), and a location (Edinburgh). However, in the example, it is unclear whether there is only one contest in the city or multiple ones, and, if so, to which one the text is pointing to. The spatial relationship \"near\" also brings more questions than answers, making it difficult to understand whether it refers to downtown Edinburgh or some constrained space within the state of Edinburgh. Spatial relationships such as these may be both geometric (obtained using coordinate systems imposed on the real world, such as latitude and longitude) and topological (spatially related, but without a measurable distance or complete direction) (Larson 1996). The current SOTA in toponym detection and disambiguation was achieved by Wang et al. (2019) on the SemEval 2019 Task 12 (Weissenbacher et al. 2019), taking as a basis PubMed articles. In their approach, called DM_NLP, the authors made use of an ensemble of multiple neural networks. In a different approach, Yan et al. (2017) proposed a solution called augmented spatial contexts that learns vector embeddings and uses them to reason about place type similarity and relatedness. When considering space in a narrative context, other issues arise. Geographic maps, landscape paintings, and other representations of space are not always narratives, yet all narratives presuppose a world with spatial extension, even when spatial information is concealed (Ryan 2014 [Online]). Generally speaking, a narrative space is a physically existing environment in which characters live and move. This space is usually identified as a setting, that is, the general socio-historic-geographical environment in which the action takes place (Ryan 2014 [Online]), i.e., grounded by referents in the real world (e.g., the Edinburgh city in our running example) or entirely fictional (Rohan kingdom in The Lord of Rings). Aiming to identify places, some works (Bamman et al. 2019;Brooke et al. 2016) have been exploring entity recognition and toponym resolution in the context of narratives, making it possible to recognize named locations, facilities, and geopolitical entities (Edinburgh, Rohan) within this kind of text (Piper et al. 2021). Another problem is related to coreference resolution. Unlike coreference resolution of named locations, long-document coreference of common items (e.g., the house and the room), which form the narrative universe for many fictional creations, can be tough. As indicated by Piper et al. (2021), many questions can only be addressed by calculating the distance between locations described in the text: how far do Frodo and Sam go on their journey? Systems that allow better inference on spatial information within narratives could provide important insights to get the reader closer to the narrative universe.",
          "Table 2 brings a summary of the current status of the tasks covered at the identification and extraction stage discussed in this section. In this table, the effectiveness of each study is shown according to the measures reported by the authors. The type column specifies the type of task addressed in the works. All the works discussed here refer to a semantic analysis of the text. This contrasts with Table 1 where semantic tasks, but, above all, lexical and syntactic, were addressed. In our supplementary material, made available with this survey, one can find additional benchmark datasets relating to the topics covered in this section. This material has datasets available that can be used as a reference for carrying out the tasks described here. Some of these datasets are available in other languages and might help identify and extract narrative components also in these languages.",
          "After the process of identifying and extracting narrative components, it is crucial to extract the link relations between such pieces. Linking narrative components comprises the core of the extraction of narratives from the text. More than extracting separate elements from texts, establishing relations and detecting structures from them becomes essential to fully understand their meaning. In this stage, extracted pieces of information are connected, structuring the narrative at a global level. Thus, temporal and event linking, entity and  ",
          "Despite concentrating the research community's attention, predicting temporal and causal relations between events and temporal references, and investigating the evolution of an event over time as a whole (Kalyanam et al. 2016), remains a major challenge of the text understanding task (Leeuwenberg and Moens 2018) that goes beyond merely identifying temporal expressions in text documents. Ultimately, understanding a text with regards to its temporal dimension requires the following three steps: (1) identification of temporal expressions and events; (2) identification of temporal relations between them; and (3) timeline constructions. An overview of each one of them is shown in Fig. 8 for our running example. Given an input text, the system determines the temporal expressions and the events (top). Next, it assigns TimeML annotations (middle). Finally, it produces a timeline as an output (bottom). Figure 8a begins by showing the input text. By looking at it, one can easily detect \"this year\" temporal expression (normalized to 2022 assuming that the Document Creation Date-DCD-refers to November 21st, 2022) and the chronology of the events. In our example, the event pulled is carried out by a magician called John. During the show, he pulled a rabbit out of a hat. The presentation was more welcomed by the public than another previous presentation. Despite all the information collected, it is not clear whether the \"last one presented\" (event) was held during the present edition or last year's contest, or even if the \"show\" consisted of only \"pulling a rabbit out of a hat\". Ambiguity in the text might also raise doubts about whether the previous presentation refers to John or another magician. In Fig. 8b, we can observe a possible interpretation of the relation phase, where temporal links are established between the temporal expressions and the events. Finally, in Fig. 8c, we can see the generated timeline. In this visualization, one can observe a reference to the document creation time, the present time (reading time), and the ordering of the events according to the time-reference \"this year\" normalized to 2022. This apparently simple example shows how difficult it may be to understand a text in detail. In the following subsections, we present research tackling the identification of temporal expressions, events and their relations, before exploring timeline constructions. Mostafazadeh et al. (2016b), in their work, proposed a pipeline strategy for extracting events and linking them temporally, without building a timeline. The authors demonstrated that events that follow a protocol or script could be related to each other through temporal associations. These relationships are based on temporal reasoning, and provide a broad understanding of language. In another study, Roemmele and Gordon (2018) applied a neural encoder-decoder to detect events before predicting the relations between adjacent events in stories. The authors assumed that subsequent events are temporally related, and as such the extraction of events and their respective relations can be inferred. The goal is to model events that are the cause of other events and also events that are the effect of other events. Similarly, Yao and Huang (2018) deemed that the order of events in a text is the same as that of the temporal order of them in a narrative. Based on this assumption, the authors used a weakly supervised technique to propose a novel strategy to extract events and temporal relations between events in news articles, blogs, and novel books across sentences in narrative paragraphs. In another proposal, Han et al. (2019a) introduced a strategy where the extraction of events and temporal relations was jointly learned. Hence, the information about one task is employed to leverage the learning of the other. This strategy selected shared representation learning and structured prediction to solve the tasks. The results achieved were superior to the previous works. Ning et al. (2017) suggested a structured learning approach to identifying temporal relations in natural language text. Their solution was evaluated on top of the TempEval-3 data (UzZaman et al. 2013), achieving a temporal awareness [metric provided by UzZaman and Allen (2011)] of 67.2%. In the health domain, Tang et al. (2013) developed a temporal information extraction system capable of identifying events, temporal expressions, and their temporal relations found in clinical texts. In this same domain, Leeuwenberg and Moens (2020) proposed an annotation scheme for extraction of implicit and explicit temporal information for clinical reports identified events, which provides probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. A comprehensive overview of the research conducted over the years on identifying events, time references, and properly connected them is given by Derczynski (2016).",
          "",
          "The next step, after establishing the relationships between events and the temporal entities, is to arrange temporal information in such a manner that the remaining narrative components can be organized with regards to time (Leeuwenberg and Moens 2019). One important aspect of this, as stated by Leeuwenberg and Moens (2019), is the temporal reasoning task, which refers to the process of combining different temporal cues into a coherent temporal view. This is of the utmost importance as previous works have shown that the information presented in the text may not match the elapsed order (Mostafazadeh et al. 2016b). Getting to know the correct order of the events may also enable to dive deeply in the analysis of particular components. Antonucci et al. (2020) employed this notion to investigate the evolution of characters of books over time. The authors trained word embeddings related to different characters in distinct parts of literary texts (e.g., chapters). These embeddings are also called dynamic or temporal embeddings (Bamler and Mandt 2017) since, from them, it is possible to analyze relations between concepts over time. This work demonstrated that characters with strong friendships have similar behavior; therefore, such characters evolve similarly over time. Another outcome of the temporal reasoning task is building the network of events in chronological order, i.e., the text's narrative. Over the years, different researches (Bethard et al. 2007;Mani and Schiffman 2005) have been proposed with one such goal. In Leeuwenberg and Moens (2018), the authors proposed two models that predict relative timelines in linear complexity and new loss functions for the training of timeline models using TimeML-style annotations. On the sidelines, Jia et al. (2021) proposed the use of temporal information as a way to provide complex question answering related to temporal information through knowledge graphs. An overview of the temporal reasoning task, on the extraction, and on how to combine temporal cues from text into a coherent temporal view is presented in a survey of Leeuwenberg and Moens (2019).",
          "Capturing the linking between the pairs of identified entities (or participants) is an important aspect of the narrative understanding. Known as relation extraction task (Qin et al. 2021), it attempts to determine a semantic link between two named entities in a sentence. Performing this task on top of our running example would result in identifying an instance of a pulled relation between \"John\" and \"a rabbit\". Such semantic relations are usually structured in the form of < e 1 , rel, e 2 > triples, where e 1 and e 2 are named-entities, and rel is a relationship type (Batista 2016 [Online]). In our case, it could be exemplified by the triple: < John, pulled, a rabbit > . Considering that in narratives, events are often directly linked, extracting such relationships is also of paramount importance. Semantic relations also encompass objectal relations (i.e., relations between discourse entities seen as extra-linguistic concepts). These relations aim to state how two discourse entities are referentially related to one another (ISO 24617-9:2019 (E) 2000). Other NLP tasks, like Question-Answering Systems (Li et al. 2019), and the creation of Knowledge Graphs (Zhang et al. 2019b), can benefit from identifying such a relation. An example of entity relation extraction employed to build Knowledge Graphs is provided in Han et al. (2019b). In this work, the authors present OpenNRE, an open-source and expandable toolkit for implementing neural models for relation extraction. Using this tool, one can train custom models to extract structured relational facts from the plain text, which can be later used to expand a knowledge graph. The traditional relation extraction can be solved by applying one of the following approaches: (1) rule-based; (2) weakly-supervised; (3) supervised; (4) distantly supervised; and (5) unsupervised. In addition to this type of relation extraction, one can also extract semantic relationships by following Open Information Extraction approaches (OIE) (Batista 2016 [Online]), a research field that obtains domain-independent relations from a text. This task is supposed to extract all kinds of n-ary relations in the text (Xavier et al. 2015). Two methods can be applied by the Open Information approaches (1) rule-based; and (2) data-based. The former relies on hand-crafted patterns derived from PoS-tagged text or dependency parse tree rules. The latter generates patterns based on training data represented as a dependency tree or PoS-tagged text. Over the last few years, neural networks have also made their path in the particular domain of relation extraction (Qin et al. 2021), through different neural network approaches (Baldini Soares et al. 2019;Hendrickx et al. 2010;Nadgeri et al. 2021). Exploiting the narrative structure, (Tang et al. 2021) proposed a Multi-tier Knowledge Projection Network (MKPNet). The strategy was designed to leverage multi-tier discourse knowledge and present a knowledge projection paradigm for event relation extraction. According to the authors, such a paradigm can effectively leverage the commonalities between discourses and narratives for event relation extraction. This study had a focus on the projection of knowledge from discourses to narratives. (Lv et al. 2016) also proposed a strategy that takes advantage of the narrative structure. The authors applied an autoencoder to a set of features, including word embeddings, in a clinical narrative dataset, the 2010 I2B2 relation challenge (Uzuner et al. 2011). Using the same dataset, the 2010 I2B2 relation challenge, and the 2018 National NLP Clinical Challenges (N2C2) dataset (Henry et al. 2020), Wei et al. (2019) proposed two different models to extract relations from clinical narratives. The models used a language model based on transformers to extract the features, and BI-LSTM neural network attention to detect the relations. Another example comes from finances related narratives, in this context a shared task was proposed by Mariko et al. (2022a) with the intend to extract causality between existing relations. In this shared task experiments were done on top of FinCausal dataset (Mariko et al. 2022b), a dataset extracted from different 2019 financial news. The best results were achieve by a team that developed an ensemble of sequence tagging models based on the BIO scheme using the RoBERTa-Largemodel, which achieved an F1 score of 94.70 to win the FinCausal 2022 challenge. In addition to these types of connections, entities can also be also linked to other kinds of data. Knowledge databases can furnish a unique identity to entities, which can enrich the narrative with information, and aid to disambiguate the entities as well. The task of linking knowledge base-entities and narrative entities is called entity linking. In the following section, we will discuss this task.",
          "Over the years, efforts have been developed to explore relations between entities in texts (Freitas et al. 2009;Hasegawa et al. 2004). This willingness to recognize individuals in a document is a significant move towards understanding what the document is all about (Balog 2018). Truly understanding a text requires, however, linking these individuals with other pieces of information. The temporal connection between events is one way to achieve this. Another way is to relate the events of the narrative with entities from a knowledge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008). Formally, the process of linking entities can be understood as the task of recognizing (named entity recognition), disambiguating (named entity disambiguation [Eshel et al. 2017)], and linking entities (named entity linking) with unique entity identifiers from a given reference knowledge-base, such as DBpedia (Auer et al. 2007), or Yago (Suchanek et al. 2007). Given the sentence \"Bush was a former president of the USA from 1989 to 1993\", the idea is to determine that \"Bush\" refers to George H. Bush, an American Politician who served as the 41st president of the United States from 1989 to 1993, and not to \"Bush\" a British rock band formed in London, England in 1992. When connecting an entity with Wikipedia data, such as Wikidata (Vrandečić and Krötzsch 2014), this task is known as Wikification. As emphasized by Szymański and Naruszewicz (2019), when a document is Wikified, the reader can better understand it because related topics and enriched knowledge from a knowledge base are easily accessible. From a system-to-system view, the meanings of a Wikified document's core concepts and entities are conveyed by anchoring them in an encyclopedia or a structurally rich ontology. In the context of a narrative, it might mean a more knowledge-rich narrative. Figure 9 illustrates the results of applying entity linking on top of our running example. By looking at the figure, one can observe the linkage of the detected entities or events in the text with their presence on external sources, such as knowledge bases. One can get insights into who is \"John\", \"Edinburgh\" or the \"XYZ contest\". This information might be helpful not only to add more knowledge but also to disambiguate the case of entities that are associated with more than one concept. In the scope of entity linking efforts, Raiman and Raiman (2018) achieved the state-ofthe-art focusing on a cross-lingual approach, a type system on an English dataset supervised with French data. In particular, they constructed a type system and used it to constrain a neural network's outputs to respect the symbolic structure.",
          "Understanding narrative stories involves the ability to recognize events and their participants, as well as to recognize the role that a participant plays in an event, as \"'who ' did 'what' to 'whom\"\", and 'when', 'where', 'why', and 'how' (ISO 24617-4:2014 ). Generally, this can be achieved by Semantic Role Labeling (SRL). This task assigns semantic roles to the constituents of the sentence (Aher et al. 2010). Contrary to syntactic analysis Fig. 9 Overview of entity linking task result when applied to our running example (such as the ones conducted on chunking, dependency parsing, etc.), it acts on a semantic level, and is responsible for capturing predicate-argument relations, such as \"who did what to whom\"' (He et al. 2018), towards making sense of a sentence's interpretation. SRL recovers the latent predicate-argument structure of a sentence, providing representations that answer basic questions about a sentence's meaning. Figure 10 illustrates the results of applying SRL on top of the first sentence of our running example, through the use of AllenNLPfoot_1 . model, which is the implementation of Shi and Lin (2019). The relation established between the different parts of the sentence can be observed by looking at the figure. In this relation, all the parts are linked by the verb \"pulled (out)\": \"John, the magician\", \"a rabbit\" and \"of a hat\" are the core arguments, while \"at the show in Edinburgh\" and \"this year\" are modifiers, the former conveying spatial information and the latter temporal information. The current state-of-the-art of this task is settled by He et al. (2018) with an F1-score of 85.5%, using a deep learning approach based on a BiL-STM neural network to predict predicates and arguments on top of the OntoNotes benchmark (Pradhan et al. 2013). In a narrative-focused approach, Mihaylov and Frank (2019) proposed the use of linguistic annotations as a basis of a discourse-aware semantic self-attention encoder for reading comprehension on narrative texts. In this work, the authors adopted the 15 fine-grained discourse relation sense types (see section 5.5 for more information about discourse relations) from the Penn Discourse Tree Bank (PDTB). According to what is mentioned by the author, combining such an annotation scheme with self-attention yields significant improvements. Following this approach, this study's results indicate that SRL significantly improves who and when questions and that discourse relations also improve the performance on why and where questions. These results are illustrated on top of the Narra-tiveQA (Kočiský et al. 2018) reading comprehension.",
          "When reading or listening to a text, the reader/listener establishes relations of meaning between the different parts, be they clauses, sentences, or paragraphs. These relations are discourse relations (DRels)-also known as rhetorical relations or coherence relationsand are crucial to explain how discourse is organized. For that reason, they have been the basis of several frameworks, such as Rhetorical Structure Theory (RST) (MANN and Thompson 1988), Segmented Discourse Representation Theory (Asher et al. 2003).",
          "Semantic labeling result when applied to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Looking at our running example, since no connective is present, the identification of the DRel would be very challenging using automatic methods. However, taking into consideration lexical and other semantic information, as well as our world knowledge, one can infer that the two sentences are related by the DRel Result, because loving the magician's presentation is a consequence of him pulling a rabbit out of the hat. The second sentence can be divided into two arguments/ text unities related by Comparison. Figure 11 illustrates the annotation of this example using an RST markup tool (O 'Donnell 2000). In NLP, the process of uncovering the DRels between text units is called discourse parsing, a very complex task, despite a few advances over the past years. One of the most recognized discourse parsers is the one used by the news corpus Penn Discourse Tree Bank (PDTB) (Prasad et al. 2018). This end-to-end discourse parser (LIN et al. 2014) follows the same steps as PDTB human annotators: identification of the discourse connectives, arguments segmentation, labeling them Arg1 and Arg2, and recognition of the explicit DRel. When no explicit relation is extracted, the second step labels the implicit DRels. The last step consists in labeling the attribution spans (see Potter (2019) for more information about attribution relations), which are text units that reveal if the content should be attributed to the writer or to another participant. The 2021 edition of the DISRPT Shared Task (Zeldes et al. 2021) added the task of Discourse Relation Classification across RST, SDRT and PDTB, to the two existing tasks from the previous edition (Zeldes et al. 2019), Elementary Discourse Unit Segmentation and Connective Detection. The Shared Task was performed in relation to 16 datasets in 11 languages. Overall, the system with the best results was DisCoDisCo (Gessler et al. 2021) with a Transformer-based neural classifier, which was able to surpass state-of-the-art scores from 2019 DISRPT regarding the first two tasks and to obtain a solid score on the 2021 benchmark for the third task. Establishing DRels between events, such as cause, result, or temporal sequence, is of paramount importance to understand the narrative. Much has been achieved with manual annotation of datasets, which has been the basis for the development of some Discourse Parsers, namely within shared tasks (see LI et al. (2021), for a review and future trends). However, many issues need further research so that Discourse Parsing can fully contribute to narrative extraction, in particular concerning the identification of implicit DRels and the specification of the high-level discourse structures. Table 3 State-of-the-art in information linking tasks Task Ref. Method Effectiveness Benchmark dataset Temporal and event linking Ning et al. (2017) Structured Learning 67.20 Temporal Awareness TempEval-3 UzZaman et al. (2013) Named entity linking Raiman and Raiman (2018) (1) Type system, and (2) use it to constrain the outputs of a neural network to respect the symbolic structure. 94.88 micro-precision CoNLL 2003 NER task Tjong Kim Sang and De Meulder (2003) 90.85 micro-precision F1-Score Relation extraction Baldini Soares et al. (2019) BERT-based Model 89.5 F1-score SemEval-2010 Task 8 Hendrickx et al. (2010) Semantic role labeling He et al. (2018) BiLSTM 85.50 F1-score OntoNote Pradhan et al. (2013) Discourse relation parsing Gessler et al. (2021) Transformer-based neural classifier EDU segmentation on Treebanked data -91.48 F1-score (average) Connective detection on Treebanked data -91.22 F1-score (average) Relation classification on Treebanked data -61.82 F1-score (average) DISRPT 2021 Zeldes et al. (2021)",
          "Table 3 brings a summary of the current status of five of the tasks (temporal reasoning, entity relation extraction, entity linking, semantic role labeling, and discourse relation parsing) covered at the information linking stage discussed in this section. The type column specifies the type of task addressed in those works. In this table, each study's effectiveness is displayed according to the measures reported by the authors. All the discussed tasks are related to semantic analysis at this pipeline stage. In our supplementary material, made available with this survey, one can find a list if additional benchmark datasets relating to the topics covered in this section. The datasets mentioned there can be used as a reference for carrying out the tasks described here. Some of these datasets are available in languages other than English and are useful for the tasks of linking narrative components in these other languages.",
          "The representation of narratives can be categorized in two levels, the conceptual and the visual level. The first one considers that the elements in a narrative are codified as concepts that allow abstracting the meaning. This codification allows the exploration and detection of the elements of a narrative, smoothing the process of analysis of a narrative. The second level of representation is built using visual elements like lines, graphs, icons, pictures, or other graphical resources used to depict the narrative. This level of representation is more accessible for people of different degrees of general expertise. Next, we detail both levels of representation.",
          "Ontologies are a flexible and complex framework in computer science that allows building schemes to represent several kinds of concepts. They aid the multi-layered meanings of a narrative to be pictured as faithfully as possible (Ciotti 2016) since the main elements are conceptually identified. Over the years, some ontologies have been proposed as a means to representing narratives. Khan et al. (2016), for example, proposed an ontology for narratives applying it to Homer's Odyssey. Damiano and Lieto (2013), in turn, described an ontology for narratives for the hero's journey, which is a traditional archetype of the histories of the western culture. The ontology is applied in digital archives for artwork, and then to aid the browsing through the digital artifacts. Tests were made in a small dataset, and some information about artworks was obtained by reasoning. In another proposal for cultural and historic events, Meghini et al. (2021) depicted the Narrative Ontology (NOnt). The authors built an ontology that comprises some rules, like laws of physics. Then, they applied it to heritage crafts, and case studies were conducted to validate the proposed model. Although, ontology is a flexible framework, building an ontology that encompasses all the elements in a narrative is a cumbersome task. Thus, few proposals undertake this challenge.",
          "Representing and learning common sense knowledge for the interpretation of a narrative is one of the fundamental problems in the quest for a profound understanding of language (Mostafazadeh et al. 2016a). Logic-based representations, like DRT (Kamp and Reyle 1993) and Abstract Meaning Representation (AMR) (Banarescu et al. 2013), express recursive formal meaning structures that have a model-theoretic interpretation, although following different formalizations. DRT adopts a dynamic and compositional perspective, which entails determining how the meaning of a sentence can change the context, and enables a straightforward and elegant representation of not only anaphoric relations, both nominal and temporal, but also of other linguistic phenomena. Within DRT's framework, the processing of the discourse is performed one sentence at a time in Discourse Representation Structures (DRSs) in an incremental manner. Each sentence is represented in the DRSs by discourse referents, which represent entities in the discourse, always displayed at the top of the box, and by conditions, which establish a relation of identity between the discourse referents and the corresponding element of the sentence, typically displayed below the universe constituted by the discourse referents. These DRSs are recursive formal meaning structures that have a model-theoretic interpretation, and can be translated into firstorder logic (FOL). Asher (1993) and Asher et al. (2003) extended DRT with the inclusion of discourse relations and with contributions from other dynamic semantic proposals and pragmatics, naming it Segmented Discourse Representation Theory (SDRT). The aforementioned semantic representations are used by the Groningen Meaning Bank (GMB) (Basile et al. 2012;Bos et al. 2017) and the Parallel Meaning Bank (PMB) (Abzianidze et al. 2017). While PMB adopts DRT as a single semantic formalism to fully represent the different annotated linguistic phenomena, GMB goes further and enhances DRT semantics with discourse relations between DRSs, using Boxer (Bos 2015;Curran et al. 2007), a system which utilizes -calculus to generate the meaning representation of texts. Another logic-based representation, but for sentences, is the AMR (Banarescu et al. 2013). The sentence is represented as a single-rooted graph (Damonte et al. 2017). The syntax of AMRs can be defined recursively, and it is possible to specify a systematic translation to first-order logic. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The AMR Bank (Banarescu et al. 2013), based on this form of representation, is a set of English sentences paired with simple, readable semantic representations in some cases manually constructed by human annotators, but also generated in a semi-automatic manner (Gruzitis et al. 2018). It is possible to represent narratives using AMR as well. Droog-Hayes et al. (2018), for instance, employ AMR to represent Russian folktales and extract the narrative structure.",
          "Narrative visualization addresses the interplay between narrative and visualization. This includes perspectives that range from the use of visualization elements to enrich traditional narratives, to the exploration of narrative techniques in visualization-rich artefacts. In this section, we focus on the visual and structured representation of a narrative as an important step, not only as a final output of the narrative extraction pipeline, but also as a machinereadable representation that can be used as a tool for human inspection and validation of each of the pipeline steps. Broadly speaking, narrative visualization can be mapped into 7 genres (Segel and Heer 2010): magazine-style, annotated chart, partitioned poster, flow chart, comic strip, slide show, and film/video/animation. As stated by the authors, such genres vary primarily in terms of the number of frames -distinct visual scenes, multiplexed in time and/or space -that each contains, and the ordering of their visual elements. None of these genres are mutually exclusive: they can act as components and be combined to create more complex visual genres. A less explored area is the visualization of narratives themselves, a perspective of particular interest in the context of this survey, where the focus is on the representation of the extracted narrative as closely as possible to its original form. Metilli et al. (2019) developed a semi-automatic software that can import knowledge from Wikidata to allow users to construct and visualize narratives, based on a proposed ontology to annotate narratives. Baikadi et al. (2011), in turn, proposed a framework for visualizing a narrative, and introduced an environment designed to explore narrative visualization to support novice writers. Milon-Flores et al. (2019), instead, applied algorithms to extract emotions and characters involved in literary work to propose a methodology that is able to generate audiovisual summaries by the combination of emotion-based music composition and graph-based animation. The representation of narrative participants and events over time can be accomplished by projecting these elements in two-dimensional layouts. Munroe (2009) introduced the concept of a narrative chart, a visual representation that encodes narrative elements and interactions in a representation where lines flows from left (past) to right (future) representing each participant, and events are depicted using ellipsis to which participants are connected. Kim et al. (2018) presented the Story Explorer, a visualization tool to explore and communicate nonlinear narratives through the representation of story curves, which contrast story order (y-axis) with narrative order (x-axis). Story Explorer is used to analyze and discuss narrative patterns in 10 popular nonlinear movies. Less elaborate visual schemes can also be used to represent the elements from a narrative and provide an accessible understanding of it. A simpler, but powerful resource to represent the elements of narrative, their relationships, and the chronological order of them is based on the use of knowledge graphs (Ehrlinger and Wöß 2016) and their linkage to knowledge-bases (von Landesberger et al. 2011). Li et al. (2018), for instance, employed a graph structure to extract and link narrative events. Amorim et al. (2021) in their work proposed the Brat2Viz tool to build a visual representation of a knowledge graph from a DRT representation of a narrative. A useful visual representation, in this context, is also the Message Sequence Chart (MSC) (Harel and Thiagarajan 2003), a diagram created to specify systems requirements. Due to its flexibility, MSCs are also suitable to schemetize other kinds of processes. In Brat2Viz tool (Amorim et al. 2021), the authors generates a MSC from a DRT as well. Further initiatives also explore MSC for narrative visualizations (Palshikar et al. 2019;Hingmire et al. 2020). A discussion of the challenges associated with narrative visualization, and the general field of information visualization, can be found in de Ponte Figueiras (2016), and also in Tong et al. (2018). The work of Edmond and Bednarz (2021) also addresses the multidisciplinary nature of the field and outline possible trajectories based on the emphasis put on the narrative itself or the data visualization components used.",
          "With the growing maturity and understanding of the narrative extraction process, new procedures have come into play to help formalizing the narrative extraction evaluation step. In particular, shared tasks were proposed to establish a common experimental setup and overcome the lack of datasets. In this section, we discuss the evaluation carried out by studies developed in the area. Representations made of the extracted narratives need to be evaluated for their level of abstraction to the application (e.g., health care systems, games, news, etc.). The evaluation of the extracted narrative is an essential step of its understandability (Riedl and Young 2010) because it aims to observe the content's understanding by the final consumers and specialists, which can provide insights and possible improvements. However, computational evaluation presents hard challenges. In the literature, few studies evaluate narratives computationally extracted or generated since this is a subjective and application-specific task. In cases where the evaluation is considered, a manual evaluation method is usually adopted. In this case, the narrative result is delivered to specific (usually hired) people, who evaluate the generated result from pre-defined domain criteria, as it is done in Motwani et al. (2019), and give their feedback and overall perception of it. However, ways of automatically evaluating the extracted narratives are sought. Goyal et al. (2010), for example, evaluated the representation of narratives texts through plot units measuring the F-score achieved by the model. Other works (Metilli et al. 2019;Zahid et al. 2019) also assessed the tasks developed in the context of narratives in terms of this measure. Research works that use this measure have the advantage of observing a given task's performance isolated, i.e., separately. Narratives, however, need to be evaluated as a whole. Narrative Cloze Test is a common evaluation framework for script learning introduced by Chambers and Jurafsky (2008). Based on the cloze task (Taylor 1953), the narrative cloze evaluation approach consists of a sequence of narrative events in a document where one event has been removed. The evaluation is then done by predicting the missing verb and typed dependency. In the context of narratives, this approach became widely used to explore the commonsense reasoning of narratives outcome. In a similar approach, Mostafazadeh et al. (2016a) proposed the Story Cloze Test. The authors presented this measure as a generic story understanding evaluation framework that can also evaluate story generation models (e.g., by calculating the log-likelihoods assigned by the story generation model to the two ending alternatives), which does not necessarily indicate a requirement for explicit narrative knowledge learning. According to the authors, models that perform well in the Story Cloze Test reveal some deeper understanding of the story. In this work, Mostafazadeh et al. (2016a) also presented a corpus of 50k five-sentence commonsense stories (ROCStories (Mostafazadeh 2016)) developed to enable a brand new framework for evaluating story understanding -bringing a great contribution to the narratives field. Another form of evaluating narratives was proposed by Kočiský et al. (2018) in the Nar-rativeQA Reading Comprehension Challenge. In this challenge, one is presented to a dataset and a respectively set of tasks in which questions about stories must be answered. As pointed out by the authors, these tasks are designed so that successfully answering their questions requires understanding of the underlying narrative rather than relying on shallow pattern matching or salience. Approaches like these are essential to the reasoning of narratives, in the sense that they aim to provide understandable narratives to the end-user focusing on the outcome and in the causality between the events. Another aspect that should be considered when evaluating a narrative is from whose point of view the story is being told (Brahman et al. 2021). As discussed in the beginning of this survey, having access to appropriate datasets is one of the most important steps behind any evaluation procedure. In this context, we make available a summarized list of the most important datasets in several languages that suit the evaluation tasks approached in this survey. We refer the interested reader to check the the supplementary material of this paper.",
          "As exposed in the previous sections, the extraction of narratives comprehends a series of interrelated tasks related to different areas, which amount to an intricate and complicated enterprise. As such, and although much has been accomplished, different challenges need to be met. In this section, we refer to the most prominent, organizing them into general and narrative extraction-oriented.",
          "",
          "Human statements frequently contain ambiguity, error, implicit information, and other sources of complexity. Therefore, the creation of cognitive agents with human-level natural language understanding capabilities involves mimicking human cognition (McShane 2017). As narratives are composed of human declarations, the study of narratives presents a high level cognitive challenge. Some of the problems inherent to narratives are coreference resolution, for instance, when the subject is null, polysemy (i.e., the multiplicity of meanings of a word or phrase), synonymy (i.e., the expression of the same idea with different terms), ambiguity, related, not only to polysemy, but also to syntactic structure, presuppositions or sarcasm. Natural Language Understanding, a research area of NLP, can aid in the resolution of some of these issues. However, a model able to deal with the aforementioned issues, and to perform at a level proximate to human reasoning is still far from being developed.",
          "As demonstrated in this survey paper, narratives are formed by a series of links between involved participants and events, and organized according to their causality over time, which may mean that different documents that report the same event can compose a familiar narrative. However, few works found in the literature of the area explore the extraction of narratives from multiple documents. Future research should propose new methods to automatically identify, interpret, and relate the different elements of a narrative, which will likely come from various sources. This dilemma is related to the fact that current models are often centered on recurrent neural networks (which, although can represent contexts longer than other types of networks, still have limitations for this type of strategy). Working in vast environments (i.e., contexts) is closely connected to natural language and requires scaling up internal processes before larger documents can be handled.",
          "Although narratives are present in a set of diverse languages, most of the data available, however, is mainly in English. One consequence of this is that low-resource languages end up receiving far less attention. For example, while the task of temporal information extraction has drawn much attention in recent years, research efforts have mostly focused in the English language (Mirza 2016), and the development of new solutions for less known languages ends up being compromised by the lack of properly annotated datasets.",
          "Pre-processing and parsing, and the identification and extraction of narrative components, are two of the components that present the best results in the process of narrative extraction. This stems from the fact that the tasks covered by these processing stages have many applications and have been studied across several NLP tasks over the years. The remaining tasks of the narrative extraction pipeline, however, still present many challenges for further enhancements. In the following, we describe some of those challenges, highlighting future directions whenever appropriate.",
          "The annotation process is a key element when working on narrative extraction. Since a significant number of the tasks from the narrative extraction pipeline hinge on the existence of large annotated datasets, the effort of annotation is huge. One of the biggest issue is precisely the lack of manually annotated datasets available with the necessary information. Checking the manual annotations can also be troublesome because, often, the multilayer annotation is so dense that the annotator can barely unravel what was annotated. A useful solution to this is to resort to visual representations such as knowledge graphs and message sequence charters to carry out the supervising task. Nonetheless, further developments on these visual representations are needed. Another problem is related to the inter-annotator agreement. As stated before, narratives are complex and not always straightforward, which reinforces the relevance of assessing the level of agreement between annotators, both during the process of creating the annotated datasets and evaluating the annotation performed by means of automatic methods.",
          "Several challenges related to the temporal domain may be understood as possible reasons that prevent the development of more elaborated solutions in different languages. For instance, the free-text temporal expression (Strötgen and Gertz 2016), which deals with general phrases not covered by standard definitions, the resolution of implicit or relative temporal expressions, or the problem posed in normalizing temporal expressions across different time-zones, are among some of the most well-known reasons. Thus, the most promising approaches rely on cross-lingual transformer language models and cross-lingual sentence embeddings that leverage universal language commonalities.",
          "From the linguistic point of view, some texts have a stereotypical structure already well established in the area (as is the case of medical reports). The knowledge of the domain is essential to understand the structure and context of a document to improve the reasoning process. The integration of different knowledge might provide, for example, a hierarchical structure of terminology that can help to identify whether two different statements refer to the same event. For a semantic and pragmatic analysis, other computational tools, such as automated reasoning systems, are useful. These tools may help to explore how inference processes can clarify pragmatic phenomena such as conversational implications and more explicitly context-based understanding.",
          "Most text processing tools concentrate on extracting relatively simple constructs from the local lexical context and focus on the document as a unit or even smaller units such as sentences or phrases, rather than on relations of different elements within the document or even cross-document relations. Correctly inducing semantic relations, between participants, participants and events or between text spans, within a natural language story is also an open issue in narrative studies. For instance, the task of discourse parsing has still a long road to go (Morey et al. 2017), to achieve discourse relation identification beyond local dependencies.",
          "Implementing a combination of three relation extraction techniques (machine learning, heuristics, and a hybrid combination of both) may be an interesting approach to be explored in the context of narratives. As narratives result from a semantic structuring of information extracted from a text, which presents n-ary relations among themselves, the Open IE exploration might provide meaningful insights to narratives studies. As far as we know, nothing of this kind has yet been proposed.",
          "The narrative perspective, also known as the point of view, is the vantage point through which the events of a story are filtered and then transmitted to the audience. Thus, the same story might have different point of views (POV) depending on the person (narrator/ character) who narrates the story or the angle from which one looks (Al-Alami 2019; Brahman et al. 2021). Extracting narratives from different points of view using the same dataset, combining them in a coherent representation of the story or deciding on which are the most relevant are still open challenges. Considering different narrative points of view also influences the task of evaluating narratives.",
          "Without using some form of a semantic representation that offers an abstraction from the details of lexical and syntactic realizations, comprehension is arguably unattainable. Representing embedded stories -stories told inside a story-is also a complex task that remains open. Gervás (2021) proposed a simple model to represent them. Nevertheless, the author stated that there is substantial further work in the depicted model. Narrative visualization is an active area of research, mostly as a result of a strong community focused on interactive storytelling and the use of data visualization techniques to improve narrative understanding. In contrast, there is ample opportunity for further research in the visual representation of narratives themselves. Existing research is mostly focused on the representation of narrative elements such as participants and events, whereas work on commonly used narrative techniques-e.g., focalization, allegory, personification, amplification-is scarce and represents a challenging opportunity for future research. With the development of the field, research on the proposal of common visual vocabularies and patterns for recurring visual solutions also constitute a pertinent opportunity for further research. Another aspect where research opportunities exist is the study of user interaction with narratives-which degree of manipulation is useful to improve narrative understanding? Which elements and dimensions of a visual representation should be open to user interaction? Finally, evaluating the effectiveness of the visualization of a narrative typically resort to user studies involving adhoc tasks-an opportunity for the development of reference evaluation benchmarks and guidelines exists.",
          "Coming up with a framework that evaluates the narrative extraction pipeline as a whole is a crucial step for further developments. One of the difficulties refers to the process of creating a gold standard dataset, a labor intensive task that is highly dependent on the subjective interpretation of several factors, such as the emphasis put on each narrative participant, or the level of detail to include. Raters may also have differing viewpoints on annotating data or disagree about what details should be kept, contributing to a lower consensus between raters and intra-raters. On the application level, the purposes of evaluation may vary according to the context addressed. In general, the proposed systems should answer questions like: What is this narrative about? Does the system accurately extract the required elements to tell a story from it properly? How accurately and efficiently does the present system represents the narrative in terms of clarity and amount of data to the application context? The processing of narrative in large-scale or from multiple documents also bumps the issue that supervision is expensive to obtain. When considering the evaluation metrics for natural language processing, another issue is related to how well these methods match and generalize to the real complexity of human languages and how many more interesting natural language inference datasets can be generated.",
          "Narratives are an essential tool for communication, representation, and understanding information. Computational systems that are able to identify narrative elements and structures can naturally interact with human users. Such systems understand collaborative contexts as an emerging narrative and can express themselves through storytelling (Riedl 2004). This survey paper provides simultaneously an account of the study of narrative extraction and a roadmap for future research. To this end, we propose a narrative extraction pipeline, defining the key tasks involved in this process identified in the literature. By doing this, we set a common ground for further studies, highlighting the different stages of the narrative process and the most prominent approaches. During the course of this survey paper, we also pointed out extensive literature focused on extracting narratives and supporting NLP tasks. Nonetheless, and despite several recent advances, there are still important open issues demonstrating that narrative extraction is a rich and promising research area that requires multidisciplinary knowledge in the crossroads between linguistics and computation."
        ],
        "ground_truth_definitions": {
          "composition of a narrative": {
            "definition": "is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events.",
            "context": "Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narra- tive can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time.",
            "type": "implicit"
          },
          "Computational Narratology": {
            "definition": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations",
            "context": "The study of narratives from the computational perspective is carried out through a study area called Computational Narratology (Mani 2014). Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "type": "explicit"
          },
          "narrative extraction": {
            "definition": "the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "context": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources. A closely related term is Computational Narrative Understanding (Piper et al. 2021), which broadens the perspective to social, personal and cultural dimensions.",
            "type": "explicit"
          },
          "event argument": {
            "definition": "a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event mention": {
            "definition": "a sentence or expression that explains an event, including a cause and multiple arguments.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event trigger": {
            "definition": "the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "argument role": {
            "definition": "the link between an argument and the event in which it is involved.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "ontologies": {
            "definition": "formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation.",
            "context": "Another way is to relate the events of the narrative with entities from a knowl- edge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of repre- sentation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008).",
            "type": "explicit"
          },
          "discourse": {
            "definition": "The expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc.",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "explicit"
          },
          "story": {
            "definition": "the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting)",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "implicit"
          },
          "event": {
            "definition": "something significant happening at a specific time and place with consequences.",
            "context": "Finding the events mentioned in the text is an essential step towards the extraction of nar- ratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "2025.acl-long.1513",
        "sections": [
          "As online news and social networks have radically altered the media landscape, understanding the dynamics of news propagation and the narratives that news convey is more crucial than ever. Online news outlets make it possible for recurring narratives to appear, often with very different phrasings, in multiple articles and propagate with very high velocity across audiences, languages, and countries. This is especially problematic with manipulative narratives, potentially containing strong biases, mis/disinformation, propaganda, or harmful content. Such a risk is even more pervasive when it is related to divisive issues and can have large societal implications, including destabilization, conflict, hate, or incitement to violence. Various definitions of narrative can be found in the literature, depending on the context and the goal of the analysis. In this paper, we define narrative as a recurring, repetitive (across and within articles), overt or implicit claim that presents and promotes a specific interpretation or viewpoint on an ongoing (and frequently dynamic) news topic. Narratives may be embedded in thousands of articles across multiple topics, themes, languages, and news genres (e.g., news reporting or opinion). They can manifest using very different vocabularies, media frames (Entman, 2007), and tones, which makes their automatic detection and extraction very challenging. Moreover, multiple narratives can be present in the same article, or even in the same paragraph or in the same sentence. An example is shown in Fig. 1, where the narrative \"Downplaying Climate Change\" occurs in two articles from very different domains: finance vs. auto enthusiasts. Thus, the effective automatic detection, extraction, and analysis of narratives in news articles is an important challenge and remains an open problem ( §2). Such capacity is crucial for analyzing the news landscape, identifying media bias, and detecting attempts to influence readers. It can also be used to warn and educate media consumers to improve their media literacy. Large Language Models (LLMs) have made the need for such capacity even more urgent, as they are vulnerable to (Xu et al., 2024) and prone to generating manipulative narratives (Vykopal et al., 2024) with modest effort. In this work, we focus on the problem of narrative detection, treating it as multi-label paragraphlevel classification problem ( §3). The biggest challenge is the lack of annotated datasets (Nunes et al., 2024), especially for long news articles (as opposed to micro-blogs or snippets). Existing narrative classification datasets for long articles ( §2) have been annotated at the document level and assign only a single label per annotation. Narratives, however, are often conveyed by small parts of the articles, and thus it is important to pinpoint those parts, which is why we model the problem as a paragraph-level classification. We consider two news domains of current relevance ( §3.1): climate change (CC) and the military conflict between Ukraine and Russia (URW). Unlike previous work on narrative classification, where the labels are flat and coarse (Li et al., 2023) ( §2), we developed a two-level taxonomy with coarse-and fine-grained narrative labels (hereafter narratives and sub-narratives) for each topic ( §3.2). We collected news articles in four languages-English, Bulgarian, Portuguese, and Russian-related to the two domains ( §4.1), and manually annotated them at the paragraph level with labels from the corresponding taxonomies ( §4.2). We report statistics about the resulting dataset ( §4.3) and discuss its quality ( §4.4). We make the dataset, dubbed PolyNarrative, publicly available, along with experimental results ( §5) and code of several strong baselines that assign narrative labels to news articles at the paragraph or the document level; in the latter case, we use the union of the paragraph-level ground-truth labels of each document as the correct prediction. Our contributions are as follows: • We propose a new two-level hierarchical taxonomy for narrative extraction. We further develop a novel datasetfoot_0 that uses this taxonomy with news article annotations from two levels (coarse-and fine-grained), at the paragraph level (previous work was document level), using multi-label annotation (previous work was mostly single-label) across two current domains (previous work was single-domain), and four languages (previous work was for a single language). • We provide a comprehensive overview of the data acquisition and annotation process, and highlight the challenges and measures taken to address them. • We perform evaluation using several strong multi-label classification methods, and draw conclusions that can benefit future work. 2 Related Work",
          "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023;Silvano et al., 2024) and capturing narrative components in the form of semantic relationships between entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012;Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes. In an effort to synthesize various formulations of the concept, Dennison (2021) proposed a refined definition of narratives as \"selective depictions of reality across at least two points that can include one or more causal claims, and are generally generalizable and can be applied to multiple situations, as opposed to specific stories\". This formulation, although well structured from a theoretical perspective, was not sufficient to properly operationalise the concept for our intention to provide a concrete taxonomy. Denmark Punishing Farmers for Cow 'Emissions' to 'Fight Global Warming' Denmark has become the first country to force farmers to comply with the goals of the World Economic Forum's (WEF) \"Net Zero\" agenda, to supposedly \"fight global warming.\" ...",
          "Although carbon dioxide is typically blamed for causing \"climate change,\" globalists claim that methane traps about 87 times more heat on a 20-year timescale but top scientists have debunked these claims as a hoax. A recent peer-reviewed study provided conclusive scientific evidence proving that carbon dioxide (CO2) emissions in Earth's atmosphere cannot cause \"global warming.\" Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change However, not only did they find that higher levels of CO2 made no difference, but they also proved that it simply isn't possible for increases in carbon dioxide to cause temperatures to rise. Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change Nevertheless, unelected globalists at the United Nations Environment Program claim that livestock accounts for about 32% of \"human-caused methane emissions.\" ... Thus, our definition of narrative ( §1), while informed by the above general formulation, is based on previous efforts to create taxonomies and datasets for practical use (Kotseva et al., 2023;Li et al., 2023;Coan et al., 2021).",
          "",
          "To apply modern NLP methods, as highlighted by Santana et al. (2023), extraction of narratives has been approached as hierarchical event detection (Glavaš et al., 2014), as frame bias detection (Zaghouani et al., 2024), as spatiotemporal entity relationship detection (Nunes et al., 2024), and as multi-class classification (Li et al., 2023). Li et al. (2023) presented a social media dataset based on short texts from microblogs and trained a BERT-based narrative classifier detecting COVID anti-vaccine claims from and using a flat taxonomy, while Weigand et al. (2022) focused on conspiracy narrative detection by applying a binary classifier to detect the presence of conspiracy theories and applied a topic modeling segmentation without a pre-defined taxonomy. Coan et al. (2021) released an English-only dataset using a two-level taxonomy on climate change denial, annotated at the snippet level from website fragments and using a single label per snippet, while Kotseva et al. (2023) provided a multi-lingual dataset annotated at document-level using a single label per document. While datasets with individual characteristics (long-articles, hierarchical taxonomies, multilinguality) have been released before, to the best of our knowledge, there has been no other dataset that provides a multi-lingual corpus, annotated at paragraph level using a hierarchical taxonomy in a multi-label multi-class fashion.",
          "Several narrative taxonomies have been created to classify recurring argumentation in online news. Kotseva et al. (2023) created a three-level narrative taxonomy on COVID-19 and used it to classify and to analyze trends over time. Hughes et al. (2021) presented a taxonomy of common anti-vax narratives, organized on several common tropes and rhetorical strategies. Coan et al. (2021) presented a two-level taxonomy for common cases of climate change denial in short snippets. Amanatullah et al. (2023) presented a flat taxonomy of common pro-Russian narratives found in the alleged pro-Kremlin influence campaigns related to the war in Ukraine. In this work, we used the last two taxonomies as a starting point for the two domains of interest.",
          "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, \"CO 2 is beneficial\" is a sub-Narrative of the \"Climate Change is beneficial\" Narrative of the Climate Change (CC) taxonomy. A (sub-)Narrative can appear in a variety of contexts and topics, and can manifest using subtle and indirect argumentation or framing, and thus its detection can be very challenging. For example, the sub-Narrative \"Western sanctions will backfire\" could be expressed throughout a financial reporting article, or in a small paragraph at the end of an opinion article commenting on military matters. Thus, in order to better tackle this challenge, our approach focuses on highlighting specific segment(s) of each news article that expresses each (sub-)Narrative. In this sense, we consider a paragraph as a minimal segment. If a (sub-)Narrative is expressed in multiple paragraphs of an article (or the entire article), all the corresponding paragraphs need to be annotated accordingly. Moreover, one paragraph can express more than one Narrative and more than one level of Narratives (Narratives and sub-Narratives). In these scenarios, the paragraph needs to be annotated with all the applicable (sub-)Narratives. Given this formulation, the task becomes a hierarchical multi-label multi-class classification problem at the paragraph level.",
          "We selected two domains (topics) that currently receive extensive news coverage, are presented through several different perspectives, and are highly susceptible to manipulation through the creation of artificial narratives. The first domain is Climate Change (CC) and the second one is the Ukraine-Russia War (URW). In both cases, we have numerous accounts of repeated argumentation in the form of narratives. We are particularly interested in potentially manipulative narratives, and this is reflected in the choice of the (sub-)Narratives of our taxonomies. However, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily a case of mis/disinformation. This is important to highlight, as mixing legitimate with mis/disinformative claims is a frequent manipulative practice (Goel et al., 2023).",
          "To create the two taxonomies, we started from existing narrative taxonomies for URW (Amanatullah et al., 2023) and CC (Coan et al., 2021). We asked media analysts to suggest modifications based on their media monitoring experience. These modifications included additions (new (sub-)Narratives), elaborations (splitting existing (sub-)Narratives into multiple different ones), modifications (refining definitions and names) and merging (grouping into common (sub-)Narratives). We asked, when possible, to phrase each (sub-)Narrative as a con-crete claim (e.g., \"Renewable energy is dangerous\" or \"Ukraine is a puppet of the West\"). When the argumentation regarding a (sub-)Narrative was very diverse or fragmented, we gave a more general descriptive label (e.g., \"Criticism of Institutions\" or \"Speculating war outcomes\"). We ended up with 38 and 36 sub-Narratives, grouped into 11 and 10 Narratives for URW and CC, respectively. The top level of the two taxonomies (Narratives) is shown in Figure 3. The full taxonomies, including sub-Narratives and definitions, can be found in Appendix A. We note that the two taxonomies are not complete; in fact, they are not intended to be complete, rather they are representative of the practical experience of the media analysts, and different valid formulations are possible. We stress again that although our taxonomies focus on potentially manipulative narratives, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily mis/disinformation or indicates strong bias. UKRAINE-RUSSIA WAR (URW) Blaming the war on others rather than the invader Discrediting Ukraine Russia is the Victim Praise of Russia Overpraising the West Speculating war outcomes Discrediting the West, Diplomacy Negative Consequences for the West Distrust towards Media Amplifying war-related fears Hidden plots by secret schemes of powerful groups CLIMATE CHANGE (CC) Criticism of climate policies Criticism of institutions and authorities Climate change is beneficial Downplaying climate change Questioning the measurements and science Criticism of climate movement Controversy about green technologies Hidden plots by secret schemes of powerful groups Amplifying Climate Fears Green policies are geopolitical instruments ",
          "",
          "To obtain news articles for annotation, we used an in-house online news scraping and indexing tool to retrieve articles on the two selected domains in the four target languages. 2 We considered both mainstream and \"alternative\" news sources (that is, sources known to repeatedly publish content with misinformation). We also developed ad-hoc scoring methods, outlined below, to heuristically evaluate the relevance of each article to the corresponding domain. More specifically: 1. We created keyword-based queries for the two domains in all the languages, and used them to retrieve a large number of articles from the news article index of our in-house tool. For the URW domain, we included documents from 2021 to 2024. For the CC domain, the documents ranged from 2015 to 2024. 2. To assess the relevance of the resulting articles, we formulated another set of key phrases, corresponding to the (sub-)Narratives of the taxonomies (e.g., \"Ukraine is corrupt\", \"Science is debunked\") or other aspects we wished to emphasize or de-emphasize. We then used bart-large-mnlifoot_2 to perform zero-shot classification with the title and the first 300 characters of each article, resulting in a classification score per article and key phrase. 3. We used an XLM-RoBERTa-basedfoot_3 multilabel classifier, trained on the Persuasion Techniques dataset (Piskorski et al., 2023a,b), and used the approach of Nikolaidis et al. (2024) to produce Persuasiveness Score metrics per article. For the CC domain, we also used a climate change denial classifier, based on the work of Coan et al. (2021), to further filter and reduce the number of articles to review. 4. We used a linear combination of the resulting scores (relevance score per key phrase, Persuasiveness Score metrics and the score from the climate change denial classifier) to automatically rank articles from most to least likely to contain relevant (sub-)Narratives. A member of the team who was familiar with the news landscape of the language, then manually inspected the resulting set of articles, focusing on articles that seemed relevant to the defined taxonomies. Since manual inspection was timeconsuming, we used the different scores computed above to reorder the articles and focus mainly on highly ranked articles.",
          "In order to clarify the annotation process, including the meaning of key concepts such as Narrative, sub-Narrative, the precise meaning of each label of the taxonomies, and good practices, we created a document with detailed annotation guidelines. This included specific definitions and examples for each label, together with a detailed description of the annotation process to be followed. The definitions and examples of the taxonomies can be found in Appendix A. As we wanted to produce a multilingual dataset with four languages, we had multiple annotator teams, each focused on one language. We assigned two annotators per news article and a curator who handled consolidation and quality assurance before the final version. We held regular calls in which hard cases were highlighted, comparisons across annotations from different languages were made, and issues with ambiguities of the guidelines were discussed. To annotate the documents, we used the Inception annotation tool (Klie et al., 2018). Each language was assigned a coordinator who had the responsibility to sort through the collected articles, assign annotators and curators, and create annotation batches incrementally. The coordinator was also responsible for training the annotators. We instructed the annotators to read each news article paragraph by paragraph, and to annotate each of them with all the applicable Narratives and sub-Narratives before moving to the next one. When no Narrative or sub-Narrative was found, the annotator simply moved to the next paragraph. Because the sub-Narrative labels were many and difficult to memorize, we configured Inception so that for each paragraph, the annotator would first select all the applicable Narratives, and then all the applicable sub-Narratives from a menu displaying the sub-Narratives of the selected Narratives only. When a paragraph was deemed to contain a Narrative, but none of its sub-Narratives, the \"[Narrative]: Other\" label was selected at the sub-Narrative level. Similarly, the \"Other\" label was selected when no Narrative was applicable. During the annotation process, several issues were highlighted. First, the distribution of labels was very uneven and varied noticeably across languages (Table 2). This reflects the reality of the news discourse, as some Narratives are more frequent in some countries than others. After each annotation batch, the collected label statistics were used by the coordinator to optimize the selection of articles for the next batch. Second, some labels were semantically very close, which led to frequent confusion. We used frequency metrics and cura-tor feedback to discuss such cases in the regular meetings, where we reiterated and further refined the guidelines. For example, the sub-Narratives \"Ukraine is a puppet of the West\" and \"The West does not care about Ukraine, only their interests\" were frequently confused. To resolve this ambiguity, we added guidelines instructing the annotators to select the former when Ukraine was the subject and the latter when the West was.",
          "Table 1 shows statistics about the documents of the PolyNarrative dataset, broken down per language, and aggregated over both domains. We can see that, except for Russian, all languages have approximately 400 annotated training documents; the test documents are approximately 35 for each language. We aimed at articles of relatively long text with a median length of 500 words and a minimum length of 250 words. Even after the rigorous data selection process, the final label distribution after annotation is highly imbalanced (see Table 2 and Appendix B for a breakdown per language). Additionally, the distribution of labels varies noticeably from language to language, due to differences in media interest across countries, as already discussed. For example, for URW, Russian is a notable outlier in terms of distribution, with \"Praise of Russia\" being one of the most common Narratives. One note to highlight, is that after the article selection process, the proportion of mainstream and alternative news varies widely across languages and could have an impact on label distribution. Moreover, 44% and 54% of the documents in CC and URW, respectively, are assigned more than one label; for paragraphs, the corresponding percentages are 12% and 15%, respectively. This observation justifies our choice to perform multi-label annotation.",
          "Table 3 reports the inter-annotator agreement scores for Narratives (coarse labels) and sub-Narratives (fine-grained labels), per language, collectively for both domains, measured as Krippendorff's α at paragraph level using the simpledorfffoot_4 library. The agreement is under the recommended value of 0.667 but is higher than the IAA of tasks of similar complexity (Piskorski et al., 2023b(Piskorski et al., , 2024)). TRAIN Language #documents #paragraphs #sent. #char. BG 400 3,951 6,728 937,150 EN 394 3,683 8,012 1,176,513 PT 395 3,877 5,739 971,356 RU 133 583 1,630 206,124 TEST Language #documents #paragraphs #sent. #char. BG 35 325 542 70,291 EN 41 584 1,106 144,401 PT 35 330 578 86,132 RU 32 154 478 56,872 Total 1,465 13,487 24,813 3,649k Table 2: Distribution statistics for coarse-grained labels (Narratives), assigned to paragraphs and documents, in both TRAIN and TEST splits. Statistics for fine-grained (sub-Narratives) are shown in Appendix B. As one would expect, the agreement on finer labels is lower, because of their subtler differences. We noticed that the CC domain caused more confusion between the annotators than URW. In both cases, we could see that the confusion was skewed by a small set of under-agreed labels (5 for URW and 7 for CC) that achieved disagreement above 40% and 60%, respectively. If we excluded these labels, the IAA for all languages rises to 0.567 and 0.560 for the coarse and 0.452 and 0.516 for fine-grained, for CC and URW respectively. There were some sub-Narratives that were commonly confused. For example, in the URW subset \"Discrediting the West, Diplomacy: West is tired of Ukraine\", \"Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests\", and \"Discrediting Ukraine: Ukraine is a puppet of the West\". Overall, on a positive note, the majority of the disagreement between the two annotators in the sub-Narrative labels was between labels of the same Narrative (e.g. sub-Narratives under \"Discrediting the West, Diplomacy\" were frequently confused with one another) On average, out of all the individual annotator disagreements (paragraphs where the two annotators picked a different sub-Narrative), 67% was with sub-Narratives of the same Narrative. A detailed breakdown of the disagreements statistics for each label can be found in Appendix E.",
          "",
          "To provide baseline results for the new PolyNarrative dataset, we performed experiments with commonly used multi-label classification methods. We assessed the following configurations: • Label granularity: We assessed the performance in both coarse (predicting Narratives) and fine-grained (predicting sub-Narratives) settings. Additionally, we assessed how the extra information about the fine-grained predictions impacted coarse-grained predictions. • Paragraph vs. document: We assessed the performance when the models were trained and evaluated to classify paragraphs vs. entire documents. The document-level labels were produced by taking the union of the labels of all of the paragraphs contained in each document. • Prompting vs. fine-tuning: We also assessed the capability of (open-source) LLMs to annotate (sub-)Narratives without further training, given instructions similar to the guidelines given to the human annotators, comparing performance to fine-tuned models. We experimented with the following models: • XLM-RoBERTa large (Conneau et al., 2020), in two versions: -XLM-RoBERTa-coarse: trained using only the coarse labels. -XLM-RoBERTa-fine: trained using the fine-grained labels. • Llama3.1-70b (Grattafiori et al., 2024), without fine-tuning, in four versions: -Llama3.1-70b-0shot-labels: Prompted with the names of the labels of the taxonomy only, in a zero-shot fashion. -Llama3.1-70b-guidelines: Prompted with the full annotation guidelines, which include hand-picked examples for each (sub-)Narrative. -Llama3.1-70b-guidelines-labels: Prompted with the names of the labels, followed by the full annotation guidelines. -Llama3.1-70b-hierarchical: two-step hierarchical prompting, where the model is instructed to pick the coarse-grained labels first, and then given these coarsegrained predictions, decide on the finegrained label; full guidelines are also provided. When using Llama, after receiving the model's response, we performed a series of post-processing operations, where the generated output labels were filtered and normalized to remove hallucinated labels and to handle cases where the model generated labels in a slightly different form (e.g., different capitalization, punctuation errors). To extract paragraph-level annotations from Llama, we inserted extra \"[paragraph_N:]\" tags at the beginning of each paragraph, N being the number of the paragraph, and modified the prompts to instruct the LLM to output each extra tag followed by the relevant labels of the corresponding paragraph. After the call to the LLM, we recombined the outputs with the input paragraphs in a post-processing phase, using regular expressions. The exact prompts used are given in Appendix C. The XLM-RoBERTa (Conneau et al., 2020) models were trained at the token level. For each token, a multi-hot vector equal in size to the number of labels was provided as ground-truth, and each token inherited the ground-truth labels of its paragraph. At inference time, we obtained a similar predicted multi-hot vector from each token, and each label was assigned (or not) based on the majority vote of the paragraph's tokens. To bypass the 512 token limitation of XLM-R, we used a sliding window with a 50% overlap. We also report the results for a naïve stratified random baseline, where we randomly draw labels from a multinomial distribution, while respecting the frequency statistics in the training set.",
          "Since our task is multi-label multi-class classification with a large number of classes, and noticeable class imbalance, we opted to use suitable metrics that are robust in these conditions. Table 4 presents the F 1 macro and sample-averaged F 1 scores for all models in both domains (CC, URW). Sample-average F 1 score is calculated by calculating the F 1 score on each instance and subsequently averaging over all instances. We observe that the XLM-RoBERTa large classifier outperforms Llama3.1-70B in the fine-grained case both for document-and paragraph-level predictions, especially in sample F 1 , where the difference is very pronounced. The Llama3.1 prompted model seems to be more competitive in coarse-grain evaluation. Interestingly, including the full guidelines in the prompt generally achieves worse results than using the taxonomy labels only. When we add both the taxonomy labels and the guidelines, performance improves, compared to using only the guidelines, but it is unclear whether it is better to include only the taxonomy labels. Hierarchical prompting does not seem to yield competitive results. In terms of per-label performance, the weakest model performance was measured on the sub-Narratives under \"Downplaying climate change\" and \"Green policies are geopolitical instruments\" Narratives in the CC domain and \"Russia is the Victim\" and \"Blaming the war on others rather than the invader\" in the URW domain. One key observation is that models exhibited relatively lower performance for the \"[Narrative]: Other\" labels, that is, the paragraphs that contained a Narrative but none of the listed sub-Narratives, that may include a very diverse set of argumentation, coming from a set of heterogeneous sub-Narratives that our taxonomy does not cover. Performance also seemed to vary noticeably per language. In case of Russian, Llamma3.1-70b models seemed to perform rather poorly on a per-sample level. On XLM-RoBERTa, English consistently exhibited poorer performance compared to other languages, with Bulgarian and Portuguese showing different performance per domain. We provide a detailed breakdown of the performance (by label and by language) of two models (XLM-RoBERTa f ine and Llama3.1-70bguidelines-labels) on Appendix D.",
          "We present PolyNarrative, a new multi-lingual, multi-label, multi-domain dataset for extraction of narratives at the paragraph level from long news articles. The dataset comprises 1,476 articles totaling 13,625 paragraphs in 4 languages (Bulgarian, English, Portuguese, and Russian), annotated using two expert-refined Narrative hierarchical taxonomies with 38 and 36 fine-grained and 10 and 11 coarse-grained narratives for each domain. We describe in detail the data acquisition and annotation process and highlighted noteworthy issues. We present preliminary experimental results using both fine-tuned encoder (XLM-R) and prompting-based decoder (Llama 3.1) Language Models in multiple granularities (fine, coarse), levels (paragraph, document), and configurations (prompting strategies). We also highlight interesting findings regarding model performance. We hope that this dataset will catalyze new research directions in narrative detection and extraction, and stimulate the development of new methods and techniques. We highlight that the taxonomies are based on expert real-world experience and they are not meant to be perceived as complete or to capture the whole breadth of the media discourse around the two domains. We encourage the research community to propose extensions that potentially capture more diverse perspectives. We encourage the direction of developing methods that are domain-and potentially taxonomy-agnostic, able to perform narrative detection in a completely unsupervised fashion, removing the dependence on human subjectivity. We believe that a multi-domain and highly granular dataset such as this one can facilitate experimentation towards this goal. Regarding future work, we plan to experiment with ways to perform generation of new narrative labels and automatic revision of taxonomy and guidelines. One way to approach this is by analyzing the structure of narrative elements in the text relate them with narrative labels and identify new structures. We also intend to use retrieval strategies to perform narrative retrieval within large news corpora.",
          "Intended Use and Misuse Potential The main drive behind the creation of our dataset was to advance research on automated narrative classification and the detection of deceptive content across multiple languages and domains. However, given that possible risk of exploiting the dataset to boost the production of biased manipulative disinformation attempts, we advise responsible use. In this context, whoever develops a Narrative detection system is also responsible for deciding which Narratives to detect, in an ethical manner. Furthermore, this research in direction can contribute to the development of independent tools such as browser plugins or independent publicly available services that could help users contextualize the information they consume, contributing to their awareness and improving citizens' media literacy. Environmental Impact The deployment of LLMs might have a large carbon footprint, especially when training new models. In the context of the reported experiments, we did not train any new LLMs, but only used existing trained models in an in-context zero-shot scenarios, which is relatively cheap in terms of computing. Fairness The majority of the annotators, primarily researchers with linguistic background and prior annotation experience, come from the institutions of the co-authors of this manuscript. They were fairly remunerated as part of their job. The remaining part of the annotator pool consisted of (a) some students from the respective academic organizations, (b) few external experienced analysts paid at rates set by their contracting institutions, and (c) experts from a contracted a professional annotation company, who were compensated according to rates based on their country of residence.",
          "Taxonomies and Dataset Representativeness Our taxonomies were edited by experienced media analysts, active in the study of misinformation and fact-checking. As such, the taxonomies overrepresent Narratives of interest of media analysts from Western institutions. The selection should not be perceived as covering the complete discourse of the two domains, but rather what such analysts encounter in practice. The original taxonomy for URW used before the heavy revision from the analysts, was not from a peer-reviewed publication but from a technical report. The dataset presented covers two widely discussed domains around the world and a wide range of media sources. However, it should not be considered as representative of the media in any specific country, nor should it be considered as balanced in any way. Also, it is important to note that while this work focuses on narratives potentially containing mis/disinformation, the operationalized definition of Narrative can also be used to detect neutral or desirable viewpoints. Biases Although the annotators were trained and made acquainted with the specifics of the two domains of interest for our task and cross-language quality control mechanisms have been put in place in the annotation process, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset. Consequently, models trained on this dataset might exhibit certain biases. and Han Qiu. 2024. The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16259-16303, Bangkok, Thailand. Association for Computational Linguistics. Wajdi Zaghouani, Mustafa Jarrar, Nizar Habash, Houda Bouamor, Imed Zitouni, Mona Diab, Samhaa El-Beltagy, and Muhammed AbuOdeh. 2024. The FIGNEWS Shared Task on News Media Narratives. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 530-547, Bangkok, Thailand. Association for Computational Linguistics.",
          "This section introduces the taxonomies of narratives and subnarratives for the two domains. A.1 Ukraine-Russia War (URW) 1. Blaming the war on others rather than the invader: statements attributing responsibility or fault to entities other than Russia in the context of Russia's invasion of Ukraine. Example: \"The economic crisis is due to Western sanctions.\" Example: \"Ukraine's actions provoked this conflict.\" Sub-Narratives: (a) Ukraine is the aggressor: Statements that shift the responsibility of the aggression to Ukraine instead of Russia and portray Ukraine as the attacker. Example: \"Ukraine secretly provoked the war because it was harassing Donbass province citizens.\" (b) The West are the aggressors: Statements that shift the responsibility for the conflict and escalation to the Western block. Example: \"The real perpetrators were US/EU. They sabotaged Minsk II agreement only to force Russia to invade.\" 2. Discrediting Ukraine: Statements that undermine the legitimacy, actions, or intentions of Ukraine or Ukrainians as a nation. Example: \"Ukraine's government is corrupt and ineffective.\" Example: \"Ukrainian soldiers are committing atrocities.\" Example: \"Ukrainian identity does not exist\" Sub-Narratives: (a) Rewriting Ukraine's history: Statements that aim to reestablish history of Ukrainian nation in a way that discredits its reputation. Example: \"Ukraine is not a real nation, it was a fabrication to split Russia and ally with Hitler.\" (b) Discrediting Ukrainian nation and society: Statements that aggressively undermine the legitimacy and reputability of Ukrainian ethnicity and people (c) Discrediting Ukrainian military: Statements that aim to undermine the capabilities, professionalism or effectiveness of the Ukrainian armed forces. (d) Discrediting Ukrainian government and officials and policies: Statements that seek to delegitimize the Ukrainian government, its leaders, and its policies, portraying them as corrupt or incompetent. (e) Ukraine is a puppet of the West: Claims that Ukraine is controlled or heavily influenced by Western powers, particularly the United States and European Union. (f) Ukraine is a hub for criminal activities: Allegations that Ukraine is a center for illegal activities such as human trafficking, drug smuggling, or organized crime (g) Ukraine is associated with nazism: Accusations that Ukrainian society or government has ties to or sympathies with Nazi ideology, often referencing historical events or extremist groups. (h) Situation in Ukraine is hopeless: Statements that portray Ukraine as having no viable perspectives or no potential positive future. Example: \"Ukraine should just give up, it is all over debt and will be exploited by the West anyway.\" 3. Russia is the Victim: Statements that portray Russia as being unfairly targeted or victimized. Example: \"Russia is being unfairly sanctioned.\" Example: \"The West is ganging up on Russia without justification.\" Example: \"Russia is doing what every country would do (e.g. protect its interests/honour etc.)\" Sub-Narratives: (a) The West is russophobic: Statements that claim that the negative reaction to Russia's actions are because of the negative perspective of western countries instead of Russia's own actions. Example: \"Politicians in the West blame Russia for everything, instead of looking at their mistakes.\" Example: \"In Country X, they banned Tchaikovsky ballets and Chechov's plays because they cannot stand Russia and its culture.\" (b) Russia actions in Ukraine are only selfdefence: Statements that justify Russia's action solely as legitimate self-defence and not a deliberate action. Example: \"There was no other way than war to defend the Russian-speaking people in Donbass.\" (c) UA is anti-RU extremists: Statements claiming that Ukraine is comprised of extremist elements that are vehemently opposed to Russia. 4. Praise of Russia: Statements that positively highlight Russia's actions, policies, or character Example: \"Russia is leading the way in international diplomacy.\" Example: \"The Russian economy is resilient and strong.\" Example: \"Glorifying mentions of Russia's weapon systems and military might.\" Sub-Narratives: (a) Praise of Russian military might: Statements that positively highlight Russia's military institutions, equipment and scale. Example: \"Russia has far more tanks and powerful artillery that US/EU would only dream of.\" (b) Praise of Russian President Vladimir Putin: Statements that present Vladimir Putin positively, including his personal and leadership qualities. Example: \"Any country would want such a strong leader as Putin to lead the way.\" (c) Russia is a guarantor of peace and prosperity: Statements that portray Russia solely in a positive manner, emphasising their potential to provide peace and prosperity to those that cooperate. Example: \"Take a look at Africa, Russia supports countries and turns them into independent nations guided by their people's interests where Western countries colonised brutally.\" (d) Russia has international support from a number of countries and people: Statements that emphasise the popularity and acceptance of Russia in the international stage. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" (e) Russian invasion has strong national support: Statements that emphasise the popularity and acceptance of the invasion inside Russia and on Russian-speaking populations. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" 5. Overpraising the West: Statements that excessively and unduly laud or extol the virtues, accomplishments, and moral superiority of Western countries, particularly in the context of international relations and military. Sub-Narratives: (a) NATO will destroy Russia: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) and its allies are capable or already in the process of eradicating Russia. (b) The West belongs in the right side of history: Statements that portray Western nations and their actions as morally superior and aligned with progress and justice and possess moral superiority. (c) The West has the strongest international support: Statements that emphasize or claim widespread backing for Western policies and actions from the international community, potentially downplaying opposition or criticism. 6. Speculating war outcomes: Statements that predict or make assumptions about the potential results or consequences of a conflict Sub-Narratives: (a) Russian army is collapsing: Statements that suggest or claim that the Russian military is experiencing a significant decline in its effectiveness, strength, or morale. (b) Russian army will lose all the occupied territories: Speculative statements that predict or assume the potential outcomes of the conflict, specifically regarding the possibility of the Russian military losing control of all the territories it currently occupies. (c) Ukrainian army is collapsing: Statements that suggest or claim that the Ukrainian military is experiencing a significant decline in its effectiveness, strength, or morale. 7. Discrediting the West, Diplomacy: Statements that criticize the Western countries, or international diplomatic efforts. Example: \"The West is hypocritical in its foreign policy.\" Example: \"Western diplomacy has failed in resolving conflicts.\" Example: \"International organizations will not solve anything because...\" Sub-Narratives: (a) The EU is divided: Statements that present the EU as a set of divided entities and interests, usually unable to take actions. Example: \"The European Council will never vote on sanctions for Russia, since they cannot agree on even the simplest of the issues.\" (b) The West is weak: Statements presenting the West overall as a non-potent group of countries (that is not as powerful as it used to be). Example: \"The weakened West is once again impotent to act in front of the will Russia.\" (c) The West is overreacting: Statements that claim that the West and its institutions are reacting to Russia's actions in a disproportionate manner. Example: \"Putin did not invade the EU but Ukraine. Imposing harsh sanctions is not the way to deal with it, dialogue and debate is.\" (d) The West does not care about Ukraine, only about its interests: Statements that claim that the West is only interested in Ukraine for its own benefits, disregard-ing the country's fate. Example: \"The West has indebted Ukraine more than XX bln of dollars, a lucrative deal for western companies to exploit.\" Example: \"NATO's actions are endangering global security.\" (e) Diplomacy does/will not work: Statements discrediting the potential of ongoing or potential diplomatic efforts. Example: \"Diplomats are desperately trying to figure out solutions but now it's too late, they have failed and Russia is free to do whatever.\" (f) West is tired of Ukraine: Claims that Western countries, particularly the United States and European nations, are becoming fatigued or disinterested in supporting Ukraine and its efforts. 8. Negative Consequences for the West: Statements that highlight or predict adverse outcomes for Western countries and their interests. Example: \"Sanctions against Russia will backfire on Europe.\" Example: \"The West is headed for an economic downturn.\" Sub-Narratives: (a) Sanctions imposed by Western countries will backfire: Statements that catastrophize on the possible negative effects for Western sanctions of Russia. Example: \"The winter is going to be cold and with current gas prices, we are talking of societal unrest.\" (b) The conflict will increase the Ukrainian refugee flows to Europe: Statements that catastrophize on the possible refugee outflows due to the conflict. Example: \"Like we did not have refugees from the Middle East, now we will have Ukrainians stressing our housing and healthcare problems.\" 9. Distrust towards Media: Statements that question the reliability or integrity of media organizations. Example: \"Western media is spreading propaganda.\" Example: \"You can't trust what the news says about Russia.\" Sub-Narratives: (a) Western media is an instrument of propaganda: Statements that discredit the media institutions of the West and claim that they are instruments of propaganda. Example: \"... but you wouldn't hear this on a western channel, only the party line from State Department.\" (b) Ukrainian media cannot be trusted: Statements that discredit the media institutions of the Ukraine and claim that they should not be trusted for reporting on the war. Example: \"Ukraine is conducting its own propaganda using their TV channels, news and social media.\" 10. Amplifying war-related fears: Statements that evoke fear or anxiety about potential threats, dangers or reactions. Example: \"The West is pushing us towards World War III.\" Example: \"It is a matter of time before war speads on the West\" Example: \"Nuclear war is imminent\" Sub-Narratives: (a) By continuing the war we risk WWIII: Statements that warn against upsetting Russia's and its leadership, evoking fear of causing WW3. Example: \"The Western elites with their fixation on Russia are sleapwalking towards WW3\" (b) Russia will also attack other countries: Statements that claim that it is imminent that Russia will attack other countries. Example: \"... and be sure, Ukraine is the first not the last country to be invaded. Others will follow.\" (c) There is a real possibility that nuclear weapons will be employed: Statements that evoke fear or anxiety about the use of nuclear weapons. Example: \"... and if Western hypocrisy continues to provoke, Putin might be forced to press the red button. . . for good\" (d) NATO should/will directly intervene: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) ought to or will take direct military action in a conflict, potentially implying a shift in policy or strategy. 11. Hidden plots by secret schemes of powerful groups: Statements that suggest hidden plots or secretive actions by powerful groups related to the war. Example: \"There's a secret plan by the elites to control global resources.\" Example: \"The war is just a cover for something much bigger.\" A.2 Climate Change (CC) 1. Criticism of climate policies: Statements that question the effectiveness, economic impact, or motives behind climate policies.",
          "(a) Climate policies are ineffective: Statements suggesting that climate policies fail to achieve their intended environmental goals. (b) Climate policies have negative impact on the economy: Statements claiming that climate policies lead to negative economic outcomes. (c) Climate policies are only for profit: Statements that argue climate policies are driven by financial or corporate gain rather than genuine environmental concerns.",
          "Statements that challenge the competence, integrity, or intentions of various institutions and authorities in relation to climate change. Sub-Narratives: (c) Temperature increase does not have significant impact: Statements claiming that the increase in temperature is not going to have any noticeable effect in nature. (d) CO2 concentrations are too small to have an impact: Statements claiming that the concentrations of CO2 will have a negligible effect. (e) Human activities do not impact climate change: Statements that support that climate change is not caused by human activity. (f) Ice is not melting: Statements claiming that there is not melting of ice. (g) Sea levels are not rising: Statements denying that sea levels have risen (or will rise). (h) Humans and nature will adapt to the changes: Statements claiming that whatever the changes in climate humans or nature will manage to find solutions to adapt. 5. Questioning the measurements and science: Statements that raise doubts about the scientific methods, data, and consensus on climate change. Sub-Narratives: (a) Methodologies/metrics used are unreliable/faulty: Statements claiming that the scientific methodologies and metrics used to measure climate change are flawed or unreliable. (b) Data shows no temperature increase: Statements asserting that available data does not support the claim of global temperature increase. (c) Greenhouse effect/carbon dioxide do not drive climate change: Statements asserting that available data does not support the claim of global temperature increase. (d) Scientific community is unreliable: Statements discrediting scientists, the scientific community and their actions. 6. Criticism of climate movement: Statements that challenge the motives, integrity, or impact of the climate movement. Sub-Narratives: (a) Climate movement is alarmist: Statements suggesting that the climate movement exaggerates the severity of climate change for dramatic effect. (b) Climate movement is corrupt: Statements alleging that the climate movement is influenced by ulterior motives, by corruption or by unethical practices. (c) Ad hominem attacks on key activists: Statements attacking the reputation of key figures (such as scientists, activists, politicians or public figures). 7. Controversy about green technologies: Statements that express skepticism or criticism of environmentally friendly technologies. Sub-Narratives: (a) Renewable energy is dangerous: Statements claiming that renewable energy sources pose significant risks or dangers. (b) Renewable energy is unreliable: Statements asserting that renewable energy sources are not dependable for widespread adoption. (c) Renewable energy is costly: Statements asserting that renewable energy sources are too expensive, inefficient and worth adopting for widespread use. (d) Nuclear energy is not climate friendly: Statements asserting that nuclear sources are or should not be considered as good for the climate. Statements that minimize the urgency of addressing climate change by suggesting that any action taken at this point is futile or too late to make a meaningful impact. 10. Green policies are geopolitical instruments: Statements claiming that that environmental policies and initiatives are used as tools for geopolitical power and influence rather than genuine environmental concern.",
          "(a) Climate-related international relations are abusive/exploitative: Statements criticizing international relations related to climate change as exploitative or economically abusive. (b) Green activities are a form of neocolonialism: Statements suggesting that green initiatives are a way for developed countries to exert control and influence over developing nations, a modern form of colonial practices.",
          "Figures 4 and 5 report the normalized count of coarse labels per language. There are substantial differences, notably between RU and other languages for URW and between EN and other languages for CC. These differences could be due either to bias in media of each country, or be due to bias in the sampling of the articles to annotate. The reason RU is absent for CC is that it was not possible to find enough articles to annotate. In Table 5, we report on the difficulty of annotating labels. We report the proportion of each label to have a disagreement between annotators both with the Other label specifically, or with any other label. Based on the numbers, we categorise each label in a class of difficulty.",
          "In this Section we provide the different prompts used in the experiments. C.1 0shot (only taxonomy) You are an experienced analyst making labeling articles with labels specific to the war in Ukraine. Labels: {codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the above list. The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst labeling articles with labels specific [\"to the war in Ukraine\"/ \"Climate Change\"] You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below. ",
          "",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You are given the coarse label and you are asked to return the fine label. You will apply a label if the article presents at any point a narrative that is given in the annotation guidelines given below.  D Selected model performance per label and language We provide detailed breakdown of the performance of our top-performing model on fine-grained level (sub-Narratives). We provide both the detailed scores per-label (Figures 6, 7) and per-language (Figures 8,9,10,11) for the two domains (CC and URW). ",
          "We report on the detailed dissagreement statistics of the annotators per each fine-grained label, and highlight the difficulty of each label, in Table 5.    label count %conf %other mcl difficulty 1: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor 169 0.309 0.315 5 Medium 2: URW: Blaming the war on others rather than the invader: The West are the aggressors 260 0.377 0.390 7 Medium 3: URW: Discrediting Ukraine: Rewriting Ukraine's history 23 0.272 0.111 28 Medium * 4: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society 38 0.380 0.053 6 Medium * 5: URW: Discrediting Ukraine: Discrediting Ukrainian military 316 0.314 0.217 1 Medium * 6: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies 502 0.215 0.253 8 Easy 7: URW: Discrediting Ukraine: Ukraine is a puppet of the West 219 0.235 0.165 28 Easy * 8: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities 127 0.368 0.243 6 Medium * 9: URW: Discrediting Ukraine: Ukraine is associated with nazism 97 0.189 0.080 6 Easy * 10: URW: Discrediting Ukraine: Situation in Ukraine is hopeless 107 0.303 0.220 6 Medium * 11: URW: Russia is the Victim: The West is russophobic 167 0.343 0.427 2 Medium 12: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence 130 0.518 0.273 1 Hard * 13: URW: Russia is the Victim: UA is anti-RU extremists 34 0.477 0.406 11 Hard * 14: URW: Praise of Russia: Praise of Russian military might 466 0.140 0.346 5 Easy 15: URW: Praise of Russia: Praise of Russian President Vladimir Putin 100 0.157 0.348 17 Easy 16: URW: Praise of Russia: Russia is a guarantor of peace and prosperity 257 0.352 0.385 1 Medium 17: URW: Praise of Russia: Russia has international support from a number of countries and people 228 0.160 0.245 16 Easy 18: URW: Praise of Russia: Russian invasion has strong national support 20 0.333 0.250 17 Medium * 19: URW: Overpraising the West: NATO will destroy Russia 11 0.263 0.400 20 Medium 20: URW: Overpraising the West: The West belongs in the right side of history 36 0.263 0.133 28 Medium * 21: URW: Overpraising the West: The West has the strongest international support 24 0.254 0.308 7 Medium 22: URW: Speculating war outcomes: Russian army is collapsing 51 0.313 0.476 23 Medium 23: URW: Speculating war outcomes: Russian army will lose all the occupied territories 10 0.312 0.200 22 Medium * 24: URW: Speculating war outcomes: Ukrainian army is collapsing 93 0.242 0.118 5 Easy * 25: URW: Discrediting the West, Diplomacy: The EU is divided 105 0.164 0.435 28 Easy 26: URW: Discrediting the West, Diplomacy: The West is weak 145 0.301 0.267 14 Medium * 27: URW: Discrediting the West, Diplomacy: The West is overreacting 34 0.395 0.294 11 Medium * 28: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests 211 0.381 0.262 7 Medium * 29: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work 109 0.550 0.434 28 Hard * 30: URW: Discrediting the West, Diplomacy: West is tired of Ukraine 43 0.583 0.171 28 Hard * 31: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire 79 0.338 0.723 12 Medium 32: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe 12 0.000 NaN Easiest 33: URW: Distrust towards Media: Western media is an instrument of propaganda 104 0.180 0.452 11 Easy 34: URW: Distrust towards Media: Ukrainian media cannot be trusted 23 0.500 NaN 6 Hard 35: URW: Amplifying war-related fears: By continuing the war we risk WWIII 135 0.285 0.339 37 Medium 36: URW: Amplifying war-related fears: Russia will also attack other countries 133 0.211 0.270 2 Easy 37: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed 292 0.259 0.486 2 Medium 38: URW: Amplifying war-related fears: NATO should/will directly intervene 63 0.305 0.273 36 Medium * 39: CC: Criticism of climate policies: Climate policies are ineffective 74 0.407 0.174 44 Hard * 40: CC: Criticism of climate policies: Climate policies have negative impact on the economy 96 0.337 0.333 45 Medium * 41: CC: Criticism of climate policies: Climate policies are only for profit 58 0.508 0.069 68 Hard * 42: CC: Criticism of institutions and authorities: Criticism of the EU 54 0.275 NaN 45 Medium 43: CC: Criticism of institutions and authorities: Criticism of international entities 104 0.434 0.196 44 Hard * 44: CC: Criticism of institutions and authorities: Criticism of national governments 225 0.375 0.274 45 Medium * 45: CC: Criticism of institutions and authorities: Criticism of political organizations and figures 190 0.487 0.172 44 Hard * 46: CC: Climate change is beneficial: CO2 is beneficial 19 0.071 NaN 51 Easiest 47: CC: Climate change is beneficial: Temperature increase is beneficial 13 0.266 0.500 54 Medium 48: CC: Downplaying climate change: Climate cycles are natural 36 0.294 0.300 50 Medium 49: CC: Downplaying climate change: Weather suggests the trend is global cooling 13 0.406 0.308 70 Hard * 50: CC: Downplaying climate change: Temperature increase does not have significant impact 5 0.736 0.286 56 Hardest * 51: CC: Downplaying climate change: CO2 concentrations are too small to have an impact 18 0.500 0.143 59 Hard * 52: CC: Downplaying climate change: Human activities do not impact climate change 34 0.387 0.083 60 Medium * 53: CC: Downplaying climate change: Ice is not melting 18 0.117 NaN 62 Easy 54: CC: Downplaying climate change: Sea levels are not rising 2 0.500 0.500 47 Hard 55: CC: Downplaying climate change: Humans and nature will adapt to the changes 5 1.000 0.400 50 Hardest * 56: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty 51 0.333 0.118 59 Medium * 57: CC: Questioning the measurements and science: Data shows no temperature increase 10 0.562 0.333 56 Hard * 58: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change 5 0.750 1.000 Hardest 59: CC: Questioning the measurements and science: Scientific community is unreliable 45 0.540 0.185 56 Hard * 60: CC: Criticism of climate movement: Climate movement is alarmist 67 0.619 0.386 61 Hardest * 61: CC: Criticism of climate movement: Climate movement is corrupt 35 0.869 0.463 60 Hardest * 62: CC: Criticism of climate movement: Ad hominem attacks on key activists 78 0.234 0.217 60 Easy * 63: CC: Controversy about green technologies: Renewable energy is dangerous 18 0.421 0.250 65 Hard * 64: CC: Controversy about green technologies: Renewable energy is unreliable 42 0.344 0.333 45 Medium * 65: CC: Controversy about green technologies: Renewable energy is costly 27 0.512 0.100 45 Hard * 66: CC: Controversy about green technologies: Nuclear energy is not climate friendly 3 1.000 0.750 45 Hardest * 67: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites 47 0.438 0.217 68 Hard * 68: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives 74 0.548 0.094 45 Hard * 69: CC: Amplifying Climate Fears: Earth will be uninhabitable soon 51 0.432 0.188 70 Hard * 70: CC: Amplifying Climate Fears: Amplifying existing fears of global warming 864 0.124 0.477 71 Easy 71: CC: Amplifying Climate Fears: Doomsday scenarios for humans 104 0.435 0.239 70 Hard * 72: CC: Amplifying Climate Fears: Whatever we do it is already too late 18 0.264 0.111 70 Medium * 73: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative 7 0.785 0.273 68 Hardest * 74: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism 10 0.375 0.333 68 Medium * Table 5: Label difficulty, %conf is the percentage of annotation with that label that results in an inconsistency, while %other is the percentage that resulted in an inconsistency with label other. \"mcl\" is the most common label with which a specific label is confused. \"difficulty\" make assess the difficulty of the label based on %conf: Easiest (≤ .1), Easy (≤ .25), Medium (≤ .4), and Hard (≤ .4) or Hardest (> 0.6), a star indicate that there is more confusion within the labels of the taxonomy than with the Other class, meaning that it is extra difficult for the annotator"
        ],
        "ground_truth_definitions": {
          "narrative in the context of news articles": {
            "definition": "descriptions of spatiotemporal events and capturing narrative components in the form of semantic relationships between entities.",
            "context": "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity.",
            "type": "implicit"
          },
          "Opinion Mining": {
            "definition": "task to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity.",
            "context": "This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes.",
            "type": "explicit"
          },
          "sub-narrative": {
            "definition": "lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative.",
            "context": "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, “CO2 is beneficial” is a sub-Narrative of the “Climate Change is beneficial” Narrative of the Climate Change (CC) taxonomy.",
            "type": "implicit"
          },
          "Narratives": {
            "definition": "overt or implicit claims, recurring across articles and languages, promoting a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinformation.",
            "context": "We present PolyNarrative, a new multilingual dataset of news articles, annotated for narra- tives. Narratives are overt or implicit claims, recurring across articles and languages, promot- ing a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinfor- mation. We developed two-level taxonomies with coarse- and fine-grained narrative labels for two domains: (i) climate change and (ii) the military conflict between Ukraine and Russia.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "frai-06-1225093",
        "sections": [
          "Similarly, Visual Question Answering (VQA) systems are known to learn heavy language priors (Agrawal et al., 2016). Thus, a lack of transparency can affect the decision-making process and may lead to the erosion of trust between humans and Artificial Intelligence (AI) systems. This can further jeopardize users' safety, ethics, and accountability if such a system is deployed publicly (Madsen et al., 2021). Considering the utilization of NLP in healthcare, finance, and law domains, all of which can directly affect human lives, it can be dangerous to blindly follow machine predictions without fully understanding them. For instance, a physician following a medical recommendation or an operation procedure for a patient without full knowledge of the system can do more harm than good. In addition, systems employing Machine Learning (ML), such as most current NLP methods, are prone to adversarial attacks where small, carefully crafted local perturbations can maximally alter model predictions, essentially misguiding the model to predict incorrectly but with high confidence (Finlayson et al., 2019). The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model's decision-making process of arriving at a prediction and the ability to be verified and audited. Therefore, models with explainability are interpretable and complete. In this survey, the focus is on explainability and mainly on the outcome explanation problem where Guidotti et al. (2018) describe explainability as \"the perspective of an end-user whose goal is to understand how a model arrives at its result\". In NLP, there exist various explainable techniques such as LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016), Integrated Gradients (Sundararajan et al., 2017), and ",
          "One of the emerging explainable techniques for NLP applications is rationalization (Atanasova et al., 2020). Rationalization provides explanations in natural language to justify a model's prediction. These explanations are rationales, which present the input features influencing the model's prediction. The reasoning behind the prediction could be understood simply by reading the explanation/rationale, thereby revealing the model's decision-making process. Rationalization can be an attractive technique because it is human-comprehensible and allows individuals without domain knowledge to understand how a model arrived at a prediction. It essentially allows the model to \"talk for themselves\" (Bastings et al., 2019;Luo et al., 2021). This technique is a part of a subset of explainability because it enables models to be interpretable and complete, as shown in Figure 1. Specifically, rationalization provides a local explanation since each prediction has a unique explanation rather than one for the entire model. Local explanations can be categorized into two groups: local post-hoc and local self-explaining. Danilevsky et al. (2020) present local post-hoc methods as explaining a single prediction after the model predicts and local self-explaining methods as simultaneously explaining and predicting. Rationalization in NLP was first introduced in 2007 (Zaidan et al., 2007). As described in Section 4.6, the objective was to use annotator rationales to improve task performance for text categorization. Interestingly, explainability was not the core objective. However, explainability is an advantage of rationalization because it makes the model inherently explainable even if used in the context of task improvement (Gurrapu et al., 2022). Our literature review found that rationalization can be further divided into two major groups: abstractive and extractive (El-Kassas et al., 2021). In extractive rationalization, important features or sentences from the input data are extracted as rationales to support the prediction. In contrast, abstractive rationalization is a generative task in which novel sentences are ). (B) An example of the Abstractive Rationalization approach (Rajani et al., ). generated using new words or paraphrasing existing sentences. This is typically accomplished through the use of a language model such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), T5 (Raffel et al., 2020), or GPT (Generative Pre-trained Transformer) (Radford and Narasimhan, 2018). Figure 2 demonstrate the usage of the two explanation types with examples. Recent advances in explainable NLP have led to a significant increase in rationalization research. Further, at present, the field of rationalization is disorganized. Thus, the motivations for this survey are-(a) formally define rationalization, (b) present and categorize the well-cited techniques based on NLP tasks, and (c) discuss current trends and future insights on the field. Thus, our primary goal is to provide future researchers with a comprehensive understanding of the previously scattered state of rationalization. The key contributions of this paper are as follows. 1. First literature to survey the field of rationalization in NLP.",
          "Explainable AI (XAI). 3. A comprehensive list of details on available rationalization models, XAI evaluations, datasets, and code are provided to guide future researchers. 4. Presents NLP Assurance as an important method for developing more trustworthy and reliable NLP systems.",
          "Table 1 indicates the related survey papers published in recent years. Danilevsky et al. (2020) note that previous surveys in XAI are broadly focused on AI without a specific narrow domain focus. Their work primarily focuses on surrogate-based explainability methods. NLP publications in recent years further demonstrate that this distinction is less relevant and valuable in the NLP domain because \"the same neural network can be used not only to make predictions but also to derive explanations.\" Therefore, surveying the field of Explainable AI (XAI) in NLP requires NLP-specific methods that are different from the standard XAI methods that are widely known. Thayaparan et al. (2020) survey the use of explanations specifically in Machine Reading Comprehension (MRC). The authors describe MRC papers that support explanations and provide a detailed overview of available benchmarks. Further, Madsen et al. (2021) briefly discuss rationalization and natural language explanations using a question and answer approach, CAGE (Commonsense Auto-Generated Explanations) (Rajani et al., 2019) as an example. Thus, this raises the questionhow can rationalization be generalized and applied to other tasks in the NLP domain? However, until now, no comprehensive literature review on rationalization has been available for the prominent NLP tasks. Thus, through this survey paper, we attempt to address this need.",
          "To provide clarity and distinguish terms that are typically used interchangeably in published literature, a list of definitions is provided in Table 2. These terms are used throughout the paper.",
          "The following are the inclusion-exclusion criteria for our publications collection methodology. The first known use of rationalization in NLP was in the year 2007. Our survey focuses on the domain of NLP from 2007 to early 2022. We have included peer-reviewed publications within this range that include a significant rationalization component as a method to provide explainability. We defined significance as rationalization being the main component of their research methodology and approach. We have eliminated a number of publications that are either not entirely in the NLP domain or do not contain a significant rationalization component. For identifying and selecting articles, the range of keywords and topics was limited to the following in the NLP domain: rationalization, explanation, justification, and explainable NLP. Thus, this survey includes reviews of the articles from journals, books, industry research, dissertations, and conference proceedings from commonplace AI/NLP venues such as ACL, EMNLP, NAACL, AAAI, NeurIPS, and others. Finally, these articles are categorized by important NLP tasks, as shown in Table 3. In recent years, there has been an increase in focus on explainability in NLP after a rise in deep learning techniques (Danilevsky et al., 2020). Due to this, a majority of the papers collected were from recent years (2016 and onwards), as illustrated in Figure 3. The availability of relevant articles was limited. After following the above approach, 33 articles were downselected to be the primary focus of this paper's discussion. Instead of providing a broad yet surface-level understanding of the work, we focus on demonstrating in-depth the most important approaches and TABLE Three related survey papers on explainability and interpretability in NLP. No. Survey title Year Papers reviewed 1 A Survey on the state of Explainable AI for Natural Language Processing (Danilevsky et al., 2020) 2020 50 2 A survey on explainability in machine reading comprehension (Thayaparan et al., 2020) 2020 69 3 Post-hoc interpretability for neural NLP: a survey (Madsen et al., 2021) 2021 27 TABLE List of common terms that are used interchangeably in published literature.",
          "",
          "A \"machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans\" (Guidotti et al., 2018).",
          "Interpretability \"aims at developing tools to understand and investigate the behavior of an AI system\" (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
          "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
          "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model's output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
          "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain. progress made in each NLP task. Overall, we selected six articles in multiple NLP domains, five on Machine Reading Comprehension and Sentiment Analysis, four on Text Classification, Fact-Checking and Commonsense Reasoning, and three on Natural Languages Inference, and two articles on Neural Machine Translation (NMT). .",
          "In this section, we discuss relevant papers and their rationalization techniques categorized by the NLP tasks listed in Table 3. Tables with important information on the papers for each subsection are presented at the beginning. TABLE Details on seven NLP tasks and their definitions which are surveyed in this paper.",
          "",
          "Enabling a model to answer questions regarding a given context (Baradaran et al., 2022).",
          "Going beyond pattern recognition to make inferences using world knowledge (Apperly, 2011;Sap et al., 2020).",
          "Determining if a hypothesis entails or contradicts a premise (MacCartney, 2009).",
          "Classifying if a claim is either true or false based on evidence (Vargo et al., 2018;Gurrapu et al., 2022).",
          "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
          "Categorizing textual data by automatically assigning labels (Minaee et al., 2021).",
          "Translating languages using deep neural networks (Sutskever et al., 2014).",
          "MRC enables a model to answer questions regarding a given context (Baradaran et al., 2022). For this reason, it also frequently referred to as Question Answering (QA) Systems. As shown in Table 4, for MRC applications, we found five recent articles from which three articles provide novel datasets (Ling et al., 2017;Mihaylov et al., 2018;Xie et al., 2020) and the remaining articles Sharp et al. (2017) and Lakhotia et al. (2021) each propose a new MRC framework. The first article, published in 2018, presented a new questionanswering dataset based on the open book exam environment for elementary-level science-OpenBookQA (Mihaylov et al., 2018). This dataset consists of two components-(i) Questions (Q): a set of 5,958 multiple choice questions and (ii) Facts (F): a set of 1,326 diverse facts about elementary level science. This dataset was further tested for evaluating the performance of existing QA systems and then compared with the human performance. The results indicated that human performance was close to 92%, but many existing QA systems showed poor performance close to the random guessing baseline of 25%. Additionally, the authors found that simple neural networks achieved an accuracy of about 50%, but it is still not close to the human performance, about 92%. Recently an extension of the WorldTree project (Jansen, 2018), i.e., WorldTree V2 (Xie et al., 2020), is presented. The main goal of this project is to generate a science domain explanation with a supporting semistructured knowledge base. The WorldTree project is a part of TABLE Selected Machine Reading Comprehension papers. References Name Year Explanation Models XAI Metric Dataset Code Sharp et al. (2017) -2017 Extractive TF-IDF, FFNN -AI2 science, Aristo Mini -Ling et al. (2017) -2017 Extractive LSTM, Seq2Seq -AQuA Mihaylov et al. (2018) OpenBookQA 2018 Abstractive BiLSTM Max-out -OpenBookQA, Xie et al. (2020) WorldTree V2 2018 Abstractive TF-IDF, BERT -WorldTree V2 Lakhotia et al. (2021) FiD-Ex 2021 Extractive T5, BERT-to-BERT -Natural Questions - explainable question answering tasks that provide answers to natural language questions and their human-readable explanations for why the answers are correct. Xie et al. (2020) notes that most multi-hop inference models could not demonstrate combining more than two or three facts to perform inference. However, here the authors merge, on average, six facts from a semi-structured knowledge base of 9216 facts. Thus, this resulted in the WorldTree V2 corpus for standardized science questions. This corpus consists of 5100 detailed explanations to support training and instrumenting multi-hop inference question answering systems. Lakhotia et al. (2021) demonstrates a new MRC framework called FiD-Ex (Extractive Fusion-in-Decoder). It has been noted that seq2seq (Sequence to Sequence) models work well at generating explanations and predictions together. However, these models require a large-labeled dataset for training and bring a host of challenges such as fabricating explanations for incorrect predictions, the difficulty of adapting to long input documents, etc. Thus, to tackle these challenges, the Fid-Ex framework includes sentence markers to encourage extractive explanations and intermediate fine-tuning for improving fewshot performance on open-domain QA datasets. This new framework is tested on ERASER (Evaluating Rationales And Simple English Reasoning) datasets and their benchmarks for evaluations (DeYoung et al., 2020). This experiment concludes that FiD-Ex significantly improves upon prior work on the explanation metrics and task accuracy on supervised and few-shot settings. Sharp et al. (2017) proposes a new neural network architecture that re-ranks answer justifications as an intermediate step in answer selection. This new approach alternates between a max pooling layer and a shallow neural network (with ten nodes, glorot uniform initializations, tanh activation, and L2 regularization of 0.1) for providing a justification. This approach contains three components: 1) retrieval component, which retrieves a pool of candidates' answer justification, 2) extractor, which extracts the features and 3) scores, which perform the scoring of the answer candidate based on the pool of justifications. The authors used 8th-grade science questions provided by Allen Institute for Artificial Intelligence (AI2) for evaluations. The training set includes 2,500 questions with four options, and the test set consists of 800 publicly released questions. Further, a pool of candidate justifications corpora containing 700k sentences from StudyStack and 25k sentences from Quizlet is used. The top 50 sentences were retrieved as a set of candidate justification. For model tuning, the authors used five-fold cross-validation, and during testing, the model architecture and hyperparameters were frozen. The authors compared results using two baselines: IR baseline and IR++. They concluded that this new approach showed better accuracy and justification quality while maintaining near state-of-the-art performance for the answer selection task. Ling et al. (2017) presented a dataset and an approach that provides answer rationales, sequences of natural language, and human-readable mathematical expressions for solving algebraic word problems. The authors proposed a sequence-to-sequence model which generates a sequence of instructions and provides the rationales after selecting the answer. For this purpose, a twolayer LSTM (Long Short-Term Memory) with a hidden size of 200 and word embedding with a size of 200 is utilized. Further, the authors also built a dataset containing 100,000 problems in which each question is decomposed into four parts -two inputs and two outputs. This new dataset is used for generating rationales for math problems and for understanding the quality of rationales as well as the ability to obtain a correct answer. Further, the authors used an attention-based sequence to sequence model as a baseline and compared results based on average sentence level perplexity and BLEU-4 (Bilingual Evaluation Understudy). The authors noted that this new approach could outperform the existing neural models in the ability to solve problems and the fluency of the generated rationales.",
          "Commonsense knowledge helps humans navigate everyday situations. Similarly, commonsense reasoning in NLP is the ability for a model to go beyond pattern recognition and use world knowledge to make inferences (Apperly, 2011;Sap et al., 2020). As shown in Table 5 on commonsense reasoning, we found four articles, and all of them provide unique solutions that contribute to the development of commonsense reasoning frameworks. Sap et al. (2020) demonstrates a solution for commonsense reasoning using LSTM encoder and decoder. The main goal was to convert the actions of an autonomous agent into natural language using neural machine translation. For this purpose, the authors built a corpus of thoughts of people as they complete tasks in the Frogger game which are then stored as states and actions. In the next step, LSTM encoder and decoder are used to translate actions as well as states into natural language. Lastly, the authors used the BLEU score to calculate sentence similarity and assessed the accuracy for selecting the best rationale. The authors also conducted a survey to evaluate the rationales based on human satisfaction. The Frogger experiment is concluded with Encoder-Decoder framework outperforming the baselines and demonstrates that the use of game theory approaches for generating rationales is a promising technique (Chang et al., 2019(Chang et al., , 2020;;Yu et al., 2019;Li et al., 2022). Further, it is noted that deep learning model performance is poor when used in tasks that require commonsense reasoning due to limitations with available datasets. To tackle this problem, Rajani et al. (2019) developed the Commonsense Auto-Generated Explanations (CAGE) framework for generating explanations for Commonsense Question Answering (CQA). The authors also created a new dataset-Common Sense Explanations (CoS-E)by collecting human explanations for commonsense reasoning and highlighting annotations. From this paper, the authors concluded that CAGE could be effectively used with pre-trained language models to increase commonsense reasoning performance. Recently, Majumder et al. (2021) and Tang et al. ( 2021) presented novel solutions for commonsense reasoning. Majumder et al. (2021) focused on the Natural Language Expiations (NLEs), which are more detailed than Extractive rationales but fall short in terms of commonsense knowledge. In this solution, the authors proposed a self-rationalizing framework RExC (Rationales, Explanations, and Commonsense). RExC first extracts rationales that act as features for the prediction then expands the extractive rationales using commonsense resources. In the last step, RExC selects the best suitable commonsense knowledge for generating NLEs and a final prediction. The authors tested RExC on five tasksthree natural language understanding tasks and two vision language understanding tasks. Overall, the results indicated improvement in the quality of extractive rationales and NLEs that bridges the gap between task performance and explainability. On the other hand, Tang et al. (2021) focused on Visual Commonsense Reasoning (VCR). They focused on a problem when a question with a corresponding input image is given to the system, and it attempts to predict an answer with a rationale statement as the justification. To explore this, author presented a multi-model approach by combining Computer Vision (CV) and NLP. Their approach leverages BERT and ResNet50 (Residual neural network) as the feature representation layer and BiLSTM (Bidirectional LSTM) and Attention for the multimodal feature fusion layer. These layers are then concatenated into an LSTM network for the encoder layer before passing into the classifier for the prediction layer. This was tested on the benchmark VCR dataset and it indicated significant improvements over existing methods and it also provided a more interpretable intuition into visual commonsense reasoning.",
          "Natural Language Inference (NLI) task helps with identifying a natural language hypothesis from a natural language premise (MacCartney, 2009). For this application, as shown in Table 6, we found three articles. The first article presents a new dataset-e-SNL (explanation-augmented Stanford Natural Language Inference) (Camburu et al., 2018)-and the other two articles discuss approaches that can improve NLI. Camburu et al. (2018) extended the Stanford NLI (SNLI) (Bowman et al., 2015a) dataset by providing human-annotated explanations for the entailment relations. This new dataset-e-SNLI-is used in a series of classification experiments involving LSTM-based networks for understanding its usefulness for providing human-interpretable full-sentence explanations. The authors also evaluated these explanations as an additional training signal for improving sentence representation and transfer capabilities of out-of-domain NLI datasets. Thus, from these experiments, the authors conclude that e-SNLI can be used for various goals mentioned above and also be utilized for improving models as well as asserting their trust. Another issue with NLI is the faithfulness of the generated explanations, tackled by Kumar and Talukdar (2020) and Wiegreffe et al. (2021). Kumar and Talukdar (2020) mentioned that existing methods do not provide a solution for understanding correlations of the explanations with the model's decision-making and this can affect the faithfulness of the generated explanations. Considering this problem, the authors proposed and presented a new framework -NILE (Natural language Inference over Labelspecific Explanations). The NILE framework can generate natural language explanations for each possible decision and process these TABLE Selected commonsense reasoning papers. References Name Year Explanation Models XAI metric Dataset Code Ehsan et al. (2018) -2018 Extractive LSTM, Seq2Seq ---Rajani et al. (2019) CAGE 2019 Abstractive GPT, BERT -CoS-E, CommonsenseQA Majumder et al. (2021) RExC 2021 Extractive Transformer -ComVE, e-SNLI, COSe, e-SNLI-VE, VCR -Tang et al. (2021) DMVCR 2021 Extractive LSTM, BERT -VCR TABLE Selected Natural Language Inference papers. References Name Year Explanation Models XAI metric Dataset Code Camburu et al. (2018) e-SNLI 2018 Abstractive BiLSTM, Seq2Seq -e-SNLI Kumar and Talukdar (2020) NILE 2020 Abstractive GPT-2, RoBERTa -e-SNLI Wiegreffe et al. (2021) -2020 Abstractive T5 -CoS-E, SNLI explanations to produce a final decision for the classification problems. To test this approach, the authors used two datasets -SNLI and e-SNLI -and compared NILE with baseline and other existing approaches based on explanation accuracy, in-domain evolution sets (SNLI), and on out-of-domain examples (train on SNLI and test on MNLI) (Williams et al., 2018). Based on the first 100 SNLI test samples, the results indicated that NILE variants are comparable with the ETPA (Explain Then Predict Attention) baseline, and NILE explanations generalize significantly better on out-of-domain examples. For out-of-domain examples (MNLI ), results showed that the percentage of correct explanations in the subset of correct label predictions was significantly better for all the NILE variants. Thus, the authors concluded that NILE is an effective approach for accurately providing both labels and explanations. Further, Kumar and Talukdar (2020) also focused on the need for faithfulness for denoting the model's decision-making process by investigating abstractive rationales. The author proposed two measurements -robustness equivalence and feature importance agreement -to investigate the association of the labels and predicted rationales, which are required for a faithful explanation. This investigation was performed on CommonsenseQA (Talmor et al., 2019) and SNLI dataset using T5based models (Narang et al., 2020). The results indicated that stateof-the-art T5-based join models demonstrate desirable properties and potential for producing faithful abstractive rationales. .",
          "Fact-checking has become a popular application of NLP in recent years given its impact on assisting with misinformation and a majority of the work has been with claim verification (Vargo et al., 2018;Gurrapu et al., 2022). Based on a paper published in 2016, there are 113 active fact-checking groups and 90 of which were established after 2010 (Graves and Cherubini, 2016). This indicates the growth of the fact-checking application. Considering the scope of this literature review, as shown in Table 7, we found four articles on fact-checking. Two of the studies in this section present novel datasets, and the remaining two provide new techniques to improve fact-checking. In 2017, a large dataset for the fact-checking community called LIAR (Wang, 2017) was introduced, including POLITIFACT data. Most works on this data were focused on using the claim and its speaker-related metadata to classify whether a verdict is true or false. The evidence-an integral part of any fact-checking processwas not part of the LIAR and was overlooked. Thus, in Alhindi et al. (2018) extended the LIAR dataset to LIAR-plus by including the evidence/justification. The authors treated the justification as a rationale for supporting and explaining the verdict. Further, they used Feature-based Machine Learning models (Logistic Regression and Support Vector Machine) and deep learning models (Bi-Directional Long Short-term Memory (BiLSTM) and Parallel-BiLSTM) for binary classification tasks to test the data. The results demonstrated a significant performance improvement in using the justification in conjunction with the claims and metadata. Further, Hanselowski et al. (2019) introduced a new corpus for training machine learning models for automated fact-checking. This new corpus is based on different sources (blogs, social media, news, etc.) and includes two granularity levels-the sources of the evidence and the stance of the evidence toward the claim-for claim identification. Authors then used this corpus to perform stance detection, evidence extraction and claim validation experiments. In these experiments, a combination of LSTMs, baseline NN, pretrained models have been used, and their results are compared based on precision, recall, and F1 macro. The results indicated that fact-checking using heterogeneous data is challenging to classify claims correctly. Further, the author claims that the fact-checking problem defined by this new corpus is more difficult compared to other datasets and needs more elaborate approaches to achieve higher performance. It has been noted that the fact-checking systems need appropriate explainability for the verdicts they predict. The justifications that are human-written can help to support and provide context for the verdicts, but they are tedious, unscalable, and expensive to produce (Atanasova et al., 2020). Considering this issue, Atanasova et al. (2020) proposed that the creation of the justifications needs to be automated to utilize them in a large-scale fact-checking system. The authors presented a novel method that automatically generates the justification from the claim's context and jointly models with veracity prediction. Further, TABLE Selected fact-checking papers. References Name Year Explanation Models XAI metric Dataset Code Alhindi et al. (2018) LIAR-PLUS 2018 Extractive SVM, BiLSTM -LIAR-PLUS Hanselowski et al. (2019) -2019 Extractive BERT -FEVER Atanasova et al. (2020) -2020 Extractive DistilBERT -LIAR-PLUS -Rana et al. (2022) RERRFACT 2022 Extractive RoBERTa, BioBERT -SCIFACT this new method is then tested on the LIAR dataset (Wang, 2017) for generating veracity explanations. The results indicated that this new method could combine predictions with veracity explanations, and manual evaluations reflected the improvement in the coverage and quality of the explanations. Another important domain in which fact-checking is useful is Science. Researching and providing substantial evidence to support or refute a scientific claim is not a straightforward task. It has been seen that scientific claim verification requires in-depth domain expertise along with tedious manual labor from experts to evaluate the credibility of a scientific claim. Considering this problem, Rana et al. (2022) proposed a new framework called RERRFACT (Reduced Evidence Retrieval Stage Representation) for classifying scientific claims by retrieving relevant abstracts and training a rationaleselection model. RERRFACT includes a two-step stance prediction that differentiates non-relevant rationales then identifies a claim's supporting and refuting rationales. This framework was tested on the SCI-FACT dataset (Wadden et al., 2020) and performed competitively against other language model benchmarks on the dataset leaderboard. .",
          "Sentiment Analysis is a subset of the text classification field (Minaee et al., 2021). It focuses specifically on the \"computational study of people's opinions, sentiments, emotions, appraisals, and attitudes toward entities such as products, services, organizations, individuals, issues, events, topics and their attributes\" (Zhang et al., 2018). The use of rationales to support sentiment analysis models in NLP is widely used compared to other NLP tasks. For this task, as shown in Table 8, we identified five papers in this field. In 2016, Lei et al. (2016) pioneered rationalization in sentiment analysis by proposing a problem: \"prediction without justification has limited applicability\". To make NLP outcomes more transparent, the authors propose an approach to extract input text which serves as justifications or rationales for a prediction. These are fragments from the input text which themselves are sufficient to make the same prediction. Their implementation approach includes a generator and an encoder architecture. The generator determines which can be potential candidates for a rationale from the input text. Those candidates are fed into the encoder to determine the prediction and the rationales are not provided during training. They employ an RCNN (Region-based Convolutional Neural Network) and an LSTM architecture and when compared with each other the RCNN performed better. The experiment was conducted on the BeerAdvocate dataset. The paper's approach outperforms attention-based baseline models. They also demonstrate their approach on a Q&A retrieval task indicating that leveraging rationales for sentiment analysis tasks is very beneficial. Similarly, Du et al. (2019) claim that explainability alone is not sufficient for a DNN (Deep Neural Network) to be viewed as credible unless the explanations align with established domain knowledge. In essence, only the correct evidences are to be used by the networks to justify predictions. In this paper, the authors define credible DNNs as models that provide explanations consistent with established knowledge. Their strategy is to use domain knowledge to improve DNNs credibility. The authors explore a specific type of domain knowledge called a rationale which are the salient features of the data. They propose an approach called CREX (Credible Explanation), which regularizes DNNs to use the appropriate evidence when making a decision for improved credibility and generalization capability. During training, instances are coupled with expert rationales and the DNN model is required to generate local explanations that conform to the rationales. They demonstrate it on three types of DNNs (CNN, LSTM, and self-attention model) and various datasets for testing. Results show that the CREX approach allows DNNs to look at the correct evidences rather than the specific bias in training dataset. Interestingly, they point that incorporating human knowledge does not always improve neural network performance unless the knowledge is very high quality. Many papers published in the rationalization field indicate that a machine learning system learning with human provided explanations or \"rationales\" can improve its predictive accuracy (Zaidan et al., 2007). Strout et al. (2019) claim that this work hasn't been connected to the XAI field where machines attempt to explain their reasoning to humans. The authors attempt to show in their paper that rationales can improve machine explanations as evaluated by human judges. Although automated evaluation works, Strout et al. (2019) believe that since the explanations are for users, therefore humans should directly evaluate them. The experiment is done by using the movie reviews dataset and by having a supervised and an unsupervised CNN model for a text classification task. They use attention mechanism and treat the rationales as supervision in one of the CNN models. Results indicate that a supervised model trained on human rationales outperforms the unsupervised on predictions. The unsupervised is the model where the rationales/explanations are learned without any human annotations. The selective rationalization mechanism is commonly used in complex neural networks which consist of two componentsrationale generator and a predictor. This approach has a problem of model interlocking which arises when the predictor overfits to the features selected by the generator. To tackle this problem this paper proposes a new framework A2R which introduces TABLE Selected sentiment analysis papers. References Name Year Explanation Models XAI metric Dataset Code Lei et al. (2016) -2016 Extractive LSTM, RCNN -BeerAdvocate, AskUbuntu Du et al. (2019) CREX 2019 Extractive CNN, LSTM -BeerAdvocate, MovieReview -Strout et al. (2019) -2019 Extractive RA-CNN, AT-CNN -MovieReview -Yu et al. (2021) A2R 2021 Extractive BiGRU -BeerAdvocate, MovieReview Antognini and Faltings (2021) ConRAT 2021 Extractive CNN, BiGRU -AmazonReviews, BeerAdvocate a third component for soft attention into the architecture (Yu et al., 2021). The authors have used BeerAdvocate and MovieReview for understanding the effectiveness of the framework. The authors compared results from A2R with the original rationalization technique RNP (Rationalizing Neural Predictions) along with 3PLAYER, HARD-KUMA and BERT-RNP. For implementation authors have used bidirectional Gated Recurrent Units (GRU) in the generators and the predictors. Furthermore, they performed two synthetic experiments using BeerAdvocate dataset by deliberately inducing interlocking dynamics and then they performed experiments in real-world setting with BeerAdvocate and MovieReview. From the results they made two conclusions-(1) A2R showed consistent performance compared to other baselines on both the experiments, ( 2) A2R helps to promote trust and interpretable AI. In the future, the authors would like to improve A2R framework for generating casually corrected rationales to overcome the lack of inherent interpretability in the rationalization models. Existing methods in rationalization compute an overall selection of input features without any specificity and this does not provide a complete explanation to support a prediction. Antognini and Faltings (2021) introduce ConRAT (Conceptbased RATionalizer), a self-interpretable model which is inspired by human decision-making where key concepts are focused using the attention mechanism. The authors use the BeerReviews dataset to not only predict the review sentiment but also predict the rationales for key concepts in the review such as Mouthfeel, Aroma, Appearance, Taste, and Overall. ConRAT is divided into three subodels, a Concept Generator which finds the concepts in the review, a Concept Selector that determines the presence or absence of a concept, and a Predictor for final review predictions. ConRAT outperforms state-of-the-art methods while using only the overall sentiment label. However, Antognini et al. (2021) have further demonstrated that attention mechanism usage can contribute to a tradeoff between noisy rationales and a decrease in prediction accuracy.",
          "Text classification, also commonly known as text categorization, is the process of assigning labels or tags to textual data such as sentences, queries, paragraphs, and documents (Minaee et al., 2021). Classifying text and extracting insights can lead to a richer understanding of the data but due to their unstructured nature, it is challenging and tedious. NLP techniques in text classification enable automatic annotation and labeling of data to make it easier to obtain those deeper insights of the data. For this task, as shown in Table 9, we identified four papers in this sfield. Traditionally, rationales provide well-defined kinds of data to nudge the model on why a prediction is the way it is given the data. Moreover, they require little additional effort for annotators and yield a better predictive model. When classifying documents, it is beneficial to obtain sentence-level supervision in addition to document-level supervision when training new classifications systems (Zhang et al., 2016). Previous work relied on linear models such as SVMs (Support Vector Machines), therefore, Zhang et al. (2016) propose a novel CNN model for text classification that exploit associated rationales of documents. Their work claims to be the \"first to incorporate rationales into neural models for text classification\". The authors propose a sentence-level CNN to estimate the probability that a sentence in a given document can be a rationale. They demonstrate that their technique outperforms baselines and CNN variants on five classification datasets. Their experimentation task uses Movie Reviews and the Risk of Bias (RoB) datasets. On the movie review dataset, their technique performs with a 90.43% accuracy with the RA-CNN (Recurrent Attention Convolutional Neural Network) model and similar strong results are also indicated on the RoB datasets. It seems intuitive that more data or information can lead to better decision-making by the neural networks. Zaidan et al. (2007) propose a new framework to improve performance for supervised machine learning by using richer \"kinds\" of data. Their approach is called the \"annotator rationales\" technique and it is to leverage a training dataset with annotated rationales. The rationales highlight the evidence supporting the prediction. Zaidan et al. (2007) test their approach on text categorization tasks, specifically, sentiment classification of movie reviews and they claim that these rationales enable the machine to learn why the prediction is the way it is. Rationales help the model learn the signal from the noise. ML algorithms face the \"credit-assigment problem\" which means that many features in the data (X) could have affected the predicted result (Y). Rationales provide a \"shortcut\" to simplifying this problem since they provide hints on which features of X were important. Zaidan et al. (2007) used a discriminative SVM for experimentation and the results indicate that this technique significantly improves results for the sentiment classification and they hypothesize that leveraging rationales might be more useful than providing more training examples. Recently, rationales have been a popular method in NLP to provide interpretability in the form of extracted subsets of texts. It is common to have spurious patterns and co-varying aspects in the TABLE Selected text classification papers. References Name Year Explanation Models XAI Metric Dataset Code Zaidan et al. (2007) -2007 Extractive SVM -MovieReview -Zhang et al. (2016) -2016 Extractive SVM, RA-CNN -Risk of Bias -Liu et al. (2019b) GEF 2019 Extractive CNN, LSTM -PCMag Reviews, Skytrax User Reviews Plyler et al. (2022) CDA 2021 Extractive RL, RNN -TripAdvisor Reviews, RateBeer dataset due to which rationale selectors do not capture the desired relationship between input text and target labels. Considering this problem this paper proposes CDA (Counterfactual Data Augmentation) framework to aid rational models trained with Maximum Mutual Information (MMI) criteria (Plyler et al., 2022). CDA consists of transforms-for rational and classificationsbecause of their effectiveness over RNNs in NLP. The authors used TripAdvisor.com and RateBeer datasets for testing CDA with three baselines -MMI, FDA (Factual Data Augmentation), and ANT (simple substitution using antonyms). The results of the rational models were compared using precision and the accuracy of the classifier is reported based on the development set. From the results, authors concluded that the models trained using the CDA framework learn higher quality rationales and it doesn't need human intervention. In the future, the authors would like to explore more on counterfactual predictors and on CDA framework that could connect with other rationalization strategies. Similarly, Liu et al. (2019b) proposed a novel Generative Explanation Framework (GEF) for classification problems that can generate fine-grained explanations. The motivation behind this explanation framework is to provide human-readable explanations without ignoring finegrained information such as textual explanations for the label. For understanding the accuracy of explanations, the authors conducted experiments on two datasets-PCMag and Skytrax User Reviewswhich were processed by the Stanford Tokenizer. Further, the authors used Encoder-Predictor architecture in which they used Conditional Variational Autoencoder (CVAR) as a base model for text explanations and Long Short-Term Memory (LSTM) for numerical explanations. The experimental results indicated that after combining base models with GEF the performance of the base model was enhanced along with improving the quality of explanations. Further, the authors also used human evaluation for evaluating the explainability of the generated text explanations. The authors noted that for 57.62% of the tested items GEF provided better or equal explanations compared with the basic model.",
          "With the advent of deep learning, Neural Machine Translation (NMT) became the successor to traditional translation methods such as Rule-based or Phrase-Based Statistical Machine Translation (PBSMT) (Yang et al., 2020). NMT models leverage Deep Neural Networks architecture to train the model end-to-end to improve translation quality and only require a fraction of the storage memory needed by PBSMT models (Sutskever et al., 2014). The use of explanations to support NMT model's prediction is relatively new, however, there has been some pioneering work to provide more explainability. For this task, as shown in Table 10, we identified two relevant papers in this area. Quality Estimation (QE) models perform well at analyzing the overall quality of translated sentences. However, determining translation errors is still a difficult task such as identifying which words are incorrect due to the limited amounts available training data. The authors explore the idea that since QE models depend on translation errors to predict the quality, using explanations or rationales extracted from these models can be used to better detect translation errors (Fomicheva et al., 2021). They propose a novel semi-supervised technique for word-level QE and demonstrate the QE task as a new benchmark for evaluating feature attribution (the interpretability of model explanations to humans). Instead of natural language explanations, their technique employs various feature attribution methods such as LIME, Integrated Gradients, Information Bottleneck, causal, and Attention. It was shown that explanations are useful and help improve model performance and provide better explainability. Deep learning models are black-boxes because they involve a large number of parameters and complex architectures which makes them uninterpretable. Considering this problem and to bring interpretability in deep learning models (Alvarez-Melis and Jaakkola, 2017) propose a model-agnostic method for providing explanations. The explanations provided by this method consist of sets of inputs and output tokens that are causally related in the black-box model. Further, these causal relations are inferred by performing perturbations on the inputs from the black-box models, generating a graph of tokens, and then solving a partitioning problem to select the most relevant components. To test the methods via runtime complexity analysis with respect to the number of LM (Linear Model) forward/backward passes. Finally for performance standard dataset-specific metrics accuracy (for SST and CoS-E) and macro-averaged F1 (for Movies) are used. The results indicated that UNIREX allows effective trade-off between performance, faithfulness, and plausibility to identify better rationale extractions. Further, authors also mention UNIREX trained rationale extractors can generalize to unseen datasets and tasks. With many ML systems demonstrating performance beyond that of human across many applications, the field of XAI is advancing techniques to improve transparency and interpretability. Das and Chernova (2020) explores XAI in the context of a question previously unexplored in ML and XAI communities: \"Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human?\". The authors investigate this question through the game of Chess where computational game engines performance surpass the average player. They present an automated technique for generating rationales for utilitybased computational methods called the Rationale-Generating Algorithm. They evaluate this with a user study against two baselines and their findings show that the machine generated rationales can lead to significant improvement in human task performance. They demonstrate that rationales can not only be used to explain the system's actions but also instruct the user to improve their performance. Sharma et al. (2020) explores an of rationalization in the mental health support field and understanding empathy. Empathy is important for mental health support and with the rise of text-based internet platforms, it becomes crucial to understanding empathy in only communication. The paper presents a computational approach to understanding empathy by developing a corpus of 10,000 pairs of posts and responses with supporting rationales as evidence. They use a multi-task RoBERTabased bi-encoder model to identify empathy in conversations and extract rationales for predictions. Their results demonstrate that their approach can effectively identify empathic conversations. To improve interpretability for NLP tasks, recent rationalization techniques include Explain-then-Predict models. In this technique, an extractive explanation from the input text is generated and then a prediction is generated. However, these models do not use the rationales appropriately and consider the task input as simply a signal to learn and extract rationales. Zhang et al. (2021) propose a novel technique to prevent this problem with their approach called ExPred where they leverage mult-task learning on the explanation phase and embed a prediction network on the extracted explanations to improve task performance. They experiment with three datasets [Movie Reviews, FEVER (Fact Extraction and VERification), MultiRC] and conclude that their model significantly outperforms existing methods.",
          "This section compares extractive and abstractive rationalization techniques. It can be observed from Figure 4 that there is more interest and focus on extractive rationalization techniques compared to abstractive. There are multiple reasons for this, and the progress in the Automatic Text Summarization (ATS) domain can help explain.",
          "In most extractive rationalization approaches, generating a rationale is similar to text summarization. These rationales contain the salient features of the input text, which users need to understand as the most influenced features of the model's prediction. Next, two steps are implemented while performing the task-(i) irrelevant information is ignored, and (ii) most crucial information is selected based on a scoring algorithm. This approach is a common foundation of summarization techniques. In extractive summarization, meaningful sentences are extracted to form a summary of the original text while still retaining the overall subject matter (El-Kassas et al., 2021). The critical difference with rationalization is that it is able to justify a neural network's prediction with evidence. In a way, extractive rationalization uses extractive summarization's fundamentals and takes it further. It frames the task as can we rationalize the output prediction where rationalize means to understand the prediction process and reason with supporting evidence. This introduces an interdependent relationship between the rationale and the prediction. This process is close to how humans rationalize with a sequence of reasons to justify a decision. This can be implemented in the NLP process to make models more explainable. As interest in ATS systems grew in the past few decades, researchers have mainly focused on extractive summarization due to its simplicity, and reliability (El-Kassas et al., 2021). The abstractive summarization needed reliable natural language generation; thus, it was in its infancy from the 2000s to the early 2010s. Therefore, an increasing body of knowledge on extractive techniques is available, which researchers interested in rationalization could leverage and build on. This intuition behind extractive summarization paves the way for extractive rationalization. The stark difference between extractive and abstractive in Figure 3 is expected and reasonable, and the fields of summarization and rationalization follow similar paths. However, summarization approaches should purely be used for inspiration -following the identical methods for rationalization would be insufficient, and it does not provide reliable model explainability. Chan et al. (2021) notes that for appropriate explainability, the desiderata for the rationale is that-(i) it must reflect the model's reasoning process (faithfulness), (ii) be convincing to the nonexpert (plausibility), and (iii) the rationale extraction should not hurt task performance. Thus, there is more work than simply extracting sentences as rationales. Moreover, extractive rationalization is insufficient because extracted sentences themselves are insufficient to provide full explainability. Humans do not fully understand without context and a coherent and logical explanation.",
          "The extensive research in extractive summarization reached its maturity, has peaked in terms of performance, and now the progress is stagnated (Gupta and Gupta, 2019). Recent advances in deep learning and the advent of the Transformer architecture in 2017 have led to more reliable and influential language models (Vaswani et al., 2017;Devlin et al., 2019). In 2019, Liu and Lapata (2019)  Abstractive techniques allowed novel words and phrases to be generated instead of extracting spans from the input. Due to these advances, the research focuses gradually shifted from extractive to abstractive summarization. It is expected that rationalization will follow a similar trend. Abstractive rationalization is still relatively new, with limited research available. However, there have been promising and pioneering approaches such as Rajani et al. (2019) and Kumar and Talukdar (2020). Almost every paper discussed with an abstractive rationalization technique in Section 4 leveraged some implementation of the Transformer architecture, such as BERT, GPT-2 (Radford et al., 2019), and T5, amongst others. BERT was the most frequently used language model. When BERT was released in 2018, it achieved SOTA results on many NLP tasks and surpassed human performance on tasks such as question answering and commonsense reasoning (Devlin et al., 2019). It made a giant leap in terms of performance compared to other language models of its time. This led to wide adoption and variations of BERT for the tasks where the Transformer-based model was required. Recently introduced models such as BART (Lewis et al., 2020), GPT-3 (Brown et al., 2020), and T5 demonstrate promising results and surpass BERT in some tasks. This is due to language models growing exponentially, and they continue to improve and perform incredibly well at natural language generation (Sanh et al., 2019). For example, in some cases, text produced by GPT-3 is almost on par if not better, than human-written text. This enables more opportunities for research in abstractive rationalization, which is needed. By leveraging SOTA language models, explanations can become more comprehensive and convincing when illustrating a model's decision-making process. As mentioned in Section 2, it is almost as if the models are \"talking for themselves\". We believe that significant progress can be made in rationalization by focusing more on improving abstractive techniques.",
          "In this section, we discuss insights from the literature reviewed, challenges, and potential future directions to propel progress on rationalization. Most importantly, we introduce a new XAI subfield called Rational AI.",
          "In Section 1, we have seen the need for explainability and the available methods in NLP. The numerical methods, such as SHAP values (Lundberg and Lee, 2017b) or Attention scores, visualization methods, such as LIME (Ribeiro et al., 2016), and saliency heatmaps, all require specialized domain knowledge to understand. At the same time, with increasing interactions with NLP-based systems, the nonexpert also deserves to know and understand how these black-box systems work because it has some degree of influence on their lives. This is formally called the right to an explanation, a right to receive an explanation for an algorithm's output (Miller, 2019b). A classic example is a bank system with an NLP model that automatically denies a loan application. In this situation, providing the loan applicant with SHAP values or saliency heatmaps to justify the bank's algorithms is not very meaningful. Thus, explainability methods are truly explainable and helpful if the nonexpert can understand them (Mittelstadt et al., 2019). We introduce Rational AI (RAI) as a potential solution.",
          "Rationalization techniques come the closest to this goal because they are built on natural language explanations (NLEs). NLEs are intuitive and human comprehensible because they are simply descriptive text. The textual information can be easily understood and translated into other languages if needed. Across all of the NLP tasks discussed in Section 4, we have seen the benefits of NLEs and the accessibility it provides to the nonexpert. We believe there is a critical need to focus on explainability techniques with NLEs. Considering these factors, we propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI. Further, rationality should not be confused or used interchangeably with the general AI term of a rational agent Russell and Norvig (2002). These are distinct topics with similar names. In this survey, RAI and rationality are purely considered in the context of model explainability. We also have not seen any usage or previous definitions of RAI within this context. We compare rationality to the other fields shown in Figure 5. Models with interpretability are interpretable, while those with explainability are interpretable and complete, as described in Section 1. Models with rationality are interpretable and complete and can rationalize their behavior through an NLE. The explainability methods described earlier in this subsection explain, but they do not justify in a way that is accessible and comprehensible to the nonexpert. In recent years, language models have become powerful and incredibly good at language generation tasks, but we have yet to see their full potential. As they continue to grow exponentially, we predict this is the beginning of explainability techniques using NLEs. The intuition behind RAI is that rationalization is one such technique, and many are yet to be developed. This calls for a more organized field to improve research focus and the need for RAI to exist.",
          "Although RAI arises from the need for better explainability for NLP tasks, it is potentially applicable in general AI and other fields in AI. Other fields, such as Computer Vision, Speech, and Robotics, could leverage rationalization methods to improve their model explainability. For example, rationalization in Computer Vision can help explain through an NLE which visual features contributed the most to an image classifier prediction in place of complex explainable techniques (Sundararajan et al., 2017;Tjoa and Guan, 2021). Many promising opportunities exist for researchers to apply rationalization in other disciplines.",
          "We have seen that rationalization is a relatively new technique, and with it, various challenges exist. In this subsection, we share challenges and potential solutions to improve the current state.",
          "No standard statistical evaluations exist currently for rationalization. There is a wide variety of metrics that are in use, such as Mean Squared Error (Lei et al., 2016), Accuracy (Zaidan et al., 2007;Du et al., 2019;Rajani et al., 2019), F1 Score (Alhindi et al., 2018;Rana et al., 2022), ANOVA (Analysis of variance) (Das and Chernova, 2020), and Precision (Plyler et al., 2022). We have observed that the most preferred statistical metric is accuracy. It is reasonable for evaluation metrics to be task-dependent and focused on the prediction. However, those alone are insufficient because the accuracy of the NLE also needs to be considered. For example, if the task prediction had high accuracy, but the NLE was unclear and incomprehensible, then it is not helpful. Metrics such as the BLEU (BiLingual Evaluation Understudy) score by Papineni et al. (2002) and the ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation) score by Lin (2004) exist for evaluating open-ended machine-generated texts. However, we have seen limited use in the literature review, such as Camburu et al. (2018). The scores work by comparing the generated text with a set of ground-truth reference texts, and often these are human-written references. These scores are helpful, especially for abstractive rationalization, where explanations can be open-ended; however, they come with limitations since the evaluation is effectively token-level matching. Since an NLE is the typical outcome of systems with rationalization, adopting a standard evaluation metric can help improve research progress. Consistent evaluations also make it easier to compare different experiments and approaches.",
          "The availability and the need for more diversity of appropriate datasets is also a problem hindering progress. Availability: Data collection is an expensive and timeconsuming task. It is possible to repurpose existing datasets, but modifying them requires manual human labor. Thus, researchers often build their datasets for a specific task they are working on. Camburu et al. (2018) developed the e-SNLI dataset by modifying the SNLI dataset from Bowman et al. (2015b). Camburu et al. TABLE Human-Centered Evaluations (HCE) are performed in out of papers surveyed in this review. Year References Method name Venue NLP domain HCE performed? 2007 Zaidan et al. (2007) -NAACL Text Classification No 2016 Ribeiro et al. (2016) LIME NAACL Multiple Domain Yes 2016 Lei et al. (2016) -EMNLP Sentiment Analysis No 2016 Zhang et al. (2016) -EMNLP Text Classification No 2017 Sharp et al. (2017) -CoNLL Machine Reading Comprehension No 2017 Ling et al. (2017) -ACL Machine Reading Comprehension No 2017 Alvarez-Melis and Jaakkola (2017) SOCRAT EMNLP Neural Machine Translation Yes 2018 Ehsan et al. (2018) -AIES Commonsense Reasoning Yes 2018 Alhindi et al. (2018) LIAR-PLUS FEVER Fact-Checking No 2018 Mihaylov et al. (2018) OpenBookQA EMNLP Machine Reading Comprehension Yes 2018 Xie et al. (2020) WorldTree V2 LREC Machine Reading Comprehension Yes 2018 Camburu et al. (2018) e-SNLI NeurlPS Natural Language Inference No 2019 Rajani et al. (2019) CAGE ACL Commonsense Reasoning Yes 2019 Hanselowski et al. (2019) -CoNLL Fact-Checking No 2019 Du et al. (2019) CREX ICDM Sentiment Analysis No 2019 Strout et al. (2019) -BlackboxNLP Sentiment Analysis Yes 2019 Liu et al. (2019b) GEF ACL Text Classification Yes 2020 Atanasova et al. (2020) -ACL Fact-Checking Yes 2020 Lakhotia et al. (2021) ERASER ACL Multiple Domain Yes 2020 Das and Chernova (2020) RGA IUI Multiple Domain Yes 2020 Sharma et al. (2020) EPITOME EMNLP Multiple Domain No 2020 Kumar and Talukdar (2020) NILE ACL Natural Language Inference Yes 2020 Wiegreffe et al. (2021) -EMNLP Natural Language Inference No 2021 Majumder et al. (2021) RExC ICLR Commonsense Reasoning Yes 2021 Tang et al. (2021) DMVCR DaWaK Commonsense Reasoning No 2021 Tang et al. (2021) FiD-Ex EMNLP Machine Reading Comprehension No 2021 Chan et al. (2021) UNIREX ICML Multiple Domain No 2021 Zhang et al. (2021) EXPred WSDM Multiple Domain No 2021 Fomicheva et al. (2021) -ACL Neural Machine Translation No 2021 Yu et al. (2021) A2R NeurlPS Sentiment Analysis Yes 2021 Antognini and Faltings (2021) ConRAT ACL Sentiment Analysis Yes 2021 Plyler et al. (2022) CDA NeurlPS Text Classification No 2022 Atanasova et al. (2020) RERRFACT AAAI Fact-Checking No (2018) achieved promising results on their task, demonstrating how their dataset can enable a wide range of new research directions by altering and repurposing existing datasets. Diversity: Without enough datasets, new research in rationalization will be limited. Researchers will be constrained to the existing datasets to make new progress. This trend is evident in the literature reviewed in MRC and Sentiment Analysis compared to NMT. In MRC, the datasets are very diverse. In sentiment analysis, most papers rely on either the BeerAdvocate (McAuley et al., 2012) or MovieReviews (Maas et al., 2011) datasets to perform their experiments. In both domains, we discovered five publications each. For a domain such as NMT, progress seems limited, and we found only two publications. The lack of appropriate rationalization datasets for NMT tasks is a possible reason for this. As we observed in our literature review, there is a direct relationship between dataset availability and the progress made. More work in creating new datasets for rationalization can help improve diversity and the progress of certain domains lagging behind, such as NMT. New datasets across all domains, in general, will increase the interest and work in rationalization because researchers will have more flexibility in designing new techniques and experimenting with a wide variety of data. Stamper et al. (2010) has organized the largest repository of learning science datasets called DataShop, and it led to improvements in research progress. Similarly, an organized central repository for rationalization supporting datasets can be beneficial. Without a centralized model evaluation and development system, reproducibility and accessibility will remain low.",
          "NLP has direct applications in many disciplines. For example, MRC and commonsense reasoning are helpful in the education discipline. Our literature review indicates using Q&A tools and commonsense injection to generate explanations for educational needs (Mihaylov et al., 2018;Li et al., 2019). Further, NLP has also been used to enhance human task performance, as we saw in Das and Chernova (2020), and to provide support for mental health (Sharma et al., 2020). Additionally, fact-checking is another application, and it is crucial in social media, fake news detection, and law (Alhindi et al., 2018). It has become common to interact with these systems, and they may have a significant influence on all aspects of our society. Due to this, the European Union recently passed a regulation that requires algorithms to provide explanations that can significantly affect users based on their userlevel predictions (Doshi-Velez and Kim, 2017).",
          "The explanations provided by the NLP systems must provide enough information to the user to help them understand its decision-making process (Putnam and Conati, 2019). Considering these aspects, the human-machine partnership is essential for evaluating and generating accurate explanations. This calls for better methods to evaluate the explanations generated. The field of HCE addresses this problem, and Sperrle et al. (2021) defines it as a \"field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios.\" In this literature survey, we found 15 out of 33 papers in which HCE is performed, and a summary is provided in Table 12. Sperrle et al. (2021) shares the increasing trend of HCE since 2017 compared to the previous years. While conducting this literature survey, this trend was not observed in the rationalization domain. Overall, we found that HEC is incorporated in most of the papers on Machine Reading Comprehension (2 out of 5), Commonsense Reasoning (3 out of 4), Fact-Checking (1 out of 4), Natural Language Inference (2 out of 5), Neural Machine Translation (1 out of 2), Sentiment Analysis (3 out of 5) and Multiple Domain (3 out 6). From our observations, researchers give more attention to performance while evaluating AI algorithms and ignore human factors such as usability, user intentions, and user experience. Thus, along with the accuracy of AI algorithms, it is also essential to focus on the interpretability and reliability of the explanations generated by AI algorithms. The articles in which HCE is used are primarily performed via crowdsourcing using Amazon Mechanical Turk, and the focus is on user-based evaluations or annotations. This pattern necessitates conducting expert evaluations to understand users' needs better because it can help improve trust in AI algorithms. HCE is a subset of the Human-Computer Interaction (HCI) field, which is integrated with the AI paradigm after the algorithmic performance evaluations as shown in Figure 6. This integration can be regarded as human-centered AI, and Riedl (2019) claims this as an AI/ML perspective that intelligent systems are part of a more extensive system that also includes human stakeholders. The literature (Hwang and Salvendy, 2010) on usability, and user experience testing demonstrated three widely used methods to perform HCE -Think Aloud (TA), Heuristic Evaluation (HE), and Cognitive Walkthrough (CW). The TA method is a standard method and can be more effective considering the evaluations of explanations in the NLP domain. In the TA method, evaluators are asked to \"think aloud\" about their experience while an experimenter observes them and listens to their thoughts (Fox, 2015). This way, an HCE method can be used in the final step to understand usability, user intentions, and user experience. This may lead to a better understanding of the interpretability and reliability of the explanations generated by rationalization. Therefore, in addition to statistical evaluation techniques, we strongly encourage researchers to integrate HCE as part of their evaluations.",
          "It is critical to perform rigorous testing and validation of NLP systems at all stages before their deployment. For example, it should be ensured that the data is unbiased, models are interpretable, and the process of arriving at the outcome is explainable to a nonexpert. In the last step of this process, it would be beneficial to use RAI techniques. Integrating rationalization with human-centered evaluations and elements of NLP Assurance can invoke human-AI trust and safety with the systems -with the recent rise of chatbots such as ChatGPT, the need for more rigorous validation is more important than any other time. This process may also transform black-box systems into white-box systems and make NLP models more comprehensible and accessible for nonexpert users."
        ],
        "ground_truth_definitions": {
          "abstractive rationalization": {
            "definition": "generating a novel explanation",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "extractive rationalization": {
            "definition": "extracting text fragments from the input",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Interpretability": {
            "definition": "Interpretability aims at developing tools to understand and investigate the behavior of an AI system.",
            "context": "Interpretability “aims at developing tools to understand and investigate the behavior of an AI system” (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to “explain or to present in understandable terms to a human” what the AI system is performing.",
            "type": "implicit"
          },
          "Rationalization": {
            "definition": "justifies a model’s output by providing a natural language explanation.",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Black-box Model": {
            "definition": "A machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans",
            "context": "Given in a Table",
            "type": "explicit"
          },
          "Explainability": {
            "definition": "the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction.",
            "context": "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
            "type": "explicit"
          },
          "interpretable": {
            "definition": "i.e., allowing humans to understand the internals of a model",
            "context": "The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models.",
            "type": "explicit"
          },
          "NLP Assurance": {
            "definition": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users.",
            "context": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain.",
            "type": "explicit"
          },
          "Completeness": {
            "definition": "The ability to accurately describe the operations of a system that allows humans to anticipate its behavior better.",
            "context": "However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model’s decision-making process of arriving at a prediction and the ability to be verified and audited.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "24589ef86242ff182e7bae495b94e7c11931f20c",
        "sections": [
          "Whitehouse: Listening to Birds in the Anthropocene / 55 The post-war Great Acceleration had its symbol in the loss of something meaningful and beautiful, but also common enough to have once been taken for granted. Listening to birds, Carson implied, was no longer something conceivable as straightforward or inevitable. Carson exemplified this shift by quoting an Alabama resident writing about the effects of a massive spraying programme to control fire ants: Our place has been a veritable bird sanctuary for over half a century. Last July we all remarked, \"There are more birds than ever.\" Then, suddenly, in the second week of August, they all disappeared … There was not a sound of the song of a bird. It was eerie, terrifying. What was man doing to our perfect and beautiful world? Finally, five months later a blue jay appeared and a wren. 8 Life in such a perfect world was thus accompanied by the presence of birds, a presence made most readily and delightfully manifest in their songs. When once familiar companions no longer accompanied local residents, the shock was unexpected and unnerving. The Anthropocene is still a new and largely academic term but one that relates to concerns that have been more widely felt, particularly since they were so clearly articulated by Carson. First, there is the concern that everyday experiences of birds and other wildlife are under threat from everyday human activities. Second, there is a belief that local changes and local activities can have global causes and effects. Finally, there is the concern that while things might seem okay in the present, nothing can be taken for granted about the future-not even the near future and what now seems commonplace. The Anthropocene is thus both a term that relates to real and observable changes in the local worlds people perceive around them and to semiotic elaborations on those perceptions that draw together local and global, human and non-human, present and future, into anxiety-laden narratives. There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"9 Anxious semiotics is central to listening to birds in the Anthropocene not only directly in the face of loss but in the tensions of daily experiences that might seem, on the face of it, to be positive. Fluctuations and changes in what can be heard lead back to possible human causes, perhaps even our own activities. Anxious semiotics can even emerge in response to an increase in some species, perhaps spreading in the wake of anthropogenic changes, or to birds singing earlier in the season, perhaps because of climate change. Anxiety points towards potential, as well as actual, loss. It also points towards concerns about human culpability for change, both locally and more generally. Conservationists monitor the fortunes of birds and their habitats assiduously, enjoying the support of much of the public as they do so. But growing desires to attend to and to care for birds and their ecology seem to be continually outstripped by our capacity to disrupt and endanger. The environmentalism spawned by Silent Spring has constantly drawn attention to the ways that ecosystems, both local and global, are affected by human activity. What prospers and what disappears are causally bound together with human actions in ways that are sometimes readily apparent and sometimes barely perceptible. I explore the associations that people perceive as emerging between bird sounds and environmental changes. Following the semiotic approach of Kohn,10 I argue that the symbolic and moral connotations of listening to birds in the Anthropocene follow from their iconic and indexical grounding in places, producing an anxious semiotics in which even positive associations can have portentous or uncertain implications. My argument progresses from recent claims made by Bernie Krause about the evolution of soundscapes and their disruption by humans to a series of narratives contributed to the Listening to Birds project: an anthropological study of people's relations to birds through sound. Finally, I invite the reader to listen to four recordings and consider the sounds they hear as a means of sensing life in the Anthropocene.",
          "Harmony and balance was a state that Carson considered to be under threat in the modern world. She began Silent Spring with the nostalgic statement, \"There was once a town in the heart of America where all life seemed to live in harmony with its surroundings.\"11 This harmony was not just something that could be seen or measured but that could also be heard in the varied mix of sounds in the environment. This point finds elaboration in the work of musician and scientist Bernie Krause, most notably in his book The Great Animal Orchestra. 12 Krause's argument is that places have evolved their own acoustic ecology and that in a natural environment the sounds will tend to fit together rather than disrupting or competing with one another. He begins The Great Animal Orchestra by speculatively describing the soundscape of the American plains, 16,000 years ago and prior to the arrival of humans in North America. An array of now extinct mammals, along with more familiar birds, insects and amphibians, fill the air with their sounds. But despite this complex mix of sounds, each animal is able to make itself heard. Inspired by Murray Schafer's soundscape approach, Krause argues that this is because each species has a sound that occupies a distinct bandwidth and so can exist in harmony with the other sounds that tend to occur around it, be they from other species or from physical factors such as running water. \"This is,\" he writes, \"the tuning of the great animal orchestra.\" 13 This tuning can be rendered visible through the use of spectrograms, which translate sound into a graph plotting time against frequency. As Mundy has pointed out, spectrograms have played a significant role in creating an \"image of evolution,\" but she argues that, more typically, this takes the form of a stable, visible object that is readily comparable and can thus be more easily incorporated into larger taxonomies.14 In Krause's argument, each spectrogram is an image of evolution because it is assumed that the sound of each species is indexically related to the other sounds in the same soundscape, i.e. there is a causal relation between the sounds in an environment and the \"bandwidth\" occupied by the sounds made by each species. This set of inter-relating sounds occupying a distinct sonic niche has emerged, according to Krause, through evolutionary processes. The nesting of these sounds is rendered as a visible object in the spectrogram. Krause divides up sound into three categories. First is the geophony, sounds made by the physical environment such as waves, wind or rain. Second is the biophony, which consists of sounds made by animals, plants and other organisms. Finally is the anthrophony or humangenerated sound. While this classification is ostensibly based on the origins of the sounds rather than what they actually sound like, Krause emphasises the evolutionary emergence of the different types of sound, arguing that: The sounds of geophony were the first sounds on Earth-and this element of the soundscape is the context in which animal voices, and even important aspects of human sonic culture, emerged. 15 So here we have a sense of the sounds of the world developing in relation through the geological eras and into the present day. The different types of sound that Krause outlines might themselves be regarded as sonic epochs in which a particular category of sound emerged or tended to predominate. As such, Krause is implying that we have now entered the epoch of anthrophony and that human sounds are drowning out the biophony and geophony in many parts of the world. The Anthropocene has also ushered in a new kind of anthrophony, with the sounds of industry, machinery, combustion engines and electronic amplification being a rather different and more disruptive type of human-induced sounds than those that would have predominated in the pre-industrial era. As such, I would argue that one can differentiate between pre-and post-Anthropocene anthrophony, the former tending to integrate more closely with other sounds and the latter often disrupting or dominating them. Indeed, Krause's main concern is not with the effects of anthrophony in general but the effects of what he calls electromechanical sounds. Problematic sounds don't so much originate in human bodies themselves but in Anthropocene technologies used by humans. Humans, Krause argues, are not just having an effect on the biophony by drowning out the sounds of other species with their own but also by disrupting ecosystems. Here his concerns are clearly aligned with those of Carson. He is troubled by the loss of sounds but also by the disruption to the harmonious sonic ecosystem. It is not simply a spring of silence that is worrying but one of disharmony. Humans have effects on other sounds not just through their own sounds but through their effects on habitats. These changes reduce the quantity of sound but also cause disruption to the relative harmony of the soundscape. When a biome is compromised, spectrograms will lose both density and diversity, along with the clear bandwidth discrimination among voices that is otherwise visible in nonstressed-habitat graphic displays. Biophonies from stressed, endangered, or altered biomes tend to show little organisational structure. 16 This emphasis on considering sounds within the context of the environments in which they emerge is a point sometimes forgotten in the analysis of bird sounds by scientists and other scholars, who take recorded sounds as discrete, decontextualized objects. Biophonic sounds are thus indexical of the conditions within which organisms live and have evolved. Krause also defines noise in relational terms: I think of noise as an acoustic event that clashes with expectation-loud heavy metal music in an intimate restaurant is noise … A straight-piping motorcycle gunning its way through the delicate landscape of Yosemite Valley shatters the numinous experience for both visitors and animals.17  Noise is thus a conflict that emerges from perceived disjunctions of one set of sounds and another, or between one aspect of the world as experienced and another. It is dissonant rather than resonant, and Krause's argument is that dissonance and noise have increased greatly to the detriment of the biophony and to human well-being. As sounds are the products of activities, this also points to increasing dissonance between human activities more generally and the activities of other species in our environment. These conclusions lead Krause to lament that, \"In order to hear the wild biophonic world, we need to get to places free from human noise.\" 18 The world as it should sound, for Krause, is not just pre-Anthropocene, like the rural idyll Carson draws on, but pre-human. It is a world not of companions but of strangers. How then should one listen to birds in the Anthropocene-in a world in which humans are not just present but dominant and noisy and in which our activities are often dissonant with those of other species? What signs are people listening for and how are they listening? The Anxious Semiotics of Humans and Other Species Krause's work provides support for what many have long realised: that the sounds of birds and other animals are importantly sounds in place. Sound-making happens in relation to other sounds and to the acoustic conditions. For birds, sound-making is also place-making; it is an act of territorialising space, of making relations with other birds and continually re-weaving the context of their lives. In order to do this, as Krause argues, birds make sound in relation to other sounds and both their lives and their evolution enable them to be responsive to signs within their environment. This emphasis on sound and communication as emerging relationally and across species boundaries can be aligned with the recent proliferation of \"more-than-human\" approaches in the social sciences. 19 These new ways of thinking about life have created fresh impetus in the environmental humanities and social sciences, but few authors in this field have explicitly addressed how to analyse the sorts of meanings and communication that emerge in more-than-human relations. A recent exception has been Eduardo Kohn, who has argued that the grounding for how humans and non-humans perceive and understand their world is similar across species. 20 Drawing on the varied works of Peirce, Bateson and von Uexküll, Kohn argues that the semiosis of life is grounded in iconic and indexical signs. The tendency of anthropology and other social sciences has been to emphasise symbolic signs, particularly language, as the primary means of communication. The apparent absence of symbolic communication in non-humans has fuelled an emphasis on human communicative exceptionalism that appears to forge a gulf in human and non-human semiotics. Kohn argues that symbols should instead be seen as nested within a semiotic field of icons and indexes from which their meaning emerges. In this respect, the ways that humans find meaning in the world is not so different to the ways that other organisms do: they are all grounded in the iconic and indexical. 21 Birds perceive the representations of other organisms, including those of their own kind, and their sound-making emerges in association while, for humans, the symbolic only \"works\" because it is emergent from the same sorts of associations. As Kohn puts it, \"The semiosis of life is iconic and indexical. Symbolic reference ... is an emergent dynamic that is nested within this broader semiosis of life from which it stems and on which it depends.\" 22 What follows from this grounded and emergent concept of the symbolic is morality. \"The moral is also distinctively human, because to think morally and to act ethically requires symbolic reference. It requires the ability to momentarily distance ourselves from the world and our actions in it to reflect on our possible modes of future conduct … This distancing is achieved through symbolic reference.\" 23 While I am more hesitant than Kohn to exclude ethical consciousness from non-humans, I agree that both the symbolic and the ethical emerge from the iconic and indexical semiosis of being-in-the-world 24 and that the symbolic also provides the potential for a conceptual, and usually partial, distancing from this immersion. Being-in-the-world is, for humans at least, double-edged because it entails a constant oscillation along a spectrum of mindfulness 25 that runs between engagement and detachment (see also the work on scientists and naturalists by Candea, MacDonald, and Ellis), 26 between Dwelling and Skill (London: Routledge, 2000); S. Eben Kirksey and Stefan Helmreich, \"The Emergence of Multispecies Ethnography,\" Cultural Anthropology 25, no. 4 (2010): 545-576. 20  Kohn,How Forests Think. 21 Ibid.,[38][39] Ibid., 55. 23 Ibid., 133. 24 By \"being-in-the-world\" I wish to convey a relational sense of life that is derived from thinking of the organism-in-its-environment as indivisible and emergent. \"Being\" cannot be separated from \"the world.\" the real and the symbolic, between is and ought. Focusing on this oscillation is, I argue, essential to understanding how people listen to birds in the Anthropocene. In order to clarify my argument, I first need to describe the semiotics of being-in-theworld that I have in mind. Developing Kohn's argument, I bring his semiotic approach into conversation with Tim Ingold's claim that life is best realised as a meshwork. By doing so, I aim to add some flesh to Kohn's semiotics and give a sense of how meanings emerge from the entanglements of the meshwork. Ingold introduces his understanding of meshwork thus: Every… trail [of movement or growth] discloses a relation. But the relation is not between one thing and another-between the organism 'here' and the environment \"there.\" It is rather a trail along which life is lived. Neither beginning here and ending there, nor vice versa, the trail winds through or amidst like the root of a plant or a stream between its banks. Each such trail is but one strand in a tissue of trails that together comprise the texture of the lifeworld. This texture is what I mean when I speak of organisms being constituted within a relational field. It is a field not of interconnected points but of interwoven lines; not a network but a meshwork. 27 As such, any organism is a tangle of interconnecting life lines and thus is emergent from these relations and associations. \"Organisms and persons … are not so much nodes in a network as knots in a tissue of knots, whose constituent strands, as they become tied up with other strands, in other knots, comprise the meshwork.\" 28 For Ingold then, life is characterised by movement and constant birth; it is a process out of which forms emerge but are never fixed. A question that arises from such a view of life is how, in such a fluid and entangled world, can any phenomenon be knowable enough to be dealt with or represented? How can living beings make a good enough \"guess\" at what is emerging to keep on living in a form that is iconic of what it has been before? It is in answer to these questions that Kohn's employment of Peirce's semiotics can be helpful. According to Kohn, Peirce \"strove to situate his entire philosophical project … within a special kind of realism that could encompass actual existence within a broader framework that would account for its relationship to spontaneity, growth, and the life of signs in human and nonhuman worlds.\" 29 This \"broader framework\" might be better thought of as a meshwork-a way of thinking about the world as emergent, fluid and entangled that chimes with both Kohn's and Peirce's ontological claims. The lines, knots and texture of the meshwork are themselves semiotic. Iconic forms are continually emerging in the continuous rebirth of the world; materialised indexical associations are what \"things\" are made of. The meshwork is semiosis made flesh. Kohn develops these themes by drawing on Peirce's concepts of \"firstness,\" \"secondness\" and \"thirdness.\" Firstness is that which is spontaneous and ephemeral-a glimpse of possibilities that are as yet unrealised and unconnected. Kohn describes the tantalising whiff of cinnamon from a freshly opened passion fruit as an example of firstness. Secondness is more startling and sometimes shocking; secondness can be unpredictable or a jolt to habits. For Kohn, a crashing palm tree exemplifies secondness, as a sudden disruption in the meshwork that might realign its relations and draw attention to troubling indexical causation. Thirdness, finally, is more general, regular and habitual. It can take the form of behaviour but can also be seen in terms of certain general tendencies of life such as selforganisation and resistance to entropy. As Kohn argues, \"This tendency is what makes the world potentially predictable and what makes life as a semiotic process, which is ultimately inferential, possible. For it is only because the world has some semblance of regularity that it can be represented. Signs are habits about habits.\"30 Thirdness is the sort of regularity that Krause hears represented in a relatively uncompromised ecosystem in which each bird responds to the regularities of other sounds and acoustic conditions, and it is essential to semiosis more generally. All signs contain aspects of firstness, secondness and thirdness but tend to amplify one of these more than the others. They are all relational. Humans and other beings can sometimes be shocked (secondness) by what is encountered, but being shocked draws deeper attention to habits and regularities. We know the world and our movements within its meshwork differently through shock, and listening to birds in the Anthropocene is habitually shocking. In examining perceptual practices in the meshwork, Ingold writes not of shock but instead marks a distinction between astonishment and surprise. 31 Astonishment, he argues, is a response to a world that is open and continually being remade. In this, even the mundane can be astonishing. Surprise is a response to a world conceived of as closed and predictable, which thus creates the possibility for surprising deviations. Astonishment involves \"riding the crest of the world's continued birth\"32 whereas surprise involves grasping the world \"within a grid of concepts and categories.\" 33 The former Ingold associates with animism and the latter with the sciences. However, I do not see the two sorts of response as being wholly inimical. Kohn's argument that the symbolic realm, in which the world might be conceived of as closed, predictable and thus potentially surprising, is itself emergent from an openness to the world, in which entanglement, continuous birth and astonishment are endemic. Moving too far into the closed, symbolic world can be pathological. As Kohn points out when describing his own temporary breakdown, symbolic semiosis that is divorced from an iconic and indexical grounding can easily induce panic: Panic and its dissipation reveal these properties of symbolic semiosis. They point both to the real dangers of unfettered symbolic thought and to how such thought can be regrounded. Watching birds regrounded my thoughts, and by extension my emerging self, by recreating the semiotic environment in which symbolic reference is itself nested. Through the artifice of my binoculars I became indexically aligned with a bird, thanks to the fact that I was able to appreciate its image now coming into sharp focus right there in front of me. 34 In his account, Kohn describes a movement from being deeply alienated and ungrounded in his semiosis while travelling on a bus to the dissipation of this panic when he takes time to look at a tanager while out walking. This move marked a 're-astonishment' with the world, while the panic was a shock that revealed more clearly his regular habits of being. The move was not so much between engagement and detachment but between immersion and alienation. This is what can happen when symbolic thoughts drift free from their anchoring in the indexical and iconic conditions of being-in-the-world. As Candea argues in relation to his work with scientists studying meerkats, detachment is not the opposite of engagement but a particular manifestation of engagement, one that enables scientists to understand the world of the meerkats more deeply and sensitively. 35 It is, in fact, a manifestation of the oscillation I described earlier between the real grounding of being-in-the-world and the symbolic and ethical. The surprise of a scientist is not so much a product of disengagement but of the oscillation towards an emergent symbolic semiosis in which the world is conceptualised as a closed model. However, in producing such a model the scientist needs to be fully engaged with a world of complex movement and constant form generation. They need to have consciously dwelt in the meshwork in order to build the model. In order to be surprised, one first needs to be astonished. The shocking surprises of the Anthropocene are most readily perceived by those who find the world constantly astonishing. My reason for discussing these relational semiotic approaches is to emphasise that the symbolic and ethical associations and interpretations of listening to birds in the Anthropocene are grounded in an experience of being-in-the-world; they are not simply cultural models that are imposed onto what is perceived. Any such models or symbolic associations are emergent from being-in-the-world rather than prior to it. Likewise moral ideas are emergent from the iconic and indexical grounding of the meshwork. Following Ingold and Kohn I argue that it is not possible to have symbolic and moral ideas without first being instantiated in the world in a meshwork of relations and associations that are at once both material and semiotic. As such, the ways that humans listen to their surroundings are not fundamentally different in their grounding to the ways that birds listen to theirs. In the narratives that follow a range of moral and symbolic ideas are represented, but these draw on the indexical associations of place and temporality. While the approach that I have outlined above is more broadly about how humans and other species perceive the world, there are some particularities to listening to birds in the Anthropocene that require elaboration. I argue that the semiotic context of the Anthropocene is peculiarly anxious because, as I pointed out earlier, it emphasises our separation from the rest of life just at the moment we connect with it. As Aldo Leopold knew, \"One of the penalties of an ecological education is that one lives alone in a world of wounds.\" 36 The Anthropocene world is increasingly not a world of reassuring thirdness and regularity but of a shocking secondness-such as Carson wrote about in Silent Spring-in which the causes of deleterious change seem to lead us back to our own actions. As the indexical and iconic grounding of our being-in-the-world shifts, those wounds become more apparent to those that are the least alienated from other forms of life. The moral and symbolic fallout is the anxious semiotics of the Anthropocene.",
          "I derive three principles from the above discussion that together provide insights into the particularities of listening to birds in the Anthropocene: 1. The semiotic grounding of perception is an indexical and iconic meshwork, both for birds and for humans. Although the meshwork is constantly emerging, there is still potential for regularities (thirdness) that are responded to by organisms in their actions and that provide a basis for the continuance of their lives. 2. The symbolic emerges from this grounding and provides potential for an oscillation between different kinds of mindfulness e.g. engagement and detachment, real and ideal etc. Human perception often involves this kind of oscillation. 3. The semiotic grounding of the Anthropocene is particularly unstable and the causes of this instability can often be traced to human action. The perception of instability and uncertainty thus provokes a distinctly anxious semiotics that can shock and unsettle senses of place and time, as well as provoking moral disquiet. To add some flesh to the bones of these principles, I turn now to the Listening to Birds project, an anthropological study of people's relations with birds through sound. Through this research I received many stories from people narrating their own experiences of listening. 37 Most came from respondents in the UK but many were also contributed from other parts of the Englishspeaking world, including Australia and New Zealand. I take these narratives as a means by which people highlight what sounds they notice, why those sounds are significant and how they respond to changes in what they hear. They also illustrate broader moral and symbolic dimensions of people's relations with their environment and the role that the sound-making of birds plays in this. What I find most striking about many of the stories is that they concern listening to birds in familiar places around where people live. They are about birds that live alongside people and inhabit their gardens and towns. These are not stories of distant soundscapes in remote and wild places but of companion species that make their own places through sound in the places that people make. What also becomes clear in these narratives is the importance to people of a sense of resonance with birds and with their environment more generally. By this I mean a kind of iconic resemblance or indexical association that arises between human activities and those of birds, as represented in their sound-making. The activities of birds are widely understood to resonate with the rhythms of time and season and, following from this understanding, many 37 These were received via the project website, which invited contributions of people's experiences of listening to birds. The website was widely publicised when the project was covered by the BBC News website. respondents expressed feelings of well-being in response to hearing birds. This seems most apparent when people's lives also resonate with these rhythms,38 as Rob from Droitwich wrote: As a farmer I'm up well before dawn at this time of year but even now there are birds singing in the morning. I heard my first dunnock yesterday. They have already started pairing up and the little owls have been amusing us with their bright chirrups every evening and morning. They certainly brighten up the dark mornings and evenings. As we move into spring I'm lucky enough to hear the dawn chorus every day and it does give me a real lift. Lambing time can be magical when you see new-born lambs taking their first steps to a sometimes deafening suite of blackcaps, robins, chiffchaffs, willow warblers and the rest. If you can't appreciate that then there's something seriously wrong with you. When people like Rob attend to the activities of birds as they go about their lives they are given, as he puts it, \"a real lift.\" This lift, I argue, stems from a sense of resonance that comes from this sympathetic attention to the activities of other beings around us. The sort of attunement of activities that Rob describes resonates, but it also requires that he be sensitive to the lives of those around him. Rob is, according to his account, astonished on a daily basis by listening to birds. His engagement with his surroundings enables him to notice the daily and seasonal changes and regularities. The signs of new and burgeoning life that he perceives serve to exemplify the stability of the place where he lives and the life he leads in it. This resonant grounding even draws Rob into making a moral point about the need for others to appreciate such an experience, perhaps reflecting an anxiety that many do not. His sensing of place and time and of his own resonance with birds is grounded in his being-in-the-world but also draws forth this generalised reflection. As Ingold argues, resonance is analogous to the rhythmic interplay of musicians, who may be playing different sounds but who are following the patterns of sounds around them in ways that are complementary and harmonious. 39 As Krause implies, resonance is also integral to the listening and sound-making of birds and other animals, as they make themselves heard through resonating with the acoustics of their own worlds. The apparent regularity of home, as signalled to Rob by the sound-making of birds, is important in forming many people's sense of the temporality of days and seasons. Rachel Carson clearly understood this connection and its power. Indeed, her book might better have been called Silenced Spring. Spring as a period on the calendar might not be silent without the birds, but for many it would scarcely have been spring either. Bird song does more than simply herald the spring; in an important sense it is spring. As Carson realised, the Anthropocene was making home seem less stable and unproblematic. Many narratives describe the effects of this instability in how home sounds through its birds. A respondent from northern England wrote: I grew up in a suburban area of a northern industrial town and was used to seeing lapwings flitting away across the fields, pee-witting as they did so. I haven't seen a lapwing anywhere near the area for years now and it saddens me to think that my daughters won't get that simple joy of seeing and hearing something otherworldly on their doorstep. Whitehouse: Listening to Birds in the Anthropocene / 65 In this example, an integral sight and sound of the rural-urban fringe has disappeared, probably because of the changes wrought by modern agriculture and our attempts to produce more food more cheaply. The silencing of lapwings is felt anxiously by a father who wants his daughters to experience some continuity with his own sense of place. Again, the grounding of sights and sounds comes to have symbolic connotations of home and of this respondent's own biography, but the differences between his own experience and that of his daughters provokes shock. Other narratives describe new birds that have moved in, like the Asian ring-necked parakeets that now add colour and sometimes discordant sound to the south-eastern suburbs of England. A respondent from Surrey writes: Over the last year or so I started hearing strange squawking in my garden. Normally the bird songs are of the usual garden variety, so this was interesting. It turns out that we have ring-necked parakeets living in the area and it was them creating the squawks and screeches. Now I listen out for them as it's lovely to see the bright green parakeets flying around and \"terrorizing\" the local woodpigeons. The ordinary suburban lives of the English Home Counties are enlivened by these exotic birds, terrorising the woodpigeons and contrasting with bird songs \"of the usual garden variety.\" Sometimes the apparent stability and quietude of suburbia needs something to shake it up; the shock of the new serves to emphasise familiar regularities. But whether these stories concern disappearing species or incoming exotics they speak of the sorts of connections with place that people make through bird sounds and of the acoustic ecologies of places. This grounding leads on to moral pronouncements about how people would like places and times to sound and of the sort of resonance they seek. In this final example, a respondent from Cumbria articulates memories of the place where he grew up before raising concerns over the increasing dissonance between farming and the activities of the birds, a dissonance that has led to the silencing of one particular bird: I grew up in Ireland and lived about three miles from Foxford, County Mayo. In the summer, and especially in the late evenings and at night, one could hear the corncrake call-lots of them-some near, and others far away. The call told me that summer was at last here. I would dearly love to hear that sound again or for my children and grandchildren to hear it too. Sadly the corncrake went into decline because of silage making-farmers cutting grass while the corncrake and other wild birds are nesting on the ground. I would love to see a comeback. The resonance that my respondents described in relation to the sound-making of birds is, I argue, an emergent ethic that draws lives and activities together. It is integral to how they experienced their surroundings but also to how they idealised experience and how they wanted their relations with a place to be. The desire for corncrakes to make comeback in County Mayo derives from this sort of idealisation of how a place should be experienced, and this ethical concern is thrown into sharp relief by their disappearance since the advent of Anthropocene agricultural intensification. This desire for resonance is not just about how things sound but about fitting in with the activities of birds in a way that is sometimes iconic and sometimes complementary. Listening to birds, for my respondents, is not simply about apprehending the sound but about trying to understand how sounds relate to their own activities and the places and temporalities they inhabit. When people move to new areas or other countries they often find the different bird sounds they encounter to be unsettling, to precipitate a sense that they don't belong and that their activities no longer resonate with the right kind of bird sounds. Even without the abruptness of moving to a new area, changes in local bird populations and the loss of species and their sounds can foster a sense of anxiety and loss. But silence, as Carson and Krause both realised, is more unsettling still. My respondents are, I think, revelling in perceiving the activities of other species as they become manifested in sound. Life makes sound and movement and their lack can easily come to be understood as a kind of death. The silence and stillness of birds killed by pesticides or motor vehicles, or habitats depleted by logging or drainage, promote an anxious semiotics of death and loss typical of the Anthropocene, when change and loss is readily attributed to disruptive human actions, sometimes perhaps even the listener's own. Silence and discordance are symptomatic of the Anthropocene.",
          "To explore these issues further, I now invite the reader to listen to four sound recordings of birds that were made in four different countries. These span a range of situations from urban gardens, to islands, to a celebrated rural location for birds, to a rainforest remnant where an enigmatic bird lingers on the edge of extinction. I offer a description of my own listening as a starting point for the reader and as a guide to what is being heard. These are not intended as exhaustive interpretations but to facilitate closer listening. I aim to assist the reader in understanding why these places sound the way they do and to encourage them to listen more closely to what they hear as they go about their lives more generally. The anxious semiotics that is never far away only emerges through active listening. Finally, I add as a proviso that listening to these recordings is not the same experience as \"being there.\" Listening is not simply a process in which sound is heard but is a whole bodily experience of being in place in which sound is a focal point. Recordings may \"transport\" one somewhere but they do not furnish the entire semiotic context and sense of being-in-the-world. Capertee Valley, New South Wales: June 2013 40 I spent a week writing a draft of this paper in the, to me, unfamiliar surroundings of the Australian bush. The Capertee Valley is a renowned area for birds a couple of hours from Sydney: a mix of woodland, fields, hills and valley. Certainly there is abundant life here: a constant twitter of fairy-wrens, thornbills and red-browed finches; the chirrups of a Jacky winter; a garrulous cry from a kookaburra or a currawong; the eerie calls of Australian magpies and ravens; the distant squawks of a flock of cockatoos and the agitated bleating of masked lapwings. What do these sounds tell me about the place where I am? At times it seems chaotic, with a whole range of disparate and similar sounds competing with one another. But I am, unusually, in a place where birds, together with a few insects and amphibians, are almost all I can hear. If this place sounds of anything it sounds of its birds. It is far from silent, or even quiet, but does seem to have what Krause calls tranquility. 41 Silence, he argues, is not a desirable state but quite disorienting. In silence, as Carson probably realised, it is hard to find a sense of place. Tranquility, on the other hand, is a state between noise and silence in which a kind of endorphin-rich serenity is attained. The only significant disruption to the sounds of birds comes when a plane flies high overhead. I likely flew over this area on my own journey to Australia and so the sound of aircraft is, in part, enabling me to hear this place. But air travel also seems to be contributing to climate change and the increasingly severe summer heat that regularly leads to devastating fires in this part of Australia. The valley is, to use Haraway's phrase, \"full of bumptious life\" but the strains of Anthropocene anxieties and the ironies of global travel still encroach. 42 Mata da Balbina, Brazil: August 2008 43 The Atlantic forest of eastern Brazil has fragmented enormously over the last century. Many of its bird species have disappeared or become extremely scarce and localised. Some are so rare they are almost like ghosts. One such bird is Stresemann's bristlefront, a thrush-sized bird with a long tail and mass of bristles around the base of its bill from which it takes its name. Until the mid-1990s this was a species only known from two museum specimens. It seemed like a shadowy myth of the past but was then discovered fleetingly in the coastal forests before disappearing as soon as it had appeared. Then in the early 2000s it was found again in a fragment of hilltop forest called Mata da Balbina close to the border of the Bahia and Minas Gerais provinces. Here the small population lingered and I was able to visit in 2008, together with another British birder. We arrived at the forest knowing little about the bird and our knowledge of its singing came from a single online recording made the previous year. The field guides that we used had no description of the sound at all, presumably because it was unknown when the guides were produced. After some time searching, a liquid, cascading song rose up from the thick undergrowth. It was soon close enough to make a recording, which we carefully played back, quietly and at long intervals, to try to entice the bird to show itself. 44 The bird we could hear in the forest sounded slightly different to the online recording because it added in an alternating ending to the song. Curiously, it seemed to respond more enthusiastically to the recording we had downloaded rather than the playback of its own song. It overlapped with the recording almost as if in a duet. The lack of scientific information on the singing of Stresemann's bristlefront rather liberated my thinking about what I was listening to; the singing had no \"facts\" to conform to. Almost an hour passed in the midst of the dense tangle of branches before we saw a female creep nervously across the forest floor, its singing encircling us enigmatically. It was a 41 Krause, The Great Animal Orchestra, 216. 42 Donna Haraway, The Companion Species Manifesto: Dogs, People and Significant Otherness (Chicago: Prickly Paradigm Press, 2003), 32. 43 See audio clip #2 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 44 The ethical issues surrounding playback of bird sounds are complex. For further discussion see David Sibley, \"The Proper Use of Playback in Birding,\" accessed 27 February 2015 http://www.sibleyguides.com/2011/04/the-proper-use-of-playback-in-birding/. beautiful but disconcerting experience; I wondered if the recording I made of its song might outlive the species itself. Stewart Island, New Zealand: November 2008 45 The small harbour of Oban is named after a port in the west of Scotland but this Oban is the main settlement on Stewart Island, an hour or two's boat ride from the southern tip of New Zealand's South Island. It's only my second day in the country, after a lengthy flight from the UK. Jet-lagged and weary, I had already had the strange experience of finding myself on the other side of the world, switching from early winter to early summer as I did so. And yet the birds were much the same as at home in Britain, thanks to a range of familiar garden birds that had been brought to New Zealand by acclimatisation societies in the late 19 th century. The impoverished avifauna of the main islands is swamped by these settler birds, most of which are well-regarded songbirds back in Europe. The sense of similarity to home is oddly disorienting. I am, in part, shocked by the familiarity of what I hear. I arrived by boat in Oban and began to hear some of the \"real\" New Zealand birds: the voluptuous warbles of tuis and the raucous gabble of kakas. These exotic sounds contrasted with the more prosaic chatter of house sparrows and the descending chirruping of chaffinches. This mix of the recognisable sounds of home and the new sounds of the Antipodes jarred, both in terms of familiarity and in the quality of sound. A duck flying over was again familiar, although most ducks in the area were a hybrid mix of introduced European mallards and native grey ducks. An oystercatcher and a gull were both familiar sounding too, much like those that careen about the coastlines of Scotland, but these were subtly different native species rather than introduced birds. Human history and the sounds of birds are inextricably bound together in New Zealand. The relatively short period of human settlement has shaped the avifauna and ecology in dramatic and readily perceptible ways. As much as the presence of the birds I hear, I sense the absence of those that have disappeared since Europeans arrived: bush wren, piopio, huia and many others. In Krause's terms it sounded like an ecosystem that has yet to become sonically integrated, with disparate types of sound, meant for different acoustic and ecological conditions, in uneasy coexistence. Aberdeen, Scotland: March 2014 46 A bright morning, just as winter is turning to spring. The winter has been the wettest on record, perhaps an effect of global warming; the switch to mild, dry weather seems to encourage a rush of activity in the botanical gardens. I'm reminded of the hubbub of sounds from the Capertee Valley: a rush of twittering finches, tits, dunnocks, treecreepers, goldcrests and robins. A great spotted woodpecker taps loudly at a trunk and, being near the coast, the wail of a herring gull cuts through the air. While the gardens seem like a green oasis in the city, the 45 See audio clip #3 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 46 See audio clip #4 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with an Olympus LS11. Whitehouse: Listening to Birds in the Anthropocene / 69 distant thrum of traffic is ever-present and soon the stream of birdsong is engulfed by a passing helicopter heading out to serve the offshore oil rigs. These garden and suburban sounds seem homely enough and the vibrancy of early spring exudes from the tumble of sounds. The clashes and contrasts are most obvious with the rumble of traffic, both aerial and terrestrial. I wonder if the birds are adapting their soundings to the ambient conditions, just as great tits are purported to have done in noisy urban areas. 47 Is the loud exuberance of singing a response to the noisy environment as much as the onset of spring warmth? The above descriptions reveal something about the sounds on the recordings and the context of their making. They also reveal a lot about how I listen to birds and how my own Anthropocene anxieties inform how I do this. My listening practices are somewhat specialised, given that listening to birds has been a central part of my life, but there are certain aspects of my own process of listening that I consider to be exemplary of how many people listen to birds in the Anthropocene. Much of what might be deemed distinctive about my own practices derives from a fine-grained knowledge of what kinds of birds I am hearing. Although in some situations I was not entirely familiar with the local avifauna, I could, with a little practice, put a name to many of the sounds. This was a result of careful listening and comparison to recordings but also through visual identification, facilitated by field guides illustrated with systematically arranged iconic images and descriptive texts. 48 But, while I was doing this to a rather precise degree, the naming of what is being heard is common to any narrative of listening. Even if the listener only thinks of the sound as being a bird then they are entering into the symbolic process of relating a perceptual encounter to a linguistic marker. This process of naming enters the experience into other sets of relations. Some named sounds might be redolent of place and time; others might be out of place or exotic. Some will be familiar and expected; others could be strange and unexpected. Like many people, I am astonished by the liveliness of the birds I listen to, but I can also be shocked, both by the strange case of a ghostly bird clinging tenuously to existence and by the familiarity of the birds I hear in another hemisphere. When people name aspects of what they directly encounter, they enter those phenomena into a whole range of other narratives, relations and reflections, which are in turn re-entered into the way one listens to one's surroundings. In the Anthropocene those new relations often lead us back to ourselves and the actions of humans, leading us into listening for discordance, disruption and absence. Even in hearing familiar and comforting sounds there is an anxiety in knowing that these can shift abruptly, or are competing with ever encroaching noise and habitat destruction.",
          "The more we care about our world and the more we pay attention to it, the worse things seem to get. This is a powerful anxiety for many in the Anthropocene. Engaging with and reflecting upon the world is essential to our own sense of well-being, but it brings with it the realisation of our own destructiveness. The more we listen to birds the more we notice the loss of birds from pesticides, the destruction of habitat, the encroaching dominance of Anthrophonic sounds, the sounds that are out of place and the ecosystems that are dissonant. There is nothing new about the way that the semiotics of our listening oscillates between our being-inthe-world and our reflection upon it. The semiotics of the Anthropocene, however, destabilises the groundings of our perceptions and draws our reflections anxiously towards our own disruptiveness. Silence and loss hang heavy in the Anthropocene, but this does not mean that our ongoing relations with birds and their sound-making are not fruitful or that we should desist attending to them to avoid the inevitable unease. These lives are ones that people often hope to resonate with, that is, to attend to them in a way that is ethical, that is aesthetic. As Donna Haraway puts it: \"All ethical relating, within or between species, is knit from the silk-strong thread of ongoing alertness to otherness-in-relation.\" 49 The \"ongoing alertness\" to the sounds of birds that my respondents told me about is just this kind of ethical relating, but it is a relating that is importantly grounded in the same kinds of semiotic processes through which birds listen to their own world. I argue that listening to birds in the Anthropocene should not encourage a separation of human activities from those of birds but should instead ground the development of relations of companionship. It elicits not simply a narrative of encroaching loss and the ever present threat that humans pose to non-humans, but one of enskillment, of how we learn to listen to birds and to the rest of our world and how we learn to make ourselves, however uneasily, at home in it. Andrew Whitehouse is an anthropologist and birder teaching at the University of Aberdeen, Scotland. His work has explored conservation conflicts in western Scotland, landscape and anthropology and, more recently, people's relations with birds through sound. He is co-editor of the volume Landscapes Beyond Land: Routes, Aesthetics, Narratives published by Berghahn in 2012."
        ],
        "ground_truth_definitions": {
          "anxious semiotics": {
            "definition": "Sense of uncertainty and concern over potential human culpability in loss.",
            "context": "There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 π r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP α for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return α (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "eb29476dd81aefedf2896db42f039f003a0ec5bf",
        "sections": [
          "Intense competition from large-scale growers has forced small-scaled farmers to find new niche markets for their commodities through value-added marketing. To discover the right niche market is a complicated task because demand is highly segmented among consumers who may be concerned with different attributes such as local, organic, and other specialty types. A recent survey of 500 Twin Cities consumers showed that consumers buy locally grown produce primarily because of its high quality and freshness (King, 2007). Previous studies that focused on consumers' choices at farmers' markets in other states showed that today's consumers support local farmers' markets because they expect higher quality (for example, freshness and taste) and lower prices (Brown, 2002(Brown, , 2003)). Organically grown produce is considered to be healthy and environmentally friendly because of the use of less-damaging pesticides (Magnusson et al., 2001;Thompson and Kidwell, 1998). For some consumers, premium price is the major obstacle in buying organic foods (Magnusson et al., 2001). Yet, ''locally grown'' produce is not necessarily ''organically grown'' produce. Locally grown produce can be produced using conventional production methods. Organic produce purchased by Minnesota consumers might be from California or New Zealand. Finding out why certain consumers prefer organically grown or locally grown produce and their willingness to pay (WTP) for fresh produce with different attributes is important for stakeholders such as producers and marketers in making their production and marketing decisions because it helps them figure out what type of fresh produce to grow and sell, what to emphasize in marketing efforts, and what are reasonable prices to charge. The literature on consumer preferences for and marketing segmentation of organically and locally grown fresh produce is sparse. Loureiro and Hine (2002) studied consumers' WTP for local (Colorado-grown), organic, and GMO-free potatoes. They found that the ''Colorado-grown'' attribute afforded potato producers the highest consumer acceptance and premium compared with organic and GMO-free potatoes. However, only consumers' attitudes toward potato freshness and nutrition were included in their estimation. Important consumer attitudes toward certain quality attributes such as appearance, taste, food safety, and so on (Yue et al., 2007(Yue et al., , 2008) ) were not included in their study. The traditional hypothetical survey method is preferred by most academic researchers in consumer preference studies. The hypothetical method asks individuals to choose the values they are willing to pay for certain products without asking them to exchange money and real goods. Although widely used, a question about this approach that has been aroused is whether values elicited from hypothetical experiments reflect the amounts individuals would actually pay. It is well documented that consumers are less reluctant to spend money in stated preference studies than in markets with real economic commitment. In the literature, this is known as the hypothetical bias (see, for example, Alfnes et al., 2006). Previous studies also show that there is significant hypothetical bias when consumers' WTP is elicited using pictures instead of real products in hypothetical studies in which participants do not need to pay for the products shown in the pictures (see, for example, Yue et al., 2008). Lusk and Schroeder (2004) found that there is no significant difference between marginal WTP elicited from hypothetical and nonhypothetical experiments when real products instead of pictures are used in both experiments. The objective of this study was to investigate consumers' WTP for organically grown and locally grown fresh produce and the marketing segmentation of these two types of produce. The results of this research are very important for small-scale farmers, market organizers, and sponsoring agencies in making their production and marketing decisions. This study also contributes to the literature on consumers' choice and demand segmentation between organically grown and locally grown produce. In addition, our research is different from previous studies in that we used and compared two methods to investigate consumer preference for and WTP for organically and locally grown fresh produce. We used both the traditional hypothetical experiment and the nonhypothetical experiment with real economic incentives. The use of a choice format is new in the consumer preference and WTP literature. Lusk and Schroeder (2004) developed the choice format with posted prices in different scenarios to investigate consumers' WTP for food quality attributes. Alfnes et al. (2006) used a nonhypothetical choice experiment to investigate consumers' WTP for salmon with different colors. The hypothetical experiment and nonhypothetical choice experiment with real economic incentives give data in similar formats and are complimentary to each other; one gives hypothetical WTP data and the other gives real WTP data. The use of the two methods helps eliminate the hypothetical bias that exists in traditional hypothetical experimental methods and get consumers' real WTP for fresh produce with different attributes.",
          "We tested the data from the hypothetical experiment and the nonhypothetical choice experiment for similarities and used the two data sources in a joint estimation. We tested if the use of real products instead of pictures can eliminate hypothetical bias. The details of both experiments are presented subsequently. Attributes. The fresh produce used for the study was tomatoes because they are widely grown locally and organically in Minnesota. In our experimental design, there were two qualitative attributes (organically grown and locally grown) and one quantitative attribute (price). Each qualitative attribute had two levels-yes or no. The number of levels for the quantitative factor, price, was six. The price ranged from $1.25 to $2.50 with $0.25 as intervals based on real tomato prices in different stores in the Twin Cities area when the experiments were conducted. Table 1 summarizes the levels of attributes. There were four combinations of the two qualitative attributes as shown in Table 2. In our experiment, we named the locally grown (no) and organically grown (no) as ''conventional,'' locally grown (no) and organically grown (yes) as ''organic,'' locally grown (yes) and organically grown (no) as ''local,'' and locally grown (yes) and organically grown (yes) as ''organic + local.'' The tomatoes were purchased from a farmers' market, a supermarket, and an organic farm in St. Paul, MN. Other product attributes such as tomato size, color, shape, size, firmness, and ripeness were not included in the design. However, the tomatoes were of the same type and when we made purchases, we carefully compared tomatoes' color, size, shape, ripeness, and firmness to make them be very similar to each other in these attributes. Therefore, the tomatoes' possible differences in these attributes were controlled and participants said that they were unable to distinguish between them. Scenarios. In the hypothetical and nonhypothetical experiments, participants were presented with a series of pricing scenarios and asked to choose one alternative in each pricing scenario. If none of the alternatives was of interest, they could choose ''none of these.'' To lessen the cognitive burden on participants, only two alternatives were usually included in each scenario in a choice experiment. In our experiments, each scenario had two alternatives described by conventional, organic, local, or organic plus local attributes and price. Because it was not practical to ask each participant to choose from all possible scenarios, a fractional factorial design was developed to minimize scenario number and maximize profile variation. The design was developed based on four principles: level balance (levels of an attribute occurred with equal frequency), orthogonality (the occurrences of any two levels of different attributes were uncorrelated), minimal overlap (cases in which attribute levels did not vary within a scenario were minimized), and utility balance (the probabilities of choosing alternatives within a scenario were kept as similar as possible) (Louviere et al., 2000). The fractional factorial design gave each participant sets of 12 scenarios to evaluate. The clearly dominated alternatives were eliminated. For a further discussion of factorial design, see Louviere et al. (2000). The following is an example of one of the scenarios: Please examine the tomatoes on the table. Consider a situation in which you are buying 1 lb of fresh tomatoes. From the following pairs of alternatives, please choose one alternative that you would like to purchase (you can choose ''none'' if you do not want either of them): Scenario 1-conventional $1.75; organic + local $2.25; none of these. Next to organic tomatoes, we put a USDA Certified Organic label; beside local tomatoes, we put a Minnesota Grown label; and beside locally grown, organic tomatoes, we put both the USDA Certified Organic label and the Minnesota Grown label. Hypothetical experiment. For the hypothetical experiment, participants were not asked to buy anything after they made choices. Real products were used in the hypothetical experiment. Participants were asked to answer a series of questions such as what kind of retailers they were most likely to patronize and a series of questions designed to extract information on their attitudes toward and preference for the organic and local attributes of fresh produce. Nonhypothetical experiment. For the nonhypothetical choice experiment, one of the pricing scenarios was randomly drawn as binding, i.e., the choices made in that scenario would be implemented. Participants would receive the products they chose and were required to pay the prices posted for the corresponding products in the binding scenario. The nonhypothetical choice experiment had the following five steps-Step 1: The experimental procedure was explained to participants and some information about the alternatives (the three attributes) was provided. Step 2: Participants studied the alternatives in scenarios and marked on a choice scheme which of the alternatives in each scenario they wanted to buy. Step 3: After participants completed all scenarios, each participant drew one card with a number determining his or her binding scenario. Step 4: Each participant received the fresh produce he or she chose in his or her binding scenario. Step 5: Participants went to the cashier and paid for their fresh produce. After participants made their choices, they were asked to answer the same set of questions as in the hypothetical experiment. Sampling. A booth was set up at the Minnesota State Fair to conduct the hypothetical and the nonhypothetical choice experiments in Aug. 2008. The Minnesota Sate Fair is the largest U.S. state fair in terms of average daily attendance (Minnesota State Fair, 2008). The sociodemographics of fair attendees are close to Minnesotan sociodemographics except that a slightly higher proportion of those aged between 35 and 44 years attend the Fair who live in Minnesota (Minnesota State Fair, 2008;U.S. Census Bureau, 2004). Therefore, we expected to get a representative sample of Minnesota consumers. State Fair participants were randomly selected to participate in the experiments. Each participant only participated in one type of experiment, either the hypothetical experiment or the nonhypothetical experiment. A small amount of cash or gifts worth $6 to $8 was given to participants to compensate for their time. To ensure that the sample was representative, only one family member per household was allowed to participate in the experiments. Most participants at the Minnesota State Fair came as families. When we recruited participants, we asked the primary food purchaser of the household to participate in the experiments. In total, 365 participants were surveyed with 115 participating in the nonhypothetical experiment and the remaining 250 participating in the hypothetical experiment. Some of the data were not included in the analysis because of one or more missing values. Observations of 343 participants with 110 participants from the nonhypothetical experiment and 233 from the hypothetical experiment were included in the analysis. Statistical model. Consumers' satisfaction derived from choice and consumption of dif-  (Train, 2003). We used the mixed logit model to capture all possible correlations for responses from the same participant and scenario. The statistical model was: where U nis was the utility of individual n from alternative i in scenario s; x nis were vectors of observed variables relating to alternative i and individual n; b was a vector of fixed coefficients; h was a vector of normally distributed random terms with mean zero used to capture the possible correlations; and nis was an identical and independent extreme value error term. The standard logit model is a special case of the mixed logit model where h has zero variance. The density of h was denoted by f (hjV), where V was the fixed parameter vector of the distribution. For a given h, the conditional choice probability of alternative i was a standard logit: Consequently, the unconditional choice probability, P, in the mixed logit model was the logit formula integrated overall values of h with density of h as weights: This integral should be solved analytically and was approximated through simulation (Brownstone and Train, 1999). The maximum likelihood estimation method was used to estimate coefficients with Stata 10.0 software (StataCorp, College Station, TX).",
          "The summary statistics of the sample are shown in Table 3. Approximately 84.51% of participants purchased fruits and vegetables more than once a week and only 0.28% of participants purchased fruits or vegetables less than once a month. These results, derived from the data, indicated that the sample of participants was representative of consumers who are regular fresh produce consumers. The average age of participants was 45 years old. Seventy-three percent of participants lived in urban areas and 23% of them belonged to certain environmental groups. Fourteen percent of participants had children younger than 12 years old in the household, 66% of participants were female, and the average education level was a college degree. On average, there were two to three people in a household and the average income level was $69,000. A large proportion of participants (68%) purchased most of their fresh produce from supermarkets, 27% from a roadside stand or farmers' market, 16% from high-end specialty stores, 11% from cooperatives, 9% from mass merchandisers, and 7% from discount stores. In the questionnaire, we also asked participants how important certain characteristics of locally grown and organically grown fresh produce were to their purchase decision using a Likert scale with 1 being not important and 5 being very important. The percentage of participants who considered different characteristics at various levels of importance are listed in Tables 4 and5. The characteristics were chosen based on previously published studies (Loureiro and Hine, 2002;King, 2007;Yue et al., 2007Yue et al., , 2008)). For locally grown fresh produce, 83% of participants considered freshness as ''very important'' and 72% of them rated safe to eat as ''very important.'' Forty-eight percent of participants thought lower price ''somewhat important'' and 17% of them regarded it as ''very important.'' Supporting the local econ-omy was a good reason for people to choose locally grown fresh produce as indicated by the fact that 74% of participants chose 4 or 5 on the Likert scale. Environmentally friendly was also considered to be important for 61% of participants. Carbon footprint was a concern for 54% of participants, whereas 25% of participants did not regard it as a major concern. For organically grown fresh produce, 52% of participants considered good for health to be ''very important'' and 26% of them thought it to be ''important'' (they chose 4). Forty percent of them regarded lower price as ''somewhat important'' and 42% of them chose 4 or 5, which means that these people thought that lower price was ''important'' or ''very important'' for organic fresh produce. Approximately 83% of participants thought safe to eat as ''important'' or ''very important'' for organic fresh produce and 67% of them considered environmentally friendly as ''important'' or ''very important.'' These results were consistent with previous studies showing that consumers associate organic production methods with a reduced health risk and may choose to reduce the risk from pesticide residues by switching to organically grown products (Magnusson and Cranfield, 2005;Williams and Hammitt, 2001). Recent survey evidence shows that consumers purchased organic foods because they perceived the foods to be fresh (68.3%), better for health, and a better source of nutrition (67.1%; Whole Foods Market, 2005). Furthermore, over 70.3% of the con- The numbers in parentheses were the percentages of participants who belong to the corresponding categories. x Participants were told to choose all that applied. sumers said they bought organic food or beverages to avoid pesticides. To get some idea of participants' past purchases of locally grown and organically grown produce, we asked participants ''When you buy fruits and vegetables, how often do you buy locally grown (organically grown) fresh produce when locally grown (organically grown) options are available?'' For locally grown produce, 14% of participants chose ''always,'' 40% chose ''most times,'' 38% chose ''sometimes,'' and 8% chose ''seldom'' or ''never.'' For organically grown produce, 6% chose ''always,'' 15% chose ''most times,'' 39% chose ''sometimes,'' and 40% chose ''seldom'' or ''never.'' The estimation results of the mixed logit model are shown in Table 6. The P value for the likelihood ratio test was <0.01, which indicated that the model fit the data well. Although some variables have a naturally interpretable metric, others do not, especially ordinal variables and interaction effects (McCall, 2001). Therefore, to simplify interpretation of parameters associated with the quality attributes, variables except organic, local, price, and real and the associated interaction effects among them were standardized with a mean of zero and a SD of one. The standardization was done by subtracting the respective variable's mean and dividing by its SD. We found that the P values for the correlations among the scenarios and the responses from the same participant were pretty large (>0.95). Therefore, the correlations were deemed to be highly insignificant. Price premium for local and organic attributes. The intercept and coefficients of organic, local, price, and organic*local were highly significant. The negative sign of the coefficient of price indicated that the higher the price of the product, the lower the proba-bility that participants would be likely to choose the product. The intercept divided by the coefficient of price is the consumer WTP for 1 lb of conventional tomatoes and the amount was $1.18. The coefficient of organic divided by that of price is the premium the consumer was willing to pay for 1 lb of organic tomatoes, which was $0.72. Similarly, the premium for the local attribute was $0.73. The premium for locally grown, organic tomatoes was not the sum of the two premiums, which was indicated by the highly significant negative coefficient of organic*local. Instead, the premium was $1.16 (the sum of coefficients of organic, local, and organic*local divided by the coefficient of price). Hypothetical bias. The marginal significant coefficients of real*price (P = 0.09) indicates that there might exist hypothetical bias for the WTP elicited using the hypothetical experiment compared with the nonhypothetical choice experiment. By calculation, we found that the hypothetical bias for organic attributes was 7.5% of the premium elicited using the nonhypothetical choice experiment. The hypothetical bias for the premium of the local attribute was 9% of the premium elicited using the nonhypothetical choice experiment. Relative to the $1.16 premium for the local plus organic attribute in the hypothetical experiment, participants were willing to pay a premium of $1.07 for 1 lb of locally grown, organic tomatoes in the nonhypothetical choice experiment. In this respect, our results are in line with the large literature on hypothetical bias in valuation studies. Previous studies have shown that the difference between the real and hypothetical study is not fixed with the changes in WTP (Yue et al., 2008). Some previous studies have found that hypothetical bias is quite high when pictures are used in a hypothetical experiment compared with a nonhypothetical experiment using real products (see, for example, Alfnes, 2003;Harrison and Rutström, 2005;Yue et al., 2008). In contrast, we found that hypothetical bias was not that high when real products were used in the hypothetical experiment. The results indicated that even if hypothetical bias might not be completely eliminated as found by Lusk and Schroeder (2004), it can be significantly decreased when real products were used to elicit consumer WTP for certain attributes in a hypothetical experiment. The findings have very important implications for future studies on consumers' WTP literature. Nonhypothetical experiments should be used; if the employment of a nonhypothetical experiment is not practical, real products rather than pictures of products should be used to lower any possible hypothetical bias. Sociodemographic background of consumers. We found that consumers' sociodemographic backgrounds affected their choice and WTP for conventional, organic, local, or locally grown, organic tomatoes. The interaction effect between age and organic was significant and negative, which indicated that older participants were less likely to choose organic tomatoes compared with younger participants. Older participants were more likely to purchase local tomatoes compared with younger participants, but the coefficient was not highly significant (P = 0.13). The significant and positive coefficient of income* organic meant that wealthier participants were more likely to purchase organic tomatoes. This result was in line with other studies on organic attributes of fruits and vegetables (see, for example, Yue et al., 2008). The coefficient of gender*local was significant and negative, which means that female participants were more likely to purchase locally grown tomatoes compared with male participants. In the questionnaire, we asked participants if they belonged to any groups associated with the environment. The estimation results showed that those participants who belonged to environmental groups were least likely to purchase conventional tomatoes and were most likely to purchase locally grown tomatoes followed by organic tomatoes. These results were similar to those by Yue et al. (2007), in that consumers who belonged to environmental groups were more likely to purchase organic apples. Participants' educational backgrounds affected their choice of tomatoes with different attributes. Participants with higher educational levels were more likely to purchase conventional tomatoes and less likely to purchase organic tomatoes. Participants who live in urban areas were most likely to purchase organic tomatoes followed by locally grown tomatoes and were least likely to purchase conventional tomatoes compared with participants from rural areas. One possible explanation is that consumers who live in urban areas may be less likely to grow fresh produce than those who live in rural areas. People experienced with growing products know that there is not as large a quality difference between locally produced fruits and nonlocally produced fruits compared with those who have no growing experience (Yue et al., 2007). Hartman (1997) also showed that the consumption of organic food was skewed in urban areas. We found that household size did not significantly differentiate participants in their choice of tomatoes with different attributes. However, participants with children younger than age 12 years had a higher chance of choosing locally grown tomatoes even if the coefficient was not highly significant (P value was 0.13). Also, these participants were more likely to choose organic tomatoes and less likely to choose conventional tomatoes as indicated by the positive sign of the coefficient of child*organic and the negative sign of the coefficient of child, even if the estimated coefficients were not statistically significant. Store categories. The estimation results showed that participants bought different types of tomatoes from different types of stores. Participants who purchased most fresh produce from mass merchandisers were most likely to choose conventional tomatoes, less likely to choose locally grown tomatoes, and least likely to choose organic tomatoes. Participants who purchased fresh produce mainly from cooperatives had the highest probability of purchasing organic tomatoes, followed by locally grown tomatoes, and they had the lowest probability of purchasing conventional tomatoes. Participants who bought the most fresh produce from specialty stores were more likely to purchase organic tomatoes and less likely to purchase locally grown tomatoes, but the results were not statistically significant. Those participants who often patronize supermarkets were most likely to purchase conventional tomatoes, followed by organic tomatoes, and they were least likely to purchase locally grown tomatoes. Participants who visited discount stores most often were less likely to purchase organic tomatoes. Those participants who often patronized roadside stands and farmers' markets were most likely to purchase locally grown tomatoes, less likely to purchase organic tomatoes, and least likely to choose conventional tomatoes. Marketing implications. These results have important marketing implications for fresh produce growers, wholesalers, and retailers. A large proportion of consumers consider ''freshness'' and ''safe to eat'' as ''very important'' for purchasing locally grown fresh produce, which indicates that these attributes should be stressed by local growers when they promote their products in advertisements, newsletters, and so on. As for organic fresh produce, ''good for health'' and ''safe to eat'' are their selling points and should be emphasized in promotional materials. There exists a group of consumers who view ''lower price'' as important for local and organic fresh produce, which was consistent with findings of previous studies that premium price was the major obstacle to buying organic foods for some consumers (Magnusson et al., 2001). To promote to this price-sensitive segment, emphasis on ''affordability'' by decreasing the price premium to a certain level will work to increase sales of organic and local fresh produce. The sociodemographics of consumers who choose conventional, local, or organic attributes have important implications for choices of target markets for fresh produce with differentiated attributes. Older female consumers with lower than average education levels who shop in urban markets are the right target market to promote locally grown fresh produce, whereas younger consumers with above-average income who live in urban areas are the main target market for organic produce. The average price premiums for organic, local, and organic plus local attributes of tomatoes were $0.67, $0.67, and $1.06, respectively, compared with conventional tomatoes after correction of hypothetical bias. These price premiums give growers guidelines when they decide what prices to charge for tomatoes with different attributes. Comparing prices that growers, wholesalers, and retailers actually charge with these premiums can provide these parties with information about whether their charged prices were consistent with consumers' perceptual values of these different attributes. Based on this information, the associated parties can make adjustments on their pricing strategies to attract more potential consumers. Finding out these price premiums can also help growers choose the right production methods and profitable value-added attributes to promote by comparing the premiums with the associated production and marketing costs. Interestingly, we found that the average premiums consumers were willing to pay for organic tomatoes and locally grown tomatoes were about the same. However, the profitability of the two types of produce might not be the same as a result of variation in production costs. Local growers can get a higher premium if they grow organic tomatoes and promote them as both locally grown and organically grown instead of organically grown alone in local markets. The results show that consumers actually patronize different stores to buy fresh produce with different attributes. Therefore, growers and wholesalers need to choose the right stores to sell their products, and retailers need to source the right types of produce for these parties to make profits. Mass merchandisers and supermarkets are the major places consumers purchase conventional produce, which means that conventional fresh produce growers need to choose these retail outlets as their major venues to sell their products. Cooperatives are where most consumers purchase organic produce, indicating that organic fresh produce growers might be able to sell more of their products and can charge a relatively higher price if they choose to sell their products at cooperatives. In contrast, discount stores and mass merchandisers where people expect to get low-priced products are not the major place for growers to sell organic produce. Supermarkets might be another choice for selling organic fresh produce. For locally grown fresh produce, consumers are most likely to purchase them from farmers' markets and roadside stands. These direct marketing channels are the best venues for local growers to sell their products. Cooperatives are also good venues to sell locally grown products, whereas mass merchandisers and supermarkets might not be able to gain enough profits by selling locally grown fresh produce. From the supply perspective, the availability of organic and locally grown produce in different stores also contributes to consumer patronage of these stores in purchasing fresh produce with different attributes. Fresh produce provides the greatest chances for direct sales to consumers through farmers' markets and roadside stands that are commonly used by small-scale growers. Given the fact that most local growers are small scale in Minnesota, it is not surprising that consumers are most likely to choose direct marketing channels to purchase locally grown produce. The growth of organic food has given rise to new marketing outlets in addition to traditional organic outlets such as cooperatives. Mass market groceries and chain stores entered the organic market place at a time of low prices for commodities at the farm gate. However, it has been documented that approximately one third of organic products are sold through mass market grocery stores with a disproportionately higher percentage of processed food and dairy products instead of fresh produce (Klonsky, 2000). In recent years, some mass merchandisers like Walmart started to source and advertise organic fresh produce and they could get a significant market share if they are able to keep prices low enough (Business Week, 2006) The hypothetical and nonhypothetical experiments were conducted at the Minnesota State Fair. One issue to consider is whether the results are representative of consumers in general (Mitchell and Carson, 1989). In our survey, the sample chosenthose who participated in the Minnesota State Fair-does not necessarily represent all fresh produce consumers in the United States, and in this regard, there is some population choice bias. The ''often'' variable indicated that most of participants were regular fresh produce consumers, so we expect the sample selection bias to be minor. We thus view our sample respondents as representing a population of the central Midwest instead of representing the total U.S. population. Given the potential biases, the extrapolation of our findings to other populations should be made with caution. We interpret the findings relative to the population attending local and regional fairs and markets.",
          "To assist small-and medium-sized growers who want to capitalize on growth in demand for differentiated produce, this article presents consumer WTP for the organic, locally grown, and locally grown organic attributes, the characteristics consumers perceive as important when they make purchase decisions of these differentiated fresh produce, and the various types of retail venues where consumers purchase these different products. A combination of the traditional hypothetical experimental method and nonhypothetical choice experiment was used to investigate consumer preference for valueadded fresh produce. The combination of these two methods can help eliminate any possible hypothetical bias in the hypothetical experimental method. We found that hypothetical bias existed using the hypothetical experimental method, but the bias was not high if real products were used in that survey. We elicited consumer WTP for local and organic attributes for fresh tomatoes. We found consumers perceived various characteristics as important for their purchase decision of organic and local fresh produce. Consumers' sociodemographic backgrounds affected their choice of fresh produce with different attributes and consumers patronized different types of stores to purchase conventional, organic, and local fresh produce."
        ],
        "ground_truth_definitions": {
          "hypothetical bias": {
            "definition": "the difference between what people say they will pay and what they would actually pay",
            "context": "We found that when real products were used in the hypothetical experiment, the hypothetical bias (the difference between what people say they will pay and what they would actually pay) was not high.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "304b9778edf07175e6964dfa3f37cacec6006197",
        "sections": [
          "Airports are symbols of mobility. They have become emblematic of our post-modern world. Yet in the early days of airports, there was no such thing as the rivers of passengers that flow through the sculpted steel and glass façades of contemporary terminal buildings. The airport consisted of a few tents and an airfield, remnants of air travel's military beginnings. Airports now cater for the millions of people who flow in, out and within countries each year. The huge mile long terminal structures are carefully built to facilitate the mobility of passengers, baggage, and cargo to their destination. To ensure the security of the country the flows are entering, while at the same time to protect the very means of their travel, these flows must be watched and controlled. The airport is well and truly a space under surveillance. And yet, the airport's role in surveillance has gone largely unnoticed by surveillance studies, reflecting the invisibility of these transient 'non-places' within the social sciences (see Crang, 2002;Gottdeiner, 2001;Rosler, 1998;Augé, 1995). Nonetheless, since the TWA flight crash in 1996 and the more recent September 11th attacks, countries such as the United States are pushing through increased measures to improve airport security and safety, launching airports into the public consciousness. Security itself has become an all-encompassing term, spreading into the current vocabulary. Widespread gun crime at schools has highlighted the 'airport-style security' measures of metal detectors and baggage checks that are now common place (BBC News Online, 1999). This paper explores several aspects of airport surveillance. In themselves, airports offer interesting case studies of how mobility may become watched and controlled. The heavy government presence in these spaces, the possibility for the invasion of privacy and the disregard of human rights must be questioned. We may also learn useful lessons at airports. For as recently discussed by Lyon (2003) airports may be seen as \"possible microcosms of wider societal surveillance trends\" (20) signposting possible surveillance developments in other spaces and places. In this paper, I shall therefore explore a number of the most recent surveillance techniques for monitoring and controlling passengers at the airport. These practices are argued to closely match designations of a 'panoptic sort' (Gandy, 1993) or surveillant sorting (Lyon, 2002b) from the surveillance studies literature. I discuss the implication of these techniques in terms of privacy issues and discrimination. In-turn, the second section to this paper examines the surveillance of non-human mobilities at airports. Informed by an actor network theory (ANT) approach and recent turns towards the agency of 'objects' and 'things', the paper provides a brief over-view of the surveillance of baggage, possessions and aircraft -indicative of a lack of research in this area and a trend towards the surveillance of non-humans. Firstly, let me contextualise this discussion within current turns towards mobility and security. I examine the theorisation of mobility within surveillance studies and trends towards surveillant sorting, before moving on to illustrate the increasing need for an ANT approach towards the subjects of surveillance.",
          "Recent progress towards the understanding of movement within surveillance studies probably reflects the current interest in mobility from disciplines such as sociology (Urry, 2002;2000), geography (Cresswell, 2001;Thrift, 1995) and cultural studies (Morris, 1988). This is no-doubt indicative of the increasingly mobile world in which we live, and must also owe its momentum to the popular fluid and mobile thought of philosophers such as Gilles Deleuze, Felix Guattari, Ian Chambers and Paul Virilio. This new mobility paradigm has moved beyond static idealizations of society towards theories that are marked by terms such as nomadism, displacement, speed and movement. Mobility has become as central as the tenets of society, space and power that fill much of our thinking. In many cases, mobility has often been understood to be an escape from a social order and as an act of resistance (Cresswell, 1993). De Certeau's (1984) tactics and Deleuze and Guattari's (1988) nomad-power transgress the fixity of the state. Gypsies and travellers cross physical boundaries and so become recognized as 'out of place' by authorities and local residents (Sibley, 1995). In many cases, physical mobility is often seen to signify the metaphorical movement from, and transgression of social norms. Mobility is therefore often viewed in terms of risk to the safe and static containers of space, territory and social order. Bauman's (1993) stranger and vagabond provide the perfect figures of risk to security as spatial and social boundaries are crossed. This is not to argue that mobility is always taken as deviant. Obviously, tourists and business travellers do not necessarily contradict social norms. Taking a relational (Cresswell, 1996) view of mobility allows us to see movement merely injected with meaning in different socio-spatial circumstances -it means different things, to different people, in different places and at different times. Therefore, where a tourist may be welcomed into a country a migrant may not be. However, the spaces at which mobility may be channelled provide the most obvious points for these social differentiations. Henceforth, it is at the borders, at airports -where movement and distinct spatial boundaries coexist -where undesirable mobilities may be distinguished from the desirable. This is increasingly achieved by surveillance. Surveillance is one method used to control mobility. As the growth of information and communication technologies allow data packets to be transferred between databases and other surveillant technologies, surveillance itself has become more mobile (Lyon, 2002b). For Lyon andothers (2002a, Phillips andCurry, 2002) however, surveillance may also struggle with mobility by stasis. What Lyon describes as the 'phenetic fix' characterizes surveillance techniques that attempt to capture the essence of movement -taking a snapshot of movement, bodies and identities. Information abstracted from moving bodies may be appropriated and fitted into neat categories and definitions. In this paper, I want to examine a particular form of the 'phenetic fix' labelled 'surveillant sorting' that is probably most illustrative of airport and border surveillance. Surveillant sorting is often used to describe the formation of those categories and profiles discussed above. Gandy's (1993) term the panoptic sort being another terminology. Issues surrounding sorting emphasise the possibility for discrimination -treating some better than others. Surveillant sorting is also able to distinguish the mobile. For example, studies have illustrated that the transfer of data packets may become shaped and prioritised, some given quicker speed and access than others (see: Graham and Wood, 2003). Mobile sorting may also work materially. Cars travelling through the Canadian-US border (Sparke, forthcoming) are put into categories of 'good' and 'bad' by the NEXUS system, the good are able to pass through quickly, where 'bad guys' are forced to wait in long queues for questioning. At airports, surveillance, rather than examination, is becoming the key method with which to distinguish passengers (Salter, 2003). In this paper, I examine a number of issues surrounding these procedures. And yet, I also want to explore a form of sorting that has gone on within the wider study of surveillance itself and which airports may be used to highlight. By this, I mean that there seems to be a tendency to limit our understanding of mobility to humans -to sort the human from the non-human. This paper could examine the mobility of passengers passing through airports and the associated methods of surveillance and control. However, to do this would be to ignore the orbiting movement of non-humans that are also subjected to the airport's controlling gaze. It is in this respect that we may turn to Actor Network Theory (ANT), the work of Bruno Latour, John Law and the recent veering of the social sciences towards the role of objects (Pels et al., 2002), things and their mobility (de Laet, 2000). These approaches are somewhat absent from surveillance studies as noted by Wood in a past issue of this journal (Wood, 2003). Wood observes Kirstie Ball's paper (2002) to be one of the only contributions to such a debate. Ball's approach pinpoints the elements of surveillance systems that \"are all socio-technically perpetuated through interactions of artefacts, technologies and people\" (586). Therefore, whilst humans and non-humans may form networks of the elements of surveillance, little emphasis has been placed upon these aspects of the subjects of surveillance. Examining how surveillance deals with mobility need not be limited to the mobile human, but extended to include things, and indeed their amalgamation with people (Haraway, 1991;Urry, 2002). Such a shift in focus is particularly pertinent when we turn to the increasing concentration upon the means of terrorism by governments and borders -shifting surveillance away from the perpetrators. Carter (2001) states that governments need better capabilities for the detection and prevention of terrorist activities. He argues that: This involves surveillance of persons and motives -a delicate matter -but also surveillance of potential means of destruction such as crop dusters, germ cultures and pilot instruction. Surveillance of means raises far fewer civil liberties issues than does surveillance of persons, and it might be much more effective. (15) The surveillance of the means of destruction is a much more attractive option for security. Governments take the view that a bag will not mind if it is x-rayed, CT scanned, and rummaged through. Examining the subjects of surveillance can enable our better understanding of the purpose, workings, and impacts of these systems. This approach is discussed in the final section. Firstly, it is useful to situate these practices within the development of air-travel, the growth of airports, and the rising need for airport security.",
          "The first airport resembling anything that we know today was built in 1922 at Königsberg in East Prussia (Zukowsky, 1996). Königsberg's terminal was a considerable advance from the windswept military landing fields and beaches. The new terminals became bridges for the transition between land and flight. Airports captured the imagination representing a symbol of progress and technological achievement in modernity. Indeed, the driving ambition of Hitler's Nazi regime was manifest in the rebuilding of Berlin's Templehof airport -the new airport reborn from the ashes of the failing past government and decaying economic system (Braun, 1995). From these early beginnings, airport space was a segregated and exclusionary one, being inhabited only by military personnel or the wealthy. Even before the First World War, aviation became an impossible dream fulfilled. Higher society frequented airfields to watch the displays of macho pilots and their flying machines (Wohl, 1994). Flights became social and cultural gatherings for the upper classes, excluding the lower. Soon, the development of the vacation and package holidays saw the growth in trips to Europe; to Spain and Portugal and the beginnings of mass transatlantic travel (Löfgren, 1999b). International air transport became less costly and connected to many destinations. Wide-body jets meant that 200 to 500 people could be carried by one flight eliminating the extortionate costs involved. The further deregulation of the American and British airlines saw airlines multiply, stimulating the competitiveness of seat prices, and also a greater choice of destination for many travellers. These changes also meant radical developments in the complexity of running airports as airlines rapidly grew and folded. Although, the growth of mass travel was still unable to fully democratize this space. VIP lounges developed for business and frequent flyers to enjoy the quiet luxury of a comfortable seat, the amenities of television, and most recently a shower or sauna. The progression of the airport culminated into the enormous multi-terminal, multi-runways sites that see millions pass through each year. They soon became airport-cities employing the work force of urban populations. Manuel Castells (1996) describes airports as a 'space of flows', nodes on the global networks of mobility. But the terminals are also built for a carefully controlled internal mobility. David Pascoe argues that the terminal was \"predicated on confluence, the precisely timed pumping of passengers into and out of [space]\", airlines then \"constructed specially designed terminals to control and direct the masses of passengers…\" (2001: 226). For as passengers increased so did the potential dangers and security risks. This was due partly to the function of airports as borders, but also to the nature of international terrorism that has developed over the past three decades. Perhaps Paul Virilio (1997) is correct where he writes that the border is now at the centre of our cities. Airports act as border zones to the vertical vectors of mobility that cross national and state boundaries. For this, Airports must function in a similar way to the borders that police the boundaries of our countries, regulating the movement of people that enter and leave. For Virilio, \"the doors to the city are its tollbooths and its custom posts are dams, filtering the fluidity of the masses, the penetrating power of the migrating hordes\" (1986: 7). Airports must therefore function as a screening or filter for the threats to a nation, but airports also become screens for quite different threats. Since the 1970's terrorism has grown, looking towards the aviation industry as a target. On the 22nd of July 1968, an El Al flight destined for Tel Aviv was hijacked. The hijacking was a bold political statement where passengers were to be traded for Palestinian terrorists imprisoned in Israel. By attacking the Israeli airline they were effectively attacking the Israeli state (Hoffman, 1998). In addition to this, the intense media coverage given to the hijacking saw the event publicized all over the world. Aviation became the perfect target for terror, which since 1968 has endured a rapid rise of these attacks. Planes were spaces that could be controlled easily; the fear of crashing subduing any passenger resistance. Airports also offered limited surveillance of the throngs of people that were travelling. The likelihood of a successful hijack has even been calculated, at 76% it is an obvious incentive for terrorists (Merari, 1998). Due to the increasing risks from terrorism and illegal immigration, airports had to find a way to take control of the airport space. Developments of airport security and surveillance were initially felt through actual changes to the space -to the architecture of the airport. John Zukowsky notes that, \"Intensified security measures changed the planning of airports, deliberately cutting up the open flow of space \" (1996: 15). The development of the sterile lounge concept (Wallis, 1998) saw baggage and security checks made before entrance to the gate. This removed the chance of threatening objects reaching the plane by person. Security checks could then be completed in a purpose built room far away from the gate to reduce queues disruptive to passengers. These techniques are still regarded as the preferred method, evident in the centralized security 'choke points' in most terminals (NRC, 1999). Surveillance has become therefore, one of the primary means of ensuring that airports are made safe and secure. 'Threat vectors' (NRC, 1999) that is, the paths by which threats may enter the airport and find their way to a plane are identified and monitored. In the next two sections, a number of these approaches and their implications are discussed.",
          "Monitoring passengers equally, proved to be far from the truth of running an international airport (Jenkins, 1998). Airport authorities needed a way of putting passengers under surveillance without having to examine every passenger rigorously. For this, methods of passenger screening and profiling have been developed to effectively sort the most probable threat to security from other passengers.",
          "The most recently publicized and perhaps most worrying form of surveillant sorting at airports has come through a surveillance technique known as profiling. Profiling is the ability for information or data about an individual to be built up. People may be sorted into profiles of particular consumer groups. The psychological profiles crime investigators use is an obvious example. Profiles are then used to predict a person's likely behaviour or the likely characteristics a criminal may embody. Profiling also usually relies upon vast quantities of information gathered about someone that are then stored and shared. In light of the growth of international terrorism, a new form of profiling has been developed called Computer Assisted Passenger Pre-Screening (CAPPS) in the United States. It is possible to discuss here some of the issues surrounding profiling and its potential impacts. Profiling was introduced in the US on the recommendations of the Al Gore led White House Commission for Aviation Safety and Security following the TWA flight crash of 1996. The Commission found that, \"passengers could be separated into a very large majority who present little or no risk, and a small minority who merit additional attention\" (1997). The rationale behind profiling is then to concentrate upon the minority -those likely to be a threat, rather than the majority, which requires much greater resources. The recommendations were based upon the practices of the airline El Al who used profiling techniques to fit the descriptions and likely behaviour of terrorists to passengers. CAPPS is reported to work by matching likely terrorist behaviour with present airline flight information. A flagged 'selectee' will then be subjected to personal checking of possessions and perhaps an interview and questioning. The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\" (1997). Problems are also clear over the secrecy of these profiles and the categories passengers become inadvertently pushed into. The lack of control passengers have over these profiles has led to concerns that innocent travellers may be unable to rid themselves of the 'selectee' status of their sorting. The repercussions of September 11th have seen the reorganisation of US airport security under the Transport Security Administration, itself under the newly formed Department for Homeland Security. Bolstered by this restructuring of governance, President Bush's developing and renamed Terrorist Information Awareness Program and rumours of the testing of CAPPS II have caused concern throughout privacy awareness groups. Posted to the US federal register on January 15th of this year, the TSA introduced a new system of records and amendment of the Privacy Act known as \"Aviation Security Screening Records\" (DOT, 2003) designed to facilitate the CAPPS II system. The proposal was open to comment and provoked a massive response from individuals and privacy awareness groups. The prime concern behind this system is again the discrimination and segregation of passengers, and also the sharing of passenger information between multiple government agencies. However, not only the sharing of Personal Name Records (PNR) are at stake (see Bennett, 1999), but the vague details of the report does not count out the possibility that more detailed banking records, tax histories and other sources of information may become easily passed between airports and other state departments. Fortunately, in Europe, different rules apply. In March 2003, the European Parliament moved to reverse an agreement made between the European Commission and the United States that would have forced PNR records to be transferred from European airlines to US airports. Another version of CAPPS II was posted to the Federal Register on August 1st with several revisions made upon the January version. At present, however, the majority of concerns lie in the current testing of CAPPS II, particularly with regard to the JetBlue controversy. The popular airline JetBlue illegally transferred a large number of passengers' records to an agency working for the US government (Singel, 2003). The information was to be used for the testing of passenger profiling on internal flights. Some of this information became public and was published to the Internet. A similar scheme has also been argued to be taking place through Galileo -the computer reservation system (CRS) that runs a large part of air travel reservation in the US.",
          "Other forms of surveillant sorting are occurring through biometric technologies. Biometry is the measurement of the body. Biometrics effectively treats the 'body as text'; identifying specific body parts such as the iris, face and palm signatures to identify the individual (see Ploeg, 1999Ploeg, , 2002)). The use of biometrics extends far beyond air travel, forensic evidence such as fingerprints are commonly used to identify suspects. Nevertheless, biometrics has also become popular at certain US and European airports. Biometrics work by containing detailed records of a passenger's body information (Delio, 2003), the most popular being retina patterns, but this may also include finger prints, DNA matching and face recognition (Agre, 2001) to name but a few. This information and passengers' identities are then stored on cards or a central database to be compared. Biometric systems are usually referred to as authentication systems, where a positive match to the stored information authenticates the identity (Clarke, 2003). However, other biometric systems, referred to as identification systems, compare the captured body data to large amounts of records kept on other databases. Here, passengers are not proving their identity; rather, airport authorities are identifying them. Biometric systems are of obvious concern to proponents of privacy rights, the critique and defence of biometrics coming from several different philosophical perspectives of technology and human agency (see van der Ploeg, 2003). One particular view is to critique biometrics for their treatment of passengers as objects. Much like a piece of baggage to be identified by a bar code, passengers' bodies offer similar bar codes to be read by authorities. Objections have also been raised over the penetration of bodies as biometric systems scan passenger identifiers. Others m ay analyse biometrics from the perspective of how the information is stored, particularly over the ease by which data may be shared with other 3rd parties. The possibility for data to be hacked and misused by external sources is another issue. Irma van der Ploeg (2003) discusses problems over the security of the American immigration service INSPASS authentication scheme at Los Angeles airport. Recorded hand geometry data was designed to be stored only on the card carried by the passenger. However, van der Ploeg illustrates that the card, if lost, would be quickly replaced by INSPASS thereby revealing the storage, somewhere, of this supposedly private information. But biometrics also has several surveillant sorting implications. For example, we can look to the effects upon the movement of passengers. Schiphol airport, Amsterdam has a fully working biometric system developed by the airport and the Dutch immigration service. The 'Privium' scheme is an 'authentication' scheme that, for 90 euros, allows enrolled passengers to bypass busy queues and check in delays. Passengers are then sorted into those enrolled by the scheme and those not. The logic behind these systems is similar to profiling in that it then gives more time for additional security measures to be placed upon those not enrolled in the biometric scheme. The systems therefore virtually sort passengers' information but also work to materially sort passenger's mobility within the terminal. The business 'kinetic elites' (Graham and Marvin, 2001;Andreu, 1998) may pass through to the VIP lounge at speed. And yet, the average traveller, forced to park long distances from the terminal, is excluded from member's club lounges and has to endure waiting in lengthy 'check-in' and security queues.",
          "Finally, the sorting of passengers has also begun with the introduction of what can be known as digital surveillance, or most commonly algorithmic surveillance (see Norris, 2002;Graham and Wood, 2003). Of course, CCTV has become one of the predominant modes of surveillance within airports as well as within cities, shops and indeed on my own university campus. The ability for CCTV to pick up deviant and threatening behaviour has been well documented, as well as its use in identifying suspects. Within airports, its continued and extended application has become a clear priority of airport security concerns. In the UK, the John Wheeler (2002) report on airport security suggested that valuable improvements have been made in this area. A technology now widely used at airports in the United States is a system called Exit Sentry developed by Cernium. Exit Sentry is able to monitor the direction of movement of passengers walking through the exit corridor of secured areas of an airport terminal such as arrivals. A passenger walking the wrong way, trying to enter the secured area through the exit corridor is warned with a flashing light. If the suspect then persists, a siren alerts security staff, and a recording of the suspects' movements is made. Systems such as Exit Sentry are exemplary of recent algorithmic surveillance technologies that in real time analyze CCTV footage of spaces. These systems understand the differences in movement of individual passengers, and may filter out static background information. Threats are not identified by a particular property of an object; rather, particular movements are inscribed with meanings of what is an allowed movement and what is considered suspicious and deviant. The uses for such technologies have been suggested to recognize the movements of car thieves and even people contemplating suicide at quiet train platforms (Norris, 2002). Indeed, research in progress at Southampton University has developed an approach called gait recognition that may be able to identify the identity of individuals by their distinctive walking styles. CCTV algorithmic surveillance effectively sorts and differentiates between mobility, in Exit Sentry's case, that is accepted (non-threatening) and that is unacceptable (threatening). The meanings are not essential to the mobility picked up by the cameras; rather the algorithms are imbued with these meanings. Sorting objects I now move beyond my discussion of airport surveillant sorting, towards a sorting of a different kind. Here I want to discuss the non-humans that have become placed under surveillance at the airport. As I have discussed airports are obviously places of human mobility. And yet, objects and things also intersect these spaces. Baggage flows through the baggage systems miraculously arriving at our destination. We carry on duty-free, cigarettes, and alcohol. But we are only aware of a tiny spectrum of the surveillance systems that place objects under this scrutiny. Initially, the sorting of objects and things at airports has occurred through the physical arrangement of airport spaces. For instance at the re-building of Templehof, the architect Ernst Sagebiel made the innovation of separating passengers, goods and baggage onto different levels of the airport (Braun, 1995). In addition, freight could arrive by an underground train opposite to the subway from which passengers arrived. This form of sorting is still used today, as automated baggage handling systems operate behind the holes in which our luggage disappears.",
          "The monitoring of our luggage at airports is probably one of the most visible methods of surveillance. Contaminating or illegal goods must be stopped. Airports are also nervous over the possibility of explosives and other weapons that may be used for terrorist activities. The identification of these objects is therefore one of the prime duties of airport security and immigration control as they concentrate upon 'threat vectors' (NRC, 1999). These objects may be identified and put under surveillance in a number of ways. If we first take hand luggage, the classic examples are probably the phase induction (PI) metal detection systems (Jenkins, 2002). Memorable scenes in films depict passengers unable to pass through these archways due to a belt or keys. At this point in a person's journey, hand luggage is usually x-rayed. X-ray machines are able to look inside a person's belongings, where operators search for suspicious looking objects, these include explosives, arms, and organic material. Although these x-ray systems, according to FAA research, could also be used upon passengers (NRC, 1996). X-ray operators could easily identify passengers carrying concealed weapons, without having to resort to personal body searches. Objects such as metals and sharp objects become a threat when held by a passenger. They have the potential to become a risk to the plane or the airport. The agency of a threat becomes possible because of the stable network of actors such as a knife, a passenger and indeed a plane. However, objects may take the shape of a threat independently of a person. For example, timer device explosives may be set in advance or barometric sensor devices respond to changes in pressure. Much like Latour's (1999) example of a sleeping policeman (speed bump), where the agency of a policeman to deter speeding becomes deferred onto the speed bump, here, the terrorists will or agency is given to the bomb. It is this deferred relationship that has led airports to implement baggage reconciliation: Positive Passenger Bag Matching (PPBM) recommended by the White House Commission (1997) in the United States. These systems are designed for the possibility that terrorists are not willing to blow themselves up when detonating a bomb. The terrorist would then check their bags onto a flight without actually boarding the plane. The PPBM system automatically flags up the bag that has been put onto the flight without an owner, the bag may then be pulled from the flight. Such a system relies upon the ability of the airport to know where a passenger's luggage is, through most recently, radio frequency (RFID) tagging and even individual trays that a bag is placed onto. Baggage handling systems most commonly use bar code technology however, so that baggage can be read to determine the identity of its owner and the flight it should be on. Surveillance technology has necessarily become more intense in an attempt to secure aviation from these objects. It is also much easier for airports to scan checked baggage than to actually interrogate a human subject. And yet, similarly to passenger screening, airports have not had the staff or equipment to check every person's bag. As such, the scanning of baggage has been linked to the CAPPS profiling systems so that a 'selectees' luggage is put under increased surveillance and identification for threats. For example, the Explosive Detection Systems (EDS) were initially used to scan 'selectee' passengers' bags for traces of explosives. Here, a 'selectee's' baggage is that scanned by the EDS machines. Since the events of 9/11 efforts have been made to increase the installation of EDS machines to ensure every passenger's bag is scanned, this has culminated in the International Civil Aviation Authority (ICAO) recommending that 100% hold scanning be made by 2006. Other systems use the computer tomography (CT) scanners used in hospitals. A slice or tomography of a bag can then be used to calculate the mass and density of materials that are then matched against explosive and hazardous materials.",
          "Perhaps the object we are most conscious of at airports is the passport. Although, considering the passport's importance to international travel there has been very little written about it in the social sciences (for an exception, see: Torpey, 2000;Caplan and Torpey, 2000;O'Bryne, 2001;Salter, 2003). And yet, the passport is one of the primary tools of states for the surveillance of their population's movements. For Torpey (2000), passports provide the means to govern a population's movement, 'penetrating' the individual to 'embrace' populations. Passports are used to identify a person, making a person legible to the state who may then enforce their authority over movement. Passports are also symbols of nation-states and our allegiance to them (O'Byrne, 2001). Paul Fussell's (1982) Abroad typically articulates the standardization that passports impose upon the traveller: where do people belong, where do they live, what is their hair colour, eye colour. All these aspects of our selves that we continually negotiate and question become squashed into the tight categories of the passport. For Löfgren (1999a): \"as a traveller you now had to live up to your passport identity to be able to prove your identity\" (19). This is nowhere more obvious than at the airport. Identification is then one of the primary means of airport surveillance; the display of the passport marks a 'reading' of the individual that occurs at multiple times during their journey. From supplying the ticket at check in, to security, to boarding the plane with your boarding pass and then landing and undergoing immigration control in the destination country, the passport must be displayed many times in the airport. It has become integral to the ritual of international travel. Airport staff and workers must also supply, not their passport, but a similar identification card to access areas of the airport. Indeed, airport vehicles must also be identifiable, clearly bearing company insignias and paint designs. In terms of mobility this is vital, passports and identity cards govern where a person, airport worker or object may go. Passports are a marker of our identity. Identities are read and given mobile and spatial limits by states and airports as they regulate mobility.",
          "We can also look outside the terminal to airspace for examples of non-human surveillance. Airspaces are territorial units but they also act as highways for the traffic of aircraft, these spaces must also become monitored and controlled for the purposes of security and safety. At the dawn of civil aviation, the organization of plane movement was described as \"decidedly sketchy in nature\" (quoted in Wegg, 1995: 115). The pilot had to rely upon skill and sight, and had no knowledge of local weather conditions. In order for the aviation industry to provide efficient and safe travel, it was essential that more could be known about environmental conditions. The first known aviation weather station in the UK was introduced at Croydon airport in 1921 (Wegg, 1995). This surveillance of weather and the general environment is perhaps an increasingly common trend for surveillance. Brazil's System for the Vigilance of the Amazon (SIVAM) developed by Raytheon provides a huge environmental monitoring system to show signs of illegal drug trafficking; mining and logging that have previously gone undetected in the rain forest. Still, the monitoring of weather is obviously not the only concern of air traffic control. As was quickly found in the 1920's crashes were most likely to occur around airports, where planes in the air and on land found it difficult to avoid each other. The need to orchestrate these movements proved paramount to safety. For this, Air Traffic Control (ATC) developed at airports, reinforcing the idea of an airspace that could be vigilantly monitored and controlled for the purposes of both safety and security. Again, at Croydon airport, the new terminal built in 1928 featured an ATC. The tower, or \"chart house\" scanned for incoming aircraft and maintained radiotelephony (RT) communications with planes. The towers therefore monitored all the positions of incoming and taking off aircraft. Un-identified or enemy planes could also be monitored from an ATC. More sophisticated radar technologies have since developed, the air traffic control radar beacon system (ATCRBS) being one. After takeoff, aircraft turn on their transponders that send signals recognized by equipment at air traffic control that may then monitor not merely the position of an aircraft but also the unique identity of the flight can be gained from the transponder signal. Air traffic control, can then manage and organize the airspace for the most efficient and safe flow of aircraft. A similar system has also developed on the ground at airports so that 'aircraft incursions' -planes wandering onto runways without permission -may be avoided. This has usually been completed using a pair of binoculars; however, problems arise during periods of low visibility in bad weather and particularly fog. Airport surface detection equipment (ASDE) is a radar system able to locate and monitor planes movement, combined with tower automated ground surveillance system (TAGS) aircraft and indeed, any other airport vehicle may also be identified on the ground surface (Wells, 1996). Although, it must be noted that newer airspace surveillance is moving considerably away from the control tower as power is given back to the planes. Instead, decentralised forms of surveillance are becoming evident in the form of intelligent on-board systems that allow each aircraft, and airport land vehicles to monitor each other's position.",
          "This paper has in part attempted to question the broad brushstrokes of 'mobility' and 'flow' that are often used to describe airports. Airports act to monitor these flows and filter out threats, sorting and categorizing mobile bodies. Whilst this may happen spatially in-terms of access to specific places of the airport, it also occurs in terms of movement. As I have shown, people may become discriminately separated into categories that also affect their passage and speed through the airport. These systems have been shown to be increasingly troubling in their invasion of privacy and the possibilities for expansive data sharing. In-turn this paper has examined a conceptual sorting within surveillance studies, using the airport to recognise how non-humans may become monitored and controlled -to explore the surveillance of not only human but also non-human mobility. This approach has been argued to be of particular significance given that governments are increasingly turning to non-human surveillance (Carter, 2001). Now the focus is on the means of terrorism. Objects such as explosives have become actants in the eyes of airport security, where barometric or timer explosives pose threats independent to the whereabouts of their makers. Indeed, the surveillance of weather and the location of the actual aircraft is also key to the safety concerns of flight. The mobility of non-humans therefore, pose new challenges for airport surveillance in a climate where 'terror', 'bomb' and 'explosives' are words on every passenger's minds. At its most basic level, I think an ANT approach can help scholars to acknowledge this monitoring of non-humans. Then we can go further than the mere description of these techniques. ANT's implication for surveillance studies is not the simple mobilisation of supposedly inaminate, non-sentient or nonhuman things that may be monitored. We can do more than merely \"follow the actors\" (Latour, 1993; see also : Hitchings, 2003). In particular, ANT may be used to give greater insight into the relationships or networks of objects that surveillance systems examine. But this exercise may seem somewhat pointless if the relationships between humans and nonhumans are not understood. Indeed, investigating the monitoring of a machine in a factory may seem interesting, but is this important for surveillance studies? Does this even fit our definition of what surveillance is? In my opinion, for now, our starting point must be human. This paper has separated its treatment of humans and non-humans in an attempt to illustrate a tendency for scholars to delimit their focus to the study of the surveillance of humans and its implications. It has not been my purpose to forget the complexity of relationships between people and things, the cyborg 'machinic ensembles' of the plane or car that transport us to our destination (Thrift, 1995;Lyon, 2001). Whilst the separations I have made have been deliberately artificial for ease of illustration, any examination of the surveillance of things, only really becomes important if we are to appreciate the impacts they may have upon the human and indeed the human associations with these things. Certainly, it seems as though the surveillance of non-humans, in the examples I have explored, are used precisely because of the difficulties posed by monitoring the human. Many of us do not wish to have a bar-code printed on us, or to be radio tagged. Therefore, non-humans are monitored instead of humans. In-turn non-human monitoring may also be used as indicators of human activity -as evidence. It is far easier to trace illegal weapons and explosives than to read the minds of passengers to test for the intent of their actions. In other cases, particularly remote monitoring, the material effects of human activity are easily seen by satellite photography. Illegal logging may therefore become recognised on vast areas of the Amazon by its initial impacts, as opposed to the individual actions and movements of a person. Of course, the network metaphor may not always fit. At airports, the relationship of passenger and baggage only really resembles a traditional network when passengers' are in contact with their luggage -when their relationship is static. More fluidic metaphors can be invoked to better describe the often shifting relationships of passenger to bag -a 'mutable mobile'. Furthermore, Law and Mol's (2001) article suggests that the metaphor of fire is useful to describe the oftenflickering presence of actors within a network. This metaphor can be seen in the instance of a passenger not boarding a flight and leaving their baggage behind. As discussed, baggage reconciliation ensures that this separation is not complete given the linkage created between passenger and bag. If a passenger fails to get on their flight, their 'absent presence' flickers into significance to ensure the removal of their luggage from the plane. At the same time, do we also limit our examination to humans, objects and things? Activists may argue that the surveillance of animals should be questioned. At airports in particular, animals are routinely transported illegally. But animals are in-fact still key to the surveillance and security operations at the airport. Animals are not only a subject of surveillance but are part of the surveillance system itself. Sniffer dogs or K9 teams can be trained as vapour detectors to monitor illegal goods such as narcotics, and explosive devices (Clutterbuck, 1994). These practices are not simply completed by people or animals but succeed because of their mutual relationship (Saunders, 1999; see also Haraway, 1991;Wolch, 2002). Questioning these relationships is long overdue in our fixation upon the high-tech. We must ask how do these interactions take place? What implications do they have? However, this is not to suggest that ANT is a magic recipe for success. There are obviously many shortcomings illustrated by critiques that query the pretension to inclusion -the universal enfranchisement of ANT (Lee and Brown, 1994), its traditional ignorance of spatiality (Hetherington and Law, 2000) and indeed the shortcomings of the network metaphor as illustrated above. That said we should not shy away from the many positives. Examining the networks of the subjects of surveillance sheds light upon the relationality of humans and things. An exploration of these networks exposes how an amalgamation of people and things may become illegitimate, worthy of surveillance. For example, a terrorist 'agency' or threat can be understood as the achievement of a person carrying scissors on a plane. Examining surveillance through this lens may reveal much about the very workings of surveillance systems, the norms and assumptions that frame them, and the implications for those they monitor. The control of the airport is in my view, a suitable starting point for such a discussion."
        ],
        "ground_truth_definitions": {
          "racial bias": {
            "definition": "biases being held towards ethnicity, and national origin - a person becoming sorted based on personal prejudice",
            "context": "The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\"(1997).",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "2d1201d181da2166ccbf1e922634b8b74a58f141",
        "sections": [
          "The Power of Political Rhetoric RONALD R. KREBS University of Minnesota, Minneapolis, USA PATRICK THADDEUS JACKSON American University, Washington, DC, USA While scholars of International Relations and comparative politics have usually treated rhetoric as epiphenomenal, one strand of constructivism has recently returned rhetoric to the heart of political analysis, especially through the mechanism of persuasion. We too maintain that rhetoric is central to political processes and outcomes, but we argue that persuasion is theoretically and methodologically problematic. We aver that rhetoric's role may be more usefully conceptualized in the context of coercion, and we advance a stylized model that illustrates how rhetorical coercion operates, explains why it works, and identifies key scope conditions. We subsequently illustrate our model's relevance through a detailed examination of a 'hard' case. This article's agenda is twofold. First, it advises scholars in these fields to avoid focusing on unanswerable questions about actors' motives and to examine instead what actors say, in what contexts, and to what audiences. Second, it lays the groundwork for a 'coercive constructivism', complementing the liberal version so prevalent today. KEY WORDS ♦ coercion ♦ constructivism ♦ discourse ♦ Habermas ♦ Israel ♦ military service ♦ persuasion ♦ rhetoric Talk is often thought to belong to the realm of diplomacy, war to the realm of action. Yet, during the 2003 Iraq War, the Bush administration was nearly as preoccupied with how the combat was portrayed as with the combat itself. Its foreign policy team invariably spoke of coalition forces rather than American forces, the war to liberate Iraq rather than the invasion of Iraq, Saddam's death squads rather than Saddam's fedayeen. Rhetoric is central to politics, even when politics takes the form of war. Yet rhetoric is curiously not central to much scholarship in comparative politics and International Relations. Politics is typically marked by rhetorical competition, but our theoretical frameworks are generally hard-pressed to make sense of its dynamics and outcomes. Whereas the dominant materialist tradition treats rhetoric as epiphenomenal, we argue, following recent constructivist work, that the rhetorical interplay itself provides leverage in explaining outcomes. We are less comfortable, however, with the argument advanced by some constructivists that political actors deploy resonant rhetorical forms and thereby persuade their interlocutors of the correctness of their preferred course of action. Although persuasion undoubtedly does occur in the political arena, it is also rare. Moreover, such mechanisms rest on a strong specification of the subjective motivations of individuals and thus are methodologically intractable. Recent mainstream constructivist research has prompted a refreshing debate on and advanced our understanding of political deliberation and argumentation. By focusing on internalized norms as the driving forces of behavior and policy, however, such research has ultimately diverted attention from the dynamics of rhetoric. Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics. 1  Employing an expansive definition of rhetoric that includes all speech actswhether they are oral or written, whether they take place under conditions approximating Habermas' 'ideal speech situation' or not -we argue that rhetorical maneuver can prove critical to success in political contests even when one's opponents have not internalized the promoted values. While claimants may deploy arguments in the hope that they will eventually persuade, their more immediate task is, through skillful framing, to leave their opponents without access to the rhetorical materials needed to craft a socially sustainable rebuttal. Rhetorical coercion occurs when this strategy proves successful: when the claimant's opponents have been talked into a corner, compelled to endorse a stance they would otherwise reject. Most narrowly conceived, this article suggests and explores a language-focused mechanism of political influence complementing recent scholarship, but it also has a broader agenda. At the level of methodology, it calls on mainstream scholarship to avoid centering causal accounts on unanswerable questions about actors' true motives and to focus instead on what actors say, in what contexts, and to what audiences. At the level of substantive theory, it joins European Journal of International Relations 13(1) other recent work in returning rhetoric to the heart of political analysis. But rhetoric is often both the medium and the locus of political contest, and thus focusing on its role in the context of deliberation can take us only so far. Rhetoric matters, we argue, when human beings power as well as when they puzzle. This article represents a step away from constructivism with a liberal flavor, focused on the transformation of values, toward constructivism with coercive characteristics, focused on the exercise of power. This article proceeds in four main sections. First, we critically review existing approaches to rhetoric in empirical political science, examining in particular the limitations of (liberal or deliberative) constructivist approaches premised on persuasion. Second, we set forth our generalizable mechanism of rhetorical coercion and explicate both its logic and its limits. Third, we illustrate this mechanism's operation by exploring in detail a single 'hard' case: the relatively successful efforts of Druze Arabs in Israel to garner greater citizenship rights by framing them as the just deserts of their military sacrifice. This outcome is not entirely explicable without close attention to the dynamics of competing rhetorical claims, and our model of rhetorical coercion helps make sense of this puzzling and important case. Fourth, we briefly discuss the particular possibilities and limits of rhetorical coercion within international politics.",
          "Most scholars of international and comparative politics disparage rhetoric as epiphenomenal. The very phrase 'mere rhetoric' captures the view that what counts is not the language people use but the material power resources upon which they can draw. This perspective is shared by 'realist' (and especially rationalist) writers, and it accords with a well-established understanding of political power. Disenchanted with narrow materialist approaches, some have in recent years sought to harness the power of ideas. Many have invoked beliefs, ideas, and culture to supply inputs and to serve as coordination devices in games with multiple equilibria (Goldstein and Keohane, 1993;Laitin, 1998). Political psychologists have long argued that ideas influence how actors interpret evidence and sift through information (Jervis, 1976). Students of collective action have suggested that principled commitments can motivate participants and render them unusually insensitive to the costs of protest (Cohen, 1985). However, scholars with an ideational bent have typically relegated rhetoric to the margins. For 'idealists', words matter only insofar as they reflect actors' true beliefs, with private statements seen as more revealing than public pronouncements. Public rhetoric is of causal consequence only from the top down, as leaders of states and social movements deploy resonant language in an effort to mobilize support (Edelman, 1964;Snow et al., 1986;Snow and Krebs and Jackson: Twisting Tongues and Twisting Arms Benford, 1988). While devotees of 'interests' and 'ideas' vigorously debate the relative causal power of their favored variables, scholars in both traditions have little use for public rhetorical contestation. But to treat rhetoric as epiphenomenal is to render much of politics puzzling and to do violence to politics as lived by its participants. Rhetoric is certainly a weapon of the weak, but those holding the reins of power can and must deploy it as well. The acquisition and maintenance of rule ultimately hinge as much on legitimacy as on physical coercion, and such legitimacy can be established only through rhetorical action (Weber, 1968). While political contestants unquestionably further their agenda by exploiting material resources, they generally at the same time 'frame' their political activity, explaining the purposes to which their material power is put (Gamson, 1992). Such contests among state leaders and societal activists should be at the center of the study of politics. Rationalists and political psychologists might both protest that they have done precisely this. Rationalist scholars have pointed out that talk is not always cheap. State leaders who renege on their public rhetorical commitments may bear substantial domestic and international costs, giving them incentives both to tell the truth and to strategically manipulate audience costs (Fearon, 1994;Sartori, 2002). Though important, this insight flattens rhetoric into a purely informational tool, a way of efficiently revealing whether one is a high-cost or low-cost actor. As such, it cannot shed light on the framing competitions that often lie at the heart of politics. Political psychologists working within the area of political communication have demonstrated that speakers who succeed in defining the relevant considerations can shape mass opinion and ultimately policy outcomes. 2 But nearly all framing experiments have abstracted far from reality in exposing subjects to just a single issue frame (Druckman, 2001). Even the exceptions have failed to interrogate fully the dynamics of framing. Recent studies have concluded that the framing effect disappears when targets are exposed to competing frames from equally credible sources and that targets then respond in accord with supposedly invariant preferences (Druckman, 2004;Sniderman and Theriault, 2004). By this account, the intense framing contests endemic to politics are peripheral in that their only effect (given the caveat of equal credibility) is to render frame competition inconsequential, and baffling in that actors continue to expend copious resources on their rhetorical efforts. Moreover, this research program has ironically diverted attention from rhetorical interplay and instead directed it towards attributes of the target (e.g. political knowledge, awareness, expertise) and the source (e.g. credibility, trustworthiness, expert status). Studies of 'message effects' are comparatively rare. 3 Our debt to the psychological literature on framing is clear, but we wish to bring rhetoric itself back into the heart of the analysis.",
          "Most approaches fail to accord public rhetoric per se causal status, but one major exception has been the vibrant constructivist literature in international and comparative politics (Checkel, 1998;Finnemore and Sikkink, 2001). A central problem for constructivists has been explaining how and why new norms emerge and why actors might obey norms despite contrary material pressures. 4 Among the universe of mechanisms through which political actors develop 'shared understandings', and arguably occupying pride of place, is persuasion: 'normative claims ', Finnemore (1996: 141) asserts, 'become powerful and prevail by being persuasive' (see also Payne, 2001;Johnston, 2001). The targets of persuasive rhetorical moves do not grudgingly comply, but rather sincerely internalize new beliefs and consequently adopt new identities and preferences. Through persuasion, 'agent action becomes social structure, ideas become norms, and the subjective becomes the intersubjective' (Finnemore and Sikkink, 1998: 914). Through persuasive rhetoric, national and transnational social movements prompt not merely compliance with but consent to existing norms, and norm entrepreneurs need not resort to coercion to effect change. Persuasion -and public rhetoric as its medium -have emerged as the coins of the constructivist realm (see, among many others, Checkel, 2001;Crawford, 2002;Keck and Sikkink, 1998;Risse et al., 1999;Risse, 2000). Though sometimes only implicitly, these scholars have often drawn on Jürgen Habermas' logic of 'communicative action' to explain the power of rhetoric (Johnson, 1993;Lynch, 1999Lynch, , 2002;;Müller, 2001;Risse, 2000). 5  Habermas acknowledges that political actors often engage in narrowly goaldirected (teleological) action, but what makes his political vision distinctive is its affirmation of the possibility and prevalence of rational dialogue, of openminded deliberation in the quest for truth. Every legitimate use of language, Habermas argues, is premised on mutual understandings, on 'idealizations that transcend any particular language game', and these 'give rise to the perspective of an agreement that is open to criticism on the basis of validity claims' (Habermas, 1990: 199). For Habermas, politics (at least in its ideal form) is less about contest than consensus, less about powering than puzzling, and deliberative exchange consequently takes center stage (Habermas, 1984). Ideally, actors leave power and rank at the door, and they seek to persuade others and are themselves open to persuasion. Experimental studies of deliberation in fact conclude that the key to sustaining reflective discussion lies in the willingness of participants to keep an open mind (Barabas, 2004). We are very much in sympathy with this emphasis on public rhetorical interchange, but we are skeptical as to the utility of privileging persuasion as a causal mechanism. As Habermas himself recognizes, the unconstrained dialogue of 'communicative action' is not an empirically accurate portrait of politics, for power and rank are omnipresent in the political sphere. Actors do not leave their identities at the door when entering into dialogue, and they do not employ language unadulterated by earlier political contestation. Discourse is therefore always structured -and not by reason alone. Rules of exclusion and employment dictate what arguments can be proffered, under what conditions, and by whom (Foucault, 1972). Insofar as empirically oriented constructivists invoke 'communicative action' to explain real-world processes, it is fair to ask whether rational deliberation characterizes actual political debate. Only on the rare occasions when it does can Habermasian discourse ethics yield empirical insight into political contestation. It would, therefore, seem more helpful to adopt a theoretical framework that explicates the power of rhetoric even when politics is not truth-seeking and truth-generating, and a conception of rhetoric that includes all speech, no matter how interlaced with power relations. We are also skeptical that analysts can overcome the methodological hurdles standing in the way of demonstrating that persuasion has occurred. Such an explanation presumes that one can pin down the 'real motives' driving individual choice, so that one can distinguish beliefs sincerely embraced from those adopted for strategic or other reasons (Payne, 2001: 39-41). But conclusive proof of persuasion is elusive, for it requires unmediated access to people's minds (Wittgenstein, 1953: §150-5). Did Gorbachev accede to the reunification of Germany because he had been persuaded by the trump card of self-determination (Risse, 2000) or because he had calculated that the net costs of resisting the United States were prohibitive? Did Europeans abandon their overseas colonies because they had been persuaded that formal empire was illegitimate (Crawford, 2002) or because they had calculated that the net expected costs of retaining the colonies were excessive? It is impossible to say based on the available evidence, nor is it clear what evidence could even in principle clinch the case. 6 Relying on statements delivered in private settings does not escape the problem, since these articulations may be as strategic as their public counterparts (Scott, 1990). Many of these questions have received attention elsewhere. Scholars of transnational activism have drawn attention to the 'mobilization of shame', and they too have concluded that ways of framing claims cannot be fabricated out of whole cloth (Crawford, 2002;Price, 1998Price, , 2003;;Risse and Sikkink, 1999). Elements of William Riker's suggestive work on 'heresthetics ' (1996) are apropos, as is Frank Schimmelfenig's research on 'rhetorical action ' (2004). We seek to build on and systematize these important contributions, but we also depart from them. First, while many constructivists recognize that the targets of activist efforts may comply at first for strategic reasons, they claim that lasting normative change requires internalization. 7 This formulation is problematic because it relies on incompatible microfoundations in stage one (instrumental adaptation) and stage two (internalization). On methodological grounds, we prefer a mechanism of political influence highlighting strategic European Journal of International Relations 13(1) action throughout the entire episode. We agree with Weber that consistency, not completeness, should be the hallmark of sound work in the social sciences (Weber, 1949). Further, we are reluctant to characterize any normative change as 'lasting' in the sense of forestalling normative contestation over the long run. We proceed from the premise that norms are inherently subject to challenge and that the rhetorical arrangements sustaining norms are never fully stabilized and are at best relatively stable. Rhetorical contestation is thus always possible in theory, although it may not always be manifest in practice. Consequently, we focus on specific bounded episodes of contestation (McAdam et al., 2001: 29-30). Finally, existing accounts focusing on the strategic uses of public language -both constructivist (e.g. Risse, 2000) and rationalist (e.g. Riker, 1996)continue to be crafted around assertions about the 'interests' served by various rhetorical deployments and the 'motives' driving both speakers and targets. We seek to place the mechanism of rhetorical coercion on firmer methodological footing and, more broadly, to demonstrate the viability of a non-purposive social science. 8 Our skepticism regarding explanations rooted in the identification of motives is not novel. Hans Morgenthau, for one, declared the search for statesmen's motives 'futile' because 'motives are the most illusive of psychological data' (Morgenthau, 1993(Morgenthau, [1948]]: 5). Scholars in the rationalchoice tradition have offered similar observations (Frieden, 1999). Our 'solution', however, is more controversial. We are not prepared to follow our rational-choice colleagues down the path of assuming and deducing preferences. Rather, we take a less-traveled road and argue that social scientists should not begin by specifying the intentions of actors. Let us be clear: we are not arguing that actors do not possess motives, that those motives do not shape actors' behavior, or that such behavior is irrelevant to political outcomes. But we do aver that purposive accounts are analytically less useful than models in which preferences need not be specified or ranked. We therefore seek to minimize the place of motives as driving forces in our accounts of political processes and outcomes. We do not recommend this course lightly, for it flies in the face of deeply held epistemological commitments among social scientists in general and political scientists in particular. Purging empirical narratives of the language of subjective motivation is consequently difficult indeed. 9 Moreover, we recognize that all methodological choices are problematic, since the modeling of complex social processes requires abstracting from reality. Yet while this path is less traveled, it is by no means virgin ground. Many social scientists have distanced themselves from subjectivist understandings of culture as a coherent system of beliefs that might be ascertained through sophisticated survey techniques. We, like they, conceptualize culture as the always contested and often contradictory intersubjective semiotic practices through which social actors seek to generate meaning (Ross, 1997;Wedeen, 2002). Public semiotic codes constrain social actors regardless of whether they truly 'believe' in these codes' content, and thus the question of 'actual' belief is of little relevance (Weber, 1968;Jackson, 2002). Relational sociologists, most notably Charles Tilly, have maintained that individualist approaches commit the sin of reductionism: even if we did know people's motives, that would not help us explain social processes and outcomes. We follow these scholars in avoiding individual consciousness as a causal factor and in shifting 'the emphasis from consciousness to conversation, from action to interaction, from selves to sociabilities' (Tilly, 1998: 400; see also Emirbayer, 1997;and Shotter, 1993). These foundational theoretical moves serve as the point of departure for our model of rhetorical coercion. 10 We cannot observe directly what people think, but we can observe what they say and how they respond to claims and counter-claims. In our view, it does not matter whether actors believe what they say, whether they are motivated by crass material interests or sincere commitment. What is important is that they can be rhetorically maneuvered into a corner, trapped into publicly endorsing positions they may, or may not, find anathema. Rhetoric affects political outcomes even when all actors are cynical operators with little interest in genuine deliberation. The resolution of political issues through public debate need not imply any significant level of intersubjective consensus. Thinking about public rhetoric in this fashion avoids the crudeness of vulgar materialism, the reductionism of ideational approaches, and the heroic optimism of persuasion. Our perspective, which one might term 'coercive constructivism', both relates to and is different from existing approaches. Unlike both materialist and ideational accounts, we argue that language has a real causal impact on political outcomes. Unlike ideational approaches and liberal constructivism, we are skeptical that politics is more often characterized by puzzling than powering; we are equally skeptical that actors can transcend coercion and participate in fully rational deliberation. 11",
          "Rhetoric lies at the heart of politics. But many accounts of politics fail to accord it much weight, and many models fail to appreciate its explanatory leverage. This section seeks to explain how and why skillful rhetorical maneuvering can underpin a successful political campaign -not by persuading one's opponents of the rectitude of one's stance, but by denying them the rhetorical materials out of which to craft a socially sustainable rebuttal. Rhetorical coercion is a political strategy that seeks to twist arms by twisting tongues. This section proceeds by clarifying what rhetorical coercion is, how it operates, and the conditions under which it can be effective.",
          "We begin with a stylized account of an episode of rhetorical contestation. Seeking to effect change in some policy or ongoing course of action, a claimant (C) directs an argument toward the opposition (O) in view of a public (P). For the purposes of the model, it does not matter whether O has exclusive control over the actions at issue (if O is, for example, the government) or whether C and O share responsibility (if they are, for example, political parties in a legislative assembly). All that matters is that O's accession or resistance is critical to the outcome. Any argument that C puts forward contains two analytically separable parts: a frame (or set of terms) that characterizes the issue at hand and a set of implications that C suggests follows from that frame. For example, C might be a group of students protesting a war; their argument, advanced against the national government O through speeches and signs at a rally, portrays the war both to O and to P as unjust and unnecessary (frame) and calls for the end of hostilities and the withdrawal of troops (implications). In responding to C, O may accept or reject either or both the frame and the implications of C's argument. These alternatives yield four different potential outcomes of this episode of political contestation, reflected in Table 1. In case 1, O accepts both the frame and the implications of C's argument, and policy changes accordingly. 12 For C, this outcome represents an unmitigated triumph. Case 2 is more mixed. In case 2, O accepts the proposed implications and changes the policy, but it rejects C's reasoning. An example would be if an environmental movement (C) urged the government (O) to restrict logging for ecological reasons, and the government agreed to restrict logging but justified the policy shift in economic terms. Though the movement would have won the substantive battle, it might reasonably perceive its victory as less than complete; the government might in the future expand logging rights on economic grounds as easily as it had earlier restricted them on that very basis. It is possible that the environmentalists' rhetorical tactics drove the government's concessions, but determining that would require Krebs and Jackson: Twisting Tongues and  access to the authorities' true motives -something we earlier argued one can never ascertain. Both case 1 and case 2 mark (at least temporary) terminuses; contestation might resume at a later stage, but these cells represent relatively stable outcomes in the short to medium run. In contrast, cases 3 and 4 are cases of continued contestation. In case 3, O accepts the terms of C's arguments, but rejects the implications that C draws. For example, a lobbying group (C) may argue that high taxes are an unfair burden on the citizenry and should be reduced through an across-the-board reduction in tax rates; a political party (O) may agree that high taxes are problematic but suggest instead targeted tax cuts. As this relatively narrow policy debate occurs within a common issue frame, it might be termed an implication contest. In case 4, the two parties disagree about the very terms of debate as well as the policies that follow, and their rhetorical efforts consequently focus on advancing their preferred issue frame in the hope that their political opponents will accept it (along with the concomitant implications). Without a common frame bounding the debate, such rhetorical interchange -a framing contestis far more fluid, wide-ranging, and fundamental than in an implication contest. Politics is replete with such framing contests. Advocates of bilingual instruction invoke diversity and inter-cultural respect, while critics charge that instruction in anything other than the dominant language would threaten national ideals and students' eventual success in the marketplace. Opponents of abortion call the fetus an unborn child, portray the act as murder, and label themselves defenders of the right to life; supporters of abortion rights depict the fetus as an insentient being, portray the act as a medical procedure, and dub themselves the defenders of a woman's right to choose. Engaged in either an implication contest or a framing contest, C seeks through its claims and counter-claims to alter the rhetorical environment within which political battle is waged. While C might ideally prefer to persuade O, it must design a political strategy that can attain its desired ends even if persuasion proves impossible. The public plays a crucial role: both C and O must craft their appeals with an eye to some audience which sits in judgment of their rhetorical moves. If C can shift the rhetorical basis on which O justifies its stance to P, C can limit the range of O's possible responses and policy options, transcending the erstwhile implication or framing contest. Put simply, rhetorical coercion has taken place when this strategy proves successful -when O, regardless of its private beliefs, can no longer sustain its public opposition.",
          "Rhetorical contestation consists of parties attempting to maneuver each other onto more favorable rhetorical terrain and thereby to close off routes of European Journal of International Relations 13(1) acceptable rebuttal. Rhetorical coercion is successful when C's rhetorical moves deprive O of materials out of which to craft a reply that falls within the bounds of what P would accept. In the end, O finds itself, against its better judgment, endorsing (or at least acquiescing in) C's stance regardless of whether O has been persuaded or believes the words it utters. The alternatives -enduring punishment at the hands of P or investing significant resources in creating new terms of debate -would be prohibitively costly and time-consuming. In our model of rhetorical coercion, neither the motives nor the sincerity of the parties is particularly relevant. 13  C can succeed in rhetorically coercing O because of two key facts of social life. First, political actors can rarely take tangible steps or advance policy positions without justifying those stances and behaviors -in short, without framing. Politics may entail coercion or distribution, but at the same time it involves the struggle over meanings. Meanings, however, cannot be imposed unilaterally or through the exercise of material power alone. They are, by their very nature, intersubjective (Laffey and Weldes, 1997), and the effort to forge shared meaning implicates some audience in the process. The debate between C and O does not transpire in a vacuum: both are continually striving to legitimate their positions in P's eyes (Perelman, 1982). Second, speakers may not say just anything they would like in the public arena: rhetoric is not infinitely elastic but is structured. 14 Every community of discourse shares a number of topoi, or rhetorical commonplaces, that both enable and constrain speakers' rhetorical possibilities. These commonplaces are not 'fully predetermined, already decided distinctions', but weakly shared notions that can be 'expressed or formulated in different ways in different, concrete circumstances' (Shotter, 1993: 170-1). While C and O are free to weave together these commonplaces in creative ways, they are not free to deploy utterly alien formulations in the course of contestation: such arguments would fall, almost literally, on deaf ears. The available space for rhetorical contestation is, therefore, locally bounded, and the parties to a contentious episode cannot introduce entirely novel arguments. They must configure their appeals utilizing rhetorical tools drawn from a chest that is, in the short term, effectively fixed (Swidler, 1986). Rhetorical innovation, while possible and even inevitable in the long run, is far less likely in the short run for three reasons. First, while structures of discourse are never fully hegemonic and are continually being reworked, coherent political action would be impossible if rhetorical universes were in a state of continuous deep flux. Relative rhetorical stabilities must emerge to permit the construction of political strategies, and thus, at any given time, 'the terrain of dominant discourse is the only plausible arena of struggle' (Scott, 1990: 102). Second, imagining, formulating, and disseminating a brand-new commonplace requires an unusual level of commitment on the part of a claimant in terms of material resources, time, and effort. Deploying existing commonplaces -which have already been imagined, formulated, and disseminated -is far less costly (McAdam et al., 2001: 47-50). The process of transforming an unusual rhetorical form into a commonplace is necessarily lengthy and uncertain, but political actors normally possess far shorter time horizons. Arguments can prove powerful only when the commonplaces on which they draw are already present in the rhetorical field, which is shaped both by the unintended consequences of prior episodes of contestation and/or by campaigns undertaken in advance with the express purpose of reconfiguring the rhetorical terrain. Third, and related, novel rhetorical resources are likely to be drowned out by existing arguments and frames. Dense networks of communication carry and regularly reinforce established commonplaces, overwhelming proffered alternatives. Rhetorical innovations consequently tend to arise at the margins, where communication networks are less dense. Overcoming such barriers is possible in principle, but it is in reality so improbable that C and O must, in a given contentious episode, play within the rhetorical arena that presents itself to them. And that arena privileges particular frames and places others beyond the pale, regardless of the material resources at the speaker's disposal. If C and O were the only two actors in our stylized story, they might introduce novel arguments costlessly and endlessly, and rhetorical coercion would then be impossible. Their rhetorical maneuvers are constrained, however, because rhetorical contestation is public in nature (Bennett, 1980: 805-6). The continual competition for public legitimation ensures that P occupies a critical position, setting limits to the policy stances that the contestants can in practice advance. The relevant audience sets the contours of rhetorical contestation, and stepping beyond or reshaping them requires an investment not attractive or even feasible in the short run. This focus on the public's role constitutes a point of intersection with scholars in the rational-choice tradition who have ascribed great importance to 'audience costs' (Fearon, 1994). In such models, the actors and the audience have clear and consistent preferences that precede the contentious episode; public pronouncements serve to signal the content and/or intensity of those preferences. By contrast, our model does not begin by identifying and ordering the parties' desires, and the outcome of a particular episode does not depend on these preferences' content or ranking. Instead, the importance of the public P lies in the imperative for appeal frames to draw on rhetorical commonplaces present in the public's everyday deliberations (Shotter, 1993: 65-9; see also Bennett, 1980). Our model presumes not that European Journal of International Relations 13(1) P has a well-thought-out position in advance on the issue being contested, but rather that there is only a limited set of arguments that P would, in principle, find minimally acceptable. 15  In sum, one argument 'wins' not because its grounds are 'valid' in the sense of satisfying the demands of universal reason or because it accords with the audience's prior normative commitments or material interests, but because its grounds are socially sustainable -because the audience deems certain rhetorical deployments acceptable and others impermissible. One cannot, however, fashion strict covering laws regarding the audience's response. Students of conversation and argumentation from across several disciplines (anthropology, communication, sociology, philosophy, psychology) have argued that context is unusually crucial to their field of study (Billig, 1996;Gumperz, 1982;Perelman, 1982;Sawyer, 2001;Shotter, 1993;Toulmin, 1958). Classifying structures of discourse, reconstructing the rhetorical possibilities, exploring the rhetorical interplay: these critical challenges require one to formulate arguments limited in time and space and limited by the arena of rhetorical competition. Thus, for example, we do not ask below what sorts of appeals are most effective in general at compelling authorities to grant oppressed minorities first-class citizenship. Rather, through an interrogation of the Israeli case, we suggest that a specific claim (framed around military sacrifice and the equation of rights and obligations) is more likely to be successful in a particular discursive context (when talk about citizenship is narrowly republican). Such an explanation is modest, yet powerful, and potentially generalizable.",
          "The logic of rhetorical coercion suggests that two factors are most important in explaining when the model will have explanatory value. First, is P a relevant party to C's claims-making? Recall that C's power is rooted in its credible threat, implicit or explicit, to bring P in on its side; C's ability to do so deprives O of the option of refusing to respond to or even acknowledge C's claims. If C cannot issue its threat credibly, then O can safely ignore C's claims, no matter how they are framed. We can readily imagine conditions under which P would not be relevant to the interaction between the protagonists in our stylized story. It may be that P simply cannot hear C -perhaps because C is being actively repressed; perhaps because C lacks sufficient resources to publicize its message; or perhaps because C's claims regard alleged misdeeds that transpire far from the public eye. 16 Moreover, in some social contexts, decisions need not be justified. 17 The prevalence of justificatory behavior in the political arena suggests, however, that we are correct to maintain that the scope of the model's application is quite large.",
          "Second, how limited is the universe of rhetorical commonplaces? We have argued that actors must, in any particular claims-making episode, forge their appeals by drawing upon existing commonplaces that represent the boundaries of legitimate framing. When these boundaries are loose, numerous rhetorical strands are available, ensuring that O can almost always put forward a meaningful rebuttal to C. In such circumstances, the terms of debate are hardly constraining. When these boundaries are relatively restricted, rhetorical coercion is more easily achieved, as O has less rhetorical 'wiggle room' -that is, fewer socially plausible ways of rebutting C's claim. We recognize that verbal appeals are multivocal, that advocates and opponents of particular policies may advance their arguments employing similar terms. But public language, in our view, is never infinitely flexible, and even so capacious a rhetorical formulation as that of 'rights' in the American context disciplines the speaker. Such disciplining is more common and thus rhetorical coercion is more likely when speakers have fewer commonplaces on which to draw. 18",
          "One of the most venerable norms in the West is that linking military service and citizenship. 'Whoever puts himself in the service of the state', observed Otto Hintze, 'must logically and fairly be granted the regular rights of citizenship' (Hintze, 1975: 211), and the call to the colors is widely considered the supreme example of state service (Janowitz, 1976). Thus the leaders of those relegated to second-class citizenship have often counseled in wartime against draft evasion and have even urged voluntary enlistment. Afterwards they have shrewdly contrasted their people's record of loyalty and sacrifice to the reality of entrenched political and social inequity. For instance, African-Americans volunteered in droves for the Union Army in the US Civil War and for the American Expeditionary Force in World War I, wrapping their postwar demands in the bloody flag. The rhetoric of military sacrifice has at times proven effective. Mobilized veterans have most obviously exploited their status to gain both political rights and distributional benefits. Oppressed and marginalized ethnic, racial, and religious groups have, based upon their collective sacrifice, also advanced claims for citizenship rights, albeit with varying degrees of success. African-Americans, for example, have repeatedly found this path to citizenship blocked. But their lack of postwar progress is hardly surprising given the high stakes of citizenship battles. The demands of subordinate groups for first-class citizenship challenge the core of the existing social order, and the opposition to meaningful change is consequently often fierce. It is not African-Americans' repeated failures that require explanation as much as the successes of others -such as American women after World War I, Japanese-Americans European Journal of International Relations 13(1) after World War II, and Sikhs in post-independence India -in deploying the rhetoric of sacrifice to win improvements in their status. The opponents of reform typically occupy powerful positions in political, social, and economic structures, and few would expect them to give way before words alone. The politics of citizenship is thus a 'hard case' for the power of rhetoric, for material interests and resources would appear to provide a ready explanation for which groups' citizenship claims are granted and which are denied. We will argue, however, that the particular rhetorical configurations advanced by Israel's various Arab minorities help explain these groups' differential rates of success in overcoming the barriers to first-class citizenship in this 'ethnic democracy' (Smooha, 1997). Drawing on their record of military service, Druze Arabs argued that equal obligations demand equal rights. Jewish politicians found themselves unable to craft a culturally meaningful rebuttal to Druze demands, and they were compelled to concede Druze claims. In contrast, Christian and Muslim Arabs, who have never been drafted into the Israel Defense Forces (IDF), insisted that Israel abide by its public commitment to liberal democracy. While this frame also drew on Israeli commonplaces, it permitted Jewish politicians to justify discrimination against Christians and Muslims by arguing that those who do not perform civic duties cannot lay claim to equivalent public rights and benefits. Although the Druze were poorer than their fellow Arabs and smaller in number, for many years they were more effective in placing their concerns on the national agenda and in garnering a favorable response from the authorities. Consequently, material factors cannot account for the success of the Druze relative to other Arabs, and Jewish politicians' continual resistance to Druze demands suggests that persuasion was not at work. Israel's relationship with its Arab minorities nicely illustrates the power and limits of rhetorical coercion. 19   Background Israel's Arab population -today well over a million strongis divided by religion, with the Muslims, the Christians, and the Druze constituting the three largest groups. Possessing basic political rights, the entire Arab community has enjoyed great socioeconomic progress since Israel's founding in 1948, particularly relative to Palestinians in the West Bank and the Gaza Strip and to Arabs throughout the Middle East. Yet they have also been openly labeled a fifth column, and they have, often without regard to religious affiliation, been the victims of variegated forms of discrimination on the part of the state and the majority Jewish population. The new Israeli government subjected Arabs to military rule, conducted pervasive surveillance within the Arab communities, limited Arabs' capacity for travel and an independent economy, expropriated the bulk of Arab-controlled land, excluded Arabs from the giant labor Krebs and Jackson: Twisting Tongues and Twisting Arms federation known as the Histadrut, and so on. Jewish leaders typically invoked the language of security to justify these policies, but its broad definition encompassed ideological, economic, and even partisan interests as well as security more narrowly conceived. Much overt discrimination came to an end by the late 1960s, but substantial discrimination persisted in more subtle forms (Kretzmer, 1988;Lustick, 1980;Shafir and Peled, 2002). Starting from this common baseline, however, the paths of Israel's Arab minorities have diverged. While all continue today to endure second-class status relative to Jewish Israelis, the Druze have made greater headway than the larger and wealthier Christian and Muslim communities. Since 1956 the Druze have been able to deploy a set of rhetorical commonplaces unavailable to their Christian and Muslim neighbors. That year, male Druze became subject to the military draft, reversing a policy of exemption that had been applied to all Arabs since the passage of Israel's universal conscription law in 1950 (Peled, 1998). Some have speculated plausibly that the Israeli authorities hoped that Druze service in the IDF would provide proof of Israeli liberalism and thereby neutralize Arab nationalists and satisfy Western critics (Firro, 1999;Lustick, 1980), but even full declassification of the relevant documents would not definitively establish the government's motives. We argue that, regardless of who wanted to draft the Druze and why, conscription helped the Druze prosper in Israeli politics.",
          "In the mid-1960s, the Druze began to mobilize, and they regularly sought a public forum in which to air their grievances, which they consistently framed around their people's military service and the equation of rights and obligations. In 1967 Druze activists circulated widely an appeal insisting that 'since we have done our duties … and see in the lack of equality … an infringement and denial of our rights, we urgently demand correction of this wrong' (Firro, 1999: 187). Surveying the history of Druze claims-making, one scholar noted that, among both Jews and the Druze, 'injustices to individual Druzes are usually attacked as ingratitude toward a man who was willing to shed his blood for his country, but now his country turns against him' (Ben Dor, 1979: 134-5). Even in recent years, the Druze have continued to gravitate to this rhetorical mode. During a series of public protests in the 1990s, mayors of Druze villages and towns 'usually denounce [ed] the Israeli government for its broken promises, and stress[ed] the price paid by the community in the form of hundreds of Druze soldiers killed during their service in the Israeli army' (Yiftachel and Segal, 1998: 487). Even the minority of Druze who have objected to the draft have accepted these basic terms. As a Communist Druze Knesset representative argued in the early 1990s, 'If this is a blood covenant, then give us the feeling that we are living in our state, in our homeland, with full rights like all citizens . . . And if there is to be no equality European Journal of International Relations 13(1) of rights, then do not demand equality of obligations, and allow Druze to choose -to serve or not to serve in the IDF' (Zohar, 1991). In constructing their claims frame, the Druze have drawn on rhetorical commonplaces deeply rooted in Israel's rhetorical traditions. Although participation in the pre-state Jewish community, the yishuv, was necessarily voluntary, it prized individual and group commitment to the public good, defined as the historical mission of the Jewish people to rebuild the land of Israel and epitomized by the kibbutz movement. The new state preserved this emphasis on civic commitment as the basis for membership, grafting a powerful statist ideology onto the earlier discourse (Liebman and Don Yihya, 1983). In an early Knesset debate, Prime Minister David Ben Gurion insisted that women too be subject to the draft, bluntly declaring, 'There are no rights without obligations. And above all is the obligation of security, the obligation of defending our existence, our freedom, our independence, and our growth' (Divre Ha-Knesset, 1950: 3:537). Even dissidents have protested within the terms of this dominant definition of citizenship: as one conscientious objector from the Lebanon War put it, 'military service, paying taxes, and obeying the law . . . That is what makes you a citizen and makes you eligible to enjoy the defense and the fruits that [the state] equally distributes' (Helman, 2000: 324). 20  This civic republican tradition has long sat uncomfortably alongside an ascriptive component in the definition of the Israeli polity. The 1948 Declaration of Independence proclaimed the new country 'the Jewish state' even as it promised that Israel would promote the welfare and protect the basic freedoms of all its inhabitants. The country's formal name, the State of Israel, suggests an abiding commitment to pluralistic secularism, but its historical-religious title, the Land of Israel, remains in popular use (Kimmerling, 1985). Some have concluded that 'Jewish ethnicity is a necessary condition for membership in the political community, while the contribution to the process of Jewish national redemption is a measure of one's civic virtue' (Peled, 1992: 435). Yet this ignores the potential contradictions between these discourses. Such tensions are muted when only Jewish citizens are drafted, for then Israeli leaders can publicly embrace civic republicanism while preserving ethnoreligious priorities. But Druze claims-making framed around collective military sacrifice challenged the coherence of Israel's 'ethnorepublican' citizenship. Confronted with such a challenge, Jewish Israeli leaders had two alternatives: either they could acknowledge their exclusive definition of the political community, or they could open its doors. While the Druze squeezed concessions out of Jewish politicians beginning in the late 1960s, there is no evidence that their appeals were persuasive: both the limited and incremental nature of Israeli concessions as well as the grudging fashion in which they were offered are not consistent with a narrative centered Krebs and Jackson: Twisting Tongues and Twisting Arms around persuasion. Rather, our model of rhetorical coercion can help explain why the Druze made headway in this ethnic democracy. Faced with Druze claims, Jewish leaders could have launched a framing contest by (a) admitting openly the discriminatory nature of the state. Alternatively, they could have conceded the Druze frame but challenged its implications by (b) claiming that the Druze were not relatively disadvantaged or (c) arguing that the Druze community's problems were of its own making. The first alternative frame was unappealing -not just because of the likely international repercussions, but also because it would have contradicted both Israel's self-proclaimed status as the sole democracy in the Middle East and its dominant way of speaking about citizenship. The second put forward a claim widely known to be false, and the third possible response, while perhaps plausible with regard to questions of economic development, could not justify administrative segregation, which was, whether cleverly or fortunately, among Druze activists' first targets. There was, however, a fourth option: Jewish politicians could have refused to reply -as long as relevant audiences were unaware of the Druze's plight. The Druze's capacity to engage in rhetorical coercion was consequently dependent on garnering attention from the Hebrew-language media. As a general rule, the Hebrew press rarely covered the Arab communities, for its readership was almost entirely Jewish. But the Druze arguments were resonant with republican rhetorical practice, producing a narrative that Jewish publics comprehended and embraced. As a result, when the Druze made noise, the press amplified it, compensating for the Druze's lack of political clout and ensuring a hearing in the halls of power (Asa'ad, 2000;Toledano, 2000). In short, the Druze trapped Jewish leaders in a rhetorical cul-de-sac in which an implication or framing contest was unsustainable. They maneuvered their Jewish opponents onto a rhetorical playing field on which the Druze could not lose, for no rebuttal would have been acceptable to key audiences, both domestic and international. Wary of calling punishment upon themselves, Jewish leaders had little choice but to concede. Consequently, 'when the Druze demanded something, the ears were much more open' (Toledano, 2000). Despite the Druze community's electoral insignificance, cases of alleged discrimination against individual Druze received attention even in the Knesset (Ben Dor, 1979: 134-6). The Druze demand in the late 1960s for full membership in the Labor Party was irrefutable: as a prominent Labor functionary wrote in a major newspaper, 'How can we explain to that Druze … that he is good enough to endanger his life for the state but that he is not good enough to be a member of the Labor Party?' (Lin, 1970). Confident that their rhetorical resources have bequeathed disproportionate influence, the Druze have resisted an alliance with their numerous fellow Arabs, with whom they share many grievances. Even those Druze otherwise critical of Israel's policies have often supported continued conscription so that European Journal of International Relations 13(1) the military sacrifice frame would remain available. Lacking abundant political resources, the Druze adopted a strategy aimed at rhetorical coercion, and they demonstrated that it could work. Jewish politicians were compelled to say things they (likely) did not believe and ultimately even to follow through on those promises. Like their Druze counterparts, Christian and Muslim Arabs drew on Israeli commonplaces, arguing that Israel needed to eliminate discriminatory measures to live up to its democratic promise. Yet, in comparison to the Druze, they have encountered far greater public resistance, for Jewish politicians had a rhetorical escape hatch. Arab rights, they often argued, were not sacrosanct, for those who did not fulfill their civic obligations (that is, military service) deserved less protection than those who had made the supreme sacrifice. In seeking to convince a wavering Knesset member to support the military administration in 1962, Ben Gurion protested, 'I have recognized all these years that rights are dependent on obligations . . . I do not understand why I may limit the freedom and steal the time of young Jewish men and women when they are in the army, but why it is forbidden to limit much less the freedom of those who do not serve in the army.' 21 That same Labor functionary who advocated offering the Druze membership in the party justified the exclusion of other Arabs on similar grounds: 'I asked them [close Arab friends] how they could sit in the party central committee … next to a bereaved father or mother, next to a party member who had just returned from the front, next to a member who stands ready to leave the next day for reserve service, while they still do not fulfill this obligation' (Lin, 1970). Faced with claims framed in liberal terms, Jewish leaders had a response at the ready, evading the rhetorical dead-end into which Christians and Muslims might have hoped to maneuver them. For decades, this rhetorical move has frustrated Arab efforts to attain first-class citizenship. As one Arab activist recently complained, 'They are trying to link our rights as citizens to performing military service. That's not how a real democracy works' (quoted in Schechter, 2003). Christian and Muslim claimants have long remained mired in a framing contest, unable to persuade or rhetorically coerce Jewish politicians. 22   Alternative Explanations How else might one explain the relative success of the Druze in attaining increments of Israeli effective citizenship? The most intuitive answer would attribute this outcome to the distribution of material resources. But the Druze's potential voter base has been tiny, their per capita income has remained relatively low, they have historically eschewed violent confrontation with the authorities, and they have often had difficulty creating and sustaining effective lobbying organizations (Firro, 1999). By all standard measures of political power, the Druze should have been less successful than other Arabs, not more so.",
          "A second alternative would invert the first. Precisely because the Druze were so small, so weak, and so moderate, Israel could countenance their inclusion in the IDF and grant them some modicum of effective citizenship without threatening the state's Jewish identity. Had the Druze's population been as large as Israel's Christian or Muslim communities, concessions would have been unthinkable. The Druze's small size made them an attractive target for cooptation (Lustick, 1980). But if this argument were correct, presumably the state should have flung its doors wide open, for the costs would have been negligible and the propaganda advantages substantial. In fact, however, while Jewish decision-makers readily offered rhetorical concessions to the Druze, they resisted implementation: 'To say that if the ears were open, things were done -there is a difference' (Toledano, 2000). The Druze made tangible progress, but they fought hard for those increments of meaningful citizenship. Nor did the Druze act like a model coopted minority. They were not content with petty favors, but rather became deeply dissatisfied with their status and continually presented Jewish leaders with difficult and uncomfortable choices. A third alternative would posit that Israel's conquest of the West Bank and Gaza Strip in 1967 and the addition of a large, politicized Palestinian population gave Jewish politicians incentives to grant Druze citizens' demands so as to prevent the Palestinization of the Druze community. Presumably, however, they faced far greater incentives to accommodate the much larger Christian and Muslim citizen populations, whose members had not yet openly allied with the Palestinian national cause. Nevertheless, the Israeli establishment remained highly resistant to Christian and Muslim claims-making. This argument thus has difficulty accounting for the divergent patterns in Israel's relations with its various Arab communities after 1967 -a difficulty not faced by the mechanism of rhetorical coercion. A fourth alternative account would attribute the Druze anomaly to their predilection for taqiyya, or the art of concealment. 23 Long viewed as heretics by their Muslim neighbors, the Druze have historically sought to shield themselves from persecution by hiding their religious observances. With regard to politics, this ultra-pragmatic doctrine prescribes neutrality when possible and 'bandwagoning' -allying with the likely victor -when necessary (Layish, 1985). One would thus have expected the Druze to avoid challenging stronger parties, but in fact the Druze displayed a rebellious streak during Ottoman and European colonial domination of the region. Despite a long history of communal tension, they made common cause with their fellow Arabs in the struggle for Lebanese autonomy early in the 20th century. And, frustrated that one foreign ruler had simply replaced another, their leadership initiated a nationalist revolt against the French in the mid-1920s -with disastrous consequences (Tauber, 1993a(Tauber, , 1993b;;Firro, 1999). The Druze were not proto-nationalists, but they were at times willing to set aside intra-Arab differences, compelling one to ask why they European Journal of International Relations 13(1) did not cast their lot with other Arabs in Israel in the decades after 1948. Taqiyya is obviously of little help in answering this question. Equally important, it lacks analytical bite. The doctrine is so flexible that it is less an explanation for Druze behavior than a tidy description of it.",
          "The Druze's success in compelling Jewish Israeli politicians to grant them a modicum of first-class citizenship illustrates nicely the operation of rhetorical coercion. Although this single case study cannot of course prove that the mechanism is generalizable, we view rhetorical coercion as among the set of 'widely operating explanatory mechanisms that combine differently and therefore produce different outcomes in one setting or another ' (McAdam et al., 2001: 13-14). The Druze case suggests the applicability of rhetorical coercion to domestic politics, but is the mechanism operative in the international arena? Extending the earlier theoretical discussion leads to the conclusion that rhetorical coercion is more likely as social ties in political communities, domestic and even international, grow increasingly extensive and dense. On the whole, however, rhetorical coercion will operate less effectively and less frequently in international settings. Key factors highlighted by the model are often present in international relations: state leaders speak to international audiences, they are sensitive to those audiences' reactions, and justificatory speech acts are prevalent. Indeed, while not systematically elaborating the mechanism's dynamics, others have shown rhetorical coercion to be of critical explanatory importance in such divergent contexts as Cold War crisis bargaining (Bially Mattern, 2004), intra-Arab alliance dynamics (Barnett, 1998), eastward enlargement of the European Union and NATO (Fierke and Wiener, 1999;Schimmelfenig, 2004), and Soviet premier Mikhail Gorbachev's proposals for nuclear disarmament (Evangelista, 2001). Yet even these examples suggest the limits of rhetorical coercion in international politics. Rhetorical coercion, like other forms of political communication, is premised on a political community that shares at least some understandings of the boundaries of acceptable discourse. The more tightly linked the community, the greater the possibilities for rhetorical coercion. Thus it is not surprising to find this mechanism primarily operative among regional groupings with relatively strong requirements for legitimation. The Soviet Union had long legitimated its leadership of the communist bloc by casting itself as the true force for global peace, and its routine calls for nuclear disarmament sustained that claim; Soviet generals could not publicly dispute Gorbachev's proposal without simultaneously undercutting Soviet authority among communist regimes. Similarly, Egypt had, under Nasser, positioned itself as the leader of the Arab world Krebs and Jackson: Twisting Tongues and Twisting Arms based on a pan-Arab discourse that represented Israel as its inherently opposed Other; Nasser reluctantly accepted the risk of war with Israel in 1967 because deviating from the norms of Arab nationalist discourse would have imperiled Egypt's status as the spokesman for Arab nationalism and might have jeopardized his regime's stability. Finally, the European Union, the most tightly bound regional arrangement on the globe today, could not have denied membership to consolidated democracies in Eastern Europe without contradicting the community's legitimating discourse which had intertwined the European concept with liberal democracy; to have vetoed or even vocally opposed eastward enlargement would have challenged the basis for community and threatened individual members' credibility. Such dynamics thus depend on the strength and density of the ties binding the community's members, and such ties vary substantially across the international system. 24 Where such ties are strong and dense -that is, within such regional groupings of states, where international anarchy (as even realists might concede) is most attenuated -rhetorical coercion remains distinctly possible. The cosmopolitan dream of a 'global public' remains today just that, although there are emerging global standards of legitimacy in several arenas and increasingly robust mechanisms through which norm violators might be held accountable (Grant and Keohane, 2005). Where such ties are weak and sparse -as in relations across such regional groupings and as in the many arenas lacking shared bases for legitimation -rhetorical coercion will be far more difficult. As the metaphor of the two-level game implies, political elites in such circumstances are likely to be engaged with and responsive to multiple and different audiences simultaneously, and these audiences are unlikely to share a single conception of what rhetorical moves are acceptable and appropriate. The more complex the social environment, the greater the opportunities for actors to evade the consequences of rhetorical interchange, and thus the more imposing the obstacles to rhetorical coercion. Where such ties have never existed or have been ruptured -perhaps as a consequence of the limits of communications technologies or perhaps as a result of enduring interstate rivalry -communication (including rhetorical coercion) is impossible, and diplomacy is a dialogue of the deaf. As a consequence, it is not the imposing capacity of foreign publics, let alone some elusive global public, to sanction state action that makes rhetorical coercion potentially successful on the international scene. When rhetorical coercion is operative internationally, the critical punishing agent is more often a domestic audience prepared to exact costs for violations of standards of rhetorical appropriateness. While foreigners have disapproved of American unilateralism in recent years, it is not clear that this opposition has driven their governments' policies (Brooks and Wohlforth, 2005), and while anti-Americanism has unquestionably been prevalent, its political consequences European Journal of International Relations 13(1) appear to have often been exaggerated (Keohane and Katzenstein, 2006). The George W. Bush administration could dismiss foreign opposition to the Iraq War, particularly because cooperation on matters of joint concern such as trade and terrorism (narrowly construed) continued seemingly unabated. But the American public's increasing disenchantment with the war has been a far greater cause for concern for the administration, as it is this public that might be directly brought into a political contest over the war -a political contest within which rhetorical coercion is far more likely to operate. 25",
          "Machiavelli famously advised that 'a ruler need not have all the positive qualities …but he must seem to have them… He must seem, to those who listen to him and watch him, entirely pious, truthful, reliable, sympathetic, and religious' (Machiavelli, 1994: 55). Realists, not inaccurately, read Machiavelli as warning against allowing moral considerations to impede actions necessary for the pursuit of political power. But, as coercive constructivists, we would add that these lines imply that social identity has an effect on social and political outcomes, regardless of whether or not the actor internalizes the components constituting this identity. A ruler pursuing policies incapable of public justification would find her path strewn with numerous practical obstacles. This lack of rhetorical resources might even compel her to alter her course of action. In accord with other constructivists, we aver that social norms and social identities matter. 26 In accord with political realists, we argue that whether or not a social actor has internalized a particular set of normative principles is not causally relevant. In accord with the broader cultural turn in the social sciences, we combine these positions by focusing on public language, on how normative principles are deployed in public debate, and by exploring the causal impact this deployment can have on what people say and consequently on what they do -not on the impossibly elusive question of what they believe. Our model of rhetorical coercion can helpfully capture the dynamics of framing contests, when there is controversy about how an issue should be characterized, and of implication contests, when narrower (yet no less important) battles are waged over the consequences of some accepted framing. Our analysis draws attention to two key conditions affecting the applicability of the model. First, rhetorical coercion can occur only when the public is a party to the debate or when the claimant can credibly threaten to involve the audience. Second, not all structures of discourse are equally conducive to rhetorical coercion. This mechanism is more likely to operate when these structures are relatively restricted (and constraining) than when they are relatively loose (and permissive). Furthermore, our model cannot illuminate the process of rhetorical contest in two circumstances: first, when an opponent accepts a claim without argument, and, second, when an opponent accepts the implications of an argument but not its terms. We contend, however, that much social and political life lies within our model's scope. 27  In short, this article seeks to invigorate the study of political rhetoricbut in a fashion relatively unfamiliar to recent debates in international and comparative politics. We argue that rhetorical contestation shapes policy outcomes and that the approaches most common in these subfields miss a significant portion of political and social reality. We further suggest that it is possible to make causal claims without trespassing into the murky waters of subjective motivation and without relying on problematic mechanisms like persuasion. Many IR constructivists would be comfortable with the first of these claims, many realists and students of political culture with the second, few from either camp with their combination. This article has sought to demonstrate that their conjunction is both logically sustainable and potentially productive.",
          "material interests' problematic and have instead embraced a linguistic focus that is not dissimilar to our own. 5. Among those drawing on social psychology are Checkel (2001); Crawford (2002: 26-7);and Johnston (2001). Others have remained agnostic as to how persuasion operates (Finnemore and Sikkink, 1998). We limit ourselves here to discussion of the Habermasian version of the persuasion argument, since in our judgment it is the most clearly articulated mechanism. For an alternative treatment of Habermas in international relations, see Mitzen (2005). 6. On the lack of evidence of changed minds in Crawford (2002), see Dessler and Owen (2005: 602). 7. We are grateful to Kathryn Sikkink for helpful discussion on this point. 8. For treatments closer to our own, see, among others, Barnett (1998); Bially Mattern (2004); Fierke (1998); Goddard (2006). 9. Such language even slips on occasion into our own case study, for the conventions of historical writing insist on subjects who possess agency, articulate goals, and pursue motives. The Foucauldian alternative -interrogating the central discursive formations -might help us escape this trap, but it would come at the cost of the narrative form, which better captures the tensions and surprising reversals of actual political contest. Such traces of motivational language are consequently inevitable, but they should not cloud or draw attention from the rhetorical dynamics themselves. We are grateful to an anonymous reviewer for pressing us on this point. 10. The distinction between persuasion and coercion as modes of political influence is by no means original, but our conceptualization of rhetorical contestation as potentially coercive is more unusual. Scholars, especially those working within a psychological framework, sometimes treat persuasion as synonymous with all attitude change, citing even nuclear deterrence as an example. In contrast, we define coercion as the imposition of constraint on choice, whereas normative persuasion seeks ideally to bypass choice. Both coercion and persuasion may lead the target to assent, but the coerced target merely complies, while the persuaded target consents. Put differently, persuasion speaks to change in preferences, while coercion seeks to alter the social environment confronting an actor while leaving preferences untouched. 11. For a related discussion of 'realist constructivism', see Jackson and Nexon (2004). 12. Note that 'acceptance' here does not imply agreement, but refers only to O's public reaction to C's argument. 13. While we make no strong assumptions regarding the content of the actors' motives, the model does rely on the thin, almost trivial, assumption that actors are fundamentally strategic -that they pursue policies that they believe will further their goals (McAdam et al., 2001). 14. For related arguments, see Cruz (2000); Spillman (1995); Steinberg (1999). 15. In this stylized model, we presume that both actors must legitimate their stance in the eyes of the same audience, but the reality of both international and domestic politics is often more complicated. Speakers in both arenas may be playing to entirely different audiences or may be competing to define which audience is in fact relevant. The dynamics of such contestation are not well addressed by this paper and will be taken up in future articles. We are grateful to anonymous reviewers for raising this question. 16. This is not infrequently the case when O is a government whose commands are implemented by a secretive bureaucracy. A potential weakness of rhetorical coercion is its implication that rhetorical concessions translate smoothly into policy change. O may renege on its commitments, particularly during implementation when there may be little need for public justification. While such breakdowns are likely, we maintain that violations will prove difficult to sustain should C hold O's feet to the public fire in a new episode of rhetorical contestation. 17. Superiors often do not explain their logic to subordinates, and exasperated parents often tell children, 'Just do as I say!' In such circumstances, rhetorical contestation is not likely to be effective. We are grateful to Robert Jervis for pointing this out. 18. For more on the logic of rhetorical coercion, see Jackson (2006); Krebs (2006). 19. For a more extensive treatment of these cases, see Krebs (2006: Chs 3-5). 20. Military service occupies so prominent a site in the Israeli cultural complex that 'civic virtue has been constructed in terms of and identified with military virtue' (Helman, 2000: 320). 21. Ben Gurion to M.K. Yizhar, 12 February 1962, State Archives (Jerusalem, Israel), Office of the Prime Minister Files, C 6304/1086. 22. Christians and Muslims could not control the rhetorical playing field, but they nevertheless made significant gains in the mid-1980s thanks to their raw political power. After the 1977 breakdown of Labor Zionism's stranglehold over the political system, the electoral competition tightened, and the Arab population was actively wooed. Candidates from across the political spectrum stumped for the first time in Arab villages, and the major parties committed themselves to improving the Arabs' lot. 23. A different version of the cultural argument, which Druze and Jews alike have relished, argues that the two peoples are natural allies as fellow minorities in the Muslim Middle East. But the assertion of a natural Jewish-Druze affinity is propagandistic rhetoric that has, since the founding of Israel, served the interests of both Israeli officials and Druze leaders. Druze religious texts are typically dismissive of other religions, and while particularly disparaging of Islam -the dominant regional religious tradition responsible for their oppression -they are hardly admiring of Jews. See Parsons (2001: 70-6); and Dana (2003: 43-51). 24. As this formulation suggests, we believe that network analysis provides a more useful set of tools for grasping such variation than do metaphors regarding, for example, the 'thickness' of the 'normative environment' (Barnett, 1998: 39). For an insightful application of network logic to imperial dynamics, see Nexon and Wright (forthcoming). 25. Consider also the examples discussed earlier. That audiences in the communist bloc outside the USSR might have questioned Soviet leadership or that Arab elites might have challenged Egypt's did matter, but primarily indirectly. Had the Soviet generals or Nasser anticipated that domestic audiences would respond to the prospective loss of regional or ideological authority with collective indifference, they would have perceived themselves to be operating with a far freer hand. That domestic audiences were sensitive to their country's standing in the relevant community of nations, and were believed to be prepared to act on that basis, was crucial in these cases. 26. The language of identity has not figured explicitly in this article, but it is our view that rhetoric and identity are inseparable. Identity is not the mental property of an individual, nor can collective identity be apprehended by aggregating the attitudes of the constituent individuals. Rather, identities are necessarily relational, and thus they are forged through the process of their articulation. Political communities are defined by a common rhetorical lexicon and coalesce around particular rhetorical configurations. 27. Our model cannot, however, capture the long-term processes through which new commonplaces enter the rhetorical space."
        ],
        "ground_truth_definitions": {
          "rhetorical coercion": {
            "definition": "an alternative means of conceptualizing the place of rhetoric in politics",
            "context": "Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4232b05abe413bf3294a560082b0b6bb72dc5a27",
        "sections": [
          "The impact of absolute material deprivationgrossly inadequate food, clothing, shelter, water, and sanitation-on health has been recognized for centuries (93); until relatively recently, discussions of socioeconomic influences on health in the United States focused primarily on links between poverty and health. Over the past 15-20 years, however, a new discourse on social factors and health-with wider relevance to the general population-has emerged in the United States, building on earlier work in Europe and Canada. Figure 1 illustrates the rapidly growing literature on the social (including economic) determinants of health (SDOH) in the United States and elsewhere. The concept is becoming far less marginal in the U.S. public health realm in general, not only in academia; the SDOH have received increasing attention from public health and nonprofit agencies (21,29,88,90,113). This growing momentum reflects a confluence of several phenomena: First, an accumulating critical mass of knowledge in social and biomedical sciences from the United ",
          "",
          "Increasing number of studies of social determinants of health in the United States and Europe. Source: Results of a PubMed search for \"social determinants.\" Literature related to health outcomes, indicators, or promotion was included; health-care literature was not included. States and other countries (1, 10, 123) has led to increased understanding of how social factors influence health and has enhanced the scientific credibility of relevant efforts. Notable recent initiatives include the World Health Organization (WHO) Commission on the Social Determinants of Health (122), the MacArthur Foundation Network on Socioeconomic Status and Health (111), and the Robert Wood Johnson Foundation (RWJF) Commission to Build a Healthier America (91). Incremental improvements in health with increasing social advantage have now been observed in the United States (14) as well as in Europe (69,92), indicating the relevance of SDOH for middle-class as well as the most disadvantaged Americans. Systematic efforts have disseminated this knowledge and made it compelling for broader U.S. audiences (17,91). An increasing focus among U.S. researchers, health agencies, and advocates on the concept of health equity has also contributed, encompassing the spectrum of causesincluding social determinants-of racial/ ethnic and other social disparities in health that raise concerns about justice (8, 15, 79,88,113). Finally, U.S. public health leaders and researchers have increasingly recognized that the dramatic health problems we face cannot be successfully addressed by medical care alone. The low U.S. ranking on key health indicators internationally has continued to fall as our medical expenditures skyrocket, far outstripping those of healthier nations.",
          "The term social determinant of health is often used to refer broadly to any nonmedical factors influencing health, including health-related knowledge, attitudes, beliefs, or behaviors (such as smoking). These factors, however, represent only the most downstream determinants in the causal pathways influencing health; they are shaped by more upstream determinants. To illustrate the upstream/downstream metaphor, consider people living near a river who become ill from drinking water contaminated by toxic chemicals originating from a factory located upstream. Although drinking the contaminated water is the most proximate or downstream cause of illness, the more fundamental (yet potentially less evident, given its temporal and physical distance from those affected) cause is the upstream dumping of chemicals. A downstream remedy might recommend that individuals buy filters to treat the contaminated water before drinking; because more affluent individuals could better afford the filters or bottled water, socioeconomic disparities in illness would be expected. The upstream solution, focused on the source of contamination, would end the factory's dumping. Although these concepts may make intuitive sense, the causal pathways linking upstream determinants with downstream determinants, and ultimately with health, are typically long and complex, often involving multiple intervening and potentially interacting factors along the way. This complexity generally makes it easier to studyand address-downstream determinants, at the risk of failing to address fundamental causes. This article focuses on the more upstream social determinants of health-the factors that play a more fundamental causal role and represent the most important opportunities for improving health and reducing health disparities. Figure 2 illustrates the conceptual framework for the RWJF Commission's work. Although the relationships are more complex, this simplified schema highlights several important concepts. First, it shows that healthrelated behaviors and receipt of recommended medical care (key downstream determinants of an individual's health) do not occur in a vacuum. Rather, these factors are shaped by more upstream determinants related to the living and working conditions that can influence health both directly (e.g., through toxic exposures or stressful experiences) and indirectly (by shaping",
          "",
          "Living and working conditions in homes and communities",
          "",
          "",
          "What influences health? Upstream and downstream determinants.",
          "): years or level of completed schooling, which does not reflect the quality of education the health-related choices that individuals have and make for themselves and their families). The diagram highlights how health is shaped not only by living and working conditions, but also by even more upstream determinants that reflect the economic and social resources and opportunities that influence an individual's access to health-promoting living and working conditions and to healthy choices.",
          "Evidence from decades of research examining associations between key social factorsprimarily educational attainment and income in the United States and occupational grade (ranking) in Europe-and health outcomes throughout the life course overwhelmingly links greater social disadvantage with poorer health (1, 10, 27,46,66,67). The striking and pervasive-albeit not universal-patterns are informative. Researchers have observed stepwise socioeconomic gradients in Europe, particularly the United Kingdom, for 30 years (69, 92) and more recently in the United States (14,73,82). As seen in Figure 3, using U.S. data, overall and in multiple racial/ethnic groups, the improvements in health with increasing social advantage-measured here by income-generally follow a stepwise, incremental gradient pattern: Although the most disadvantaged-those with belowpoverty-level incomes or without high-school completion-typically experience the worst health, even those with intermediate income or education levels appear less healthy than the most affluent/educated (14). The social gradients in health provide clues to understanding the SDOH. Although other research is needed to clarify the underlying pathways, the dose-response relationship suggested by the gradient patterns supports the biological plausibility of a fundamental causal role for one or more upstream SDOH. Gradients by income, education, or occupational grade could reflect relatively direct health benefits of having more economic resources (e.g., healthier nutrition, housing, or neighborhood conditions, or less stress due to more resources to cope with daily challenges), unmeasured socioeconomic factors, and/or associated psychosocial/ behavioral factors, such as health-related behaviors (109), self-perceived social status (121), or perceived control (68). Reverse causation as an alternative explanation is discussed below.  ",
          "Following the framework depicted in Figure 2, we briefly review current knowledge of how several important upstream social factors influence health.",
          "Neighborhoods can influence health through their physical characteristics, such as air and water quality and proximity to facilities that produce or store hazardous substances; exposures to lead paint, mold, dust, or pest infestation in housing; access to nutritious foods and safe places to exercise; or risk of pedestrian accidents (6, 23,48,49,51,77,97). The availability and quality of neighborhood servicesincluding schools, transportation, medical care, and employment resources-can also influence health, e.g., by shaping residents' opportunities to earn a living (43,83,117). Neighborhoods' physical and service characteristics can create and reinforce socioeconomic and racial/ethnic disparities in health. Health is also shaped by social relationships. For example, neighborhoods where residents express mutual trust and are willing to intervene for the public good have been linked with lower homicide rates (76, 98); conversely, less closely knit neighborhoods and more social disorder have been related to anxiety and depression (32,84,94). Many-but not all-studies have found that neighborhood features are associated with health even after considering residents' individual-level characteristics (37). Surprisingly, some researchers-albeit not manyhave found poorer health among disadvantaged individuals living in relatively advantaged neighborhoods (85,89,120), possibly because of adverse psychological effects of feeling worse off than one's neighbors and/or stronger social ties or reduced exposure to discrimination associated with a greater geographic concentration of one's own group (119). Working conditions and health. The physical aspects of work-the traditional domain of occupational health and safety-represent an obvious pathway through which work influences health. For example, jobs requiring repetitive movements and/or high physical workload put workers at higher risk for musculoskeletal injuries and disorders (81), whereas physically inactive workers in sedentary jobs are at increased risk of obesity and chronic diseases such as diabetes and heart disease (115); physical conditions in the workplace such as inadequate ventilation, high noise levels, and hazardous chemical exposures can also harm health. Psychosocial aspects of work represent another pathway to health. For example, working overtime has been associated with injury, illness, and mortality (20). Workers in jobs characterized by high demands coupled with low control or by perceived imbalance of efforts and rewards are at higher risk of poor health (34,56); control at work may be a major contributor to socioeconomic differences in health among employed persons (56,68). Social support at work has also been linked with health (104-107); environments facilitating mutual support among coworkers may buffer against physical and mental health stressors (60). Work-related opportunities and resources can also influence health. Employment-related earnings represent most Americans' primary economic resource, shaping health-related decisions made for themselves and their families; work-related benefits-including medical insurance, paid leave, schedule flexibility, workplace wellness programs, child-and elder-care resources and retirement benefits-could also be important. Well-paying jobs are more likely to provide benefits, greater financial security, and ability to afford healthier living conditions. In contrast, the working poor-estimated at 7.4 million U.S. workers in 2006 (112)-generally do not earn enough to cover basic necessities and are less likely to have health-related benefits (27,54). Different pathways linking work and health may interact to exacerbate social disparities in health: Socially disadvantaged groups Multiple pathways linking education to health. are more likely to have health-harming physical and psychosocial working conditions, along with disadvantaged living conditions associated with lower pay (38). Education and health. Figure 4 depicts three interrelated pathways through which educational attainment (completed schooling) is linked with health. It is widely recognized that education can lead to improved health by increasing health knowledge and healthy behaviors. This may be explained in part by literacy, allowing more-educated individuals to make better-informed, healthrelated decisions-including about receipt and management of medical care-for themselves and their families (36,99). Greater educational attainment has been associated with health-promoting behaviors (3) and earlier adoption of health-related recommendations (31). Education also plays an important role in health by shaping employment opportunities, which are major determinants of economic resources. More-educated individuals experience lower rates of unemployment, which is strongly associated with worse health and higher mortality (4); they are more likely to have jobs with healthier physical and psychosocial working conditions, better health-related benefits (44), and higher compensation (30) (which determines affordability of health-promoting living conditions). Education may also affect health by influencing social and psychological factors. More education has been associated with greater perceived personal control (74), which has frequently been linked with better health and health-related behaviors (63,74,75). Greater educational attainment is generally associated with higher relative social standing; subjective social status (an individual's perception of his or her ranking in a social hierarchy) may predict health even after controlling for more objective indicators of social status (35). More education also has been linked with increased social support (72), which is associated with better physical and mental health (5); social support may buffer the health-damaging effects of stress (110), influence health-related behaviors (24), and if one's social networks are socially advantaged, enhance access to employment, housing, and other opportunities and resources that can influence health (19). The role of educational quality-e.g., the employment opportunities, prestige, social networks, and other advantages accompanying a degree from an elite institution-is rarely considered in health studies. Educational attainment thus can underestimate health-related differences related to education (75,95). Income, wealth, and health. Economic resources reflect access to material goods and services, including income (monetary earnings during a specified time period) and wealth (accumulated material assets, such as the value of one's home, household possessions, vehicles and other property, bank accounts, and investments). Theoretically, wealth may better reflect economic resources overall, but it is more difficult to measure than income and hence less frequently measured in health studies. Among studies that have included both, many (but not all) have found links between wealth and health after considering income (87). Racial/ethnic differences in income markedly underestimate differences in wealth (13). Reverse causation (income loss due to poor health) occurs but does not fully account for the observed associations of income/wealth and health (58,78). Many longitudinal studies show that economic resources predict health or its proximate determinants, even after adjustment for education (2, 33, 52) [although education is a stronger predictor for other outcomes (52) and both are likely to matter (13, 58)]. Health effects of increasing income have been observed in randomized and natural experiments (58). Several researchers have observed health effects of income/wealth even after adjusting for many other relevant factors (33,58,62). Particularly when other socioeconomic factors are inadequately measured, however, observed associations between income/wealth and health may reflect effects of other socioeconomically linked factors such as educational attainment and quality, childhood socioeconomic circumstances, neighborhood characteristics, physical and psychosocial working conditions, and subjective social status. The health effects of low economic resources may be ameliorated by access to other resources and opportunities; for example, some relatively low-income countries/states (e.g., Cuba, Costa Rica, and Kerala, India) have favorable health indicators that may be explained by long-standing societal investments in education, social safety nets, and/or prevention-oriented medical care (41). Income inequality (measured at an aggregate level) has often been linked with health (116), although a causal link is debated (65,116). Income inequality could affect health by eroding social cohesion (59). The link could also be explained by other factors strongly associated with both income inequality and health, such as lack of social solidarity, which could be both a cause and an effect of income inequality. Race, racism, and health. In the United States and many other societies, race or ethnic group is another important social factor that influences health, primarily because of racism. Racism refers not only to overt, intentionally discriminatory actions and attitudes, but also to deep-seated societal structures that-even without intent to discriminate-systematically constrain some individuals' opportunities and resources on the basis of their race or ethnic group. Racial residential segregation is a key mechanism through which racism produces and perpetuates social disadvantage (22,117). Blacks and Latinos are more likely to reside in disadvantaged neighborhoods with inadequately resourced schools and hence to have lower educational attainment and quality Racial discrimination, racism: includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate (96), with resultant health effects through pathways discussed above. Racism may also affect health more directly through pathways involving stress; chronic stress related to experiences of racial/ethnic bias, including relatively subtle experiences arising even without consciously prejudicial intent, may contribute to racial/ethnic disparities in health, regardless of one's neighborhood, income, or education (80,118). More education or income may paradoxically expose blacks or Latinos to more discrimination because of more contact with (non-Latino) whites. Race-health links could also be shaped by perceptions of how one's race-and its associations with social influence, prestige, and acceptance-affects one's relative place in social hierarchies. Associations between discrimination and health similar to those observed in the United States are being found in other countries (118). The pervasive role of stress. Coping with daily challenges can be particularly stressful when one's financial and social resources are limited. Recent evidence implicates chronic stress in the causal pathways linking multiple upstream social determinants with health, through neuroendocrine, inflammatory, immune, and/or vascular mechanisms (71,108). Stressful experiences-such as those associated with social disadvantage, including economic hardship (12, 40) and racial discrimination (118)-may trigger the release of cortisol, cytokines, and other substances that can damage immune defenses, vital organs, and physiologic systems (71,101). This mechanism can lead to more rapid onset or progression of chronic illnesses, including cardiovascular disease (108), and the bodily wear and tear associated with chronic stress may accelerate aging (70,102,103). The accumulated strain from trying, with inadequate resources, to cope with daily challenges may, over time, lead to more physiological damage than would a single dramatically stressful event (70). A recent collection of papers summarizes current knowledge of pathways and biological mechanisms likely to be involved in the health effects of stress and other psychosocial factorsincluding perceived control, subjective social status, and social support (1).",
          "The importance of early childhood experiences. Among the strongest bodies of SDOH evidence is work considering adverse health effects of early childhood experiences associated with family social disadvantage. Many studies have shown that early experiences affect children's cognitive, behavioral, and physical development (7, 25, 53, 55), which predicts health; developmental differences have been associated with socioeconomically linked differences in children's home environments, including differences in stimulation from parents/caregivers (7, 39,50,114,123). Biological changes due to adverse socioeconomic conditions in infancy and toddler years appear to become \"embedded\" in children's bodies, determining their developmental capacity (53). Several longitudinal studies following children from early childhood through young adulthood have linked childhood developmental outcomes with subsequent educational attainment (18,47,100), which is strongly associated with adult health (discussed above). Substantial evidence indicates that pathways initiated by childhood adversity can be interrupted. Studies show that high-quality early childhood development interventionsincluding center-based programs to nurture and stimulate children and to support and educate parents-greatly ameliorate the effects of social disadvantage on children's cognitive, emotional/behavioral, and physical development (57); the first five years of life appear to be most crucial (55), although opportunities for intervention continue throughout childhood (55) and adolescence (42). over lifetimes and across generations (9, 26,40,45,46,53,55,64,66,102,108,124). As illustrated in Figure 5, developed for the RWJF Commission, upstream social factors influence health at each life stage, with accumulating social advantage/disadvantage and health advantage/disadvantage over time. Children of socially disadvantaged parents are less healthy and have more limited educational opportunities, both of which diminish their chances for good health and social advantage in adulthood. Emerging research on geneenvironment interactions suggests that the intergenerational transmission of social advantage and health may be partially explained by epigenetic changes in gene expression, which in turn are passed on to subsequent generations (61).",
          "",
          "",
          "A large body of evidence from observational research strongly and repeatedly links multiple upstream social (including economic) factors with a wide array of health outcomes, and understanding-albeit incomplete-of underlying pathways and biological mechanisms has been growing. With notable exceptions, however, we know little about effective ways to address social factors to improve health and reduce health disparities-about when, where, and how to intervene. The gaps in knowledge reflect several challenges. More often than not, the relationships between upstream social factors and health are complex and play out over long periods of time, involving multiple intermediate outcomes subject to effect modification by characteristics of people and settings along the causal chain. This complexity makes it difficult to learn about the specific pathways through which upstream social factors shape health and to identify priorities for intervention. Addressing the knowledge gaps is also complicated by our limited ability to measure upstream social factors. Current measures do not fully capture-or tease out the distinct effects of-relevant aspects of income, wealth, education, or occupational rank. For example, the observed effects of race/ethnicity on adult health after adjustment for available socioeconomic measures suggest a potential role for unmeasured social influences (13)e.g., childhood circumstances, neighborhood characteristics, accumulated wealth, racial discrimination. Development of better measures of these influences is in its infancy (13,37,80,118). Research funding is also an issue. Most U.S. research funding supports studies of single diseases rather than causal or contributory factors with effects that manifest across multiple diseases, putting SDOH research at a disadvantage. The health effects of upstream social factors-or interventions to address themmay not manifest for decades or generations; longitudinal studies are expensive and access to longitudinal databases is particularly limited in the United States (9). Conducting randomized trials, the gold standard for establishing effectiveness in health sciences, is particularly challenging for upstream interventions.",
          "Investment at this time would be particularly strategic in several areas. Research to improve the measurement of social factors is an important requisite for effective efforts in all of the following areas. Descriptive studies and monitoring. Ongoing descriptive research is needed to monitor changes over time both in the distributions of key upstream social factors (e.g., income, wealth, and education) across groups defined by race/ethnicity, geography, and gender, and in their associations with health outcomes in specific populations and settings. Interpreted in light of relevant policies, these findings can indicate the extent to which social and health disadvantage affects different groups and gauge progress toward improving health and reducing health disparities. Longitudinal research. We need more life-course research, including longitudinal studies to build public-use databases with comprehensive information on both social factors and health, collected over time frames long enough-ideally multiple generations-for health consequences of early childhood experiences to manifest. A more reasonable balance is needed between investments in studying adult disease and examining children's trajectories of health and social advantage across the life course. Connecting the dots: linking knowledge to elucidate pathways and assess interventions. Even robust longitudinal data are unlikely to provide sufficient information for tracing the effects of an upstream determinant (A) through relevant pathways to its ultimate health outcomes (Z), particularly if exposure to A occurs in childhood and outcome Z occurs much later. Attempting to document and quantify the effects of A on Z in a single study represents an important obstacle to understanding how social factors influence health-and how to intervene. Considering the potential for effect modification by characteristics of people and contexts at each step of multiple complex causal pathways, the consistency of existing findings linking upstream social determinants with distal health outcomes seems remarkable. To strengthen our understanding of how upstream social factors shape health, we need to connect the dots by building the knowledge base incrementally through linking a series of distinct studies (perhaps spanning multiple disciplines) that examine specific segments of the pathways connecting A to Z. For example, one study could test the effects of an upstream determinant on an intermediate outcome, which then could be the independent variable in subsequent studies of increasingly downstream intermediate outcomes; no single study would be expected to span all steps from A to Z. Once the links in the causal chain are documented, a similar incremental approach could be applied to study the effectiveness of interventions, e.g., testing the effects of an upstream intervention on an intermediate outcome with established links to health. This approach to advancing knowledge is not new: Medicine and public health often rely on evidence from studies of intermediate outcomes (e.g., obesity) with demonstrated links to other outcomes (e.g., diabetes or cardiovascular disease) (11). Although not definitive, the knowledge gained from connecting the dots can be compelling when confirmed in multiple studies; furthermore, policy makers must recognize that the limited generalizability of findings from randomized experiments introduces uncertainty as well (11).",
          "We need research to inform translation of existing knowledge about the SDOH into effective and efficient policies. Often, the rate-limiting step may not be insufficient knowledge of pathways but rather lack of solid evidence about what, specifically and concretely, works best in different settings to reduce social inequalities in health. For example, although we have convincing evidence that educational quality and attainment powerfully influence health through multiple pathways, lack of consensus about interventions is often invoked to justify inaction. Knowledge of pathways can point to promising or at least plausible approaches but generally cannot indicate which actions will be effective and efficient under different conditions; that knowledge can come only from well-designed intervention research, including both randomized experiments (when possible and appropriate) and nonrandomized studies with rigorous attention to comparability and bias. Intervention research often seeks to identify the magic bullet that will yield results on its own, a stand-alone intervention with independent effects after adjusting for other factors. This notion may be reasonable when considering surgery, but the complex pathways linking social disadvantage to health suggest that seeking a single magic bullet is unrealistic. Interventions with individuals may require simultaneous efforts with families and communities. Recognizing the expense and methodologic challenges, we need multifaceted approaches that operate simultaneously across domains to interrupt damaging (and activate favorable) pathways at multiple points at which the underlying differences in social advantage and the consequent health inequalities are produced, exacerbated, and perpetuated. Other issues must also be addressed. Research funding must be expanded beyond a focus on single diseases and/or biomedical factors exclusively. The time frame for evaluating program or policy effectiveness should be extended (11). Researchers must be trained in the concepts, measures, and methods needed both to study SDOH and their interactions with biomedical factors (e.g., gene-environment interactions) and to consider social factors in clinical and laboratory studies focused on other questions. Political barriers to translating knowledge to action. The field of SDOH is coming of age in many ways, with respect to increased attention within and beyond academia; documentation of strong and pervasive links between social and economic factors and health; and the accumulation of knowledge of pathways and biological mechanisms that provide a scientific foundation for appreciating the role of social factors in health. Although associations between social factors and health are no longer in question, we have much to learn, both about the underlying processes linking upstream social determinants and most health outcomes and about effective ways to intervene. Lack of evidence, however, is not always the major barrier to action. Often, the chief obstacle is lack of political will; particularly in the United States, our deeply embedded culture of individualism can impede actions that require a sense of social solidarity. For example, as noted in an Institute of Medicine report, \"whether early childhood programs can make a difference has been asked and answered in the affirmative innumerable times\"; the remaining questions are about the most effective and efficient interventions (55). Even after major business groups have advocated universal highquality preschool as essential for achieving a productive-i.e., healthy and educated-future workforce (16,28,86), this goal remains elusive. Descriptive, explanatory, and interventional research can play a supportive role in building consensus about the need for action by increasing public and policy-maker awareness of unacceptable conditions such as racial and socioeconomic disparities in health; by making the links between social factors and health meaningful and plausible to the public and policy makers; and by suggesting, testing, and helping to estimate the costs of promising science-based approaches. Information about the pathways and mechanisms through which social advantage influences health can provide an important counterweight to victim-blaming, which too often impedes policies focused on upstream social and economic factors. Based on current understanding of the role of stress in the links between multiple social factors and health outcomes, studies of pathways involving stressful circumstances and physiological responses to stress may yield knowledge needed to help achieve consensus for action. Research on the SDOH can provide practical guidance for policies and add meaning and credibility to ethical and economic arguments for the need to act, not only to ameliorate the adverse health consequences but also to reduce social disadvantage itself.",
          "1. A critical mass of knowledge related to the social determinants of health has accumulated in the United States and elsewhere, documenting associations, exploring pathways and biological mechanisms, and providing a previously unavailable scientific foundation for appreciating the fundamental role of social factors in health. 2. The questions are no longer about whether social factors are important influences on health, but rather about how social factors operate and how we can most effectively intervene to activate health-promoting pathways and interrupt health-damaging ones. 3. Too little attention has been given to the upstream social determinants of health, such as economic resources, education, and racial discrimination. Although most research has focused on the more easily studied and addressed downstream factors, these upstream determinants represent the fundamental causes in pathways that influence downstream factors and ultimately lead to health effects. 4. One barrier to expanding our understanding of how upstream social determinants influence health is a widespread expectation that a single research study can encompass an entire pathway from upstream factor to downstream health effects. Such studies are unlikely to be achieved, however, given the complex causal chains and long time periods involved. Rather, we should focus on advancing knowledge of pathways incrementally by linking results from studies of specific pathway segments. 5. More research on pathways and biological mechanisms is needed, but this will not necessarily yield sufficient information for identifying the most effective and efficient interventions. Well-designed studies of interventions are essential. 6. On the basis of experience and awareness of the complexity of pathways, isolated interventions focused on single discrete (upstream or downstream) social factors may not be effective. The challenge is to design and adequately study multidimensional interventions that address multiple factors simultaneously. 7. Lack of evidence is not always the major barrier to action on the social determinants of health. Particularly in the United States, the crucial obstacle is often lack of political will. A strategic research agenda on the social determinants of health should also address the factors that can enhance or impede political will to translate knowledge into effective action.",
          "The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review. ",
          ""
        ],
        "ground_truth_definitions": {
          "racial discrimination/racism": {
            "definition": "includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate",
            "context": "Racial discrimination, racism: includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "To elucidate the psychological motivations underlying fake news sharing, we also need to understand what news content people share. Specifically, to adjudicate between the three theories, the analyses differentiate news stories along two dimensions: (1) Whether a news story originates from a fake news source (i.e., a source containing registered debunked stories) or a real news source (i.e., a generally credible source), and (2) whether a news story comes from a *pro*-Republican or a *pro*-Democratic source. The three theories make very different predictions about which dimension matters for people's sharing preferences. The ignorance theory predicts that the veracity dimension matters most and that people with high cognitive abilities will be less willing to share fake news stories and more likely to share real news stories, irrespective of the political slant of the stories. The latter two theories make the opposite prediction: they posit that people place less value on whether stories are true or false as long as the stories are politically useful. Specifically, the disruption theory implies that disruptive individuals should selectively share as many fake news as possible to sow confusion and uncertainty. For the polarization theory, however, the content-related predictions are more complex, and so it is these predictions we unfold now. The polarization theory holds that fake news sharing is not an accident caused by ignorance, it is partisan business as usual. The media landscape in the US is politically fragmented, with some news sources catering to Republicans while others offer content that fit Democrats' taste (Groeling 2013). From the perspective of the polarization theory, fake news is an extreme version of traditional partisan news that polarized partisans may find attractive due to its rarity and extremity. Accordingly, partisans motivated to selectively share hyper-partisan content will tend to view the online media landscape of news sources as a one-dimensional continuum reflecting the political usefulness of the stories they provide. For a polarized Republican social media user, the order of the usefulness of news sources goes from *pro*-Republican fake news sources, to *pro*-Republican real news sources over centrist sources to *pro*-Democratic real news sources, and finally, to *pro*-Democratic fake news. For Democrats, the order reverses. This insight, we contend, leads to three observable implications about the content of shared news. First, partisan social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. Accordingly, we expect that sharing fake news favoring one party over the other should be best predicted by sharing real news strongly favoring the same party. Conversely, sharing fake news favoring one party should correlate negatively with sharing fake news favoring the other party. Second, if news sources differ systematically in how strongly they cater to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Specifically, fake news should show the strongest partisan sorting in terms of who wants to share from these sources. Third, and finally, the negativity of news source content should change as we move across the partisan news continuum. If partisans selectively share stories with content that fits their political tastes, the observable implication is that stories shared from news sources located towards the Democratic extreme on the partisan news continuum should portray Republican elites negatively while stories from the Republican extreme should cast Democratic elites in a negative light.",
        "reasoning": "The text discusses theories related to fake news sharing but does not provide explicit definitions of the terms like \"ignorance theory,\" \"disruption theory,\" or \"polarization theory.\" Instead, it explains their implications and predictions.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The section clearly defines racial discrimination as the unequal treatment of individuals based on their race or ethnicity.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "Our review provides a detailed exposition of various aspects of the field, including relevant concepts, the interplay of issues, generic system architectures, terminology extraction methods, related resources and metrics, and prospective research topics. We delve into the evolution of different approaches to terminology extraction, encompassing traditional linguistics-based and statistically-based methods, as well as more recent developments such as graph-based algorithms, machine learning, deep learning, and LMs. These methods represent different research directions within the terminology extraction domain. Traditional linguistics-based and statistically-based methods, such as PoS tagging, KPE, and TF analysis, have laid a robust foundation for terminology extraction. However, they exhibit certain limitations in handling polysemy, domain adaption, and semantic relatedness. Machine learning-based approaches rely on feature engineering and classical supervised learning algorithms. They have demonstrated favorable results in terminology extraction tasks, albeit typically requiring substantial manual effort and domain knowledge. Deep learning methods, on the other hand, leverage end-to-end training of neural network models to automatically learn higher-level feature representations but often consume large-scale annotated data. Embedding techniques map text into lower-dimensional vector spaces, encoding terminology semantically and facilitating the capture of semantic relationships among terminologies. Additionally, graphbased methods construct term-concept networks, offering enhanced representations of intricate relationships among terminologies. However, one of the most remarkable developments is the rise of LMs, particularly massive LMs. Models like BERT and GPT-4 have achieved tremendous success in the field of NLP and have profoundly impacted terminology extraction. These models undergo self-training on vast amounts of textual data, endowing them with formidable language comprehension capabilities, enabling them to better understand the context and meanings of terminologies. Furthermore, their powerful representation enhances the accuracy of terminology extraction, especially when dealing with polysemy and domain-specific terminologies, as they excel in capturing linguistic nuances. In the future, as LLMs evolving and adapting to different domains, we can anticipate further improvements in terminology extraction methods. Additionally, researchers can explore how to better integrate various approaches to address the challenges in terminology extraction. Recently, many articles [247][248][249] have proposed methods that combine LLMs with knowledge graphs for knowledge extraction. The integration of LLMs with knowledge graphs can become a new trend in addressing terminology extraction tasks. LLMs can play a pivotal role in future terminology extraction research, providing us with more advanced solutions for terminology extraction, and advancing the fields of information retrieval and knowledge management. NER Named entity recognition NLP Natural language processing NPs Noun phrases PD Phrase-document PMI Pointwise mutual information PLMs Pre-trained language models PoS Part-of-Speech P Precision R Recall RIDF ResidualIDF RIPPER Repeated incremental pruning to produce error reduction RNN Recurrent neural networks SOTA State-of-the-art SVM Support vector machine TF Term frequency TF-IDF Term frequency-inverse document frequency",
        "reasoning": "The section does not contain an obvious definition of a term.",
        "is_definition": false
      },
      {
        "augmented": true,
        "section": "The baseline empirical specification is a standard difference-in-difference estimation. We compare towns where Fox News entered the cable market by the year 2000 with towns where Fox News was not available by the year 2000. We consider the impact of the entry of Fox News on the change in the Republican vote share between 1996 and 2000. This strategy exploits the timing of the entry of Fox News. By the November 1996 elections, Fox News had been launched in only a few markets, and, even in those markets, just one month before the elections. By the November 2000 elections, Fox News had an audience that was smaller, but nonetheless comparable to that of CNN. Our baseline specification is As in We first implement a simple difference-in-difference estimator and estimate (5) without controls (Γ 2000 = Γ 00-90 = 0 and Γ C = 0) (Column 1 in Table 4). On average, in our sample the vote share for Republicans increased by 3.47 percentage points (α = .0347) between the 1996 and the 2000 elections. Compared to this overall increase, towns with Fox News became (insignificantly) less Republican by two tenths of a percentage point ( βF = -.0026) relative to towns without Fox News. The standard error (.0037), however, is sufficiently large that we cannot rule out that the entry of Fox News increased the Republican vote share by half a percentage point. In Column 2, we add demographic controls, raising the R 2 of the regression from .0008 to .5199. The estimate for β F , βF = .0027, becomes positive, but is still insignificant. The change in vote share for Republicans is more positive in towns with fewer college graduates, fewer blacks and Hispanics, and higher unemployment in 2000. The change in vote share is also positively correlated with decreases in the number of blacks and decreases in the number of unemployed workers between 1990 and 2000. In Column 3, we add controls for cable size C k,2000 , rendering the Fox News coefficient positive and significant ( βF = .0078). Controlling for population, a higher number of potential cable subscribers is negatively correlated with the Republican vote share. Introducing control variables increases the point estimate of β F , suggesting that the unobservables bias the estimate of the Fox News effect downward. In the two benchmark specifications we include district fixed effects (Column 4) and county fixed effects (Column 5) in addition to the full set of controls. In these specifications the identification of β F depends on the comparison of neighboring towns with and without Fox News. These specifications control for unobserved trends in voting that are common to a geographic area, and that may be correlated with Fox News availability. In Column 4, the comparison of neighboring towns takes place within a congressional district, while in Column 5 it takes place within the finer geographical unit of the county. The key advantage of specifications with cable, demographic, and geographic controls is that, conditional on these variables, the availability of Fox News is idiosyncratic (Table 3). The estimate of the effect of Fox News is positive and significant in both cases, .0040 and .0069 respectively. In both specifications, the standard errors on the coefficient β F are halved compared to Column 1, indicating that the additional controls increase substantially the precision of the estimates. In the specifications that best control for heterogeneity, availability of Fox News increases the Republican vote share by 4 to 7 tenths of a percentage point, a sizeable and precisely estimated effect. In Columns 6 and 7, we replicate the results of Columns 4 and 5 after adding the change in Republican vote share between 1988 and 1992 as an additional control. Over this substantially smaller sample (3,722 observations), the effect of Fox News availability is less precisely estimated and somewhat smaller, but still significant in the specification with county fixed effects. The coefficient on 1988-1992 voting trends is close to zero with district fixed effects and significant but small with county fixed effects. Since previous voting trends are not substantial predictors of current voting trends, and since including them would lower the sample size substantially, we omit them in the remaining regressions. Robustness. In Table 5, we examine the robustness of these results to a number of alternative assumptions. Throughout, we use the full set of controls as well as fixed effects for congressional districts, as in Column 4 of Table 4 17 . First, we test for robustness to functional form assumptions. In Column 1, we regress the vote share in 2000, v R,Pres k,2000 , on the vote share in 1996, v R,Pres k,1996 , instead of taking their difference. The coefficient on the 1996 vote share is .9359, close enough to 1 to support the use of the difference v R,Pres k,2000v R,Pres k,1996 in the benchmark specifications 18 . In Column 2, we allow for a non-linear relationship between the 1996 and 2000 vote shares, and regress v R,Pres k,2000 on a quartic polynomial of v R,Pres k,1996 . In Column 3, we use as an alternative dependent variable the change in the log of the vote share: log(v R,Pres k,2000 )-log(v R,Pres k,1996 ). In all three specifications, the estimate for βF is positive and significant and very similar to the benchmark estimate with district fixed effects. In particular, the estimate of a .85 percent effect ( β = .0085) in Column 3 is consistent with the benchmark estimate (Table 4, Column 4) of .4 percentage points, given an average Republican vote share of around .5. In the next set of specifications, we consider alternative specifications which take into account the role of third parties, that is, Ross Perot in 1996 and, to a lesser extent, Ralph Nader in 2000. To the extent that towns with Fox News in 2000 may be more informed about third party candidates, neglecting third parties could lead to a bias in the estimates. Therefore, we consider as alternative dependent variables the all-party Republican vote share v 0R,Pres ) (Column 4) and the right-wing-party vote share ) (Column 5). Across these specifications, the estimate for βF is positive and significant and hovers around 4 tenths of a percentage point, as in the benchmark specification with district fixed effects. Next, we consider alternative specifications of the Fox News variable d F OX k,2000 . Exposure to Fox News is likely to be higher for towns in which Fox News is provided as part of the Basic cable package, rather than as part of the (more expensive) Expanded package. We add a dummy for towns with Fox News in the Basic package (Column 6). The estimated coefficient on this dummy is positive but insignificant. The lack of a significant difference likely reflects the near equivalence between availability in a Basic or Extended package: two thirds of Basic subscribers purchase the Extended package whenever available. While this specification accounts for differences in exposure to different packages, it does not account for variation across towns in subscription rates to cable. To account for this, we compute r k,t , the ratio of the number of Fox News subscribers to the number of potential subscribers. This ratio is zero for towns that do not offer Fox News. 19 The coefficient on this ratio (Column 7), .0058, is positive and marginally significant. The lower precision in this estimate may be due to measurement error in the subscriber data, which is updated less frequently. We also test the sensitivity of the results to alternative samples. To check the robustness to the weighting procedure, we run an unweighted regression restricted to the 3,115 towns with average turnout in 1996 and 2000 of at least 2000 votes (Column 8). To examine the potential impact of outliers, we trim the top and bottom 1 percent of the dependent variable (Column 9). In both of these specifications, the effect of Fox News is positive and significant. Finally, we adopt the optimal trimming approach of Crump et al. (2005) (Column 10). This approach acknowledges that treatment and control group often differ significantly on observables, making the interpretation of average treatment effects problematic. As a solution, it focuses the analysis on an optimal subsample in which treatment and control observations are more comparable. We implement this method by estimating a propensity score for the availability of Fox News based on the controls v R,Pres k,1996 , X k,2000 , X k,00-90 , and C k,2000 . We then eliminate the 5,079 observations with propensity score smaller than .1 or larger than .9, the cutoffs suggested by Crump et al. (2005). The trimming eliminates most towns with fewer than 20 cable channels, since Fox News is almost never included as one of these channels. To the extent that cable companies in these towns under-report their offering, including potentially Fox News, their exclusion eliminates a source of contamination of the control group. Over the optimally trimmed sample of 4,177 towns, the point estimate for βF is significant and larger than in the baseline specification, consistently with attenuation over the eliminated sample. We use this subsample in the rest of the paper as a specification check. In Appendix Table 2, we explore the role of weighting and of measurement error. The estimates of the Fox News effect are very similar if we weight the observations by voting-age population in 1996 (Columns 1 and 2) instead of by total votes cast. The estimates are .25 percentage points lower (.0014 and .0040) if we do not weight (Columns 3 and 4). The Fox News effect is still significant with county fixed effects but not with district fixed effects. The lower point estimates may be due to higher measurement error in very small towns or to heterogeneous treatment effects in smaller, more rural towns (see Table 6). We also present the results of another unweighted estimation procedure, nearest-neighbor matching (Abadie et  al., 2001). Each town with Fox News (treatment town) is matched to the non-Fox News town (control town) with the closest value of the controls. We match observations based on cable and Census controls (Column 5), and also on District indicator variables (Column 6). 20 The average treatment effect of Fox News (.0054 and .0042) is similar to our benchmark findings, significant, and larger than the OLS estimates without weighting (Columns 3 and 4). Different weighting procedures, therefore, have only a limited effect on the results. In Columns 7 and 8 we enlarge the baseline sample by including observations that are likely to be subject to greater measurement error. We include (as treated) 289 towns where Fox News is offered in parts but not all of the town, and 257 towns with likely voting data problems (see Section 4 for details). Consistently with increased measurement error in the Fox News variable, the estimates of the Fox News effect are lower. Finally, in Columns 9 and 10 we present a specification that is both unweighted and includes the observations with measurement error; in addition, we exclude the data from the states of Hi, Nd, Nj, and Wy. This specification is essentially the one that appeared in an older version of this paper. The failure to reject the null hypothesis stems from the inclusion of observations with greater measurement error, the equal weighting of towns of different sizes, and the smaller sample of States. Interaction effects. In Table 6, we examine how the Fox News effect interacts with town characteristics for the benchmark specifications with district and county fixed effects (Columns 1-2 and 5-6) and in the optimally trimmed sample (Columns 3-4 and 7-8). The impact of Fox News is larger in urban towns, although this effect is not significant in the optimally trimmed sample. This result may simply reflect the fact (documented below) that the Fox News effect is larger in more Democratic towns, which tend to be more urban. The Fox News effect is smaller in towns with more cable channels: an increase of 10 cable channels (.7 standard deviations) reduces the effect by .2 percentage points. When the Fox News message competes with a larger number of channels, its impact appears diminished (Mullainathan and Shleifer, 2005). The lower Fox News impact result could reflect exposure to more balanced reporting (though CNN and the network news are available in all towns in the sample) or merely lower audience rates for Fox News when more channels are available. We consider also the heterogeneity across geographic regions. The effect of Fox News is highest in the North East and in the West, and significantly lower in the Midwest. The effect is significantly lower also in the South in the benchmark specifications, but this pattern is not significant and even reverses in the optimally trimmed sample. These results should be taken with caution in light of the small number of observations in the South (725 towns) and West (689 towns). We also interact the results with the political orientation in the District. We split congressional districts into thirds by the 2000 Republican vote share. The Fox News effect is lower for the more Republican districts, significantly so with county fixed effects. The lower impact of Fox News in the Republican districts and states (the South) and in rural towns areas may be explained by the fact that in these towns most people already voted Republican, and therefore the share of the population at risk of being convinced was smaller. Magnitudes. Across the different specifications, the entry of Fox News into a cable market by the year 2000 had a significant effect on the Republican vote share in Presidential elections. The implied confidence intervals for the benchmark estimates (Columns 4 and 5 of Table 4) are (.0008, .0072) with district fixed effects and (.0041,.0097) with county fixed effects. The findings, therefore, are consistent with both a small (but positive) effect of Fox News and a fairly large effect, close to one percentage point. How large are these effects relative to shifts in vote share between 1996 and 2000? The average weighted change in vote share between 1996 and 2000 in our sample is 3.47 percentage points, with a standard deviation of 4.02. The estimated impact of Fox News is one tenth of a standard deviation with district fixed effects, and one sixth of a standard deviation with county fixed effects. The impact of Fox News is small, but not negligible. As a second measure, we estimate the number of votes that Fox News is likely to have shifted. We assume a treatment effect of Fox News of .54 percentage points, the midpoint of the benchmark estimates, and a diffusion of Fox News of 34 percent of the population, also for the 22 States for which we do not have data. The estimated impact of Fox News on the Republican vote share is then .34 * (.0054) = .0018, that is, .18 percentage points. Assuming that Fox News did not affect turnout substantially, Fox News shifted approximately 200,000 votes from the Democratic candidate to the Republican candidate. We also predict the number of votes shifted by Fox News in Florida, the pivotal State. In 2000, Fox News reached 32.8 percent of the Florida population. We assume that the Fox News effect on the 5,963,110 Florida votes cast is the same as in our sample. Under this assumption, the introduction of Fox News shifted .328 * (.0054) * 5, 963, 110 = 10, 561 votes, a number substantially larger than Bush's official margin of victory of 537 votes. The estimated impact of Fox News would be smaller, and potentially negative, under the alternative assumption that the effect in Florida is the same as in the Southern states (Table 6, Columns 5 through 8). 21 Overall, the impact of the entry of Fox News can be measured as about one tenth of a standard deviation of the shift toward Republicans between 1996 and 2000. While this estimate is moderately small, the entry of Fox News may still have contributed to the Bush victory in the unusually close 2000 election. Moreover, the impact may become larger over time as the Fox News audience and diffusion grows.",
        "reasoning": "The section provided is part of a statistical analysis discussing the impact of Fox News on the Republican vote share in U.S. towns. It describes the methodology, variables, and specifications used in the study, including the use of difference-in-difference estimation, fixed effects, and various control variables. The section does not contain a definition of any term but rather explains the analytical framework and variables used.",
        "is_definition": false
      },
      {
        "paper_id": "2025.acl-long.1513",
        "sections": [
          "As online news and social networks have radically altered the media landscape, understanding the dynamics of news propagation and the narratives that news convey is more crucial than ever. Online news outlets make it possible for recurring narratives to appear, often with very different phrasings, in multiple articles and propagate with very high velocity across audiences, languages, and countries. This is especially problematic with manipulative narratives, potentially containing strong biases, mis/disinformation, propaganda, or harmful content. Such a risk is even more pervasive when it is related to divisive issues and can have large societal implications, including destabilization, conflict, hate, or incitement to violence. Various definitions of narrative can be found in the literature, depending on the context and the goal of the analysis. In this paper, we define narrative as a recurring, repetitive (across and within articles), overt or implicit claim that presents and promotes a specific interpretation or viewpoint on an ongoing (and frequently dynamic) news topic. Narratives may be embedded in thousands of articles across multiple topics, themes, languages, and news genres (e.g., news reporting or opinion). They can manifest using very different vocabularies, media frames (Entman, 2007), and tones, which makes their automatic detection and extraction very challenging. Moreover, multiple narratives can be present in the same article, or even in the same paragraph or in the same sentence. An example is shown in Fig. 1, where the narrative \"Downplaying Climate Change\" occurs in two articles from very different domains: finance vs. auto enthusiasts. Thus, the effective automatic detection, extraction, and analysis of narratives in news articles is an important challenge and remains an open problem ( §2). Such capacity is crucial for analyzing the news landscape, identifying media bias, and detecting attempts to influence readers. It can also be used to warn and educate media consumers to improve their media literacy. Large Language Models (LLMs) have made the need for such capacity even more urgent, as they are vulnerable to (Xu et al., 2024) and prone to generating manipulative narratives (Vykopal et al., 2024) with modest effort. In this work, we focus on the problem of narrative detection, treating it as multi-label paragraphlevel classification problem ( §3). The biggest challenge is the lack of annotated datasets (Nunes et al., 2024), especially for long news articles (as opposed to micro-blogs or snippets). Existing narrative classification datasets for long articles ( §2) have been annotated at the document level and assign only a single label per annotation. Narratives, however, are often conveyed by small parts of the articles, and thus it is important to pinpoint those parts, which is why we model the problem as a paragraph-level classification. We consider two news domains of current relevance ( §3.1): climate change (CC) and the military conflict between Ukraine and Russia (URW). Unlike previous work on narrative classification, where the labels are flat and coarse (Li et al., 2023) ( §2), we developed a two-level taxonomy with coarse-and fine-grained narrative labels (hereafter narratives and sub-narratives) for each topic ( §3.2). We collected news articles in four languages-English, Bulgarian, Portuguese, and Russian-related to the two domains ( §4.1), and manually annotated them at the paragraph level with labels from the corresponding taxonomies ( §4.2). We report statistics about the resulting dataset ( §4.3) and discuss its quality ( §4.4). We make the dataset, dubbed PolyNarrative, publicly available, along with experimental results ( §5) and code of several strong baselines that assign narrative labels to news articles at the paragraph or the document level; in the latter case, we use the union of the paragraph-level ground-truth labels of each document as the correct prediction. Our contributions are as follows: • We propose a new two-level hierarchical taxonomy for narrative extraction. We further develop a novel datasetfoot_0 that uses this taxonomy with news article annotations from two levels (coarse-and fine-grained), at the paragraph level (previous work was document level), using multi-label annotation (previous work was mostly single-label) across two current domains (previous work was single-domain), and four languages (previous work was for a single language). • We provide a comprehensive overview of the data acquisition and annotation process, and highlight the challenges and measures taken to address them. • We perform evaluation using several strong multi-label classification methods, and draw conclusions that can benefit future work. 2 Related Work",
          "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023;Silvano et al., 2024) and capturing narrative components in the form of semantic relationships between entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012;Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes. In an effort to synthesize various formulations of the concept, Dennison (2021) proposed a refined definition of narratives as \"selective depictions of reality across at least two points that can include one or more causal claims, and are generally generalizable and can be applied to multiple situations, as opposed to specific stories\". This formulation, although well structured from a theoretical perspective, was not sufficient to properly operationalise the concept for our intention to provide a concrete taxonomy. Denmark Punishing Farmers for Cow 'Emissions' to 'Fight Global Warming' Denmark has become the first country to force farmers to comply with the goals of the World Economic Forum's (WEF) \"Net Zero\" agenda, to supposedly \"fight global warming.\" ...",
          "Although carbon dioxide is typically blamed for causing \"climate change,\" globalists claim that methane traps about 87 times more heat on a 20-year timescale but top scientists have debunked these claims as a hoax. A recent peer-reviewed study provided conclusive scientific evidence proving that carbon dioxide (CO2) emissions in Earth's atmosphere cannot cause \"global warming.\" Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change However, not only did they find that higher levels of CO2 made no difference, but they also proved that it simply isn't possible for increases in carbon dioxide to cause temperatures to rise. Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change Nevertheless, unelected globalists at the United Nations Environment Program claim that livestock accounts for about 32% of \"human-caused methane emissions.\" ... Thus, our definition of narrative ( §1), while informed by the above general formulation, is based on previous efforts to create taxonomies and datasets for practical use (Kotseva et al., 2023;Li et al., 2023;Coan et al., 2021).",
          "",
          "To apply modern NLP methods, as highlighted by Santana et al. (2023), extraction of narratives has been approached as hierarchical event detection (Glavaš et al., 2014), as frame bias detection (Zaghouani et al., 2024), as spatiotemporal entity relationship detection (Nunes et al., 2024), and as multi-class classification (Li et al., 2023). Li et al. (2023) presented a social media dataset based on short texts from microblogs and trained a BERT-based narrative classifier detecting COVID anti-vaccine claims from and using a flat taxonomy, while Weigand et al. (2022) focused on conspiracy narrative detection by applying a binary classifier to detect the presence of conspiracy theories and applied a topic modeling segmentation without a pre-defined taxonomy. Coan et al. (2021) released an English-only dataset using a two-level taxonomy on climate change denial, annotated at the snippet level from website fragments and using a single label per snippet, while Kotseva et al. (2023) provided a multi-lingual dataset annotated at document-level using a single label per document. While datasets with individual characteristics (long-articles, hierarchical taxonomies, multilinguality) have been released before, to the best of our knowledge, there has been no other dataset that provides a multi-lingual corpus, annotated at paragraph level using a hierarchical taxonomy in a multi-label multi-class fashion.",
          "Several narrative taxonomies have been created to classify recurring argumentation in online news. Kotseva et al. (2023) created a three-level narrative taxonomy on COVID-19 and used it to classify and to analyze trends over time. Hughes et al. (2021) presented a taxonomy of common anti-vax narratives, organized on several common tropes and rhetorical strategies. Coan et al. (2021) presented a two-level taxonomy for common cases of climate change denial in short snippets. Amanatullah et al. (2023) presented a flat taxonomy of common pro-Russian narratives found in the alleged pro-Kremlin influence campaigns related to the war in Ukraine. In this work, we used the last two taxonomies as a starting point for the two domains of interest.",
          "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, \"CO 2 is beneficial\" is a sub-Narrative of the \"Climate Change is beneficial\" Narrative of the Climate Change (CC) taxonomy. A (sub-)Narrative can appear in a variety of contexts and topics, and can manifest using subtle and indirect argumentation or framing, and thus its detection can be very challenging. For example, the sub-Narrative \"Western sanctions will backfire\" could be expressed throughout a financial reporting article, or in a small paragraph at the end of an opinion article commenting on military matters. Thus, in order to better tackle this challenge, our approach focuses on highlighting specific segment(s) of each news article that expresses each (sub-)Narrative. In this sense, we consider a paragraph as a minimal segment. If a (sub-)Narrative is expressed in multiple paragraphs of an article (or the entire article), all the corresponding paragraphs need to be annotated accordingly. Moreover, one paragraph can express more than one Narrative and more than one level of Narratives (Narratives and sub-Narratives). In these scenarios, the paragraph needs to be annotated with all the applicable (sub-)Narratives. Given this formulation, the task becomes a hierarchical multi-label multi-class classification problem at the paragraph level.",
          "We selected two domains (topics) that currently receive extensive news coverage, are presented through several different perspectives, and are highly susceptible to manipulation through the creation of artificial narratives. The first domain is Climate Change (CC) and the second one is the Ukraine-Russia War (URW). In both cases, we have numerous accounts of repeated argumentation in the form of narratives. We are particularly interested in potentially manipulative narratives, and this is reflected in the choice of the (sub-)Narratives of our taxonomies. However, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily a case of mis/disinformation. This is important to highlight, as mixing legitimate with mis/disinformative claims is a frequent manipulative practice (Goel et al., 2023).",
          "To create the two taxonomies, we started from existing narrative taxonomies for URW (Amanatullah et al., 2023) and CC (Coan et al., 2021). We asked media analysts to suggest modifications based on their media monitoring experience. These modifications included additions (new (sub-)Narratives), elaborations (splitting existing (sub-)Narratives into multiple different ones), modifications (refining definitions and names) and merging (grouping into common (sub-)Narratives). We asked, when possible, to phrase each (sub-)Narrative as a con-crete claim (e.g., \"Renewable energy is dangerous\" or \"Ukraine is a puppet of the West\"). When the argumentation regarding a (sub-)Narrative was very diverse or fragmented, we gave a more general descriptive label (e.g., \"Criticism of Institutions\" or \"Speculating war outcomes\"). We ended up with 38 and 36 sub-Narratives, grouped into 11 and 10 Narratives for URW and CC, respectively. The top level of the two taxonomies (Narratives) is shown in Figure 3. The full taxonomies, including sub-Narratives and definitions, can be found in Appendix A. We note that the two taxonomies are not complete; in fact, they are not intended to be complete, rather they are representative of the practical experience of the media analysts, and different valid formulations are possible. We stress again that although our taxonomies focus on potentially manipulative narratives, we do not assume that every claim corresponding to a (sub-)Narrative of our taxonomies is necessarily mis/disinformation or indicates strong bias. UKRAINE-RUSSIA WAR (URW) Blaming the war on others rather than the invader Discrediting Ukraine Russia is the Victim Praise of Russia Overpraising the West Speculating war outcomes Discrediting the West, Diplomacy Negative Consequences for the West Distrust towards Media Amplifying war-related fears Hidden plots by secret schemes of powerful groups CLIMATE CHANGE (CC) Criticism of climate policies Criticism of institutions and authorities Climate change is beneficial Downplaying climate change Questioning the measurements and science Criticism of climate movement Controversy about green technologies Hidden plots by secret schemes of powerful groups Amplifying Climate Fears Green policies are geopolitical instruments ",
          "",
          "To obtain news articles for annotation, we used an in-house online news scraping and indexing tool to retrieve articles on the two selected domains in the four target languages. 2 We considered both mainstream and \"alternative\" news sources (that is, sources known to repeatedly publish content with misinformation). We also developed ad-hoc scoring methods, outlined below, to heuristically evaluate the relevance of each article to the corresponding domain. More specifically: 1. We created keyword-based queries for the two domains in all the languages, and used them to retrieve a large number of articles from the news article index of our in-house tool. For the URW domain, we included documents from 2021 to 2024. For the CC domain, the documents ranged from 2015 to 2024. 2. To assess the relevance of the resulting articles, we formulated another set of key phrases, corresponding to the (sub-)Narratives of the taxonomies (e.g., \"Ukraine is corrupt\", \"Science is debunked\") or other aspects we wished to emphasize or de-emphasize. We then used bart-large-mnlifoot_2 to perform zero-shot classification with the title and the first 300 characters of each article, resulting in a classification score per article and key phrase. 3. We used an XLM-RoBERTa-basedfoot_3 multilabel classifier, trained on the Persuasion Techniques dataset (Piskorski et al., 2023a,b), and used the approach of Nikolaidis et al. (2024) to produce Persuasiveness Score metrics per article. For the CC domain, we also used a climate change denial classifier, based on the work of Coan et al. (2021), to further filter and reduce the number of articles to review. 4. We used a linear combination of the resulting scores (relevance score per key phrase, Persuasiveness Score metrics and the score from the climate change denial classifier) to automatically rank articles from most to least likely to contain relevant (sub-)Narratives. A member of the team who was familiar with the news landscape of the language, then manually inspected the resulting set of articles, focusing on articles that seemed relevant to the defined taxonomies. Since manual inspection was timeconsuming, we used the different scores computed above to reorder the articles and focus mainly on highly ranked articles.",
          "In order to clarify the annotation process, including the meaning of key concepts such as Narrative, sub-Narrative, the precise meaning of each label of the taxonomies, and good practices, we created a document with detailed annotation guidelines. This included specific definitions and examples for each label, together with a detailed description of the annotation process to be followed. The definitions and examples of the taxonomies can be found in Appendix A. As we wanted to produce a multilingual dataset with four languages, we had multiple annotator teams, each focused on one language. We assigned two annotators per news article and a curator who handled consolidation and quality assurance before the final version. We held regular calls in which hard cases were highlighted, comparisons across annotations from different languages were made, and issues with ambiguities of the guidelines were discussed. To annotate the documents, we used the Inception annotation tool (Klie et al., 2018). Each language was assigned a coordinator who had the responsibility to sort through the collected articles, assign annotators and curators, and create annotation batches incrementally. The coordinator was also responsible for training the annotators. We instructed the annotators to read each news article paragraph by paragraph, and to annotate each of them with all the applicable Narratives and sub-Narratives before moving to the next one. When no Narrative or sub-Narrative was found, the annotator simply moved to the next paragraph. Because the sub-Narrative labels were many and difficult to memorize, we configured Inception so that for each paragraph, the annotator would first select all the applicable Narratives, and then all the applicable sub-Narratives from a menu displaying the sub-Narratives of the selected Narratives only. When a paragraph was deemed to contain a Narrative, but none of its sub-Narratives, the \"[Narrative]: Other\" label was selected at the sub-Narrative level. Similarly, the \"Other\" label was selected when no Narrative was applicable. During the annotation process, several issues were highlighted. First, the distribution of labels was very uneven and varied noticeably across languages (Table 2). This reflects the reality of the news discourse, as some Narratives are more frequent in some countries than others. After each annotation batch, the collected label statistics were used by the coordinator to optimize the selection of articles for the next batch. Second, some labels were semantically very close, which led to frequent confusion. We used frequency metrics and cura-tor feedback to discuss such cases in the regular meetings, where we reiterated and further refined the guidelines. For example, the sub-Narratives \"Ukraine is a puppet of the West\" and \"The West does not care about Ukraine, only their interests\" were frequently confused. To resolve this ambiguity, we added guidelines instructing the annotators to select the former when Ukraine was the subject and the latter when the West was.",
          "Table 1 shows statistics about the documents of the PolyNarrative dataset, broken down per language, and aggregated over both domains. We can see that, except for Russian, all languages have approximately 400 annotated training documents; the test documents are approximately 35 for each language. We aimed at articles of relatively long text with a median length of 500 words and a minimum length of 250 words. Even after the rigorous data selection process, the final label distribution after annotation is highly imbalanced (see Table 2 and Appendix B for a breakdown per language). Additionally, the distribution of labels varies noticeably from language to language, due to differences in media interest across countries, as already discussed. For example, for URW, Russian is a notable outlier in terms of distribution, with \"Praise of Russia\" being one of the most common Narratives. One note to highlight, is that after the article selection process, the proportion of mainstream and alternative news varies widely across languages and could have an impact on label distribution. Moreover, 44% and 54% of the documents in CC and URW, respectively, are assigned more than one label; for paragraphs, the corresponding percentages are 12% and 15%, respectively. This observation justifies our choice to perform multi-label annotation.",
          "Table 3 reports the inter-annotator agreement scores for Narratives (coarse labels) and sub-Narratives (fine-grained labels), per language, collectively for both domains, measured as Krippendorff's α at paragraph level using the simpledorfffoot_4 library. The agreement is under the recommended value of 0.667 but is higher than the IAA of tasks of similar complexity (Piskorski et al., 2023b(Piskorski et al., , 2024)). TRAIN Language #documents #paragraphs #sent. #char. BG 400 3,951 6,728 937,150 EN 394 3,683 8,012 1,176,513 PT 395 3,877 5,739 971,356 RU 133 583 1,630 206,124 TEST Language #documents #paragraphs #sent. #char. BG 35 325 542 70,291 EN 41 584 1,106 144,401 PT 35 330 578 86,132 RU 32 154 478 56,872 Total 1,465 13,487 24,813 3,649k Table 2: Distribution statistics for coarse-grained labels (Narratives), assigned to paragraphs and documents, in both TRAIN and TEST splits. Statistics for fine-grained (sub-Narratives) are shown in Appendix B. As one would expect, the agreement on finer labels is lower, because of their subtler differences. We noticed that the CC domain caused more confusion between the annotators than URW. In both cases, we could see that the confusion was skewed by a small set of under-agreed labels (5 for URW and 7 for CC) that achieved disagreement above 40% and 60%, respectively. If we excluded these labels, the IAA for all languages rises to 0.567 and 0.560 for the coarse and 0.452 and 0.516 for fine-grained, for CC and URW respectively. There were some sub-Narratives that were commonly confused. For example, in the URW subset \"Discrediting the West, Diplomacy: West is tired of Ukraine\", \"Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests\", and \"Discrediting Ukraine: Ukraine is a puppet of the West\". Overall, on a positive note, the majority of the disagreement between the two annotators in the sub-Narrative labels was between labels of the same Narrative (e.g. sub-Narratives under \"Discrediting the West, Diplomacy\" were frequently confused with one another) On average, out of all the individual annotator disagreements (paragraphs where the two annotators picked a different sub-Narrative), 67% was with sub-Narratives of the same Narrative. A detailed breakdown of the disagreements statistics for each label can be found in Appendix E.",
          "",
          "To provide baseline results for the new PolyNarrative dataset, we performed experiments with commonly used multi-label classification methods. We assessed the following configurations: • Label granularity: We assessed the performance in both coarse (predicting Narratives) and fine-grained (predicting sub-Narratives) settings. Additionally, we assessed how the extra information about the fine-grained predictions impacted coarse-grained predictions. • Paragraph vs. document: We assessed the performance when the models were trained and evaluated to classify paragraphs vs. entire documents. The document-level labels were produced by taking the union of the labels of all of the paragraphs contained in each document. • Prompting vs. fine-tuning: We also assessed the capability of (open-source) LLMs to annotate (sub-)Narratives without further training, given instructions similar to the guidelines given to the human annotators, comparing performance to fine-tuned models. We experimented with the following models: • XLM-RoBERTa large (Conneau et al., 2020), in two versions: -XLM-RoBERTa-coarse: trained using only the coarse labels. -XLM-RoBERTa-fine: trained using the fine-grained labels. • Llama3.1-70b (Grattafiori et al., 2024), without fine-tuning, in four versions: -Llama3.1-70b-0shot-labels: Prompted with the names of the labels of the taxonomy only, in a zero-shot fashion. -Llama3.1-70b-guidelines: Prompted with the full annotation guidelines, which include hand-picked examples for each (sub-)Narrative. -Llama3.1-70b-guidelines-labels: Prompted with the names of the labels, followed by the full annotation guidelines. -Llama3.1-70b-hierarchical: two-step hierarchical prompting, where the model is instructed to pick the coarse-grained labels first, and then given these coarsegrained predictions, decide on the finegrained label; full guidelines are also provided. When using Llama, after receiving the model's response, we performed a series of post-processing operations, where the generated output labels were filtered and normalized to remove hallucinated labels and to handle cases where the model generated labels in a slightly different form (e.g., different capitalization, punctuation errors). To extract paragraph-level annotations from Llama, we inserted extra \"[paragraph_N:]\" tags at the beginning of each paragraph, N being the number of the paragraph, and modified the prompts to instruct the LLM to output each extra tag followed by the relevant labels of the corresponding paragraph. After the call to the LLM, we recombined the outputs with the input paragraphs in a post-processing phase, using regular expressions. The exact prompts used are given in Appendix C. The XLM-RoBERTa (Conneau et al., 2020) models were trained at the token level. For each token, a multi-hot vector equal in size to the number of labels was provided as ground-truth, and each token inherited the ground-truth labels of its paragraph. At inference time, we obtained a similar predicted multi-hot vector from each token, and each label was assigned (or not) based on the majority vote of the paragraph's tokens. To bypass the 512 token limitation of XLM-R, we used a sliding window with a 50% overlap. We also report the results for a naïve stratified random baseline, where we randomly draw labels from a multinomial distribution, while respecting the frequency statistics in the training set.",
          "Since our task is multi-label multi-class classification with a large number of classes, and noticeable class imbalance, we opted to use suitable metrics that are robust in these conditions. Table 4 presents the F 1 macro and sample-averaged F 1 scores for all models in both domains (CC, URW). Sample-average F 1 score is calculated by calculating the F 1 score on each instance and subsequently averaging over all instances. We observe that the XLM-RoBERTa large classifier outperforms Llama3.1-70B in the fine-grained case both for document-and paragraph-level predictions, especially in sample F 1 , where the difference is very pronounced. The Llama3.1 prompted model seems to be more competitive in coarse-grain evaluation. Interestingly, including the full guidelines in the prompt generally achieves worse results than using the taxonomy labels only. When we add both the taxonomy labels and the guidelines, performance improves, compared to using only the guidelines, but it is unclear whether it is better to include only the taxonomy labels. Hierarchical prompting does not seem to yield competitive results. In terms of per-label performance, the weakest model performance was measured on the sub-Narratives under \"Downplaying climate change\" and \"Green policies are geopolitical instruments\" Narratives in the CC domain and \"Russia is the Victim\" and \"Blaming the war on others rather than the invader\" in the URW domain. One key observation is that models exhibited relatively lower performance for the \"[Narrative]: Other\" labels, that is, the paragraphs that contained a Narrative but none of the listed sub-Narratives, that may include a very diverse set of argumentation, coming from a set of heterogeneous sub-Narratives that our taxonomy does not cover. Performance also seemed to vary noticeably per language. In case of Russian, Llamma3.1-70b models seemed to perform rather poorly on a per-sample level. On XLM-RoBERTa, English consistently exhibited poorer performance compared to other languages, with Bulgarian and Portuguese showing different performance per domain. We provide a detailed breakdown of the performance (by label and by language) of two models (XLM-RoBERTa f ine and Llama3.1-70bguidelines-labels) on Appendix D.",
          "We present PolyNarrative, a new multi-lingual, multi-label, multi-domain dataset for extraction of narratives at the paragraph level from long news articles. The dataset comprises 1,476 articles totaling 13,625 paragraphs in 4 languages (Bulgarian, English, Portuguese, and Russian), annotated using two expert-refined Narrative hierarchical taxonomies with 38 and 36 fine-grained and 10 and 11 coarse-grained narratives for each domain. We describe in detail the data acquisition and annotation process and highlighted noteworthy issues. We present preliminary experimental results using both fine-tuned encoder (XLM-R) and prompting-based decoder (Llama 3.1) Language Models in multiple granularities (fine, coarse), levels (paragraph, document), and configurations (prompting strategies). We also highlight interesting findings regarding model performance. We hope that this dataset will catalyze new research directions in narrative detection and extraction, and stimulate the development of new methods and techniques. We highlight that the taxonomies are based on expert real-world experience and they are not meant to be perceived as complete or to capture the whole breadth of the media discourse around the two domains. We encourage the research community to propose extensions that potentially capture more diverse perspectives. We encourage the direction of developing methods that are domain-and potentially taxonomy-agnostic, able to perform narrative detection in a completely unsupervised fashion, removing the dependence on human subjectivity. We believe that a multi-domain and highly granular dataset such as this one can facilitate experimentation towards this goal. Regarding future work, we plan to experiment with ways to perform generation of new narrative labels and automatic revision of taxonomy and guidelines. One way to approach this is by analyzing the structure of narrative elements in the text relate them with narrative labels and identify new structures. We also intend to use retrieval strategies to perform narrative retrieval within large news corpora.",
          "Intended Use and Misuse Potential The main drive behind the creation of our dataset was to advance research on automated narrative classification and the detection of deceptive content across multiple languages and domains. However, given that possible risk of exploiting the dataset to boost the production of biased manipulative disinformation attempts, we advise responsible use. In this context, whoever develops a Narrative detection system is also responsible for deciding which Narratives to detect, in an ethical manner. Furthermore, this research in direction can contribute to the development of independent tools such as browser plugins or independent publicly available services that could help users contextualize the information they consume, contributing to their awareness and improving citizens' media literacy. Environmental Impact The deployment of LLMs might have a large carbon footprint, especially when training new models. In the context of the reported experiments, we did not train any new LLMs, but only used existing trained models in an in-context zero-shot scenarios, which is relatively cheap in terms of computing. Fairness The majority of the annotators, primarily researchers with linguistic background and prior annotation experience, come from the institutions of the co-authors of this manuscript. They were fairly remunerated as part of their job. The remaining part of the annotator pool consisted of (a) some students from the respective academic organizations, (b) few external experienced analysts paid at rates set by their contracting institutions, and (c) experts from a contracted a professional annotation company, who were compensated according to rates based on their country of residence.",
          "Taxonomies and Dataset Representativeness Our taxonomies were edited by experienced media analysts, active in the study of misinformation and fact-checking. As such, the taxonomies overrepresent Narratives of interest of media analysts from Western institutions. The selection should not be perceived as covering the complete discourse of the two domains, but rather what such analysts encounter in practice. The original taxonomy for URW used before the heavy revision from the analysts, was not from a peer-reviewed publication but from a technical report. The dataset presented covers two widely discussed domains around the world and a wide range of media sources. However, it should not be considered as representative of the media in any specific country, nor should it be considered as balanced in any way. Also, it is important to note that while this work focuses on narratives potentially containing mis/disinformation, the operationalized definition of Narrative can also be used to detect neutral or desirable viewpoints. Biases Although the annotators were trained and made acquainted with the specifics of the two domains of interest for our task and cross-language quality control mechanisms have been put in place in the annotation process, we are aware that some degree of intrinsic subjectivity will inevitably be present in the dataset. Consequently, models trained on this dataset might exhibit certain biases. and Han Qiu. 2024. The earth is flat because...: Investigating LLMs' belief towards misinformation via persuasive conversation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16259-16303, Bangkok, Thailand. Association for Computational Linguistics. Wajdi Zaghouani, Mustafa Jarrar, Nizar Habash, Houda Bouamor, Imed Zitouni, Mona Diab, Samhaa El-Beltagy, and Muhammed AbuOdeh. 2024. The FIGNEWS Shared Task on News Media Narratives. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 530-547, Bangkok, Thailand. Association for Computational Linguistics.",
          "This section introduces the taxonomies of narratives and subnarratives for the two domains. A.1 Ukraine-Russia War (URW) 1. Blaming the war on others rather than the invader: statements attributing responsibility or fault to entities other than Russia in the context of Russia's invasion of Ukraine. Example: \"The economic crisis is due to Western sanctions.\" Example: \"Ukraine's actions provoked this conflict.\" Sub-Narratives: (a) Ukraine is the aggressor: Statements that shift the responsibility of the aggression to Ukraine instead of Russia and portray Ukraine as the attacker. Example: \"Ukraine secretly provoked the war because it was harassing Donbass province citizens.\" (b) The West are the aggressors: Statements that shift the responsibility for the conflict and escalation to the Western block. Example: \"The real perpetrators were US/EU. They sabotaged Minsk II agreement only to force Russia to invade.\" 2. Discrediting Ukraine: Statements that undermine the legitimacy, actions, or intentions of Ukraine or Ukrainians as a nation. Example: \"Ukraine's government is corrupt and ineffective.\" Example: \"Ukrainian soldiers are committing atrocities.\" Example: \"Ukrainian identity does not exist\" Sub-Narratives: (a) Rewriting Ukraine's history: Statements that aim to reestablish history of Ukrainian nation in a way that discredits its reputation. Example: \"Ukraine is not a real nation, it was a fabrication to split Russia and ally with Hitler.\" (b) Discrediting Ukrainian nation and society: Statements that aggressively undermine the legitimacy and reputability of Ukrainian ethnicity and people (c) Discrediting Ukrainian military: Statements that aim to undermine the capabilities, professionalism or effectiveness of the Ukrainian armed forces. (d) Discrediting Ukrainian government and officials and policies: Statements that seek to delegitimize the Ukrainian government, its leaders, and its policies, portraying them as corrupt or incompetent. (e) Ukraine is a puppet of the West: Claims that Ukraine is controlled or heavily influenced by Western powers, particularly the United States and European Union. (f) Ukraine is a hub for criminal activities: Allegations that Ukraine is a center for illegal activities such as human trafficking, drug smuggling, or organized crime (g) Ukraine is associated with nazism: Accusations that Ukrainian society or government has ties to or sympathies with Nazi ideology, often referencing historical events or extremist groups. (h) Situation in Ukraine is hopeless: Statements that portray Ukraine as having no viable perspectives or no potential positive future. Example: \"Ukraine should just give up, it is all over debt and will be exploited by the West anyway.\" 3. Russia is the Victim: Statements that portray Russia as being unfairly targeted or victimized. Example: \"Russia is being unfairly sanctioned.\" Example: \"The West is ganging up on Russia without justification.\" Example: \"Russia is doing what every country would do (e.g. protect its interests/honour etc.)\" Sub-Narratives: (a) The West is russophobic: Statements that claim that the negative reaction to Russia's actions are because of the negative perspective of western countries instead of Russia's own actions. Example: \"Politicians in the West blame Russia for everything, instead of looking at their mistakes.\" Example: \"In Country X, they banned Tchaikovsky ballets and Chechov's plays because they cannot stand Russia and its culture.\" (b) Russia actions in Ukraine are only selfdefence: Statements that justify Russia's action solely as legitimate self-defence and not a deliberate action. Example: \"There was no other way than war to defend the Russian-speaking people in Donbass.\" (c) UA is anti-RU extremists: Statements claiming that Ukraine is comprised of extremist elements that are vehemently opposed to Russia. 4. Praise of Russia: Statements that positively highlight Russia's actions, policies, or character Example: \"Russia is leading the way in international diplomacy.\" Example: \"The Russian economy is resilient and strong.\" Example: \"Glorifying mentions of Russia's weapon systems and military might.\" Sub-Narratives: (a) Praise of Russian military might: Statements that positively highlight Russia's military institutions, equipment and scale. Example: \"Russia has far more tanks and powerful artillery that US/EU would only dream of.\" (b) Praise of Russian President Vladimir Putin: Statements that present Vladimir Putin positively, including his personal and leadership qualities. Example: \"Any country would want such a strong leader as Putin to lead the way.\" (c) Russia is a guarantor of peace and prosperity: Statements that portray Russia solely in a positive manner, emphasising their potential to provide peace and prosperity to those that cooperate. Example: \"Take a look at Africa, Russia supports countries and turns them into independent nations guided by their people's interests where Western countries colonised brutally.\" (d) Russia has international support from a number of countries and people: Statements that emphasise the popularity and acceptance of Russia in the international stage. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" (e) Russian invasion has strong national support: Statements that emphasise the popularity and acceptance of the invasion inside Russia and on Russian-speaking populations. Example: \"The majority of the countries population sides with Russia as per last UN General Assembly vote.\" 5. Overpraising the West: Statements that excessively and unduly laud or extol the virtues, accomplishments, and moral superiority of Western countries, particularly in the context of international relations and military. Sub-Narratives: (a) NATO will destroy Russia: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) and its allies are capable or already in the process of eradicating Russia. (b) The West belongs in the right side of history: Statements that portray Western nations and their actions as morally superior and aligned with progress and justice and possess moral superiority. (c) The West has the strongest international support: Statements that emphasize or claim widespread backing for Western policies and actions from the international community, potentially downplaying opposition or criticism. 6. Speculating war outcomes: Statements that predict or make assumptions about the potential results or consequences of a conflict Sub-Narratives: (a) Russian army is collapsing: Statements that suggest or claim that the Russian military is experiencing a significant decline in its effectiveness, strength, or morale. (b) Russian army will lose all the occupied territories: Speculative statements that predict or assume the potential outcomes of the conflict, specifically regarding the possibility of the Russian military losing control of all the territories it currently occupies. (c) Ukrainian army is collapsing: Statements that suggest or claim that the Ukrainian military is experiencing a significant decline in its effectiveness, strength, or morale. 7. Discrediting the West, Diplomacy: Statements that criticize the Western countries, or international diplomatic efforts. Example: \"The West is hypocritical in its foreign policy.\" Example: \"Western diplomacy has failed in resolving conflicts.\" Example: \"International organizations will not solve anything because...\" Sub-Narratives: (a) The EU is divided: Statements that present the EU as a set of divided entities and interests, usually unable to take actions. Example: \"The European Council will never vote on sanctions for Russia, since they cannot agree on even the simplest of the issues.\" (b) The West is weak: Statements presenting the West overall as a non-potent group of countries (that is not as powerful as it used to be). Example: \"The weakened West is once again impotent to act in front of the will Russia.\" (c) The West is overreacting: Statements that claim that the West and its institutions are reacting to Russia's actions in a disproportionate manner. Example: \"Putin did not invade the EU but Ukraine. Imposing harsh sanctions is not the way to deal with it, dialogue and debate is.\" (d) The West does not care about Ukraine, only about its interests: Statements that claim that the West is only interested in Ukraine for its own benefits, disregard-ing the country's fate. Example: \"The West has indebted Ukraine more than XX bln of dollars, a lucrative deal for western companies to exploit.\" Example: \"NATO's actions are endangering global security.\" (e) Diplomacy does/will not work: Statements discrediting the potential of ongoing or potential diplomatic efforts. Example: \"Diplomats are desperately trying to figure out solutions but now it's too late, they have failed and Russia is free to do whatever.\" (f) West is tired of Ukraine: Claims that Western countries, particularly the United States and European nations, are becoming fatigued or disinterested in supporting Ukraine and its efforts. 8. Negative Consequences for the West: Statements that highlight or predict adverse outcomes for Western countries and their interests. Example: \"Sanctions against Russia will backfire on Europe.\" Example: \"The West is headed for an economic downturn.\" Sub-Narratives: (a) Sanctions imposed by Western countries will backfire: Statements that catastrophize on the possible negative effects for Western sanctions of Russia. Example: \"The winter is going to be cold and with current gas prices, we are talking of societal unrest.\" (b) The conflict will increase the Ukrainian refugee flows to Europe: Statements that catastrophize on the possible refugee outflows due to the conflict. Example: \"Like we did not have refugees from the Middle East, now we will have Ukrainians stressing our housing and healthcare problems.\" 9. Distrust towards Media: Statements that question the reliability or integrity of media organizations. Example: \"Western media is spreading propaganda.\" Example: \"You can't trust what the news says about Russia.\" Sub-Narratives: (a) Western media is an instrument of propaganda: Statements that discredit the media institutions of the West and claim that they are instruments of propaganda. Example: \"... but you wouldn't hear this on a western channel, only the party line from State Department.\" (b) Ukrainian media cannot be trusted: Statements that discredit the media institutions of the Ukraine and claim that they should not be trusted for reporting on the war. Example: \"Ukraine is conducting its own propaganda using their TV channels, news and social media.\" 10. Amplifying war-related fears: Statements that evoke fear or anxiety about potential threats, dangers or reactions. Example: \"The West is pushing us towards World War III.\" Example: \"It is a matter of time before war speads on the West\" Example: \"Nuclear war is imminent\" Sub-Narratives: (a) By continuing the war we risk WWIII: Statements that warn against upsetting Russia's and its leadership, evoking fear of causing WW3. Example: \"The Western elites with their fixation on Russia are sleapwalking towards WW3\" (b) Russia will also attack other countries: Statements that claim that it is imminent that Russia will attack other countries. Example: \"... and be sure, Ukraine is the first not the last country to be invaded. Others will follow.\" (c) There is a real possibility that nuclear weapons will be employed: Statements that evoke fear or anxiety about the use of nuclear weapons. Example: \"... and if Western hypocrisy continues to provoke, Putin might be forced to press the red button. . . for good\" (d) NATO should/will directly intervene: Statements that suggest or claim that the North Atlantic Treaty Organization (NATO) ought to or will take direct military action in a conflict, potentially implying a shift in policy or strategy. 11. Hidden plots by secret schemes of powerful groups: Statements that suggest hidden plots or secretive actions by powerful groups related to the war. Example: \"There's a secret plan by the elites to control global resources.\" Example: \"The war is just a cover for something much bigger.\" A.2 Climate Change (CC) 1. Criticism of climate policies: Statements that question the effectiveness, economic impact, or motives behind climate policies.",
          "(a) Climate policies are ineffective: Statements suggesting that climate policies fail to achieve their intended environmental goals. (b) Climate policies have negative impact on the economy: Statements claiming that climate policies lead to negative economic outcomes. (c) Climate policies are only for profit: Statements that argue climate policies are driven by financial or corporate gain rather than genuine environmental concerns.",
          "Statements that challenge the competence, integrity, or intentions of various institutions and authorities in relation to climate change. Sub-Narratives: (c) Temperature increase does not have significant impact: Statements claiming that the increase in temperature is not going to have any noticeable effect in nature. (d) CO2 concentrations are too small to have an impact: Statements claiming that the concentrations of CO2 will have a negligible effect. (e) Human activities do not impact climate change: Statements that support that climate change is not caused by human activity. (f) Ice is not melting: Statements claiming that there is not melting of ice. (g) Sea levels are not rising: Statements denying that sea levels have risen (or will rise). (h) Humans and nature will adapt to the changes: Statements claiming that whatever the changes in climate humans or nature will manage to find solutions to adapt. 5. Questioning the measurements and science: Statements that raise doubts about the scientific methods, data, and consensus on climate change. Sub-Narratives: (a) Methodologies/metrics used are unreliable/faulty: Statements claiming that the scientific methodologies and metrics used to measure climate change are flawed or unreliable. (b) Data shows no temperature increase: Statements asserting that available data does not support the claim of global temperature increase. (c) Greenhouse effect/carbon dioxide do not drive climate change: Statements asserting that available data does not support the claim of global temperature increase. (d) Scientific community is unreliable: Statements discrediting scientists, the scientific community and their actions. 6. Criticism of climate movement: Statements that challenge the motives, integrity, or impact of the climate movement. Sub-Narratives: (a) Climate movement is alarmist: Statements suggesting that the climate movement exaggerates the severity of climate change for dramatic effect. (b) Climate movement is corrupt: Statements alleging that the climate movement is influenced by ulterior motives, by corruption or by unethical practices. (c) Ad hominem attacks on key activists: Statements attacking the reputation of key figures (such as scientists, activists, politicians or public figures). 7. Controversy about green technologies: Statements that express skepticism or criticism of environmentally friendly technologies. Sub-Narratives: (a) Renewable energy is dangerous: Statements claiming that renewable energy sources pose significant risks or dangers. (b) Renewable energy is unreliable: Statements asserting that renewable energy sources are not dependable for widespread adoption. (c) Renewable energy is costly: Statements asserting that renewable energy sources are too expensive, inefficient and worth adopting for widespread use. (d) Nuclear energy is not climate friendly: Statements asserting that nuclear sources are or should not be considered as good for the climate. Statements that minimize the urgency of addressing climate change by suggesting that any action taken at this point is futile or too late to make a meaningful impact. 10. Green policies are geopolitical instruments: Statements claiming that that environmental policies and initiatives are used as tools for geopolitical power and influence rather than genuine environmental concern.",
          "(a) Climate-related international relations are abusive/exploitative: Statements criticizing international relations related to climate change as exploitative or economically abusive. (b) Green activities are a form of neocolonialism: Statements suggesting that green initiatives are a way for developed countries to exert control and influence over developing nations, a modern form of colonial practices.",
          "Figures 4 and 5 report the normalized count of coarse labels per language. There are substantial differences, notably between RU and other languages for URW and between EN and other languages for CC. These differences could be due either to bias in media of each country, or be due to bias in the sampling of the articles to annotate. The reason RU is absent for CC is that it was not possible to find enough articles to annotate. In Table 5, we report on the difficulty of annotating labels. We report the proportion of each label to have a disagreement between annotators both with the Other label specifically, or with any other label. Based on the numbers, we categorise each label in a class of difficulty.",
          "In this Section we provide the different prompts used in the experiments. C.1 0shot (only taxonomy) You are an experienced analyst making labeling articles with labels specific to the war in Ukraine. Labels: {codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the above list. The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst labeling articles with labels specific [\"to the war in Ukraine\"/ \"Climate Change\"] You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You will apply a label to each paragraph if it presents at any point a narrative that is given in the annotation guidelines given below. ",
          "",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "The beginning of each paragraph is marked by a [paragraph_X:] tag. Answer strictly only with the paragraph tag and the name of the labels detected and not anything else. If you find no label, just output 'Other'. Strip down the markdown artifacts. Join the labels with semicolon. Do not output anything from the text.",
          "You are an experienced analyst making labeling articles with labels specific to the war in Ukraine.",
          "{codebook_urw} You are given the coarse label and you are asked to return the fine label. You will apply a label if the article presents at any point a narrative that is given in the annotation guidelines given below.  D Selected model performance per label and language We provide detailed breakdown of the performance of our top-performing model on fine-grained level (sub-Narratives). We provide both the detailed scores per-label (Figures 6, 7) and per-language (Figures 8,9,10,11) for the two domains (CC and URW). ",
          "We report on the detailed dissagreement statistics of the annotators per each fine-grained label, and highlight the difficulty of each label, in Table 5.    label count %conf %other mcl difficulty 1: URW: Blaming the war on others rather than the invader: Ukraine is the aggressor 169 0.309 0.315 5 Medium 2: URW: Blaming the war on others rather than the invader: The West are the aggressors 260 0.377 0.390 7 Medium 3: URW: Discrediting Ukraine: Rewriting Ukraine's history 23 0.272 0.111 28 Medium * 4: URW: Discrediting Ukraine: Discrediting Ukrainian nation and society 38 0.380 0.053 6 Medium * 5: URW: Discrediting Ukraine: Discrediting Ukrainian military 316 0.314 0.217 1 Medium * 6: URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies 502 0.215 0.253 8 Easy 7: URW: Discrediting Ukraine: Ukraine is a puppet of the West 219 0.235 0.165 28 Easy * 8: URW: Discrediting Ukraine: Ukraine is a hub for criminal activities 127 0.368 0.243 6 Medium * 9: URW: Discrediting Ukraine: Ukraine is associated with nazism 97 0.189 0.080 6 Easy * 10: URW: Discrediting Ukraine: Situation in Ukraine is hopeless 107 0.303 0.220 6 Medium * 11: URW: Russia is the Victim: The West is russophobic 167 0.343 0.427 2 Medium 12: URW: Russia is the Victim: Russia actions in Ukraine are only self-defence 130 0.518 0.273 1 Hard * 13: URW: Russia is the Victim: UA is anti-RU extremists 34 0.477 0.406 11 Hard * 14: URW: Praise of Russia: Praise of Russian military might 466 0.140 0.346 5 Easy 15: URW: Praise of Russia: Praise of Russian President Vladimir Putin 100 0.157 0.348 17 Easy 16: URW: Praise of Russia: Russia is a guarantor of peace and prosperity 257 0.352 0.385 1 Medium 17: URW: Praise of Russia: Russia has international support from a number of countries and people 228 0.160 0.245 16 Easy 18: URW: Praise of Russia: Russian invasion has strong national support 20 0.333 0.250 17 Medium * 19: URW: Overpraising the West: NATO will destroy Russia 11 0.263 0.400 20 Medium 20: URW: Overpraising the West: The West belongs in the right side of history 36 0.263 0.133 28 Medium * 21: URW: Overpraising the West: The West has the strongest international support 24 0.254 0.308 7 Medium 22: URW: Speculating war outcomes: Russian army is collapsing 51 0.313 0.476 23 Medium 23: URW: Speculating war outcomes: Russian army will lose all the occupied territories 10 0.312 0.200 22 Medium * 24: URW: Speculating war outcomes: Ukrainian army is collapsing 93 0.242 0.118 5 Easy * 25: URW: Discrediting the West, Diplomacy: The EU is divided 105 0.164 0.435 28 Easy 26: URW: Discrediting the West, Diplomacy: The West is weak 145 0.301 0.267 14 Medium * 27: URW: Discrediting the West, Diplomacy: The West is overreacting 34 0.395 0.294 11 Medium * 28: URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests 211 0.381 0.262 7 Medium * 29: URW: Discrediting the West, Diplomacy: Diplomacy does/will not work 109 0.550 0.434 28 Hard * 30: URW: Discrediting the West, Diplomacy: West is tired of Ukraine 43 0.583 0.171 28 Hard * 31: URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire 79 0.338 0.723 12 Medium 32: URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe 12 0.000 NaN Easiest 33: URW: Distrust towards Media: Western media is an instrument of propaganda 104 0.180 0.452 11 Easy 34: URW: Distrust towards Media: Ukrainian media cannot be trusted 23 0.500 NaN 6 Hard 35: URW: Amplifying war-related fears: By continuing the war we risk WWIII 135 0.285 0.339 37 Medium 36: URW: Amplifying war-related fears: Russia will also attack other countries 133 0.211 0.270 2 Easy 37: URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed 292 0.259 0.486 2 Medium 38: URW: Amplifying war-related fears: NATO should/will directly intervene 63 0.305 0.273 36 Medium * 39: CC: Criticism of climate policies: Climate policies are ineffective 74 0.407 0.174 44 Hard * 40: CC: Criticism of climate policies: Climate policies have negative impact on the economy 96 0.337 0.333 45 Medium * 41: CC: Criticism of climate policies: Climate policies are only for profit 58 0.508 0.069 68 Hard * 42: CC: Criticism of institutions and authorities: Criticism of the EU 54 0.275 NaN 45 Medium 43: CC: Criticism of institutions and authorities: Criticism of international entities 104 0.434 0.196 44 Hard * 44: CC: Criticism of institutions and authorities: Criticism of national governments 225 0.375 0.274 45 Medium * 45: CC: Criticism of institutions and authorities: Criticism of political organizations and figures 190 0.487 0.172 44 Hard * 46: CC: Climate change is beneficial: CO2 is beneficial 19 0.071 NaN 51 Easiest 47: CC: Climate change is beneficial: Temperature increase is beneficial 13 0.266 0.500 54 Medium 48: CC: Downplaying climate change: Climate cycles are natural 36 0.294 0.300 50 Medium 49: CC: Downplaying climate change: Weather suggests the trend is global cooling 13 0.406 0.308 70 Hard * 50: CC: Downplaying climate change: Temperature increase does not have significant impact 5 0.736 0.286 56 Hardest * 51: CC: Downplaying climate change: CO2 concentrations are too small to have an impact 18 0.500 0.143 59 Hard * 52: CC: Downplaying climate change: Human activities do not impact climate change 34 0.387 0.083 60 Medium * 53: CC: Downplaying climate change: Ice is not melting 18 0.117 NaN 62 Easy 54: CC: Downplaying climate change: Sea levels are not rising 2 0.500 0.500 47 Hard 55: CC: Downplaying climate change: Humans and nature will adapt to the changes 5 1.000 0.400 50 Hardest * 56: CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty 51 0.333 0.118 59 Medium * 57: CC: Questioning the measurements and science: Data shows no temperature increase 10 0.562 0.333 56 Hard * 58: CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change 5 0.750 1.000 Hardest 59: CC: Questioning the measurements and science: Scientific community is unreliable 45 0.540 0.185 56 Hard * 60: CC: Criticism of climate movement: Climate movement is alarmist 67 0.619 0.386 61 Hardest * 61: CC: Criticism of climate movement: Climate movement is corrupt 35 0.869 0.463 60 Hardest * 62: CC: Criticism of climate movement: Ad hominem attacks on key activists 78 0.234 0.217 60 Easy * 63: CC: Controversy about green technologies: Renewable energy is dangerous 18 0.421 0.250 65 Hard * 64: CC: Controversy about green technologies: Renewable energy is unreliable 42 0.344 0.333 45 Medium * 65: CC: Controversy about green technologies: Renewable energy is costly 27 0.512 0.100 45 Hard * 66: CC: Controversy about green technologies: Nuclear energy is not climate friendly 3 1.000 0.750 45 Hardest * 67: CC: Hidden plots by secret schemes of powerful groups: Blaming global elites 47 0.438 0.217 68 Hard * 68: CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives 74 0.548 0.094 45 Hard * 69: CC: Amplifying Climate Fears: Earth will be uninhabitable soon 51 0.432 0.188 70 Hard * 70: CC: Amplifying Climate Fears: Amplifying existing fears of global warming 864 0.124 0.477 71 Easy 71: CC: Amplifying Climate Fears: Doomsday scenarios for humans 104 0.435 0.239 70 Hard * 72: CC: Amplifying Climate Fears: Whatever we do it is already too late 18 0.264 0.111 70 Medium * 73: CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative 7 0.785 0.273 68 Hardest * 74: CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism 10 0.375 0.333 68 Medium * Table 5: Label difficulty, %conf is the percentage of annotation with that label that results in an inconsistency, while %other is the percentage that resulted in an inconsistency with label other. \"mcl\" is the most common label with which a specific label is confused. \"difficulty\" make assess the difficulty of the label based on %conf: Easiest (≤ .1), Easy (≤ .25), Medium (≤ .4), and Hard (≤ .4) or Hardest (> 0.6), a star indicate that there is more confusion within the labels of the taxonomy than with the Other class, meaning that it is extra difficult for the annotator"
        ],
        "ground_truth_definitions": {
          "narrative in the context of news articles": {
            "definition": "descriptions of spatiotemporal events and capturing narrative components in the form of semantic relationships between entities.",
            "context": "The term narrative in the context of news articles has been used in a variety of different formulations. This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity.",
            "type": "implicit"
          },
          "Opinion Mining": {
            "definition": "task to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and determine its polarity.",
            "context": "This includes modeling them as descriptions of spatiotemporal events (Norambuena et al., 2023; Silvano et al., 2024) and capturing narrative com- ponents in the form of semantic relationships be- tween entities (Nunes et al., 2024). Related but distinct is the problem of Opinion Mining (Liu and Zhang, 2012; Cortis and Davis, 2021), where the task is to extract the opinion of an entity (opinion holder) on an entity or issue (or group thereof), based on an aspect and deter- mine its polarity. Although a restricted subset of narratives can be expressed as opinions, the two concepts serve very different purposes.",
            "type": "explicit"
          },
          "sub-narrative": {
            "definition": "lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative.",
            "context": "Hereafter, we use Narrative (capitalized) to refer to a top-level (coarse) label of our taxonomies. A sub-Narrative is a lower-level (fine) label of our taxonomies, representing a narrower claim within a Narrative. For example, “CO2 is beneficial” is a sub-Narrative of the “Climate Change is beneficial” Narrative of the Climate Change (CC) taxonomy.",
            "type": "implicit"
          },
          "Narratives": {
            "definition": "overt or implicit claims, recurring across articles and languages, promoting a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinformation.",
            "context": "We present PolyNarrative, a new multilingual dataset of news articles, annotated for narra- tives. Narratives are overt or implicit claims, recurring across articles and languages, promot- ing a specific interpretation or viewpoint on an ongoing topic, often propagating mis/disinfor- mation. We developed two-level taxonomies with coarse- and fine-grained narrative labels for two domains: (i) climate change and (ii) the military conflict between Ukraine and Russia.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "eb29476dd81aefedf2896db42f039f003a0ec5bf",
        "sections": [
          "Intense competition from large-scale growers has forced small-scaled farmers to find new niche markets for their commodities through value-added marketing. To discover the right niche market is a complicated task because demand is highly segmented among consumers who may be concerned with different attributes such as local, organic, and other specialty types. A recent survey of 500 Twin Cities consumers showed that consumers buy locally grown produce primarily because of its high quality and freshness (King, 2007). Previous studies that focused on consumers' choices at farmers' markets in other states showed that today's consumers support local farmers' markets because they expect higher quality (for example, freshness and taste) and lower prices (Brown, 2002(Brown, , 2003)). Organically grown produce is considered to be healthy and environmentally friendly because of the use of less-damaging pesticides (Magnusson et al., 2001;Thompson and Kidwell, 1998). For some consumers, premium price is the major obstacle in buying organic foods (Magnusson et al., 2001). Yet, ''locally grown'' produce is not necessarily ''organically grown'' produce. Locally grown produce can be produced using conventional production methods. Organic produce purchased by Minnesota consumers might be from California or New Zealand. Finding out why certain consumers prefer organically grown or locally grown produce and their willingness to pay (WTP) for fresh produce with different attributes is important for stakeholders such as producers and marketers in making their production and marketing decisions because it helps them figure out what type of fresh produce to grow and sell, what to emphasize in marketing efforts, and what are reasonable prices to charge. The literature on consumer preferences for and marketing segmentation of organically and locally grown fresh produce is sparse. Loureiro and Hine (2002) studied consumers' WTP for local (Colorado-grown), organic, and GMO-free potatoes. They found that the ''Colorado-grown'' attribute afforded potato producers the highest consumer acceptance and premium compared with organic and GMO-free potatoes. However, only consumers' attitudes toward potato freshness and nutrition were included in their estimation. Important consumer attitudes toward certain quality attributes such as appearance, taste, food safety, and so on (Yue et al., 2007(Yue et al., , 2008) ) were not included in their study. The traditional hypothetical survey method is preferred by most academic researchers in consumer preference studies. The hypothetical method asks individuals to choose the values they are willing to pay for certain products without asking them to exchange money and real goods. Although widely used, a question about this approach that has been aroused is whether values elicited from hypothetical experiments reflect the amounts individuals would actually pay. It is well documented that consumers are less reluctant to spend money in stated preference studies than in markets with real economic commitment. In the literature, this is known as the hypothetical bias (see, for example, Alfnes et al., 2006). Previous studies also show that there is significant hypothetical bias when consumers' WTP is elicited using pictures instead of real products in hypothetical studies in which participants do not need to pay for the products shown in the pictures (see, for example, Yue et al., 2008). Lusk and Schroeder (2004) found that there is no significant difference between marginal WTP elicited from hypothetical and nonhypothetical experiments when real products instead of pictures are used in both experiments. The objective of this study was to investigate consumers' WTP for organically grown and locally grown fresh produce and the marketing segmentation of these two types of produce. The results of this research are very important for small-scale farmers, market organizers, and sponsoring agencies in making their production and marketing decisions. This study also contributes to the literature on consumers' choice and demand segmentation between organically grown and locally grown produce. In addition, our research is different from previous studies in that we used and compared two methods to investigate consumer preference for and WTP for organically and locally grown fresh produce. We used both the traditional hypothetical experiment and the nonhypothetical experiment with real economic incentives. The use of a choice format is new in the consumer preference and WTP literature. Lusk and Schroeder (2004) developed the choice format with posted prices in different scenarios to investigate consumers' WTP for food quality attributes. Alfnes et al. (2006) used a nonhypothetical choice experiment to investigate consumers' WTP for salmon with different colors. The hypothetical experiment and nonhypothetical choice experiment with real economic incentives give data in similar formats and are complimentary to each other; one gives hypothetical WTP data and the other gives real WTP data. The use of the two methods helps eliminate the hypothetical bias that exists in traditional hypothetical experimental methods and get consumers' real WTP for fresh produce with different attributes.",
          "We tested the data from the hypothetical experiment and the nonhypothetical choice experiment for similarities and used the two data sources in a joint estimation. We tested if the use of real products instead of pictures can eliminate hypothetical bias. The details of both experiments are presented subsequently. Attributes. The fresh produce used for the study was tomatoes because they are widely grown locally and organically in Minnesota. In our experimental design, there were two qualitative attributes (organically grown and locally grown) and one quantitative attribute (price). Each qualitative attribute had two levels-yes or no. The number of levels for the quantitative factor, price, was six. The price ranged from $1.25 to $2.50 with $0.25 as intervals based on real tomato prices in different stores in the Twin Cities area when the experiments were conducted. Table 1 summarizes the levels of attributes. There were four combinations of the two qualitative attributes as shown in Table 2. In our experiment, we named the locally grown (no) and organically grown (no) as ''conventional,'' locally grown (no) and organically grown (yes) as ''organic,'' locally grown (yes) and organically grown (no) as ''local,'' and locally grown (yes) and organically grown (yes) as ''organic + local.'' The tomatoes were purchased from a farmers' market, a supermarket, and an organic farm in St. Paul, MN. Other product attributes such as tomato size, color, shape, size, firmness, and ripeness were not included in the design. However, the tomatoes were of the same type and when we made purchases, we carefully compared tomatoes' color, size, shape, ripeness, and firmness to make them be very similar to each other in these attributes. Therefore, the tomatoes' possible differences in these attributes were controlled and participants said that they were unable to distinguish between them. Scenarios. In the hypothetical and nonhypothetical experiments, participants were presented with a series of pricing scenarios and asked to choose one alternative in each pricing scenario. If none of the alternatives was of interest, they could choose ''none of these.'' To lessen the cognitive burden on participants, only two alternatives were usually included in each scenario in a choice experiment. In our experiments, each scenario had two alternatives described by conventional, organic, local, or organic plus local attributes and price. Because it was not practical to ask each participant to choose from all possible scenarios, a fractional factorial design was developed to minimize scenario number and maximize profile variation. The design was developed based on four principles: level balance (levels of an attribute occurred with equal frequency), orthogonality (the occurrences of any two levels of different attributes were uncorrelated), minimal overlap (cases in which attribute levels did not vary within a scenario were minimized), and utility balance (the probabilities of choosing alternatives within a scenario were kept as similar as possible) (Louviere et al., 2000). The fractional factorial design gave each participant sets of 12 scenarios to evaluate. The clearly dominated alternatives were eliminated. For a further discussion of factorial design, see Louviere et al. (2000). The following is an example of one of the scenarios: Please examine the tomatoes on the table. Consider a situation in which you are buying 1 lb of fresh tomatoes. From the following pairs of alternatives, please choose one alternative that you would like to purchase (you can choose ''none'' if you do not want either of them): Scenario 1-conventional $1.75; organic + local $2.25; none of these. Next to organic tomatoes, we put a USDA Certified Organic label; beside local tomatoes, we put a Minnesota Grown label; and beside locally grown, organic tomatoes, we put both the USDA Certified Organic label and the Minnesota Grown label. Hypothetical experiment. For the hypothetical experiment, participants were not asked to buy anything after they made choices. Real products were used in the hypothetical experiment. Participants were asked to answer a series of questions such as what kind of retailers they were most likely to patronize and a series of questions designed to extract information on their attitudes toward and preference for the organic and local attributes of fresh produce. Nonhypothetical experiment. For the nonhypothetical choice experiment, one of the pricing scenarios was randomly drawn as binding, i.e., the choices made in that scenario would be implemented. Participants would receive the products they chose and were required to pay the prices posted for the corresponding products in the binding scenario. The nonhypothetical choice experiment had the following five steps-Step 1: The experimental procedure was explained to participants and some information about the alternatives (the three attributes) was provided. Step 2: Participants studied the alternatives in scenarios and marked on a choice scheme which of the alternatives in each scenario they wanted to buy. Step 3: After participants completed all scenarios, each participant drew one card with a number determining his or her binding scenario. Step 4: Each participant received the fresh produce he or she chose in his or her binding scenario. Step 5: Participants went to the cashier and paid for their fresh produce. After participants made their choices, they were asked to answer the same set of questions as in the hypothetical experiment. Sampling. A booth was set up at the Minnesota State Fair to conduct the hypothetical and the nonhypothetical choice experiments in Aug. 2008. The Minnesota Sate Fair is the largest U.S. state fair in terms of average daily attendance (Minnesota State Fair, 2008). The sociodemographics of fair attendees are close to Minnesotan sociodemographics except that a slightly higher proportion of those aged between 35 and 44 years attend the Fair who live in Minnesota (Minnesota State Fair, 2008;U.S. Census Bureau, 2004). Therefore, we expected to get a representative sample of Minnesota consumers. State Fair participants were randomly selected to participate in the experiments. Each participant only participated in one type of experiment, either the hypothetical experiment or the nonhypothetical experiment. A small amount of cash or gifts worth $6 to $8 was given to participants to compensate for their time. To ensure that the sample was representative, only one family member per household was allowed to participate in the experiments. Most participants at the Minnesota State Fair came as families. When we recruited participants, we asked the primary food purchaser of the household to participate in the experiments. In total, 365 participants were surveyed with 115 participating in the nonhypothetical experiment and the remaining 250 participating in the hypothetical experiment. Some of the data were not included in the analysis because of one or more missing values. Observations of 343 participants with 110 participants from the nonhypothetical experiment and 233 from the hypothetical experiment were included in the analysis. Statistical model. Consumers' satisfaction derived from choice and consumption of dif-  (Train, 2003). We used the mixed logit model to capture all possible correlations for responses from the same participant and scenario. The statistical model was: where U nis was the utility of individual n from alternative i in scenario s; x nis were vectors of observed variables relating to alternative i and individual n; b was a vector of fixed coefficients; h was a vector of normally distributed random terms with mean zero used to capture the possible correlations; and nis was an identical and independent extreme value error term. The standard logit model is a special case of the mixed logit model where h has zero variance. The density of h was denoted by f (hjV), where V was the fixed parameter vector of the distribution. For a given h, the conditional choice probability of alternative i was a standard logit: Consequently, the unconditional choice probability, P, in the mixed logit model was the logit formula integrated overall values of h with density of h as weights: This integral should be solved analytically and was approximated through simulation (Brownstone and Train, 1999). The maximum likelihood estimation method was used to estimate coefficients with Stata 10.0 software (StataCorp, College Station, TX).",
          "The summary statistics of the sample are shown in Table 3. Approximately 84.51% of participants purchased fruits and vegetables more than once a week and only 0.28% of participants purchased fruits or vegetables less than once a month. These results, derived from the data, indicated that the sample of participants was representative of consumers who are regular fresh produce consumers. The average age of participants was 45 years old. Seventy-three percent of participants lived in urban areas and 23% of them belonged to certain environmental groups. Fourteen percent of participants had children younger than 12 years old in the household, 66% of participants were female, and the average education level was a college degree. On average, there were two to three people in a household and the average income level was $69,000. A large proportion of participants (68%) purchased most of their fresh produce from supermarkets, 27% from a roadside stand or farmers' market, 16% from high-end specialty stores, 11% from cooperatives, 9% from mass merchandisers, and 7% from discount stores. In the questionnaire, we also asked participants how important certain characteristics of locally grown and organically grown fresh produce were to their purchase decision using a Likert scale with 1 being not important and 5 being very important. The percentage of participants who considered different characteristics at various levels of importance are listed in Tables 4 and5. The characteristics were chosen based on previously published studies (Loureiro and Hine, 2002;King, 2007;Yue et al., 2007Yue et al., , 2008)). For locally grown fresh produce, 83% of participants considered freshness as ''very important'' and 72% of them rated safe to eat as ''very important.'' Forty-eight percent of participants thought lower price ''somewhat important'' and 17% of them regarded it as ''very important.'' Supporting the local econ-omy was a good reason for people to choose locally grown fresh produce as indicated by the fact that 74% of participants chose 4 or 5 on the Likert scale. Environmentally friendly was also considered to be important for 61% of participants. Carbon footprint was a concern for 54% of participants, whereas 25% of participants did not regard it as a major concern. For organically grown fresh produce, 52% of participants considered good for health to be ''very important'' and 26% of them thought it to be ''important'' (they chose 4). Forty percent of them regarded lower price as ''somewhat important'' and 42% of them chose 4 or 5, which means that these people thought that lower price was ''important'' or ''very important'' for organic fresh produce. Approximately 83% of participants thought safe to eat as ''important'' or ''very important'' for organic fresh produce and 67% of them considered environmentally friendly as ''important'' or ''very important.'' These results were consistent with previous studies showing that consumers associate organic production methods with a reduced health risk and may choose to reduce the risk from pesticide residues by switching to organically grown products (Magnusson and Cranfield, 2005;Williams and Hammitt, 2001). Recent survey evidence shows that consumers purchased organic foods because they perceived the foods to be fresh (68.3%), better for health, and a better source of nutrition (67.1%; Whole Foods Market, 2005). Furthermore, over 70.3% of the con- The numbers in parentheses were the percentages of participants who belong to the corresponding categories. x Participants were told to choose all that applied. sumers said they bought organic food or beverages to avoid pesticides. To get some idea of participants' past purchases of locally grown and organically grown produce, we asked participants ''When you buy fruits and vegetables, how often do you buy locally grown (organically grown) fresh produce when locally grown (organically grown) options are available?'' For locally grown produce, 14% of participants chose ''always,'' 40% chose ''most times,'' 38% chose ''sometimes,'' and 8% chose ''seldom'' or ''never.'' For organically grown produce, 6% chose ''always,'' 15% chose ''most times,'' 39% chose ''sometimes,'' and 40% chose ''seldom'' or ''never.'' The estimation results of the mixed logit model are shown in Table 6. The P value for the likelihood ratio test was <0.01, which indicated that the model fit the data well. Although some variables have a naturally interpretable metric, others do not, especially ordinal variables and interaction effects (McCall, 2001). Therefore, to simplify interpretation of parameters associated with the quality attributes, variables except organic, local, price, and real and the associated interaction effects among them were standardized with a mean of zero and a SD of one. The standardization was done by subtracting the respective variable's mean and dividing by its SD. We found that the P values for the correlations among the scenarios and the responses from the same participant were pretty large (>0.95). Therefore, the correlations were deemed to be highly insignificant. Price premium for local and organic attributes. The intercept and coefficients of organic, local, price, and organic*local were highly significant. The negative sign of the coefficient of price indicated that the higher the price of the product, the lower the proba-bility that participants would be likely to choose the product. The intercept divided by the coefficient of price is the consumer WTP for 1 lb of conventional tomatoes and the amount was $1.18. The coefficient of organic divided by that of price is the premium the consumer was willing to pay for 1 lb of organic tomatoes, which was $0.72. Similarly, the premium for the local attribute was $0.73. The premium for locally grown, organic tomatoes was not the sum of the two premiums, which was indicated by the highly significant negative coefficient of organic*local. Instead, the premium was $1.16 (the sum of coefficients of organic, local, and organic*local divided by the coefficient of price). Hypothetical bias. The marginal significant coefficients of real*price (P = 0.09) indicates that there might exist hypothetical bias for the WTP elicited using the hypothetical experiment compared with the nonhypothetical choice experiment. By calculation, we found that the hypothetical bias for organic attributes was 7.5% of the premium elicited using the nonhypothetical choice experiment. The hypothetical bias for the premium of the local attribute was 9% of the premium elicited using the nonhypothetical choice experiment. Relative to the $1.16 premium for the local plus organic attribute in the hypothetical experiment, participants were willing to pay a premium of $1.07 for 1 lb of locally grown, organic tomatoes in the nonhypothetical choice experiment. In this respect, our results are in line with the large literature on hypothetical bias in valuation studies. Previous studies have shown that the difference between the real and hypothetical study is not fixed with the changes in WTP (Yue et al., 2008). Some previous studies have found that hypothetical bias is quite high when pictures are used in a hypothetical experiment compared with a nonhypothetical experiment using real products (see, for example, Alfnes, 2003;Harrison and Rutström, 2005;Yue et al., 2008). In contrast, we found that hypothetical bias was not that high when real products were used in the hypothetical experiment. The results indicated that even if hypothetical bias might not be completely eliminated as found by Lusk and Schroeder (2004), it can be significantly decreased when real products were used to elicit consumer WTP for certain attributes in a hypothetical experiment. The findings have very important implications for future studies on consumers' WTP literature. Nonhypothetical experiments should be used; if the employment of a nonhypothetical experiment is not practical, real products rather than pictures of products should be used to lower any possible hypothetical bias. Sociodemographic background of consumers. We found that consumers' sociodemographic backgrounds affected their choice and WTP for conventional, organic, local, or locally grown, organic tomatoes. The interaction effect between age and organic was significant and negative, which indicated that older participants were less likely to choose organic tomatoes compared with younger participants. Older participants were more likely to purchase local tomatoes compared with younger participants, but the coefficient was not highly significant (P = 0.13). The significant and positive coefficient of income* organic meant that wealthier participants were more likely to purchase organic tomatoes. This result was in line with other studies on organic attributes of fruits and vegetables (see, for example, Yue et al., 2008). The coefficient of gender*local was significant and negative, which means that female participants were more likely to purchase locally grown tomatoes compared with male participants. In the questionnaire, we asked participants if they belonged to any groups associated with the environment. The estimation results showed that those participants who belonged to environmental groups were least likely to purchase conventional tomatoes and were most likely to purchase locally grown tomatoes followed by organic tomatoes. These results were similar to those by Yue et al. (2007), in that consumers who belonged to environmental groups were more likely to purchase organic apples. Participants' educational backgrounds affected their choice of tomatoes with different attributes. Participants with higher educational levels were more likely to purchase conventional tomatoes and less likely to purchase organic tomatoes. Participants who live in urban areas were most likely to purchase organic tomatoes followed by locally grown tomatoes and were least likely to purchase conventional tomatoes compared with participants from rural areas. One possible explanation is that consumers who live in urban areas may be less likely to grow fresh produce than those who live in rural areas. People experienced with growing products know that there is not as large a quality difference between locally produced fruits and nonlocally produced fruits compared with those who have no growing experience (Yue et al., 2007). Hartman (1997) also showed that the consumption of organic food was skewed in urban areas. We found that household size did not significantly differentiate participants in their choice of tomatoes with different attributes. However, participants with children younger than age 12 years had a higher chance of choosing locally grown tomatoes even if the coefficient was not highly significant (P value was 0.13). Also, these participants were more likely to choose organic tomatoes and less likely to choose conventional tomatoes as indicated by the positive sign of the coefficient of child*organic and the negative sign of the coefficient of child, even if the estimated coefficients were not statistically significant. Store categories. The estimation results showed that participants bought different types of tomatoes from different types of stores. Participants who purchased most fresh produce from mass merchandisers were most likely to choose conventional tomatoes, less likely to choose locally grown tomatoes, and least likely to choose organic tomatoes. Participants who purchased fresh produce mainly from cooperatives had the highest probability of purchasing organic tomatoes, followed by locally grown tomatoes, and they had the lowest probability of purchasing conventional tomatoes. Participants who bought the most fresh produce from specialty stores were more likely to purchase organic tomatoes and less likely to purchase locally grown tomatoes, but the results were not statistically significant. Those participants who often patronize supermarkets were most likely to purchase conventional tomatoes, followed by organic tomatoes, and they were least likely to purchase locally grown tomatoes. Participants who visited discount stores most often were less likely to purchase organic tomatoes. Those participants who often patronized roadside stands and farmers' markets were most likely to purchase locally grown tomatoes, less likely to purchase organic tomatoes, and least likely to choose conventional tomatoes. Marketing implications. These results have important marketing implications for fresh produce growers, wholesalers, and retailers. A large proportion of consumers consider ''freshness'' and ''safe to eat'' as ''very important'' for purchasing locally grown fresh produce, which indicates that these attributes should be stressed by local growers when they promote their products in advertisements, newsletters, and so on. As for organic fresh produce, ''good for health'' and ''safe to eat'' are their selling points and should be emphasized in promotional materials. There exists a group of consumers who view ''lower price'' as important for local and organic fresh produce, which was consistent with findings of previous studies that premium price was the major obstacle to buying organic foods for some consumers (Magnusson et al., 2001). To promote to this price-sensitive segment, emphasis on ''affordability'' by decreasing the price premium to a certain level will work to increase sales of organic and local fresh produce. The sociodemographics of consumers who choose conventional, local, or organic attributes have important implications for choices of target markets for fresh produce with differentiated attributes. Older female consumers with lower than average education levels who shop in urban markets are the right target market to promote locally grown fresh produce, whereas younger consumers with above-average income who live in urban areas are the main target market for organic produce. The average price premiums for organic, local, and organic plus local attributes of tomatoes were $0.67, $0.67, and $1.06, respectively, compared with conventional tomatoes after correction of hypothetical bias. These price premiums give growers guidelines when they decide what prices to charge for tomatoes with different attributes. Comparing prices that growers, wholesalers, and retailers actually charge with these premiums can provide these parties with information about whether their charged prices were consistent with consumers' perceptual values of these different attributes. Based on this information, the associated parties can make adjustments on their pricing strategies to attract more potential consumers. Finding out these price premiums can also help growers choose the right production methods and profitable value-added attributes to promote by comparing the premiums with the associated production and marketing costs. Interestingly, we found that the average premiums consumers were willing to pay for organic tomatoes and locally grown tomatoes were about the same. However, the profitability of the two types of produce might not be the same as a result of variation in production costs. Local growers can get a higher premium if they grow organic tomatoes and promote them as both locally grown and organically grown instead of organically grown alone in local markets. The results show that consumers actually patronize different stores to buy fresh produce with different attributes. Therefore, growers and wholesalers need to choose the right stores to sell their products, and retailers need to source the right types of produce for these parties to make profits. Mass merchandisers and supermarkets are the major places consumers purchase conventional produce, which means that conventional fresh produce growers need to choose these retail outlets as their major venues to sell their products. Cooperatives are where most consumers purchase organic produce, indicating that organic fresh produce growers might be able to sell more of their products and can charge a relatively higher price if they choose to sell their products at cooperatives. In contrast, discount stores and mass merchandisers where people expect to get low-priced products are not the major place for growers to sell organic produce. Supermarkets might be another choice for selling organic fresh produce. For locally grown fresh produce, consumers are most likely to purchase them from farmers' markets and roadside stands. These direct marketing channels are the best venues for local growers to sell their products. Cooperatives are also good venues to sell locally grown products, whereas mass merchandisers and supermarkets might not be able to gain enough profits by selling locally grown fresh produce. From the supply perspective, the availability of organic and locally grown produce in different stores also contributes to consumer patronage of these stores in purchasing fresh produce with different attributes. Fresh produce provides the greatest chances for direct sales to consumers through farmers' markets and roadside stands that are commonly used by small-scale growers. Given the fact that most local growers are small scale in Minnesota, it is not surprising that consumers are most likely to choose direct marketing channels to purchase locally grown produce. The growth of organic food has given rise to new marketing outlets in addition to traditional organic outlets such as cooperatives. Mass market groceries and chain stores entered the organic market place at a time of low prices for commodities at the farm gate. However, it has been documented that approximately one third of organic products are sold through mass market grocery stores with a disproportionately higher percentage of processed food and dairy products instead of fresh produce (Klonsky, 2000). In recent years, some mass merchandisers like Walmart started to source and advertise organic fresh produce and they could get a significant market share if they are able to keep prices low enough (Business Week, 2006) The hypothetical and nonhypothetical experiments were conducted at the Minnesota State Fair. One issue to consider is whether the results are representative of consumers in general (Mitchell and Carson, 1989). In our survey, the sample chosenthose who participated in the Minnesota State Fair-does not necessarily represent all fresh produce consumers in the United States, and in this regard, there is some population choice bias. The ''often'' variable indicated that most of participants were regular fresh produce consumers, so we expect the sample selection bias to be minor. We thus view our sample respondents as representing a population of the central Midwest instead of representing the total U.S. population. Given the potential biases, the extrapolation of our findings to other populations should be made with caution. We interpret the findings relative to the population attending local and regional fairs and markets.",
          "To assist small-and medium-sized growers who want to capitalize on growth in demand for differentiated produce, this article presents consumer WTP for the organic, locally grown, and locally grown organic attributes, the characteristics consumers perceive as important when they make purchase decisions of these differentiated fresh produce, and the various types of retail venues where consumers purchase these different products. A combination of the traditional hypothetical experimental method and nonhypothetical choice experiment was used to investigate consumer preference for valueadded fresh produce. The combination of these two methods can help eliminate any possible hypothetical bias in the hypothetical experimental method. We found that hypothetical bias existed using the hypothetical experimental method, but the bias was not high if real products were used in that survey. We elicited consumer WTP for local and organic attributes for fresh tomatoes. We found consumers perceived various characteristics as important for their purchase decision of organic and local fresh produce. Consumers' sociodemographic backgrounds affected their choice of fresh produce with different attributes and consumers patronized different types of stores to purchase conventional, organic, and local fresh produce."
        ],
        "ground_truth_definitions": {
          "hypothetical bias": {
            "definition": "the difference between what people say they will pay and what they would actually pay",
            "context": "We found that when real products were used in the hypothetical experiment, the hypothetical bias (the difference between what people say they will pay and what they would actually pay) was not high.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10462-022-10338-7",
        "sections": [
          "Recent years have shown a growing interest in the application of natural language processing techniques for extracting, summarizing, and creating new data from text. Today's digital media ecosystem generates massive streams of unstructured data, such as news articles, web pages, blog posts and tweets, thus posing a set of challenges related to their automatic understanding. Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narrative can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time. Although several attempts have been conducted in the domain of narrative extraction, some problems remain unsolved, which are not easily solved by the currently available Natural Language Processing (NLP) and Information Extraction (IE) techniques. Important challenges in this domain involve defining annotation schemes that are comprehensive enough to include the relevant features of the narrative elements, but, at the same time, not too cumbersome to avoid overloading the process of extraction; automatically extracting cohesive narratives from unstructured data with high effectiveness; detecting and characterizing storylines over single and multiple documents; identifying narrative segments within large portions of text; dealing with informal language and with different types of documents (e.g., social media and micro-blogs); coping with different human languages and with combinations of languages; defining meaningful formal representation of narratives as an intermediate step to visualization schemes; devising a standard evaluating narrative framework (made of datasets, baselines and metrics) as an entry-point for researchers interested on properly evaluating their methods; and contributing with the development of specialized linguistic resources for low-resource languages, such as narrative annotated data, to avoid creating niches. To help the research community address these challenges, different techniques can be applied depending on the type and scale of the narrative, e.g. single news articles versus whole fiction books. In this paper, we aim to survey existing approaches and techniques related to the process of extracting narratives from a text, an emergent theme of research within the field of computational linguistics and artificial intelligence. To the best of our knowledge, there is currently no survey addressing the full process of narrative extraction from textual data. Hence, collecting, organizing, documenting and describing fundamental concepts of this process is of paramount importance for different related areas, where the extraction of narratives plays an important role. Domains such as journalism (Caselli et al. 2016), finance (El-Haj 2022), health (Sheikhalishahi et al. 2019) , information access in digital libraries (Kroll et al. 2022), and other fields that require analyzing narratives over a common topic using NLP approaches, are among the main beneficiaries of this survey. Such communities are very active in the area, which makes it quite hard for a researcher to be aware of all the relevant contributions. It should be noted that in the different contexts where narratives can be extracted, there are common problems; however, there is also terminology, notation and specificities for each one. While in a financial narrative, there is a need for structure extraction (El-Haj et al. 2019) given the format in which the data are generally available, in narratives related to historical events (Lai et al. 2021), there is no such need. In this work, such particularities will not be discussed; instead, we will seek to focus on tasks that are common to narrative extraction from a general point of view. In an attempt to provide a thorough account of relevant research developed within the field of computational linguistics and artificial intelligence (AI), we conduct a survey where the different approaches are organized according to the pipeline of tasks one can find in the extraction of narratives from text. The development of this study began with the selection of fundamental articles understood as key to the subareas that make up the extraction of narratives. From these articles, we apply the snowball procedure (Wohlin 2014) to expand the pool of articles by considering relevant research published in high-quality conferences and journals in the fields of natural language processing and artificial intelligence. The remainder of this paper is organized as follows. Section 2 gives an overview of the process of extracting narratives from a text by introducing the fundamental concepts, the data acquisition and annotation effort, and the narrative extraction pipeline. Section 3 presents the initial step of narrative extraction that comprises the pre-processing and parsing of a text. Section 4 introduces research developed on the identification and extraction of narrative components, namely lexical and syntactical components. Section 5 presents the techniques found in the literature to establish the linkage between the identified narrative components. Section 6 refers to research work focused on the representation of narratives structures. Section 7 provides a snapshot of the metrics and the datasets behind the evaluation efforts. Section 8 promotes a discussion of some important aspects of narratives research including open issues. Finally, Sect. 9 concludes this paper by presenting its final remarks.",
          "In this section, we begin by presenting a detailed definition of the concept of narratives and its associated terminology. Following, we discuss important steps related to data acquisition and annotation. Finally, we describe the general pipeline behind the process of extracting narratives from textual data through the lens of a computational scope. The pipeline here introduced will define the structure of the rest of this paper.",
          "Narratives have long been studied in the field of linguistics. One of the first authors to introduce a formal definition was Adam (1992), who considers narratives to be a prototypical sequence obeying a thematic unity, regarding a chronological succession of events involving characters. The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theatrical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community. Riedl (2004) considers narratives as a cognitive tool for situated understanding, i.e., a structure designed to better understand the world around us. Motta (2005), instead, understands narratives as forms of relationships that are established due to culture, the coexistence between living beings that have interests, desires, and that are under the constraints and social conditions of hierarchy and power. As the author states, whoever narrates has some purpose in narrating, therefore no narrative is naive. The same narrative can be seen from a different point of view, i.e., from a focal point for seeing, hearing, smelling, and experiencing the accessible. Stripping the data of unnecessary information and making sure that relevant metadata is kept is also key during this process. A case in point related to narrative extracted from news is the publication date, which is relevant to determine its timeline. What follows is designing a suitable annotation scheme that is simultaneously tailored to encompass all the particularities of the target language(s), and comprehensive enough to be applied to other datasets or even be broaden. Bearing in mind, on the one hand, the diversity of annotation frameworks, and, on the other hand, the usefulness of establishing comparisons between annotated corpora from different genres in the same language, but also across languages, many proposals try to achieve this balance utilizing acknowledged standards, which have resulted in, for e.g., Ontologies of Linguistic Annotation (OLiA) (Chiarcos 2014), and ISO 24617-Language resource management-Semantic annotation framework. The decision about the annotation framework, and about the different layers of annotation, depends necessarily on a variety of factors, like the annotation purpose, the texts genre, among others (Pustejovsky et al. 2017). In the case of the narratives annotation, since it is relevant to feature participants, events, time and space, as well the relationships between them, the annotation scheme can be designed to include several intertwined semantic layers enabling temporal, referential, thematic, and spatial annotations (see for example Silvano et al. (2021)). The adequacy of the annotation tool is also of great relevance for the efficiency of different tasks, namely creation, browsing, visualization, and querying of linguistic annotations. Although one can choose to tailor a tool to the specific features of one's project, it may be labor-saving to resort to the existing ones and, if necessary, proceed with some modifications. Some of the existing annotation tools that enable annotating different markables with several attributes, as well as establishing links between those markables are the following: MMAX2 (Müller and Strube 2006), MAE and MAI Stubbs (2011), BRAT (Stenetorp et al. 2012), or ANNIS (Krause and Zeldes 2014). During the process of narrative extraction, some degree of human participation is necessary, either to verify the feasibility of the annotation scheme, to annotate linguistic aspects for which automatic models are insufficient, or to supervise the automatic annotation. Different strategies can be adopted depending, namely, on the type of annotators that the project wants or needs, with or without linguistic training, specifically: crowdsourcing (Estellés-Arolas and de Guevara 2012), class sourcing (Christopher Blackwell 2009) or gamification (Stieglitz et al. 2016). When using human annotation, calculating interannotator agreement (IAA) is crucial to ensure its reliability and accuracy. The two most used metrics in computational and corpus linguistics are: Cohen's Kappa (and its variation, Fleiss's Kapppa) and Krippendorff 's Alpha (Pustejovsky and Stubbs 2012).",
          "The study of narratives from the perspective of natural language texts can be summarized into five major stages: (1) Pre-Processing and Parsing; (2) Identification and Extraction of Narrative Components; (3) Linking Components; (4) Representation of Narratives and (5) Evaluation. Each of these tasks gives rise to the structure adopted in the rest of this survey. An overall picture is shown in Fig. 1, and a detailed description of each task can be found in the following sections.",
          "Pre-processing and parsing of a text comprise a set of lexical and syntactic tasks. Lexical activities aim to split the text into basic units, called tokens, and to normalize them into different forms; syntactical analysis identifies the grammar class of tokens, and identifies the dependency between the tokens, producing trees or chunks that relate the tokens to each other. The following subsections detail both tasks.",
          "Lexical tasks seek to standardize the input text as a way to better prepare the data content for the following steps (Sun et al. 2014). Despite being a simple procedure, they are considered to be a key factor to achieve effective results (Denny and Spirling 2018). The main lexical tasks required for the narrative extraction process are: (1) Sentence segmentation, i.e., dividing the whole text into sentences and tokens to gather the lexicon of the language (Palmer 2007); for this task, using deep learning (DL) methods Schweter and Ahmed (2019) achieved SOTA results through the use of a multi-lingual system based on three different architectures of neural networks. (2) Text tokenization, applied to break sentences into tokens, the smallest unit of a text (Vijayarani et al. 2016); and (3) Text Cleaning and Normalization, which may involve a number of optional cleaning steps, including removing numbers, punctuation marks, accent marks, or stopwords, as well as apply stemming (Jabbar et al. 2020) and lemmatization (Bergmanis and Goldwater 2018) to observe other possible discrepancies between equal or similar words. These are foundational NLP tasks; hence they will not be explored in detail in this article. More details regarding these tasks can be found in Raina and Krishnamurthy (2022).",
          "The lexical analysis precedes a more profound observation, which aims to identify the grammar class of the words and to infer the relationship between words in a sentence to help understand the meaning of a text. To go over this process, we use the text presented in Fig. 2 as a running example in the remainder of this survey. ",
          "The first step in this stage is to assign parts-of-speech tags to each word of a given text (e.g., nouns, verbs, adjectives) based on its definition and its context, which is called the Part-of-Speech (PoS) tagging task. The current state-of-the-art is settled by Bohnet et al. (2018) in several languages. In this work, the authors used recurrent neural networks with sentence-level context for initial character and word-based representations, achieving an accuracy of 97.96% on top of the Penn Treebank dataset Marcus et al. (1993). With respect to narrative texts, since their elements can be typically associated to different word classes (for instance, nouns to participants, verbs to events), POS tagging plays a fundamental role (Palshikar et al. 2019;Quaresma et al. 2019;Yu and Kim 2021). Figure 3 shows the result of applying the Stanzafoot_0 library (Qi et al. 2020) PoS tagger to the first sentence of our running example. In the figure, PROPN refers to a proper noun, PUNCT to punctuation, DET to determiner, VERB to a verb, and ADP to adposition. These tags are defined under the Universal Dependencies guidelines (de Marneffe et al. 2021).",
          "Following PoS tagging, the parsing of the text can be conducted. The chunking task (a.k.a, shallow parsing) is responsible for identifying constituent parts of the sentences (nouns, verbs, adjectives, etc.) and linking them to higher-order units that have discrete grammatical meanings (noun groups or phrases, verb groups, and others). Figure 4 illustrates the results of applying chunking on top of the first sentence of our running example through the use of the benepar library (Kitaev et al. 2019). By looking at the figure, we can observe that both \"a\" (determiner -DT), as well as \"rabbit\" (noun-NN), belong to a higher group of noun phrases (NP). Note that adpositions (tagged as \"ADP\" in Fig. 3) are now represented in Fig. 4 with the \"IN\" tag. As in other tasks, the application of deep learning approaches (Hashimoto et al. 2017;Zhai et al. 2017;Akbik et al. 2018) has brought important improvements to this particular task. The next step of the pipeline is to understand how all the words of the sentence relate to each other, which is done by a dependency parsing analysis. The objective, as referred by Jurafsky and Martin (2009), is to assign a single headword to each dependent word in the sentence through labeled arcs. The root node of the tree, that is, the head of the entire Fig. 2 Running example Fig. 3 Results of applying the stanza PoS tagging processor to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" structure, will be the main verb in the sentence (e.g., \"pulled\" in our running example). This task can enhance the performance of named entity recognition (further discussed in Sect. 4.2) models and be used to increase information extraction (de Oliveira et al. 2022). Figure 5 shows a dependency analysis of the first sentence of our running example alongside the type of relationship predicted to occur between two tokens. The results obtained stem from applying the Stanford CoreNLP Parser (Chen and Manning 2014), one of the most well-known tools in this regard. A quick look at the figure emphasizes the association between several terms, among which we highlight the relationship hierarchy between \"John\" and \"magician\". The current state-of-the-art of this task, in the English and Chinese languages, is settled by Mrini et al. (2020). The model proposed by the authors used a combination of a label attention layer with Head-driven Phrase Structure Grammar (HPSG), and pre-trained XLNet embeddings Yang et al. (2019). In a narrative context, the application of these tasks can be particular useful to identify relations between narrative components (further discussed in Sect. 4). Fig. 4 Results of applying the benepar chunking model to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Fig. 5 Results of applying the Stanford CoreNLP Parser Dependency to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\"",
          "Coreference resolution, also known as anaphora resolution, uses information about the semantic relation of coreferent expressions to infer the reference between them (Jiang and Cohn 2021). In addition to that, all the terms that reference the same real-world entity are marked (Mitkov 2014). For example, the word \"presentation\" (found in sentence 2 of our running example) refers to \"show\" (which can be found in sentence 1). Coreference resolution is essential for deep language understanding, and has shown its potential in various language processing problems (Poesio et al. 2016). Bringing it to the context of narrative extraction, some works highlight the importance of this task in the most different scenarios, such as clinical narratives (Jindal and Roth 2013a), newswire (Do et al. 2015), and also in violent death narratives from the USA's Centers for Disease Control's (CDC) National Violent Death Reporting System (Uppunda et al. 2021). In a general domain, several approaches have been proposed over the years (Pradhan et al. 2012;Mitkov 2014;Lee et al. 2013), but recently there has been a shift towards adopting transformers models (Ferreira Cruz et al. 2020;Kantor and Globerson 2019) to improve the connection between textual elements and, as such, enhance the coreference task. The current SOTA of coreference resolution is marked by Attree (2019) and Kirstain et al. (2021), both sticking to deep learning methods.",
          "Table 1 presents the current status of the tasks covered in this section. This includes lexical tasks (sentence segmentation, text tokenization, data cleaning, normalization, stopwords removal, stemming, lemmatization) and syntactic (PoS tagging, chunking, dependency parsing, coreference resolution). Herein, however, we only list those that have been undergoing constant improvements over the most recent years. In this table, the effectiveness of each study is shown according to the measures reported by the authors.",
          "The next step in the narrative extraction pipeline is to identify and extract the main elements that compose a narrative. This step comprises tasks like the detection and classification of events, the recognition and the classification of named entities (participants), the extraction of temporal information and of spatial data.",
          "Finding the events mentioned in the text is an essential step towards the extraction of narratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person. Xie et al. (2008) went a little bit further, and suggested describing real-world events through the use of the 5W1H interrogatives: when, where, who, what, why, and how, anchored on journalistic practices. Based on this reference, an event such as the one referenced in our running example might be depicted along with the six aspects: who-John, the magician; when-2022 (taking into account the date of writing the text); Table 1 State-of-the-art in pre-processing and parsing tasks Task Ref. Method Effectiveness Benchmark dataset Sentence segmentation Schweter and Ahmed (2019) BiLSTM 98.62 F1-score (English) Europarl Corpus Koehn (2005) 99.70 F1-score (Greek) SETimes Alperen and Tyers (2010) 99.86 F1-score (Arabic) Leipzig Corpus Goldhahn et al. (2012) Text tokenization Graën et al. (2018) Language-specific and language independent token identification rules 1% Error rate SMULTRON Treebank Volk et al. (2015) Lemmatization Bergmanis and Goldwater (2018) Standard encoder-decoder architecture + character-level sentence context 92.4 Accuracy (ambiguous words) Universal Dependency Treebank v2.0 Nivre et al. (2020) 84.30 Accuracy (unseen words) Part-of-speech tagging Bohnet et al. (2018) Recurrent network model 97.96 Accuracy Penn Treebank Marcus et al. (1993) Chunking Akbik et al. (2018) Neural character-level language modeling 96.72 F1-score Penn Treebank Marcus et al. (1993) 96.34 F1-Score Dependency parsing Mrini et al. (2020) Label attention layer + HPSG + XLNet 97.39 UAS Penn Treebank Marcus et al. (1993) 96.29 LAS Coreference resolution Attree (2019) Neural attention model 92.50 F1-Score GAP Dataset Webster et al. (2018) where -XYZ contest realized in Edinburgh; what-the magician pleased the audience during his presentation; how-taking a rabbit out of his hat; why-his presentation was better than the last one. The task of event detection has its roots in the late 90's when the Topic Detection and Tracking (TDT) project emerged (Allan et al. 1998) as the first attempt to cope with the rising of huge volumes of data. Among the entire project, the detection of new events, also known as the first story detection (FSD) (Kontostathis et al. 2004), was a subtask of the TDT project, concerned with the detection and subsequent tracking of the first and coming stories of a given news event. One way to detect events is to rely on event triggers, cues that express an event's occurrence, most often single verbs or phrasal verbs, as referred by Boroş (2018), but also nouns, noun phrases, pronouns, adverbs, and adjectives. According to Araki (2018), 95% of the ACE (Automatic Content Extraction) event triggers consist of a single token. This is the case of our running example, where the events pulled, loved, presented are triggered by verbs, and show, contest and presentation are also events represented by nouns. Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argument role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 subtypes, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example. The ACE 2005's predefined event scheme is presented in the left-hand part of the figure. The event trigger (\"pulled\"), the 3 argument roles (\"Arg-Entity\"), and 2 modifiers (\"Arg-Time\" and \"Arg-Place\") identified for the type \"Contact-Meet\" type are illustrated on the right-hand side of the figure. In this visualization, the time-reference \"this year\" is normalized to 2022 based on the document creation time (DCT). The current state-of-the-art in event extraction task for the English language is provided by Feng et al. (2018), who developed a language-independent neural network. The authors tested on top of the ACE 2005 English event detection task (Walker et al. 2006) data, achieving an F1-score of 73.40%. More detailed information about the event detection task can be found in Saeed et al. (2019) and a recent survey from Xiang and Wang (2019). In recent years, the task of monitoring streams of data and detection events has made a shift towards microblogs platforms, such as Twitter (Atefeh and Khreich 2015), a very different scenario when compared to the TDT era, when algorithms were developed to track news stories over time from traditional media like newspapers (Kalyanam et al. 2016). The detection of events also plays a fundamental role in different kinds of applications, domains, and languages, as is the case of clinical narratives (Jindal and Roth 2013b;Adams et al. 2021), and historical events (Lai et al. 2021). Despite the significant advances in the last few years, the application of event extraction techniques in the context of narrative representation has been quite limited (Metilli et al. 2019), and only recently a few works began to emerge. Metilli et al. (2019), for example, made use of numerous novel discourse and narrative features, besides common relations, such as the time at which the event takes place, or coreference events, which detects whether two mentions of events refer to the same event (Araki 2018). Aldawsari and Finlayson ( 2019) presented a supervised model to automatically identify when one event is a sub-event of another, a problem known as sub-event detection or event hierarchy construction. Another strand tackles the problem of nested event structures, a common occurrence in both open domain (not limited to a single topic or subject) and domain specific (dedicated to a particular problem representation or solution) extraction tasks (McClosky et al. 2011). For instance, a \"crime\" event can lead to an \"investigation\" event, which can lead to an \"arrest\" event (Chambers and Jurafsky 2009). The research of Metilli et al. (2019) applied a recurrent neural network model to event detection applied to biography texts obtained from Wikipedia. The model used a set of event categories (e.g., birth, conflict, marriage, and others) to depict a narrative. In the context of news stories, Zahid et al. (2019) proposed the development of heuristics to segment news according to a scheme that defines the organization and order of the events. The strategy followed in that research employed mechanisms that journalists usually exploit to compose news.",
          "Another essential category of narrative elements is the participants. They are the \"who\" of the story, relevant to the \"what\" and the \"why\". Participants, sometimes also referred to as actors, are involved in events in a varied number of ways and often correspond to entities in NLP. These, are usually identified through Named Entity Recognition (NER), a generic task aiming at seeking, locating and categorizing the entities mentioned in the text into predefined categories. Although the three most common named entities are person, organization, and location, further types such as numeric expressions (e.g., time, date, money, and percent expressions) can also be considered (Nadeau and Sekine 2007). In narrative terms, participants are more frequently restricted to the person and organization categories, though other categories such as animals may also be used. In our running example this would result in annotating \"John\" and \"audience\" as a person, and \"rabbit\" as an animal as can be observed in Fig. 7. Overall, research on named entity recognition can be categorized into four main categories: rule-based approaches, unsupervised learning approaches, feature-based supervised learning approaches, and deep-learning-based approaches (Li et al. 2022). Beyond this, other aspects, such as textual genres, entity types, and language, are also considered by research conducted on named entity identification (Goyal et al. 2018). The current stateof-the-art in named entity recognition was obtained by Wang et al. (2021) on the CoNLL 2003NER task (Tjong Kim Sang and De Meulder 2003). To automate the process of finding better concatenations of embeddings for structured prediction tasks, the authors proposed Automated Concatenation of Embeddings (ACE) based on a formulation inspired by recent progress on neural architecture search. The method achieved an F1-score of 94.60%. In narrative contexts, Lee et al. (2021) present a proposal that goes beyond the identification of entities in narratives, proposing a multi-relational graph contextualization to capture the implicit state of the participants in the story (i.e., characters' motivations, goals, and mental states). Piper et al. (2021) point out that NLP works focusing on agents have emphasized broadening the understanding of characters beyond named entities. This has been done through the concept of animacy detection, as in agents like \"the coachman\" or \"the frog\", while also distinguishing characters from other named referents (Piper et al. 2021). According to Oza and Dietz (2021)'s proposal, selecting a set of relevant entities for a story construction could be best achieved by using entity co-occurrences in retrieved text passages -especially when the relative relevance of passages is incorporated as link strength. In line with what the authors claim, their proposal is between 80% and 30% more effective than the best link-based approach.",
          "Temporal references are of the utmost importance for the understanding of the narrative and its timeline by anchoring each event or scene at a point in time. The process of identifying temporal references is commonly referred to as Temporal Tagging, and can be split into two sub-tasks: extraction and normalization (Strötgen and Gertz 2013). The former aims to correctly identify temporal expressions, and can be seen as a classification problem. The latter aims to normalize the identified temporal expressions, and can be seen as a more challenging process where different temporal expressions, carrying out the same meaning, need to be anchored at the same time-point. In this regard, three temporal taggers take the lead, Heideltime (Strötgen and Gertz 2013), SuTime (Chang and Manning 2012) and GuTime (Mani and Wilson 2000), which support the four basic types of temporal objects defined by Pustejovsky et al. (2005) in TimeML, the standard markup language for temporal annotation containing TIMEX3 tags for temporal expressions: Dates (e.g., \"December 3, 2021\"), Time (e.g., \"5:37 a.m.\"), Durations (e.g., \"four weeks\", \"several years\"), and Sets (e.g., \"every day\", \"twice a month\"). Alongside with these, other realizations of temporal expressions can be found in a text (Strötgen et al. 2012;Campos et al. 2017), which pose additional challenges. We refer to explicit (e.g., \"April 14, 2020\"), implicit (e.g., \"Christmas day 2019\"), and relative temporal expressions (e.g., \"yesterday\"), a kind of temporal expression that requires further knowledge to be normalized. A discussion of the challenges associated with each one of them can be found in Strötgen andGertz (2013, 2016). Other researchers (Jatowt et al. 2013) devised methods concerning document dating of non-timestamped documents. The rise and continuous development of temporal taggers has paved the way to the emergence of upstream research at the intersection between information extraction and several NLP tasks, where temporal expressions play an important role. For example, in the information retrieval domain, temporal information (Campos et al. 2014) can be used for temporal clustering of documents (Campos et al. 2012) or temporal ranking (Berberich et al. 2010;Campos et al. 2016); in question answering to query knowledge bases (Sun et al. 2018); in document summarization to construct timeline summaries (Campos et al. 2021); and in web archives to estimate the relevance of past news (Sato et al. 2021). At this stage of identification and extraction of narrative components, several works address the intersections between the tasks of identifying and extracting events, entities, and temporal information. Strötgen and Gertz (2012) explored event-centric aspects, considering related temporal information. In particular, they introduced the concept of event sequences, a set (or sequence) of chronological ordered events extracted from several documents. In their work, the geographic dimension played an important role by mapping event-sequences onto a map as well. In contrast, studies such as Agarwal et al. (2018) and Rijhwani and Preotiuc-Pietro (2020) explored the task of recognizing named entities considering time-aware aspects. The incipient annotation of documents to support related research has also led to the development of markup languages (e.g., TimeML (Pustejovsky et al. 2005), a formal specification language for temporal and event expressions anchored on TIMEX3 tags), annotated corpora (e.g., TimeBank corpus (Pustejovsky et al. 2006), a standard set of English news articles annotated with temporal information under the TimeML 1.2.1 (Saurı et al. 2006) guidelines) and research competitions [e.g., TempEval series (Lim et al. 2019)].",
          "The problem of extracting geographic references from texts is longstanding (Martins et al. 2008), and is highly related to the field of Geographic Information Retrieval (GIR) (Purves et al. 2018). Much of this information can be found in unstructured texts through references to places and locations (Purves et al. 2018). Many difficulties arise when attempting to comprehend geographic information in natural language or free text (e.g., under-specified and ambiguous queries) (Purves et al. 2018). Challenges related to space involve the detection and resolution of references to locations, typically, but not exclusively, in the form of place names, or more formally toponyms, from unstructured text documents (Jones and Purves 2008). In our running example, to extract the location of the main event, one can consider \"contests near Edinburgh\", which consists of three important parts, a theme (contests), a spatial relationship (near), and a location (Edinburgh). However, in the example, it is unclear whether there is only one contest in the city or multiple ones, and, if so, to which one the text is pointing to. The spatial relationship \"near\" also brings more questions than answers, making it difficult to understand whether it refers to downtown Edinburgh or some constrained space within the state of Edinburgh. Spatial relationships such as these may be both geometric (obtained using coordinate systems imposed on the real world, such as latitude and longitude) and topological (spatially related, but without a measurable distance or complete direction) (Larson 1996). The current SOTA in toponym detection and disambiguation was achieved by Wang et al. (2019) on the SemEval 2019 Task 12 (Weissenbacher et al. 2019), taking as a basis PubMed articles. In their approach, called DM_NLP, the authors made use of an ensemble of multiple neural networks. In a different approach, Yan et al. (2017) proposed a solution called augmented spatial contexts that learns vector embeddings and uses them to reason about place type similarity and relatedness. When considering space in a narrative context, other issues arise. Geographic maps, landscape paintings, and other representations of space are not always narratives, yet all narratives presuppose a world with spatial extension, even when spatial information is concealed (Ryan 2014 [Online]). Generally speaking, a narrative space is a physically existing environment in which characters live and move. This space is usually identified as a setting, that is, the general socio-historic-geographical environment in which the action takes place (Ryan 2014 [Online]), i.e., grounded by referents in the real world (e.g., the Edinburgh city in our running example) or entirely fictional (Rohan kingdom in The Lord of Rings). Aiming to identify places, some works (Bamman et al. 2019;Brooke et al. 2016) have been exploring entity recognition and toponym resolution in the context of narratives, making it possible to recognize named locations, facilities, and geopolitical entities (Edinburgh, Rohan) within this kind of text (Piper et al. 2021). Another problem is related to coreference resolution. Unlike coreference resolution of named locations, long-document coreference of common items (e.g., the house and the room), which form the narrative universe for many fictional creations, can be tough. As indicated by Piper et al. (2021), many questions can only be addressed by calculating the distance between locations described in the text: how far do Frodo and Sam go on their journey? Systems that allow better inference on spatial information within narratives could provide important insights to get the reader closer to the narrative universe.",
          "Table 2 brings a summary of the current status of the tasks covered at the identification and extraction stage discussed in this section. In this table, the effectiveness of each study is shown according to the measures reported by the authors. The type column specifies the type of task addressed in the works. All the works discussed here refer to a semantic analysis of the text. This contrasts with Table 1 where semantic tasks, but, above all, lexical and syntactic, were addressed. In our supplementary material, made available with this survey, one can find additional benchmark datasets relating to the topics covered in this section. This material has datasets available that can be used as a reference for carrying out the tasks described here. Some of these datasets are available in other languages and might help identify and extract narrative components also in these languages.",
          "After the process of identifying and extracting narrative components, it is crucial to extract the link relations between such pieces. Linking narrative components comprises the core of the extraction of narratives from the text. More than extracting separate elements from texts, establishing relations and detecting structures from them becomes essential to fully understand their meaning. In this stage, extracted pieces of information are connected, structuring the narrative at a global level. Thus, temporal and event linking, entity and  ",
          "Despite concentrating the research community's attention, predicting temporal and causal relations between events and temporal references, and investigating the evolution of an event over time as a whole (Kalyanam et al. 2016), remains a major challenge of the text understanding task (Leeuwenberg and Moens 2018) that goes beyond merely identifying temporal expressions in text documents. Ultimately, understanding a text with regards to its temporal dimension requires the following three steps: (1) identification of temporal expressions and events; (2) identification of temporal relations between them; and (3) timeline constructions. An overview of each one of them is shown in Fig. 8 for our running example. Given an input text, the system determines the temporal expressions and the events (top). Next, it assigns TimeML annotations (middle). Finally, it produces a timeline as an output (bottom). Figure 8a begins by showing the input text. By looking at it, one can easily detect \"this year\" temporal expression (normalized to 2022 assuming that the Document Creation Date-DCD-refers to November 21st, 2022) and the chronology of the events. In our example, the event pulled is carried out by a magician called John. During the show, he pulled a rabbit out of a hat. The presentation was more welcomed by the public than another previous presentation. Despite all the information collected, it is not clear whether the \"last one presented\" (event) was held during the present edition or last year's contest, or even if the \"show\" consisted of only \"pulling a rabbit out of a hat\". Ambiguity in the text might also raise doubts about whether the previous presentation refers to John or another magician. In Fig. 8b, we can observe a possible interpretation of the relation phase, where temporal links are established between the temporal expressions and the events. Finally, in Fig. 8c, we can see the generated timeline. In this visualization, one can observe a reference to the document creation time, the present time (reading time), and the ordering of the events according to the time-reference \"this year\" normalized to 2022. This apparently simple example shows how difficult it may be to understand a text in detail. In the following subsections, we present research tackling the identification of temporal expressions, events and their relations, before exploring timeline constructions. Mostafazadeh et al. (2016b), in their work, proposed a pipeline strategy for extracting events and linking them temporally, without building a timeline. The authors demonstrated that events that follow a protocol or script could be related to each other through temporal associations. These relationships are based on temporal reasoning, and provide a broad understanding of language. In another study, Roemmele and Gordon (2018) applied a neural encoder-decoder to detect events before predicting the relations between adjacent events in stories. The authors assumed that subsequent events are temporally related, and as such the extraction of events and their respective relations can be inferred. The goal is to model events that are the cause of other events and also events that are the effect of other events. Similarly, Yao and Huang (2018) deemed that the order of events in a text is the same as that of the temporal order of them in a narrative. Based on this assumption, the authors used a weakly supervised technique to propose a novel strategy to extract events and temporal relations between events in news articles, blogs, and novel books across sentences in narrative paragraphs. In another proposal, Han et al. (2019a) introduced a strategy where the extraction of events and temporal relations was jointly learned. Hence, the information about one task is employed to leverage the learning of the other. This strategy selected shared representation learning and structured prediction to solve the tasks. The results achieved were superior to the previous works. Ning et al. (2017) suggested a structured learning approach to identifying temporal relations in natural language text. Their solution was evaluated on top of the TempEval-3 data (UzZaman et al. 2013), achieving a temporal awareness [metric provided by UzZaman and Allen (2011)] of 67.2%. In the health domain, Tang et al. (2013) developed a temporal information extraction system capable of identifying events, temporal expressions, and their temporal relations found in clinical texts. In this same domain, Leeuwenberg and Moens (2020) proposed an annotation scheme for extraction of implicit and explicit temporal information for clinical reports identified events, which provides probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. A comprehensive overview of the research conducted over the years on identifying events, time references, and properly connected them is given by Derczynski (2016).",
          "",
          "The next step, after establishing the relationships between events and the temporal entities, is to arrange temporal information in such a manner that the remaining narrative components can be organized with regards to time (Leeuwenberg and Moens 2019). One important aspect of this, as stated by Leeuwenberg and Moens (2019), is the temporal reasoning task, which refers to the process of combining different temporal cues into a coherent temporal view. This is of the utmost importance as previous works have shown that the information presented in the text may not match the elapsed order (Mostafazadeh et al. 2016b). Getting to know the correct order of the events may also enable to dive deeply in the analysis of particular components. Antonucci et al. (2020) employed this notion to investigate the evolution of characters of books over time. The authors trained word embeddings related to different characters in distinct parts of literary texts (e.g., chapters). These embeddings are also called dynamic or temporal embeddings (Bamler and Mandt 2017) since, from them, it is possible to analyze relations between concepts over time. This work demonstrated that characters with strong friendships have similar behavior; therefore, such characters evolve similarly over time. Another outcome of the temporal reasoning task is building the network of events in chronological order, i.e., the text's narrative. Over the years, different researches (Bethard et al. 2007;Mani and Schiffman 2005) have been proposed with one such goal. In Leeuwenberg and Moens (2018), the authors proposed two models that predict relative timelines in linear complexity and new loss functions for the training of timeline models using TimeML-style annotations. On the sidelines, Jia et al. (2021) proposed the use of temporal information as a way to provide complex question answering related to temporal information through knowledge graphs. An overview of the temporal reasoning task, on the extraction, and on how to combine temporal cues from text into a coherent temporal view is presented in a survey of Leeuwenberg and Moens (2019).",
          "Capturing the linking between the pairs of identified entities (or participants) is an important aspect of the narrative understanding. Known as relation extraction task (Qin et al. 2021), it attempts to determine a semantic link between two named entities in a sentence. Performing this task on top of our running example would result in identifying an instance of a pulled relation between \"John\" and \"a rabbit\". Such semantic relations are usually structured in the form of < e 1 , rel, e 2 > triples, where e 1 and e 2 are named-entities, and rel is a relationship type (Batista 2016 [Online]). In our case, it could be exemplified by the triple: < John, pulled, a rabbit > . Considering that in narratives, events are often directly linked, extracting such relationships is also of paramount importance. Semantic relations also encompass objectal relations (i.e., relations between discourse entities seen as extra-linguistic concepts). These relations aim to state how two discourse entities are referentially related to one another (ISO 24617-9:2019 (E) 2000). Other NLP tasks, like Question-Answering Systems (Li et al. 2019), and the creation of Knowledge Graphs (Zhang et al. 2019b), can benefit from identifying such a relation. An example of entity relation extraction employed to build Knowledge Graphs is provided in Han et al. (2019b). In this work, the authors present OpenNRE, an open-source and expandable toolkit for implementing neural models for relation extraction. Using this tool, one can train custom models to extract structured relational facts from the plain text, which can be later used to expand a knowledge graph. The traditional relation extraction can be solved by applying one of the following approaches: (1) rule-based; (2) weakly-supervised; (3) supervised; (4) distantly supervised; and (5) unsupervised. In addition to this type of relation extraction, one can also extract semantic relationships by following Open Information Extraction approaches (OIE) (Batista 2016 [Online]), a research field that obtains domain-independent relations from a text. This task is supposed to extract all kinds of n-ary relations in the text (Xavier et al. 2015). Two methods can be applied by the Open Information approaches (1) rule-based; and (2) data-based. The former relies on hand-crafted patterns derived from PoS-tagged text or dependency parse tree rules. The latter generates patterns based on training data represented as a dependency tree or PoS-tagged text. Over the last few years, neural networks have also made their path in the particular domain of relation extraction (Qin et al. 2021), through different neural network approaches (Baldini Soares et al. 2019;Hendrickx et al. 2010;Nadgeri et al. 2021). Exploiting the narrative structure, (Tang et al. 2021) proposed a Multi-tier Knowledge Projection Network (MKPNet). The strategy was designed to leverage multi-tier discourse knowledge and present a knowledge projection paradigm for event relation extraction. According to the authors, such a paradigm can effectively leverage the commonalities between discourses and narratives for event relation extraction. This study had a focus on the projection of knowledge from discourses to narratives. (Lv et al. 2016) also proposed a strategy that takes advantage of the narrative structure. The authors applied an autoencoder to a set of features, including word embeddings, in a clinical narrative dataset, the 2010 I2B2 relation challenge (Uzuner et al. 2011). Using the same dataset, the 2010 I2B2 relation challenge, and the 2018 National NLP Clinical Challenges (N2C2) dataset (Henry et al. 2020), Wei et al. (2019) proposed two different models to extract relations from clinical narratives. The models used a language model based on transformers to extract the features, and BI-LSTM neural network attention to detect the relations. Another example comes from finances related narratives, in this context a shared task was proposed by Mariko et al. (2022a) with the intend to extract causality between existing relations. In this shared task experiments were done on top of FinCausal dataset (Mariko et al. 2022b), a dataset extracted from different 2019 financial news. The best results were achieve by a team that developed an ensemble of sequence tagging models based on the BIO scheme using the RoBERTa-Largemodel, which achieved an F1 score of 94.70 to win the FinCausal 2022 challenge. In addition to these types of connections, entities can also be also linked to other kinds of data. Knowledge databases can furnish a unique identity to entities, which can enrich the narrative with information, and aid to disambiguate the entities as well. The task of linking knowledge base-entities and narrative entities is called entity linking. In the following section, we will discuss this task.",
          "Over the years, efforts have been developed to explore relations between entities in texts (Freitas et al. 2009;Hasegawa et al. 2004). This willingness to recognize individuals in a document is a significant move towards understanding what the document is all about (Balog 2018). Truly understanding a text requires, however, linking these individuals with other pieces of information. The temporal connection between events is one way to achieve this. Another way is to relate the events of the narrative with entities from a knowledge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008). Formally, the process of linking entities can be understood as the task of recognizing (named entity recognition), disambiguating (named entity disambiguation [Eshel et al. 2017)], and linking entities (named entity linking) with unique entity identifiers from a given reference knowledge-base, such as DBpedia (Auer et al. 2007), or Yago (Suchanek et al. 2007). Given the sentence \"Bush was a former president of the USA from 1989 to 1993\", the idea is to determine that \"Bush\" refers to George H. Bush, an American Politician who served as the 41st president of the United States from 1989 to 1993, and not to \"Bush\" a British rock band formed in London, England in 1992. When connecting an entity with Wikipedia data, such as Wikidata (Vrandečić and Krötzsch 2014), this task is known as Wikification. As emphasized by Szymański and Naruszewicz (2019), when a document is Wikified, the reader can better understand it because related topics and enriched knowledge from a knowledge base are easily accessible. From a system-to-system view, the meanings of a Wikified document's core concepts and entities are conveyed by anchoring them in an encyclopedia or a structurally rich ontology. In the context of a narrative, it might mean a more knowledge-rich narrative. Figure 9 illustrates the results of applying entity linking on top of our running example. By looking at the figure, one can observe the linkage of the detected entities or events in the text with their presence on external sources, such as knowledge bases. One can get insights into who is \"John\", \"Edinburgh\" or the \"XYZ contest\". This information might be helpful not only to add more knowledge but also to disambiguate the case of entities that are associated with more than one concept. In the scope of entity linking efforts, Raiman and Raiman (2018) achieved the state-ofthe-art focusing on a cross-lingual approach, a type system on an English dataset supervised with French data. In particular, they constructed a type system and used it to constrain a neural network's outputs to respect the symbolic structure.",
          "Understanding narrative stories involves the ability to recognize events and their participants, as well as to recognize the role that a participant plays in an event, as \"'who ' did 'what' to 'whom\"\", and 'when', 'where', 'why', and 'how' (ISO 24617-4:2014 ). Generally, this can be achieved by Semantic Role Labeling (SRL). This task assigns semantic roles to the constituents of the sentence (Aher et al. 2010). Contrary to syntactic analysis Fig. 9 Overview of entity linking task result when applied to our running example (such as the ones conducted on chunking, dependency parsing, etc.), it acts on a semantic level, and is responsible for capturing predicate-argument relations, such as \"who did what to whom\"' (He et al. 2018), towards making sense of a sentence's interpretation. SRL recovers the latent predicate-argument structure of a sentence, providing representations that answer basic questions about a sentence's meaning. Figure 10 illustrates the results of applying SRL on top of the first sentence of our running example, through the use of AllenNLPfoot_1 . model, which is the implementation of Shi and Lin (2019). The relation established between the different parts of the sentence can be observed by looking at the figure. In this relation, all the parts are linked by the verb \"pulled (out)\": \"John, the magician\", \"a rabbit\" and \"of a hat\" are the core arguments, while \"at the show in Edinburgh\" and \"this year\" are modifiers, the former conveying spatial information and the latter temporal information. The current state-of-the-art of this task is settled by He et al. (2018) with an F1-score of 85.5%, using a deep learning approach based on a BiL-STM neural network to predict predicates and arguments on top of the OntoNotes benchmark (Pradhan et al. 2013). In a narrative-focused approach, Mihaylov and Frank (2019) proposed the use of linguistic annotations as a basis of a discourse-aware semantic self-attention encoder for reading comprehension on narrative texts. In this work, the authors adopted the 15 fine-grained discourse relation sense types (see section 5.5 for more information about discourse relations) from the Penn Discourse Tree Bank (PDTB). According to what is mentioned by the author, combining such an annotation scheme with self-attention yields significant improvements. Following this approach, this study's results indicate that SRL significantly improves who and when questions and that discourse relations also improve the performance on why and where questions. These results are illustrated on top of the Narra-tiveQA (Kočiský et al. 2018) reading comprehension.",
          "When reading or listening to a text, the reader/listener establishes relations of meaning between the different parts, be they clauses, sentences, or paragraphs. These relations are discourse relations (DRels)-also known as rhetorical relations or coherence relationsand are crucial to explain how discourse is organized. For that reason, they have been the basis of several frameworks, such as Rhetorical Structure Theory (RST) (MANN and Thompson 1988), Segmented Discourse Representation Theory (Asher et al. 2003).",
          "Semantic labeling result when applied to the sentence: \"John, the magician, pulled a rabbit out of a hat at the show in Edinburgh this year.\" Looking at our running example, since no connective is present, the identification of the DRel would be very challenging using automatic methods. However, taking into consideration lexical and other semantic information, as well as our world knowledge, one can infer that the two sentences are related by the DRel Result, because loving the magician's presentation is a consequence of him pulling a rabbit out of the hat. The second sentence can be divided into two arguments/ text unities related by Comparison. Figure 11 illustrates the annotation of this example using an RST markup tool (O 'Donnell 2000). In NLP, the process of uncovering the DRels between text units is called discourse parsing, a very complex task, despite a few advances over the past years. One of the most recognized discourse parsers is the one used by the news corpus Penn Discourse Tree Bank (PDTB) (Prasad et al. 2018). This end-to-end discourse parser (LIN et al. 2014) follows the same steps as PDTB human annotators: identification of the discourse connectives, arguments segmentation, labeling them Arg1 and Arg2, and recognition of the explicit DRel. When no explicit relation is extracted, the second step labels the implicit DRels. The last step consists in labeling the attribution spans (see Potter (2019) for more information about attribution relations), which are text units that reveal if the content should be attributed to the writer or to another participant. The 2021 edition of the DISRPT Shared Task (Zeldes et al. 2021) added the task of Discourse Relation Classification across RST, SDRT and PDTB, to the two existing tasks from the previous edition (Zeldes et al. 2019), Elementary Discourse Unit Segmentation and Connective Detection. The Shared Task was performed in relation to 16 datasets in 11 languages. Overall, the system with the best results was DisCoDisCo (Gessler et al. 2021) with a Transformer-based neural classifier, which was able to surpass state-of-the-art scores from 2019 DISRPT regarding the first two tasks and to obtain a solid score on the 2021 benchmark for the third task. Establishing DRels between events, such as cause, result, or temporal sequence, is of paramount importance to understand the narrative. Much has been achieved with manual annotation of datasets, which has been the basis for the development of some Discourse Parsers, namely within shared tasks (see LI et al. (2021), for a review and future trends). However, many issues need further research so that Discourse Parsing can fully contribute to narrative extraction, in particular concerning the identification of implicit DRels and the specification of the high-level discourse structures. Table 3 State-of-the-art in information linking tasks Task Ref. Method Effectiveness Benchmark dataset Temporal and event linking Ning et al. (2017) Structured Learning 67.20 Temporal Awareness TempEval-3 UzZaman et al. (2013) Named entity linking Raiman and Raiman (2018) (1) Type system, and (2) use it to constrain the outputs of a neural network to respect the symbolic structure. 94.88 micro-precision CoNLL 2003 NER task Tjong Kim Sang and De Meulder (2003) 90.85 micro-precision F1-Score Relation extraction Baldini Soares et al. (2019) BERT-based Model 89.5 F1-score SemEval-2010 Task 8 Hendrickx et al. (2010) Semantic role labeling He et al. (2018) BiLSTM 85.50 F1-score OntoNote Pradhan et al. (2013) Discourse relation parsing Gessler et al. (2021) Transformer-based neural classifier EDU segmentation on Treebanked data -91.48 F1-score (average) Connective detection on Treebanked data -91.22 F1-score (average) Relation classification on Treebanked data -61.82 F1-score (average) DISRPT 2021 Zeldes et al. (2021)",
          "Table 3 brings a summary of the current status of five of the tasks (temporal reasoning, entity relation extraction, entity linking, semantic role labeling, and discourse relation parsing) covered at the information linking stage discussed in this section. The type column specifies the type of task addressed in those works. In this table, each study's effectiveness is displayed according to the measures reported by the authors. All the discussed tasks are related to semantic analysis at this pipeline stage. In our supplementary material, made available with this survey, one can find a list if additional benchmark datasets relating to the topics covered in this section. The datasets mentioned there can be used as a reference for carrying out the tasks described here. Some of these datasets are available in languages other than English and are useful for the tasks of linking narrative components in these other languages.",
          "The representation of narratives can be categorized in two levels, the conceptual and the visual level. The first one considers that the elements in a narrative are codified as concepts that allow abstracting the meaning. This codification allows the exploration and detection of the elements of a narrative, smoothing the process of analysis of a narrative. The second level of representation is built using visual elements like lines, graphs, icons, pictures, or other graphical resources used to depict the narrative. This level of representation is more accessible for people of different degrees of general expertise. Next, we detail both levels of representation.",
          "Ontologies are a flexible and complex framework in computer science that allows building schemes to represent several kinds of concepts. They aid the multi-layered meanings of a narrative to be pictured as faithfully as possible (Ciotti 2016) since the main elements are conceptually identified. Over the years, some ontologies have been proposed as a means to representing narratives. Khan et al. (2016), for example, proposed an ontology for narratives applying it to Homer's Odyssey. Damiano and Lieto (2013), in turn, described an ontology for narratives for the hero's journey, which is a traditional archetype of the histories of the western culture. The ontology is applied in digital archives for artwork, and then to aid the browsing through the digital artifacts. Tests were made in a small dataset, and some information about artworks was obtained by reasoning. In another proposal for cultural and historic events, Meghini et al. (2021) depicted the Narrative Ontology (NOnt). The authors built an ontology that comprises some rules, like laws of physics. Then, they applied it to heritage crafts, and case studies were conducted to validate the proposed model. Although, ontology is a flexible framework, building an ontology that encompasses all the elements in a narrative is a cumbersome task. Thus, few proposals undertake this challenge.",
          "Representing and learning common sense knowledge for the interpretation of a narrative is one of the fundamental problems in the quest for a profound understanding of language (Mostafazadeh et al. 2016a). Logic-based representations, like DRT (Kamp and Reyle 1993) and Abstract Meaning Representation (AMR) (Banarescu et al. 2013), express recursive formal meaning structures that have a model-theoretic interpretation, although following different formalizations. DRT adopts a dynamic and compositional perspective, which entails determining how the meaning of a sentence can change the context, and enables a straightforward and elegant representation of not only anaphoric relations, both nominal and temporal, but also of other linguistic phenomena. Within DRT's framework, the processing of the discourse is performed one sentence at a time in Discourse Representation Structures (DRSs) in an incremental manner. Each sentence is represented in the DRSs by discourse referents, which represent entities in the discourse, always displayed at the top of the box, and by conditions, which establish a relation of identity between the discourse referents and the corresponding element of the sentence, typically displayed below the universe constituted by the discourse referents. These DRSs are recursive formal meaning structures that have a model-theoretic interpretation, and can be translated into firstorder logic (FOL). Asher (1993) and Asher et al. (2003) extended DRT with the inclusion of discourse relations and with contributions from other dynamic semantic proposals and pragmatics, naming it Segmented Discourse Representation Theory (SDRT). The aforementioned semantic representations are used by the Groningen Meaning Bank (GMB) (Basile et al. 2012;Bos et al. 2017) and the Parallel Meaning Bank (PMB) (Abzianidze et al. 2017). While PMB adopts DRT as a single semantic formalism to fully represent the different annotated linguistic phenomena, GMB goes further and enhances DRT semantics with discourse relations between DRSs, using Boxer (Bos 2015;Curran et al. 2007), a system which utilizes -calculus to generate the meaning representation of texts. Another logic-based representation, but for sentences, is the AMR (Banarescu et al. 2013). The sentence is represented as a single-rooted graph (Damonte et al. 2017). The syntax of AMRs can be defined recursively, and it is possible to specify a systematic translation to first-order logic. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The AMR Bank (Banarescu et al. 2013), based on this form of representation, is a set of English sentences paired with simple, readable semantic representations in some cases manually constructed by human annotators, but also generated in a semi-automatic manner (Gruzitis et al. 2018). It is possible to represent narratives using AMR as well. Droog-Hayes et al. (2018), for instance, employ AMR to represent Russian folktales and extract the narrative structure.",
          "Narrative visualization addresses the interplay between narrative and visualization. This includes perspectives that range from the use of visualization elements to enrich traditional narratives, to the exploration of narrative techniques in visualization-rich artefacts. In this section, we focus on the visual and structured representation of a narrative as an important step, not only as a final output of the narrative extraction pipeline, but also as a machinereadable representation that can be used as a tool for human inspection and validation of each of the pipeline steps. Broadly speaking, narrative visualization can be mapped into 7 genres (Segel and Heer 2010): magazine-style, annotated chart, partitioned poster, flow chart, comic strip, slide show, and film/video/animation. As stated by the authors, such genres vary primarily in terms of the number of frames -distinct visual scenes, multiplexed in time and/or space -that each contains, and the ordering of their visual elements. None of these genres are mutually exclusive: they can act as components and be combined to create more complex visual genres. A less explored area is the visualization of narratives themselves, a perspective of particular interest in the context of this survey, where the focus is on the representation of the extracted narrative as closely as possible to its original form. Metilli et al. (2019) developed a semi-automatic software that can import knowledge from Wikidata to allow users to construct and visualize narratives, based on a proposed ontology to annotate narratives. Baikadi et al. (2011), in turn, proposed a framework for visualizing a narrative, and introduced an environment designed to explore narrative visualization to support novice writers. Milon-Flores et al. (2019), instead, applied algorithms to extract emotions and characters involved in literary work to propose a methodology that is able to generate audiovisual summaries by the combination of emotion-based music composition and graph-based animation. The representation of narrative participants and events over time can be accomplished by projecting these elements in two-dimensional layouts. Munroe (2009) introduced the concept of a narrative chart, a visual representation that encodes narrative elements and interactions in a representation where lines flows from left (past) to right (future) representing each participant, and events are depicted using ellipsis to which participants are connected. Kim et al. (2018) presented the Story Explorer, a visualization tool to explore and communicate nonlinear narratives through the representation of story curves, which contrast story order (y-axis) with narrative order (x-axis). Story Explorer is used to analyze and discuss narrative patterns in 10 popular nonlinear movies. Less elaborate visual schemes can also be used to represent the elements from a narrative and provide an accessible understanding of it. A simpler, but powerful resource to represent the elements of narrative, their relationships, and the chronological order of them is based on the use of knowledge graphs (Ehrlinger and Wöß 2016) and their linkage to knowledge-bases (von Landesberger et al. 2011). Li et al. (2018), for instance, employed a graph structure to extract and link narrative events. Amorim et al. (2021) in their work proposed the Brat2Viz tool to build a visual representation of a knowledge graph from a DRT representation of a narrative. A useful visual representation, in this context, is also the Message Sequence Chart (MSC) (Harel and Thiagarajan 2003), a diagram created to specify systems requirements. Due to its flexibility, MSCs are also suitable to schemetize other kinds of processes. In Brat2Viz tool (Amorim et al. 2021), the authors generates a MSC from a DRT as well. Further initiatives also explore MSC for narrative visualizations (Palshikar et al. 2019;Hingmire et al. 2020). A discussion of the challenges associated with narrative visualization, and the general field of information visualization, can be found in de Ponte Figueiras (2016), and also in Tong et al. (2018). The work of Edmond and Bednarz (2021) also addresses the multidisciplinary nature of the field and outline possible trajectories based on the emphasis put on the narrative itself or the data visualization components used.",
          "With the growing maturity and understanding of the narrative extraction process, new procedures have come into play to help formalizing the narrative extraction evaluation step. In particular, shared tasks were proposed to establish a common experimental setup and overcome the lack of datasets. In this section, we discuss the evaluation carried out by studies developed in the area. Representations made of the extracted narratives need to be evaluated for their level of abstraction to the application (e.g., health care systems, games, news, etc.). The evaluation of the extracted narrative is an essential step of its understandability (Riedl and Young 2010) because it aims to observe the content's understanding by the final consumers and specialists, which can provide insights and possible improvements. However, computational evaluation presents hard challenges. In the literature, few studies evaluate narratives computationally extracted or generated since this is a subjective and application-specific task. In cases where the evaluation is considered, a manual evaluation method is usually adopted. In this case, the narrative result is delivered to specific (usually hired) people, who evaluate the generated result from pre-defined domain criteria, as it is done in Motwani et al. (2019), and give their feedback and overall perception of it. However, ways of automatically evaluating the extracted narratives are sought. Goyal et al. (2010), for example, evaluated the representation of narratives texts through plot units measuring the F-score achieved by the model. Other works (Metilli et al. 2019;Zahid et al. 2019) also assessed the tasks developed in the context of narratives in terms of this measure. Research works that use this measure have the advantage of observing a given task's performance isolated, i.e., separately. Narratives, however, need to be evaluated as a whole. Narrative Cloze Test is a common evaluation framework for script learning introduced by Chambers and Jurafsky (2008). Based on the cloze task (Taylor 1953), the narrative cloze evaluation approach consists of a sequence of narrative events in a document where one event has been removed. The evaluation is then done by predicting the missing verb and typed dependency. In the context of narratives, this approach became widely used to explore the commonsense reasoning of narratives outcome. In a similar approach, Mostafazadeh et al. (2016a) proposed the Story Cloze Test. The authors presented this measure as a generic story understanding evaluation framework that can also evaluate story generation models (e.g., by calculating the log-likelihoods assigned by the story generation model to the two ending alternatives), which does not necessarily indicate a requirement for explicit narrative knowledge learning. According to the authors, models that perform well in the Story Cloze Test reveal some deeper understanding of the story. In this work, Mostafazadeh et al. (2016a) also presented a corpus of 50k five-sentence commonsense stories (ROCStories (Mostafazadeh 2016)) developed to enable a brand new framework for evaluating story understanding -bringing a great contribution to the narratives field. Another form of evaluating narratives was proposed by Kočiský et al. (2018) in the Nar-rativeQA Reading Comprehension Challenge. In this challenge, one is presented to a dataset and a respectively set of tasks in which questions about stories must be answered. As pointed out by the authors, these tasks are designed so that successfully answering their questions requires understanding of the underlying narrative rather than relying on shallow pattern matching or salience. Approaches like these are essential to the reasoning of narratives, in the sense that they aim to provide understandable narratives to the end-user focusing on the outcome and in the causality between the events. Another aspect that should be considered when evaluating a narrative is from whose point of view the story is being told (Brahman et al. 2021). As discussed in the beginning of this survey, having access to appropriate datasets is one of the most important steps behind any evaluation procedure. In this context, we make available a summarized list of the most important datasets in several languages that suit the evaluation tasks approached in this survey. We refer the interested reader to check the the supplementary material of this paper.",
          "As exposed in the previous sections, the extraction of narratives comprehends a series of interrelated tasks related to different areas, which amount to an intricate and complicated enterprise. As such, and although much has been accomplished, different challenges need to be met. In this section, we refer to the most prominent, organizing them into general and narrative extraction-oriented.",
          "",
          "Human statements frequently contain ambiguity, error, implicit information, and other sources of complexity. Therefore, the creation of cognitive agents with human-level natural language understanding capabilities involves mimicking human cognition (McShane 2017). As narratives are composed of human declarations, the study of narratives presents a high level cognitive challenge. Some of the problems inherent to narratives are coreference resolution, for instance, when the subject is null, polysemy (i.e., the multiplicity of meanings of a word or phrase), synonymy (i.e., the expression of the same idea with different terms), ambiguity, related, not only to polysemy, but also to syntactic structure, presuppositions or sarcasm. Natural Language Understanding, a research area of NLP, can aid in the resolution of some of these issues. However, a model able to deal with the aforementioned issues, and to perform at a level proximate to human reasoning is still far from being developed.",
          "As demonstrated in this survey paper, narratives are formed by a series of links between involved participants and events, and organized according to their causality over time, which may mean that different documents that report the same event can compose a familiar narrative. However, few works found in the literature of the area explore the extraction of narratives from multiple documents. Future research should propose new methods to automatically identify, interpret, and relate the different elements of a narrative, which will likely come from various sources. This dilemma is related to the fact that current models are often centered on recurrent neural networks (which, although can represent contexts longer than other types of networks, still have limitations for this type of strategy). Working in vast environments (i.e., contexts) is closely connected to natural language and requires scaling up internal processes before larger documents can be handled.",
          "Although narratives are present in a set of diverse languages, most of the data available, however, is mainly in English. One consequence of this is that low-resource languages end up receiving far less attention. For example, while the task of temporal information extraction has drawn much attention in recent years, research efforts have mostly focused in the English language (Mirza 2016), and the development of new solutions for less known languages ends up being compromised by the lack of properly annotated datasets.",
          "Pre-processing and parsing, and the identification and extraction of narrative components, are two of the components that present the best results in the process of narrative extraction. This stems from the fact that the tasks covered by these processing stages have many applications and have been studied across several NLP tasks over the years. The remaining tasks of the narrative extraction pipeline, however, still present many challenges for further enhancements. In the following, we describe some of those challenges, highlighting future directions whenever appropriate.",
          "The annotation process is a key element when working on narrative extraction. Since a significant number of the tasks from the narrative extraction pipeline hinge on the existence of large annotated datasets, the effort of annotation is huge. One of the biggest issue is precisely the lack of manually annotated datasets available with the necessary information. Checking the manual annotations can also be troublesome because, often, the multilayer annotation is so dense that the annotator can barely unravel what was annotated. A useful solution to this is to resort to visual representations such as knowledge graphs and message sequence charters to carry out the supervising task. Nonetheless, further developments on these visual representations are needed. Another problem is related to the inter-annotator agreement. As stated before, narratives are complex and not always straightforward, which reinforces the relevance of assessing the level of agreement between annotators, both during the process of creating the annotated datasets and evaluating the annotation performed by means of automatic methods.",
          "Several challenges related to the temporal domain may be understood as possible reasons that prevent the development of more elaborated solutions in different languages. For instance, the free-text temporal expression (Strötgen and Gertz 2016), which deals with general phrases not covered by standard definitions, the resolution of implicit or relative temporal expressions, or the problem posed in normalizing temporal expressions across different time-zones, are among some of the most well-known reasons. Thus, the most promising approaches rely on cross-lingual transformer language models and cross-lingual sentence embeddings that leverage universal language commonalities.",
          "From the linguistic point of view, some texts have a stereotypical structure already well established in the area (as is the case of medical reports). The knowledge of the domain is essential to understand the structure and context of a document to improve the reasoning process. The integration of different knowledge might provide, for example, a hierarchical structure of terminology that can help to identify whether two different statements refer to the same event. For a semantic and pragmatic analysis, other computational tools, such as automated reasoning systems, are useful. These tools may help to explore how inference processes can clarify pragmatic phenomena such as conversational implications and more explicitly context-based understanding.",
          "Most text processing tools concentrate on extracting relatively simple constructs from the local lexical context and focus on the document as a unit or even smaller units such as sentences or phrases, rather than on relations of different elements within the document or even cross-document relations. Correctly inducing semantic relations, between participants, participants and events or between text spans, within a natural language story is also an open issue in narrative studies. For instance, the task of discourse parsing has still a long road to go (Morey et al. 2017), to achieve discourse relation identification beyond local dependencies.",
          "Implementing a combination of three relation extraction techniques (machine learning, heuristics, and a hybrid combination of both) may be an interesting approach to be explored in the context of narratives. As narratives result from a semantic structuring of information extracted from a text, which presents n-ary relations among themselves, the Open IE exploration might provide meaningful insights to narratives studies. As far as we know, nothing of this kind has yet been proposed.",
          "The narrative perspective, also known as the point of view, is the vantage point through which the events of a story are filtered and then transmitted to the audience. Thus, the same story might have different point of views (POV) depending on the person (narrator/ character) who narrates the story or the angle from which one looks (Al-Alami 2019; Brahman et al. 2021). Extracting narratives from different points of view using the same dataset, combining them in a coherent representation of the story or deciding on which are the most relevant are still open challenges. Considering different narrative points of view also influences the task of evaluating narratives.",
          "Without using some form of a semantic representation that offers an abstraction from the details of lexical and syntactic realizations, comprehension is arguably unattainable. Representing embedded stories -stories told inside a story-is also a complex task that remains open. Gervás (2021) proposed a simple model to represent them. Nevertheless, the author stated that there is substantial further work in the depicted model. Narrative visualization is an active area of research, mostly as a result of a strong community focused on interactive storytelling and the use of data visualization techniques to improve narrative understanding. In contrast, there is ample opportunity for further research in the visual representation of narratives themselves. Existing research is mostly focused on the representation of narrative elements such as participants and events, whereas work on commonly used narrative techniques-e.g., focalization, allegory, personification, amplification-is scarce and represents a challenging opportunity for future research. With the development of the field, research on the proposal of common visual vocabularies and patterns for recurring visual solutions also constitute a pertinent opportunity for further research. Another aspect where research opportunities exist is the study of user interaction with narratives-which degree of manipulation is useful to improve narrative understanding? Which elements and dimensions of a visual representation should be open to user interaction? Finally, evaluating the effectiveness of the visualization of a narrative typically resort to user studies involving adhoc tasks-an opportunity for the development of reference evaluation benchmarks and guidelines exists.",
          "Coming up with a framework that evaluates the narrative extraction pipeline as a whole is a crucial step for further developments. One of the difficulties refers to the process of creating a gold standard dataset, a labor intensive task that is highly dependent on the subjective interpretation of several factors, such as the emphasis put on each narrative participant, or the level of detail to include. Raters may also have differing viewpoints on annotating data or disagree about what details should be kept, contributing to a lower consensus between raters and intra-raters. On the application level, the purposes of evaluation may vary according to the context addressed. In general, the proposed systems should answer questions like: What is this narrative about? Does the system accurately extract the required elements to tell a story from it properly? How accurately and efficiently does the present system represents the narrative in terms of clarity and amount of data to the application context? The processing of narrative in large-scale or from multiple documents also bumps the issue that supervision is expensive to obtain. When considering the evaluation metrics for natural language processing, another issue is related to how well these methods match and generalize to the real complexity of human languages and how many more interesting natural language inference datasets can be generated.",
          "Narratives are an essential tool for communication, representation, and understanding information. Computational systems that are able to identify narrative elements and structures can naturally interact with human users. Such systems understand collaborative contexts as an emerging narrative and can express themselves through storytelling (Riedl 2004). This survey paper provides simultaneously an account of the study of narrative extraction and a roadmap for future research. To this end, we propose a narrative extraction pipeline, defining the key tasks involved in this process identified in the literature. By doing this, we set a common ground for further studies, highlighting the different stages of the narrative process and the most prominent approaches. During the course of this survey paper, we also pointed out extensive literature focused on extracting narratives and supporting NLP tasks. Nonetheless, and despite several recent advances, there are still important open issues demonstrating that narrative extraction is a rich and promising research area that requires multidisciplinary knowledge in the crossroads between linguistics and computation."
        ],
        "ground_truth_definitions": {
          "composition of a narrative": {
            "definition": "is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events.",
            "context": "Much of this content is communicated using natural language containing narratives that refer to stories involving multiple actors and events, occurring in varied locations according to a timeline. The composition of a narrative is given by the entities participating in the story, the events in which they are agents, and the temporal data that defines the sequence of narrated events. In this sense, as stated by Toolan (2013), a narra- tive can be seen as a perceived sequence of non-randomly connected events, a sequence of interconnected facts that are observed over a period of time involving basic elements such as organizations, persons, locations or time.",
            "type": "implicit"
          },
          "Computational Narratology": {
            "definition": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations",
            "context": "The study of narratives from the computational perspective is carried out through a study area called Computational Narratology (Mani 2014). Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "type": "explicit"
          },
          "narrative extraction": {
            "definition": "the use of computational tools for identifying, linking and visualizing narrative elements from textual sources.",
            "context": "Its purpose is to study narrative from the computational and information processing point of view focusing on the algorith- mic processes involved in creating and interpreting narratives, and the modeling of nar- rative structure in terms of formal, computable representations (Mani 2012). In parallel, computational narrative extraction, or simply Narrative Extraction can be defined as the use of computational tools for identifying, linking and visualizing narrative elements from textual sources. A closely related term is Computational Narrative Understanding (Piper et al. 2021), which broadens the perspective to social, personal and cultural dimensions.",
            "type": "explicit"
          },
          "event argument": {
            "definition": "a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event mention": {
            "definition": "a sentence or expression that explains an event, including a cause and multiple arguments.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "event trigger": {
            "definition": "the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "argument role": {
            "definition": "the link between an argument and the event in which it is involved.",
            "context": "Other works consider the case where multiple events appear in the same sentence (Balali et al. 2020). Following ACE (LDC 2005) terminology, we consider an event structure based on four subtasks: (1) Event mention (i.e., a sentence or expression that explains an event, including a cause and multiple arguments); (2) Event trigger (i.e., the key term that demonstrates the occurrence of an event most clearly, usually a verb or a noun); (3) Event argument (i.e., a reference to an entity, a temporal expression, or a value that works as an attribute or individual with a particular role in an event); and (4) Argu- ment role (i.e., the link between an argument and the event in which it is involved). In its annotation guidelines for events, ACE 2005 (LDC 2005) defined 8 event types and 33 sub- types, where each event subtype corresponds to a set of argument roles. Figure 6 illustrates the event extraction process for our running example.",
            "type": "explicit"
          },
          "ontologies": {
            "definition": "formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of representation.",
            "context": "Another way is to relate the events of the narrative with entities from a knowl- edge base, which stores data about entities using ontological schemes (Ehrlinger and Wöß 2016). According to Gruber (1993), ontologies are formal and explicit specifications of a shared conceptualization that can serve as a modeling basis for different purposes of repre- sentation. Representational primitives defined by ontologies are typically classes (or sets), attributes (or properties), and relationships (or relations among class members) (Gruber 2008).",
            "type": "explicit"
          },
          "discourse": {
            "definition": "The expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc.",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "explicit"
          },
          "story": {
            "definition": "the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting)",
            "context": "The events are linked by causality relations, and compose a story with an initial situation, followed by a complication, reactions, resolution and final situation. From a structuralist theory point of view, narratives can be defined, as stated by Chatman (1980), as structures consisting of two parts: (1) a story, the content or chain of events (actions, happenings), plus what may be called the existents (characters, items of setting); and (2) a discourse, that is, the expression, how the content is communicated, for instance, by word (verbal language: oral and written), image (visual language), representation (theat- rical language), etc. The definition of what a narrative is has been dissected by several other authors over the years, reflecting the difficulty in reaching a broad consensus within the community.",
            "type": "implicit"
          },
          "event": {
            "definition": "something significant happening at a specific time and place with consequences.",
            "context": "Finding the events mentioned in the text is an essential step towards the extraction of nar- ratives from a pre-processed text. Formally, an event may be defined (Allan et al. 1998) as something significant happening at a specific time and place with consequences. In the real world, this can be an explosion caused by a bomb, the birth of an heir, or the death of a famous person.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "frai-06-1225093",
        "sections": [
          "Similarly, Visual Question Answering (VQA) systems are known to learn heavy language priors (Agrawal et al., 2016). Thus, a lack of transparency can affect the decision-making process and may lead to the erosion of trust between humans and Artificial Intelligence (AI) systems. This can further jeopardize users' safety, ethics, and accountability if such a system is deployed publicly (Madsen et al., 2021). Considering the utilization of NLP in healthcare, finance, and law domains, all of which can directly affect human lives, it can be dangerous to blindly follow machine predictions without fully understanding them. For instance, a physician following a medical recommendation or an operation procedure for a patient without full knowledge of the system can do more harm than good. In addition, systems employing Machine Learning (ML), such as most current NLP methods, are prone to adversarial attacks where small, carefully crafted local perturbations can maximally alter model predictions, essentially misguiding the model to predict incorrectly but with high confidence (Finlayson et al., 2019). The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model's decision-making process of arriving at a prediction and the ability to be verified and audited. Therefore, models with explainability are interpretable and complete. In this survey, the focus is on explainability and mainly on the outcome explanation problem where Guidotti et al. (2018) describe explainability as \"the perspective of an end-user whose goal is to understand how a model arrives at its result\". In NLP, there exist various explainable techniques such as LIME (Local Interpretable Model-Agnostic Explanations) (Ribeiro et al., 2016), Integrated Gradients (Sundararajan et al., 2017), and ",
          "One of the emerging explainable techniques for NLP applications is rationalization (Atanasova et al., 2020). Rationalization provides explanations in natural language to justify a model's prediction. These explanations are rationales, which present the input features influencing the model's prediction. The reasoning behind the prediction could be understood simply by reading the explanation/rationale, thereby revealing the model's decision-making process. Rationalization can be an attractive technique because it is human-comprehensible and allows individuals without domain knowledge to understand how a model arrived at a prediction. It essentially allows the model to \"talk for themselves\" (Bastings et al., 2019;Luo et al., 2021). This technique is a part of a subset of explainability because it enables models to be interpretable and complete, as shown in Figure 1. Specifically, rationalization provides a local explanation since each prediction has a unique explanation rather than one for the entire model. Local explanations can be categorized into two groups: local post-hoc and local self-explaining. Danilevsky et al. (2020) present local post-hoc methods as explaining a single prediction after the model predicts and local self-explaining methods as simultaneously explaining and predicting. Rationalization in NLP was first introduced in 2007 (Zaidan et al., 2007). As described in Section 4.6, the objective was to use annotator rationales to improve task performance for text categorization. Interestingly, explainability was not the core objective. However, explainability is an advantage of rationalization because it makes the model inherently explainable even if used in the context of task improvement (Gurrapu et al., 2022). Our literature review found that rationalization can be further divided into two major groups: abstractive and extractive (El-Kassas et al., 2021). In extractive rationalization, important features or sentences from the input data are extracted as rationales to support the prediction. In contrast, abstractive rationalization is a generative task in which novel sentences are ). (B) An example of the Abstractive Rationalization approach (Rajani et al., ). generated using new words or paraphrasing existing sentences. This is typically accomplished through the use of a language model such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), T5 (Raffel et al., 2020), or GPT (Generative Pre-trained Transformer) (Radford and Narasimhan, 2018). Figure 2 demonstrate the usage of the two explanation types with examples. Recent advances in explainable NLP have led to a significant increase in rationalization research. Further, at present, the field of rationalization is disorganized. Thus, the motivations for this survey are-(a) formally define rationalization, (b) present and categorize the well-cited techniques based on NLP tasks, and (c) discuss current trends and future insights on the field. Thus, our primary goal is to provide future researchers with a comprehensive understanding of the previously scattered state of rationalization. The key contributions of this paper are as follows. 1. First literature to survey the field of rationalization in NLP.",
          "Explainable AI (XAI). 3. A comprehensive list of details on available rationalization models, XAI evaluations, datasets, and code are provided to guide future researchers. 4. Presents NLP Assurance as an important method for developing more trustworthy and reliable NLP systems.",
          "Table 1 indicates the related survey papers published in recent years. Danilevsky et al. (2020) note that previous surveys in XAI are broadly focused on AI without a specific narrow domain focus. Their work primarily focuses on surrogate-based explainability methods. NLP publications in recent years further demonstrate that this distinction is less relevant and valuable in the NLP domain because \"the same neural network can be used not only to make predictions but also to derive explanations.\" Therefore, surveying the field of Explainable AI (XAI) in NLP requires NLP-specific methods that are different from the standard XAI methods that are widely known. Thayaparan et al. (2020) survey the use of explanations specifically in Machine Reading Comprehension (MRC). The authors describe MRC papers that support explanations and provide a detailed overview of available benchmarks. Further, Madsen et al. (2021) briefly discuss rationalization and natural language explanations using a question and answer approach, CAGE (Commonsense Auto-Generated Explanations) (Rajani et al., 2019) as an example. Thus, this raises the questionhow can rationalization be generalized and applied to other tasks in the NLP domain? However, until now, no comprehensive literature review on rationalization has been available for the prominent NLP tasks. Thus, through this survey paper, we attempt to address this need.",
          "To provide clarity and distinguish terms that are typically used interchangeably in published literature, a list of definitions is provided in Table 2. These terms are used throughout the paper.",
          "The following are the inclusion-exclusion criteria for our publications collection methodology. The first known use of rationalization in NLP was in the year 2007. Our survey focuses on the domain of NLP from 2007 to early 2022. We have included peer-reviewed publications within this range that include a significant rationalization component as a method to provide explainability. We defined significance as rationalization being the main component of their research methodology and approach. We have eliminated a number of publications that are either not entirely in the NLP domain or do not contain a significant rationalization component. For identifying and selecting articles, the range of keywords and topics was limited to the following in the NLP domain: rationalization, explanation, justification, and explainable NLP. Thus, this survey includes reviews of the articles from journals, books, industry research, dissertations, and conference proceedings from commonplace AI/NLP venues such as ACL, EMNLP, NAACL, AAAI, NeurIPS, and others. Finally, these articles are categorized by important NLP tasks, as shown in Table 3. In recent years, there has been an increase in focus on explainability in NLP after a rise in deep learning techniques (Danilevsky et al., 2020). Due to this, a majority of the papers collected were from recent years (2016 and onwards), as illustrated in Figure 3. The availability of relevant articles was limited. After following the above approach, 33 articles were downselected to be the primary focus of this paper's discussion. Instead of providing a broad yet surface-level understanding of the work, we focus on demonstrating in-depth the most important approaches and TABLE Three related survey papers on explainability and interpretability in NLP. No. Survey title Year Papers reviewed 1 A Survey on the state of Explainable AI for Natural Language Processing (Danilevsky et al., 2020) 2020 50 2 A survey on explainability in machine reading comprehension (Thayaparan et al., 2020) 2020 69 3 Post-hoc interpretability for neural NLP: a survey (Madsen et al., 2021) 2021 27 TABLE List of common terms that are used interchangeably in published literature.",
          "",
          "A \"machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans\" (Guidotti et al., 2018).",
          "Interpretability \"aims at developing tools to understand and investigate the behavior of an AI system\" (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to \"explain or to present in understandable terms to a human\" what the AI system is performing.",
          "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
          "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model's output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
          "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain. progress made in each NLP task. Overall, we selected six articles in multiple NLP domains, five on Machine Reading Comprehension and Sentiment Analysis, four on Text Classification, Fact-Checking and Commonsense Reasoning, and three on Natural Languages Inference, and two articles on Neural Machine Translation (NMT). .",
          "In this section, we discuss relevant papers and their rationalization techniques categorized by the NLP tasks listed in Table 3. Tables with important information on the papers for each subsection are presented at the beginning. TABLE Details on seven NLP tasks and their definitions which are surveyed in this paper.",
          "",
          "Enabling a model to answer questions regarding a given context (Baradaran et al., 2022).",
          "Going beyond pattern recognition to make inferences using world knowledge (Apperly, 2011;Sap et al., 2020).",
          "Determining if a hypothesis entails or contradicts a premise (MacCartney, 2009).",
          "Classifying if a claim is either true or false based on evidence (Vargo et al., 2018;Gurrapu et al., 2022).",
          "Quantifying whether the textual data has a positive, negative, or neutral emotion (Zhang et al., 2018).",
          "Categorizing textual data by automatically assigning labels (Minaee et al., 2021).",
          "Translating languages using deep neural networks (Sutskever et al., 2014).",
          "MRC enables a model to answer questions regarding a given context (Baradaran et al., 2022). For this reason, it also frequently referred to as Question Answering (QA) Systems. As shown in Table 4, for MRC applications, we found five recent articles from which three articles provide novel datasets (Ling et al., 2017;Mihaylov et al., 2018;Xie et al., 2020) and the remaining articles Sharp et al. (2017) and Lakhotia et al. (2021) each propose a new MRC framework. The first article, published in 2018, presented a new questionanswering dataset based on the open book exam environment for elementary-level science-OpenBookQA (Mihaylov et al., 2018). This dataset consists of two components-(i) Questions (Q): a set of 5,958 multiple choice questions and (ii) Facts (F): a set of 1,326 diverse facts about elementary level science. This dataset was further tested for evaluating the performance of existing QA systems and then compared with the human performance. The results indicated that human performance was close to 92%, but many existing QA systems showed poor performance close to the random guessing baseline of 25%. Additionally, the authors found that simple neural networks achieved an accuracy of about 50%, but it is still not close to the human performance, about 92%. Recently an extension of the WorldTree project (Jansen, 2018), i.e., WorldTree V2 (Xie et al., 2020), is presented. The main goal of this project is to generate a science domain explanation with a supporting semistructured knowledge base. The WorldTree project is a part of TABLE Selected Machine Reading Comprehension papers. References Name Year Explanation Models XAI Metric Dataset Code Sharp et al. (2017) -2017 Extractive TF-IDF, FFNN -AI2 science, Aristo Mini -Ling et al. (2017) -2017 Extractive LSTM, Seq2Seq -AQuA Mihaylov et al. (2018) OpenBookQA 2018 Abstractive BiLSTM Max-out -OpenBookQA, Xie et al. (2020) WorldTree V2 2018 Abstractive TF-IDF, BERT -WorldTree V2 Lakhotia et al. (2021) FiD-Ex 2021 Extractive T5, BERT-to-BERT -Natural Questions - explainable question answering tasks that provide answers to natural language questions and their human-readable explanations for why the answers are correct. Xie et al. (2020) notes that most multi-hop inference models could not demonstrate combining more than two or three facts to perform inference. However, here the authors merge, on average, six facts from a semi-structured knowledge base of 9216 facts. Thus, this resulted in the WorldTree V2 corpus for standardized science questions. This corpus consists of 5100 detailed explanations to support training and instrumenting multi-hop inference question answering systems. Lakhotia et al. (2021) demonstrates a new MRC framework called FiD-Ex (Extractive Fusion-in-Decoder). It has been noted that seq2seq (Sequence to Sequence) models work well at generating explanations and predictions together. However, these models require a large-labeled dataset for training and bring a host of challenges such as fabricating explanations for incorrect predictions, the difficulty of adapting to long input documents, etc. Thus, to tackle these challenges, the Fid-Ex framework includes sentence markers to encourage extractive explanations and intermediate fine-tuning for improving fewshot performance on open-domain QA datasets. This new framework is tested on ERASER (Evaluating Rationales And Simple English Reasoning) datasets and their benchmarks for evaluations (DeYoung et al., 2020). This experiment concludes that FiD-Ex significantly improves upon prior work on the explanation metrics and task accuracy on supervised and few-shot settings. Sharp et al. (2017) proposes a new neural network architecture that re-ranks answer justifications as an intermediate step in answer selection. This new approach alternates between a max pooling layer and a shallow neural network (with ten nodes, glorot uniform initializations, tanh activation, and L2 regularization of 0.1) for providing a justification. This approach contains three components: 1) retrieval component, which retrieves a pool of candidates' answer justification, 2) extractor, which extracts the features and 3) scores, which perform the scoring of the answer candidate based on the pool of justifications. The authors used 8th-grade science questions provided by Allen Institute for Artificial Intelligence (AI2) for evaluations. The training set includes 2,500 questions with four options, and the test set consists of 800 publicly released questions. Further, a pool of candidate justifications corpora containing 700k sentences from StudyStack and 25k sentences from Quizlet is used. The top 50 sentences were retrieved as a set of candidate justification. For model tuning, the authors used five-fold cross-validation, and during testing, the model architecture and hyperparameters were frozen. The authors compared results using two baselines: IR baseline and IR++. They concluded that this new approach showed better accuracy and justification quality while maintaining near state-of-the-art performance for the answer selection task. Ling et al. (2017) presented a dataset and an approach that provides answer rationales, sequences of natural language, and human-readable mathematical expressions for solving algebraic word problems. The authors proposed a sequence-to-sequence model which generates a sequence of instructions and provides the rationales after selecting the answer. For this purpose, a twolayer LSTM (Long Short-Term Memory) with a hidden size of 200 and word embedding with a size of 200 is utilized. Further, the authors also built a dataset containing 100,000 problems in which each question is decomposed into four parts -two inputs and two outputs. This new dataset is used for generating rationales for math problems and for understanding the quality of rationales as well as the ability to obtain a correct answer. Further, the authors used an attention-based sequence to sequence model as a baseline and compared results based on average sentence level perplexity and BLEU-4 (Bilingual Evaluation Understudy). The authors noted that this new approach could outperform the existing neural models in the ability to solve problems and the fluency of the generated rationales.",
          "Commonsense knowledge helps humans navigate everyday situations. Similarly, commonsense reasoning in NLP is the ability for a model to go beyond pattern recognition and use world knowledge to make inferences (Apperly, 2011;Sap et al., 2020). As shown in Table 5 on commonsense reasoning, we found four articles, and all of them provide unique solutions that contribute to the development of commonsense reasoning frameworks. Sap et al. (2020) demonstrates a solution for commonsense reasoning using LSTM encoder and decoder. The main goal was to convert the actions of an autonomous agent into natural language using neural machine translation. For this purpose, the authors built a corpus of thoughts of people as they complete tasks in the Frogger game which are then stored as states and actions. In the next step, LSTM encoder and decoder are used to translate actions as well as states into natural language. Lastly, the authors used the BLEU score to calculate sentence similarity and assessed the accuracy for selecting the best rationale. The authors also conducted a survey to evaluate the rationales based on human satisfaction. The Frogger experiment is concluded with Encoder-Decoder framework outperforming the baselines and demonstrates that the use of game theory approaches for generating rationales is a promising technique (Chang et al., 2019(Chang et al., , 2020;;Yu et al., 2019;Li et al., 2022). Further, it is noted that deep learning model performance is poor when used in tasks that require commonsense reasoning due to limitations with available datasets. To tackle this problem, Rajani et al. (2019) developed the Commonsense Auto-Generated Explanations (CAGE) framework for generating explanations for Commonsense Question Answering (CQA). The authors also created a new dataset-Common Sense Explanations (CoS-E)by collecting human explanations for commonsense reasoning and highlighting annotations. From this paper, the authors concluded that CAGE could be effectively used with pre-trained language models to increase commonsense reasoning performance. Recently, Majumder et al. (2021) and Tang et al. ( 2021) presented novel solutions for commonsense reasoning. Majumder et al. (2021) focused on the Natural Language Expiations (NLEs), which are more detailed than Extractive rationales but fall short in terms of commonsense knowledge. In this solution, the authors proposed a self-rationalizing framework RExC (Rationales, Explanations, and Commonsense). RExC first extracts rationales that act as features for the prediction then expands the extractive rationales using commonsense resources. In the last step, RExC selects the best suitable commonsense knowledge for generating NLEs and a final prediction. The authors tested RExC on five tasksthree natural language understanding tasks and two vision language understanding tasks. Overall, the results indicated improvement in the quality of extractive rationales and NLEs that bridges the gap between task performance and explainability. On the other hand, Tang et al. (2021) focused on Visual Commonsense Reasoning (VCR). They focused on a problem when a question with a corresponding input image is given to the system, and it attempts to predict an answer with a rationale statement as the justification. To explore this, author presented a multi-model approach by combining Computer Vision (CV) and NLP. Their approach leverages BERT and ResNet50 (Residual neural network) as the feature representation layer and BiLSTM (Bidirectional LSTM) and Attention for the multimodal feature fusion layer. These layers are then concatenated into an LSTM network for the encoder layer before passing into the classifier for the prediction layer. This was tested on the benchmark VCR dataset and it indicated significant improvements over existing methods and it also provided a more interpretable intuition into visual commonsense reasoning.",
          "Natural Language Inference (NLI) task helps with identifying a natural language hypothesis from a natural language premise (MacCartney, 2009). For this application, as shown in Table 6, we found three articles. The first article presents a new dataset-e-SNL (explanation-augmented Stanford Natural Language Inference) (Camburu et al., 2018)-and the other two articles discuss approaches that can improve NLI. Camburu et al. (2018) extended the Stanford NLI (SNLI) (Bowman et al., 2015a) dataset by providing human-annotated explanations for the entailment relations. This new dataset-e-SNLI-is used in a series of classification experiments involving LSTM-based networks for understanding its usefulness for providing human-interpretable full-sentence explanations. The authors also evaluated these explanations as an additional training signal for improving sentence representation and transfer capabilities of out-of-domain NLI datasets. Thus, from these experiments, the authors conclude that e-SNLI can be used for various goals mentioned above and also be utilized for improving models as well as asserting their trust. Another issue with NLI is the faithfulness of the generated explanations, tackled by Kumar and Talukdar (2020) and Wiegreffe et al. (2021). Kumar and Talukdar (2020) mentioned that existing methods do not provide a solution for understanding correlations of the explanations with the model's decision-making and this can affect the faithfulness of the generated explanations. Considering this problem, the authors proposed and presented a new framework -NILE (Natural language Inference over Labelspecific Explanations). The NILE framework can generate natural language explanations for each possible decision and process these TABLE Selected commonsense reasoning papers. References Name Year Explanation Models XAI metric Dataset Code Ehsan et al. (2018) -2018 Extractive LSTM, Seq2Seq ---Rajani et al. (2019) CAGE 2019 Abstractive GPT, BERT -CoS-E, CommonsenseQA Majumder et al. (2021) RExC 2021 Extractive Transformer -ComVE, e-SNLI, COSe, e-SNLI-VE, VCR -Tang et al. (2021) DMVCR 2021 Extractive LSTM, BERT -VCR TABLE Selected Natural Language Inference papers. References Name Year Explanation Models XAI metric Dataset Code Camburu et al. (2018) e-SNLI 2018 Abstractive BiLSTM, Seq2Seq -e-SNLI Kumar and Talukdar (2020) NILE 2020 Abstractive GPT-2, RoBERTa -e-SNLI Wiegreffe et al. (2021) -2020 Abstractive T5 -CoS-E, SNLI explanations to produce a final decision for the classification problems. To test this approach, the authors used two datasets -SNLI and e-SNLI -and compared NILE with baseline and other existing approaches based on explanation accuracy, in-domain evolution sets (SNLI), and on out-of-domain examples (train on SNLI and test on MNLI) (Williams et al., 2018). Based on the first 100 SNLI test samples, the results indicated that NILE variants are comparable with the ETPA (Explain Then Predict Attention) baseline, and NILE explanations generalize significantly better on out-of-domain examples. For out-of-domain examples (MNLI ), results showed that the percentage of correct explanations in the subset of correct label predictions was significantly better for all the NILE variants. Thus, the authors concluded that NILE is an effective approach for accurately providing both labels and explanations. Further, Kumar and Talukdar (2020) also focused on the need for faithfulness for denoting the model's decision-making process by investigating abstractive rationales. The author proposed two measurements -robustness equivalence and feature importance agreement -to investigate the association of the labels and predicted rationales, which are required for a faithful explanation. This investigation was performed on CommonsenseQA (Talmor et al., 2019) and SNLI dataset using T5based models (Narang et al., 2020). The results indicated that stateof-the-art T5-based join models demonstrate desirable properties and potential for producing faithful abstractive rationales. .",
          "Fact-checking has become a popular application of NLP in recent years given its impact on assisting with misinformation and a majority of the work has been with claim verification (Vargo et al., 2018;Gurrapu et al., 2022). Based on a paper published in 2016, there are 113 active fact-checking groups and 90 of which were established after 2010 (Graves and Cherubini, 2016). This indicates the growth of the fact-checking application. Considering the scope of this literature review, as shown in Table 7, we found four articles on fact-checking. Two of the studies in this section present novel datasets, and the remaining two provide new techniques to improve fact-checking. In 2017, a large dataset for the fact-checking community called LIAR (Wang, 2017) was introduced, including POLITIFACT data. Most works on this data were focused on using the claim and its speaker-related metadata to classify whether a verdict is true or false. The evidence-an integral part of any fact-checking processwas not part of the LIAR and was overlooked. Thus, in Alhindi et al. (2018) extended the LIAR dataset to LIAR-plus by including the evidence/justification. The authors treated the justification as a rationale for supporting and explaining the verdict. Further, they used Feature-based Machine Learning models (Logistic Regression and Support Vector Machine) and deep learning models (Bi-Directional Long Short-term Memory (BiLSTM) and Parallel-BiLSTM) for binary classification tasks to test the data. The results demonstrated a significant performance improvement in using the justification in conjunction with the claims and metadata. Further, Hanselowski et al. (2019) introduced a new corpus for training machine learning models for automated fact-checking. This new corpus is based on different sources (blogs, social media, news, etc.) and includes two granularity levels-the sources of the evidence and the stance of the evidence toward the claim-for claim identification. Authors then used this corpus to perform stance detection, evidence extraction and claim validation experiments. In these experiments, a combination of LSTMs, baseline NN, pretrained models have been used, and their results are compared based on precision, recall, and F1 macro. The results indicated that fact-checking using heterogeneous data is challenging to classify claims correctly. Further, the author claims that the fact-checking problem defined by this new corpus is more difficult compared to other datasets and needs more elaborate approaches to achieve higher performance. It has been noted that the fact-checking systems need appropriate explainability for the verdicts they predict. The justifications that are human-written can help to support and provide context for the verdicts, but they are tedious, unscalable, and expensive to produce (Atanasova et al., 2020). Considering this issue, Atanasova et al. (2020) proposed that the creation of the justifications needs to be automated to utilize them in a large-scale fact-checking system. The authors presented a novel method that automatically generates the justification from the claim's context and jointly models with veracity prediction. Further, TABLE Selected fact-checking papers. References Name Year Explanation Models XAI metric Dataset Code Alhindi et al. (2018) LIAR-PLUS 2018 Extractive SVM, BiLSTM -LIAR-PLUS Hanselowski et al. (2019) -2019 Extractive BERT -FEVER Atanasova et al. (2020) -2020 Extractive DistilBERT -LIAR-PLUS -Rana et al. (2022) RERRFACT 2022 Extractive RoBERTa, BioBERT -SCIFACT this new method is then tested on the LIAR dataset (Wang, 2017) for generating veracity explanations. The results indicated that this new method could combine predictions with veracity explanations, and manual evaluations reflected the improvement in the coverage and quality of the explanations. Another important domain in which fact-checking is useful is Science. Researching and providing substantial evidence to support or refute a scientific claim is not a straightforward task. It has been seen that scientific claim verification requires in-depth domain expertise along with tedious manual labor from experts to evaluate the credibility of a scientific claim. Considering this problem, Rana et al. (2022) proposed a new framework called RERRFACT (Reduced Evidence Retrieval Stage Representation) for classifying scientific claims by retrieving relevant abstracts and training a rationaleselection model. RERRFACT includes a two-step stance prediction that differentiates non-relevant rationales then identifies a claim's supporting and refuting rationales. This framework was tested on the SCI-FACT dataset (Wadden et al., 2020) and performed competitively against other language model benchmarks on the dataset leaderboard. .",
          "Sentiment Analysis is a subset of the text classification field (Minaee et al., 2021). It focuses specifically on the \"computational study of people's opinions, sentiments, emotions, appraisals, and attitudes toward entities such as products, services, organizations, individuals, issues, events, topics and their attributes\" (Zhang et al., 2018). The use of rationales to support sentiment analysis models in NLP is widely used compared to other NLP tasks. For this task, as shown in Table 8, we identified five papers in this field. In 2016, Lei et al. (2016) pioneered rationalization in sentiment analysis by proposing a problem: \"prediction without justification has limited applicability\". To make NLP outcomes more transparent, the authors propose an approach to extract input text which serves as justifications or rationales for a prediction. These are fragments from the input text which themselves are sufficient to make the same prediction. Their implementation approach includes a generator and an encoder architecture. The generator determines which can be potential candidates for a rationale from the input text. Those candidates are fed into the encoder to determine the prediction and the rationales are not provided during training. They employ an RCNN (Region-based Convolutional Neural Network) and an LSTM architecture and when compared with each other the RCNN performed better. The experiment was conducted on the BeerAdvocate dataset. The paper's approach outperforms attention-based baseline models. They also demonstrate their approach on a Q&A retrieval task indicating that leveraging rationales for sentiment analysis tasks is very beneficial. Similarly, Du et al. (2019) claim that explainability alone is not sufficient for a DNN (Deep Neural Network) to be viewed as credible unless the explanations align with established domain knowledge. In essence, only the correct evidences are to be used by the networks to justify predictions. In this paper, the authors define credible DNNs as models that provide explanations consistent with established knowledge. Their strategy is to use domain knowledge to improve DNNs credibility. The authors explore a specific type of domain knowledge called a rationale which are the salient features of the data. They propose an approach called CREX (Credible Explanation), which regularizes DNNs to use the appropriate evidence when making a decision for improved credibility and generalization capability. During training, instances are coupled with expert rationales and the DNN model is required to generate local explanations that conform to the rationales. They demonstrate it on three types of DNNs (CNN, LSTM, and self-attention model) and various datasets for testing. Results show that the CREX approach allows DNNs to look at the correct evidences rather than the specific bias in training dataset. Interestingly, they point that incorporating human knowledge does not always improve neural network performance unless the knowledge is very high quality. Many papers published in the rationalization field indicate that a machine learning system learning with human provided explanations or \"rationales\" can improve its predictive accuracy (Zaidan et al., 2007). Strout et al. (2019) claim that this work hasn't been connected to the XAI field where machines attempt to explain their reasoning to humans. The authors attempt to show in their paper that rationales can improve machine explanations as evaluated by human judges. Although automated evaluation works, Strout et al. (2019) believe that since the explanations are for users, therefore humans should directly evaluate them. The experiment is done by using the movie reviews dataset and by having a supervised and an unsupervised CNN model for a text classification task. They use attention mechanism and treat the rationales as supervision in one of the CNN models. Results indicate that a supervised model trained on human rationales outperforms the unsupervised on predictions. The unsupervised is the model where the rationales/explanations are learned without any human annotations. The selective rationalization mechanism is commonly used in complex neural networks which consist of two componentsrationale generator and a predictor. This approach has a problem of model interlocking which arises when the predictor overfits to the features selected by the generator. To tackle this problem this paper proposes a new framework A2R which introduces TABLE Selected sentiment analysis papers. References Name Year Explanation Models XAI metric Dataset Code Lei et al. (2016) -2016 Extractive LSTM, RCNN -BeerAdvocate, AskUbuntu Du et al. (2019) CREX 2019 Extractive CNN, LSTM -BeerAdvocate, MovieReview -Strout et al. (2019) -2019 Extractive RA-CNN, AT-CNN -MovieReview -Yu et al. (2021) A2R 2021 Extractive BiGRU -BeerAdvocate, MovieReview Antognini and Faltings (2021) ConRAT 2021 Extractive CNN, BiGRU -AmazonReviews, BeerAdvocate a third component for soft attention into the architecture (Yu et al., 2021). The authors have used BeerAdvocate and MovieReview for understanding the effectiveness of the framework. The authors compared results from A2R with the original rationalization technique RNP (Rationalizing Neural Predictions) along with 3PLAYER, HARD-KUMA and BERT-RNP. For implementation authors have used bidirectional Gated Recurrent Units (GRU) in the generators and the predictors. Furthermore, they performed two synthetic experiments using BeerAdvocate dataset by deliberately inducing interlocking dynamics and then they performed experiments in real-world setting with BeerAdvocate and MovieReview. From the results they made two conclusions-(1) A2R showed consistent performance compared to other baselines on both the experiments, ( 2) A2R helps to promote trust and interpretable AI. In the future, the authors would like to improve A2R framework for generating casually corrected rationales to overcome the lack of inherent interpretability in the rationalization models. Existing methods in rationalization compute an overall selection of input features without any specificity and this does not provide a complete explanation to support a prediction. Antognini and Faltings (2021) introduce ConRAT (Conceptbased RATionalizer), a self-interpretable model which is inspired by human decision-making where key concepts are focused using the attention mechanism. The authors use the BeerReviews dataset to not only predict the review sentiment but also predict the rationales for key concepts in the review such as Mouthfeel, Aroma, Appearance, Taste, and Overall. ConRAT is divided into three subodels, a Concept Generator which finds the concepts in the review, a Concept Selector that determines the presence or absence of a concept, and a Predictor for final review predictions. ConRAT outperforms state-of-the-art methods while using only the overall sentiment label. However, Antognini et al. (2021) have further demonstrated that attention mechanism usage can contribute to a tradeoff between noisy rationales and a decrease in prediction accuracy.",
          "Text classification, also commonly known as text categorization, is the process of assigning labels or tags to textual data such as sentences, queries, paragraphs, and documents (Minaee et al., 2021). Classifying text and extracting insights can lead to a richer understanding of the data but due to their unstructured nature, it is challenging and tedious. NLP techniques in text classification enable automatic annotation and labeling of data to make it easier to obtain those deeper insights of the data. For this task, as shown in Table 9, we identified four papers in this sfield. Traditionally, rationales provide well-defined kinds of data to nudge the model on why a prediction is the way it is given the data. Moreover, they require little additional effort for annotators and yield a better predictive model. When classifying documents, it is beneficial to obtain sentence-level supervision in addition to document-level supervision when training new classifications systems (Zhang et al., 2016). Previous work relied on linear models such as SVMs (Support Vector Machines), therefore, Zhang et al. (2016) propose a novel CNN model for text classification that exploit associated rationales of documents. Their work claims to be the \"first to incorporate rationales into neural models for text classification\". The authors propose a sentence-level CNN to estimate the probability that a sentence in a given document can be a rationale. They demonstrate that their technique outperforms baselines and CNN variants on five classification datasets. Their experimentation task uses Movie Reviews and the Risk of Bias (RoB) datasets. On the movie review dataset, their technique performs with a 90.43% accuracy with the RA-CNN (Recurrent Attention Convolutional Neural Network) model and similar strong results are also indicated on the RoB datasets. It seems intuitive that more data or information can lead to better decision-making by the neural networks. Zaidan et al. (2007) propose a new framework to improve performance for supervised machine learning by using richer \"kinds\" of data. Their approach is called the \"annotator rationales\" technique and it is to leverage a training dataset with annotated rationales. The rationales highlight the evidence supporting the prediction. Zaidan et al. (2007) test their approach on text categorization tasks, specifically, sentiment classification of movie reviews and they claim that these rationales enable the machine to learn why the prediction is the way it is. Rationales help the model learn the signal from the noise. ML algorithms face the \"credit-assigment problem\" which means that many features in the data (X) could have affected the predicted result (Y). Rationales provide a \"shortcut\" to simplifying this problem since they provide hints on which features of X were important. Zaidan et al. (2007) used a discriminative SVM for experimentation and the results indicate that this technique significantly improves results for the sentiment classification and they hypothesize that leveraging rationales might be more useful than providing more training examples. Recently, rationales have been a popular method in NLP to provide interpretability in the form of extracted subsets of texts. It is common to have spurious patterns and co-varying aspects in the TABLE Selected text classification papers. References Name Year Explanation Models XAI Metric Dataset Code Zaidan et al. (2007) -2007 Extractive SVM -MovieReview -Zhang et al. (2016) -2016 Extractive SVM, RA-CNN -Risk of Bias -Liu et al. (2019b) GEF 2019 Extractive CNN, LSTM -PCMag Reviews, Skytrax User Reviews Plyler et al. (2022) CDA 2021 Extractive RL, RNN -TripAdvisor Reviews, RateBeer dataset due to which rationale selectors do not capture the desired relationship between input text and target labels. Considering this problem this paper proposes CDA (Counterfactual Data Augmentation) framework to aid rational models trained with Maximum Mutual Information (MMI) criteria (Plyler et al., 2022). CDA consists of transforms-for rational and classificationsbecause of their effectiveness over RNNs in NLP. The authors used TripAdvisor.com and RateBeer datasets for testing CDA with three baselines -MMI, FDA (Factual Data Augmentation), and ANT (simple substitution using antonyms). The results of the rational models were compared using precision and the accuracy of the classifier is reported based on the development set. From the results, authors concluded that the models trained using the CDA framework learn higher quality rationales and it doesn't need human intervention. In the future, the authors would like to explore more on counterfactual predictors and on CDA framework that could connect with other rationalization strategies. Similarly, Liu et al. (2019b) proposed a novel Generative Explanation Framework (GEF) for classification problems that can generate fine-grained explanations. The motivation behind this explanation framework is to provide human-readable explanations without ignoring finegrained information such as textual explanations for the label. For understanding the accuracy of explanations, the authors conducted experiments on two datasets-PCMag and Skytrax User Reviewswhich were processed by the Stanford Tokenizer. Further, the authors used Encoder-Predictor architecture in which they used Conditional Variational Autoencoder (CVAR) as a base model for text explanations and Long Short-Term Memory (LSTM) for numerical explanations. The experimental results indicated that after combining base models with GEF the performance of the base model was enhanced along with improving the quality of explanations. Further, the authors also used human evaluation for evaluating the explainability of the generated text explanations. The authors noted that for 57.62% of the tested items GEF provided better or equal explanations compared with the basic model.",
          "With the advent of deep learning, Neural Machine Translation (NMT) became the successor to traditional translation methods such as Rule-based or Phrase-Based Statistical Machine Translation (PBSMT) (Yang et al., 2020). NMT models leverage Deep Neural Networks architecture to train the model end-to-end to improve translation quality and only require a fraction of the storage memory needed by PBSMT models (Sutskever et al., 2014). The use of explanations to support NMT model's prediction is relatively new, however, there has been some pioneering work to provide more explainability. For this task, as shown in Table 10, we identified two relevant papers in this area. Quality Estimation (QE) models perform well at analyzing the overall quality of translated sentences. However, determining translation errors is still a difficult task such as identifying which words are incorrect due to the limited amounts available training data. The authors explore the idea that since QE models depend on translation errors to predict the quality, using explanations or rationales extracted from these models can be used to better detect translation errors (Fomicheva et al., 2021). They propose a novel semi-supervised technique for word-level QE and demonstrate the QE task as a new benchmark for evaluating feature attribution (the interpretability of model explanations to humans). Instead of natural language explanations, their technique employs various feature attribution methods such as LIME, Integrated Gradients, Information Bottleneck, causal, and Attention. It was shown that explanations are useful and help improve model performance and provide better explainability. Deep learning models are black-boxes because they involve a large number of parameters and complex architectures which makes them uninterpretable. Considering this problem and to bring interpretability in deep learning models (Alvarez-Melis and Jaakkola, 2017) propose a model-agnostic method for providing explanations. The explanations provided by this method consist of sets of inputs and output tokens that are causally related in the black-box model. Further, these causal relations are inferred by performing perturbations on the inputs from the black-box models, generating a graph of tokens, and then solving a partitioning problem to select the most relevant components. To test the methods via runtime complexity analysis with respect to the number of LM (Linear Model) forward/backward passes. Finally for performance standard dataset-specific metrics accuracy (for SST and CoS-E) and macro-averaged F1 (for Movies) are used. The results indicated that UNIREX allows effective trade-off between performance, faithfulness, and plausibility to identify better rationale extractions. Further, authors also mention UNIREX trained rationale extractors can generalize to unseen datasets and tasks. With many ML systems demonstrating performance beyond that of human across many applications, the field of XAI is advancing techniques to improve transparency and interpretability. Das and Chernova (2020) explores XAI in the context of a question previously unexplored in ML and XAI communities: \"Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human?\". The authors investigate this question through the game of Chess where computational game engines performance surpass the average player. They present an automated technique for generating rationales for utilitybased computational methods called the Rationale-Generating Algorithm. They evaluate this with a user study against two baselines and their findings show that the machine generated rationales can lead to significant improvement in human task performance. They demonstrate that rationales can not only be used to explain the system's actions but also instruct the user to improve their performance. Sharma et al. (2020) explores an of rationalization in the mental health support field and understanding empathy. Empathy is important for mental health support and with the rise of text-based internet platforms, it becomes crucial to understanding empathy in only communication. The paper presents a computational approach to understanding empathy by developing a corpus of 10,000 pairs of posts and responses with supporting rationales as evidence. They use a multi-task RoBERTabased bi-encoder model to identify empathy in conversations and extract rationales for predictions. Their results demonstrate that their approach can effectively identify empathic conversations. To improve interpretability for NLP tasks, recent rationalization techniques include Explain-then-Predict models. In this technique, an extractive explanation from the input text is generated and then a prediction is generated. However, these models do not use the rationales appropriately and consider the task input as simply a signal to learn and extract rationales. Zhang et al. (2021) propose a novel technique to prevent this problem with their approach called ExPred where they leverage mult-task learning on the explanation phase and embed a prediction network on the extracted explanations to improve task performance. They experiment with three datasets [Movie Reviews, FEVER (Fact Extraction and VERification), MultiRC] and conclude that their model significantly outperforms existing methods.",
          "This section compares extractive and abstractive rationalization techniques. It can be observed from Figure 4 that there is more interest and focus on extractive rationalization techniques compared to abstractive. There are multiple reasons for this, and the progress in the Automatic Text Summarization (ATS) domain can help explain.",
          "In most extractive rationalization approaches, generating a rationale is similar to text summarization. These rationales contain the salient features of the input text, which users need to understand as the most influenced features of the model's prediction. Next, two steps are implemented while performing the task-(i) irrelevant information is ignored, and (ii) most crucial information is selected based on a scoring algorithm. This approach is a common foundation of summarization techniques. In extractive summarization, meaningful sentences are extracted to form a summary of the original text while still retaining the overall subject matter (El-Kassas et al., 2021). The critical difference with rationalization is that it is able to justify a neural network's prediction with evidence. In a way, extractive rationalization uses extractive summarization's fundamentals and takes it further. It frames the task as can we rationalize the output prediction where rationalize means to understand the prediction process and reason with supporting evidence. This introduces an interdependent relationship between the rationale and the prediction. This process is close to how humans rationalize with a sequence of reasons to justify a decision. This can be implemented in the NLP process to make models more explainable. As interest in ATS systems grew in the past few decades, researchers have mainly focused on extractive summarization due to its simplicity, and reliability (El-Kassas et al., 2021). The abstractive summarization needed reliable natural language generation; thus, it was in its infancy from the 2000s to the early 2010s. Therefore, an increasing body of knowledge on extractive techniques is available, which researchers interested in rationalization could leverage and build on. This intuition behind extractive summarization paves the way for extractive rationalization. The stark difference between extractive and abstractive in Figure 3 is expected and reasonable, and the fields of summarization and rationalization follow similar paths. However, summarization approaches should purely be used for inspiration -following the identical methods for rationalization would be insufficient, and it does not provide reliable model explainability. Chan et al. (2021) notes that for appropriate explainability, the desiderata for the rationale is that-(i) it must reflect the model's reasoning process (faithfulness), (ii) be convincing to the nonexpert (plausibility), and (iii) the rationale extraction should not hurt task performance. Thus, there is more work than simply extracting sentences as rationales. Moreover, extractive rationalization is insufficient because extracted sentences themselves are insufficient to provide full explainability. Humans do not fully understand without context and a coherent and logical explanation.",
          "The extensive research in extractive summarization reached its maturity, has peaked in terms of performance, and now the progress is stagnated (Gupta and Gupta, 2019). Recent advances in deep learning and the advent of the Transformer architecture in 2017 have led to more reliable and influential language models (Vaswani et al., 2017;Devlin et al., 2019). In 2019, Liu and Lapata (2019)  Abstractive techniques allowed novel words and phrases to be generated instead of extracting spans from the input. Due to these advances, the research focuses gradually shifted from extractive to abstractive summarization. It is expected that rationalization will follow a similar trend. Abstractive rationalization is still relatively new, with limited research available. However, there have been promising and pioneering approaches such as Rajani et al. (2019) and Kumar and Talukdar (2020). Almost every paper discussed with an abstractive rationalization technique in Section 4 leveraged some implementation of the Transformer architecture, such as BERT, GPT-2 (Radford et al., 2019), and T5, amongst others. BERT was the most frequently used language model. When BERT was released in 2018, it achieved SOTA results on many NLP tasks and surpassed human performance on tasks such as question answering and commonsense reasoning (Devlin et al., 2019). It made a giant leap in terms of performance compared to other language models of its time. This led to wide adoption and variations of BERT for the tasks where the Transformer-based model was required. Recently introduced models such as BART (Lewis et al., 2020), GPT-3 (Brown et al., 2020), and T5 demonstrate promising results and surpass BERT in some tasks. This is due to language models growing exponentially, and they continue to improve and perform incredibly well at natural language generation (Sanh et al., 2019). For example, in some cases, text produced by GPT-3 is almost on par if not better, than human-written text. This enables more opportunities for research in abstractive rationalization, which is needed. By leveraging SOTA language models, explanations can become more comprehensive and convincing when illustrating a model's decision-making process. As mentioned in Section 2, it is almost as if the models are \"talking for themselves\". We believe that significant progress can be made in rationalization by focusing more on improving abstractive techniques.",
          "In this section, we discuss insights from the literature reviewed, challenges, and potential future directions to propel progress on rationalization. Most importantly, we introduce a new XAI subfield called Rational AI.",
          "In Section 1, we have seen the need for explainability and the available methods in NLP. The numerical methods, such as SHAP values (Lundberg and Lee, 2017b) or Attention scores, visualization methods, such as LIME (Ribeiro et al., 2016), and saliency heatmaps, all require specialized domain knowledge to understand. At the same time, with increasing interactions with NLP-based systems, the nonexpert also deserves to know and understand how these black-box systems work because it has some degree of influence on their lives. This is formally called the right to an explanation, a right to receive an explanation for an algorithm's output (Miller, 2019b). A classic example is a bank system with an NLP model that automatically denies a loan application. In this situation, providing the loan applicant with SHAP values or saliency heatmaps to justify the bank's algorithms is not very meaningful. Thus, explainability methods are truly explainable and helpful if the nonexpert can understand them (Mittelstadt et al., 2019). We introduce Rational AI (RAI) as a potential solution.",
          "Rationalization techniques come the closest to this goal because they are built on natural language explanations (NLEs). NLEs are intuitive and human comprehensible because they are simply descriptive text. The textual information can be easily understood and translated into other languages if needed. Across all of the NLP tasks discussed in Section 4, we have seen the benefits of NLEs and the accessibility it provides to the nonexpert. We believe there is a critical need to focus on explainability techniques with NLEs. Considering these factors, we propose a new subfield in Explainable AI called Rational AI as shown in Figure 5. We define Rational AI as follows. Rational AI: A field of methods that enable a black-box system to rationalize and produce a natural language explanation (rationale) to justify its output. Rationality is the process of applying RAI to make models more explainable through an NLE. This is similar to the relationship between explainability and XAI. Further, rationality should not be confused or used interchangeably with the general AI term of a rational agent Russell and Norvig (2002). These are distinct topics with similar names. In this survey, RAI and rationality are purely considered in the context of model explainability. We also have not seen any usage or previous definitions of RAI within this context. We compare rationality to the other fields shown in Figure 5. Models with interpretability are interpretable, while those with explainability are interpretable and complete, as described in Section 1. Models with rationality are interpretable and complete and can rationalize their behavior through an NLE. The explainability methods described earlier in this subsection explain, but they do not justify in a way that is accessible and comprehensible to the nonexpert. In recent years, language models have become powerful and incredibly good at language generation tasks, but we have yet to see their full potential. As they continue to grow exponentially, we predict this is the beginning of explainability techniques using NLEs. The intuition behind RAI is that rationalization is one such technique, and many are yet to be developed. This calls for a more organized field to improve research focus and the need for RAI to exist.",
          "Although RAI arises from the need for better explainability for NLP tasks, it is potentially applicable in general AI and other fields in AI. Other fields, such as Computer Vision, Speech, and Robotics, could leverage rationalization methods to improve their model explainability. For example, rationalization in Computer Vision can help explain through an NLE which visual features contributed the most to an image classifier prediction in place of complex explainable techniques (Sundararajan et al., 2017;Tjoa and Guan, 2021). Many promising opportunities exist for researchers to apply rationalization in other disciplines.",
          "We have seen that rationalization is a relatively new technique, and with it, various challenges exist. In this subsection, we share challenges and potential solutions to improve the current state.",
          "No standard statistical evaluations exist currently for rationalization. There is a wide variety of metrics that are in use, such as Mean Squared Error (Lei et al., 2016), Accuracy (Zaidan et al., 2007;Du et al., 2019;Rajani et al., 2019), F1 Score (Alhindi et al., 2018;Rana et al., 2022), ANOVA (Analysis of variance) (Das and Chernova, 2020), and Precision (Plyler et al., 2022). We have observed that the most preferred statistical metric is accuracy. It is reasonable for evaluation metrics to be task-dependent and focused on the prediction. However, those alone are insufficient because the accuracy of the NLE also needs to be considered. For example, if the task prediction had high accuracy, but the NLE was unclear and incomprehensible, then it is not helpful. Metrics such as the BLEU (BiLingual Evaluation Understudy) score by Papineni et al. (2002) and the ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation) score by Lin (2004) exist for evaluating open-ended machine-generated texts. However, we have seen limited use in the literature review, such as Camburu et al. (2018). The scores work by comparing the generated text with a set of ground-truth reference texts, and often these are human-written references. These scores are helpful, especially for abstractive rationalization, where explanations can be open-ended; however, they come with limitations since the evaluation is effectively token-level matching. Since an NLE is the typical outcome of systems with rationalization, adopting a standard evaluation metric can help improve research progress. Consistent evaluations also make it easier to compare different experiments and approaches.",
          "The availability and the need for more diversity of appropriate datasets is also a problem hindering progress. Availability: Data collection is an expensive and timeconsuming task. It is possible to repurpose existing datasets, but modifying them requires manual human labor. Thus, researchers often build their datasets for a specific task they are working on. Camburu et al. (2018) developed the e-SNLI dataset by modifying the SNLI dataset from Bowman et al. (2015b). Camburu et al. TABLE Human-Centered Evaluations (HCE) are performed in out of papers surveyed in this review. Year References Method name Venue NLP domain HCE performed? 2007 Zaidan et al. (2007) -NAACL Text Classification No 2016 Ribeiro et al. (2016) LIME NAACL Multiple Domain Yes 2016 Lei et al. (2016) -EMNLP Sentiment Analysis No 2016 Zhang et al. (2016) -EMNLP Text Classification No 2017 Sharp et al. (2017) -CoNLL Machine Reading Comprehension No 2017 Ling et al. (2017) -ACL Machine Reading Comprehension No 2017 Alvarez-Melis and Jaakkola (2017) SOCRAT EMNLP Neural Machine Translation Yes 2018 Ehsan et al. (2018) -AIES Commonsense Reasoning Yes 2018 Alhindi et al. (2018) LIAR-PLUS FEVER Fact-Checking No 2018 Mihaylov et al. (2018) OpenBookQA EMNLP Machine Reading Comprehension Yes 2018 Xie et al. (2020) WorldTree V2 LREC Machine Reading Comprehension Yes 2018 Camburu et al. (2018) e-SNLI NeurlPS Natural Language Inference No 2019 Rajani et al. (2019) CAGE ACL Commonsense Reasoning Yes 2019 Hanselowski et al. (2019) -CoNLL Fact-Checking No 2019 Du et al. (2019) CREX ICDM Sentiment Analysis No 2019 Strout et al. (2019) -BlackboxNLP Sentiment Analysis Yes 2019 Liu et al. (2019b) GEF ACL Text Classification Yes 2020 Atanasova et al. (2020) -ACL Fact-Checking Yes 2020 Lakhotia et al. (2021) ERASER ACL Multiple Domain Yes 2020 Das and Chernova (2020) RGA IUI Multiple Domain Yes 2020 Sharma et al. (2020) EPITOME EMNLP Multiple Domain No 2020 Kumar and Talukdar (2020) NILE ACL Natural Language Inference Yes 2020 Wiegreffe et al. (2021) -EMNLP Natural Language Inference No 2021 Majumder et al. (2021) RExC ICLR Commonsense Reasoning Yes 2021 Tang et al. (2021) DMVCR DaWaK Commonsense Reasoning No 2021 Tang et al. (2021) FiD-Ex EMNLP Machine Reading Comprehension No 2021 Chan et al. (2021) UNIREX ICML Multiple Domain No 2021 Zhang et al. (2021) EXPred WSDM Multiple Domain No 2021 Fomicheva et al. (2021) -ACL Neural Machine Translation No 2021 Yu et al. (2021) A2R NeurlPS Sentiment Analysis Yes 2021 Antognini and Faltings (2021) ConRAT ACL Sentiment Analysis Yes 2021 Plyler et al. (2022) CDA NeurlPS Text Classification No 2022 Atanasova et al. (2020) RERRFACT AAAI Fact-Checking No (2018) achieved promising results on their task, demonstrating how their dataset can enable a wide range of new research directions by altering and repurposing existing datasets. Diversity: Without enough datasets, new research in rationalization will be limited. Researchers will be constrained to the existing datasets to make new progress. This trend is evident in the literature reviewed in MRC and Sentiment Analysis compared to NMT. In MRC, the datasets are very diverse. In sentiment analysis, most papers rely on either the BeerAdvocate (McAuley et al., 2012) or MovieReviews (Maas et al., 2011) datasets to perform their experiments. In both domains, we discovered five publications each. For a domain such as NMT, progress seems limited, and we found only two publications. The lack of appropriate rationalization datasets for NMT tasks is a possible reason for this. As we observed in our literature review, there is a direct relationship between dataset availability and the progress made. More work in creating new datasets for rationalization can help improve diversity and the progress of certain domains lagging behind, such as NMT. New datasets across all domains, in general, will increase the interest and work in rationalization because researchers will have more flexibility in designing new techniques and experimenting with a wide variety of data. Stamper et al. (2010) has organized the largest repository of learning science datasets called DataShop, and it led to improvements in research progress. Similarly, an organized central repository for rationalization supporting datasets can be beneficial. Without a centralized model evaluation and development system, reproducibility and accessibility will remain low.",
          "NLP has direct applications in many disciplines. For example, MRC and commonsense reasoning are helpful in the education discipline. Our literature review indicates using Q&A tools and commonsense injection to generate explanations for educational needs (Mihaylov et al., 2018;Li et al., 2019). Further, NLP has also been used to enhance human task performance, as we saw in Das and Chernova (2020), and to provide support for mental health (Sharma et al., 2020). Additionally, fact-checking is another application, and it is crucial in social media, fake news detection, and law (Alhindi et al., 2018). It has become common to interact with these systems, and they may have a significant influence on all aspects of our society. Due to this, the European Union recently passed a regulation that requires algorithms to provide explanations that can significantly affect users based on their userlevel predictions (Doshi-Velez and Kim, 2017).",
          "The explanations provided by the NLP systems must provide enough information to the user to help them understand its decision-making process (Putnam and Conati, 2019). Considering these aspects, the human-machine partnership is essential for evaluating and generating accurate explanations. This calls for better methods to evaluate the explanations generated. The field of HCE addresses this problem, and Sperrle et al. (2021) defines it as a \"field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios.\" In this literature survey, we found 15 out of 33 papers in which HCE is performed, and a summary is provided in Table 12. Sperrle et al. (2021) shares the increasing trend of HCE since 2017 compared to the previous years. While conducting this literature survey, this trend was not observed in the rationalization domain. Overall, we found that HEC is incorporated in most of the papers on Machine Reading Comprehension (2 out of 5), Commonsense Reasoning (3 out of 4), Fact-Checking (1 out of 4), Natural Language Inference (2 out of 5), Neural Machine Translation (1 out of 2), Sentiment Analysis (3 out of 5) and Multiple Domain (3 out 6). From our observations, researchers give more attention to performance while evaluating AI algorithms and ignore human factors such as usability, user intentions, and user experience. Thus, along with the accuracy of AI algorithms, it is also essential to focus on the interpretability and reliability of the explanations generated by AI algorithms. The articles in which HCE is used are primarily performed via crowdsourcing using Amazon Mechanical Turk, and the focus is on user-based evaluations or annotations. This pattern necessitates conducting expert evaluations to understand users' needs better because it can help improve trust in AI algorithms. HCE is a subset of the Human-Computer Interaction (HCI) field, which is integrated with the AI paradigm after the algorithmic performance evaluations as shown in Figure 6. This integration can be regarded as human-centered AI, and Riedl (2019) claims this as an AI/ML perspective that intelligent systems are part of a more extensive system that also includes human stakeholders. The literature (Hwang and Salvendy, 2010) on usability, and user experience testing demonstrated three widely used methods to perform HCE -Think Aloud (TA), Heuristic Evaluation (HE), and Cognitive Walkthrough (CW). The TA method is a standard method and can be more effective considering the evaluations of explanations in the NLP domain. In the TA method, evaluators are asked to \"think aloud\" about their experience while an experimenter observes them and listens to their thoughts (Fox, 2015). This way, an HCE method can be used in the final step to understand usability, user intentions, and user experience. This may lead to a better understanding of the interpretability and reliability of the explanations generated by rationalization. Therefore, in addition to statistical evaluation techniques, we strongly encourage researchers to integrate HCE as part of their evaluations.",
          "It is critical to perform rigorous testing and validation of NLP systems at all stages before their deployment. For example, it should be ensured that the data is unbiased, models are interpretable, and the process of arriving at the outcome is explainable to a nonexpert. In the last step of this process, it would be beneficial to use RAI techniques. Integrating rationalization with human-centered evaluations and elements of NLP Assurance can invoke human-AI trust and safety with the systems -with the recent rise of chatbots such as ChatGPT, the need for more rigorous validation is more important than any other time. This process may also transform black-box systems into white-box systems and make NLP models more comprehensible and accessible for nonexpert users."
        ],
        "ground_truth_definitions": {
          "abstractive rationalization": {
            "definition": "generating a novel explanation",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "extractive rationalization": {
            "definition": "extracting text fragments from the input",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Interpretability": {
            "definition": "Interpretability aims at developing tools to understand and investigate the behavior of an AI system.",
            "context": "Interpretability “aims at developing tools to understand and investigate the behavior of an AI system” (Thayaparan et al., 2020). Doshi-Velez and Kim (2017) augments this definition by adding that interpretability tools allow us to “explain or to present in understandable terms to a human” what the AI system is performing.",
            "type": "implicit"
          },
          "Rationalization": {
            "definition": "justifies a model’s output by providing a natural language explanation.",
            "context": "The term rationalization is interchangeable with explanation or justification. Rationalization has rarely been formally defined in the context of NLP. Therefore, we propose the following definition: Rationalization justifies a model’s output by providing a natural language explanation. This is accomplished by either extracting text fragments from the input (extractive rationalization) or by generating a novel explanation (abstractive rationalization).",
            "type": "explicit"
          },
          "Black-box Model": {
            "definition": "A machine-learning obscure model, whose [architecture] internals are either unknown to the observer, or they are known but uninterpretable by humans",
            "context": "Given in a Table",
            "type": "explicit"
          },
          "Explainability": {
            "definition": "the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction.",
            "context": "There is no consensus on the nature of explanations since they are entirely task-dependent, and AI embraces a wide variety of tasks (Miller, 2019a). We treat explainability as a specialization of interpretability where the aim of explainability is to design inherently interpretable models capable of performing transparent inference through the generation of an explanation for the final prediction (Thayaparan et al., 2020). In this survey, we focus on black-box explainability, specifically as the outcome explanation problem.",
            "type": "explicit"
          },
          "interpretable": {
            "definition": "i.e., allowing humans to understand the internals of a model",
            "context": "The bar for the ethical standards and the accountability required to maintain NLP systems continue to increase as these systems become more opaque with increasingly complex networks and algorithms. There has been significant research focus on enabling models to be more interpretable, i.e., allowing humans to understand the internals of a model (Gilpin et al., 2018). However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models.",
            "type": "explicit"
          },
          "NLP Assurance": {
            "definition": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users.",
            "context": "A process that is applied at all stages of the NLP development lifecycle to ensure that all outcomes are valid, verified, trustworthy, and explainable to a non-expert, ethical in the context of its deployment, unbiased in its learning, and fair to its users. This definition is adopted from Batarseh et al. (2021) and modified to fit the scope of the NLP domain.",
            "type": "explicit"
          },
          "Completeness": {
            "definition": "The ability to accurately describe the operations of a system that allows humans to anticipate its behavior better.",
            "context": "However, due to the lack of completeness, interpretability alone is not enough for humans to trust black-box models. Completeness is the ability to accurately describe the operations of a system that allows humans to anticipate its behavior better. Gilpin et al. (2018) argue that explainability improves on interpretability as a technique to describe the model’s decision-making process of arriving at a prediction and the ability to be verified and audited.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "304b9778edf07175e6964dfa3f37cacec6006197",
        "sections": [
          "Airports are symbols of mobility. They have become emblematic of our post-modern world. Yet in the early days of airports, there was no such thing as the rivers of passengers that flow through the sculpted steel and glass façades of contemporary terminal buildings. The airport consisted of a few tents and an airfield, remnants of air travel's military beginnings. Airports now cater for the millions of people who flow in, out and within countries each year. The huge mile long terminal structures are carefully built to facilitate the mobility of passengers, baggage, and cargo to their destination. To ensure the security of the country the flows are entering, while at the same time to protect the very means of their travel, these flows must be watched and controlled. The airport is well and truly a space under surveillance. And yet, the airport's role in surveillance has gone largely unnoticed by surveillance studies, reflecting the invisibility of these transient 'non-places' within the social sciences (see Crang, 2002;Gottdeiner, 2001;Rosler, 1998;Augé, 1995). Nonetheless, since the TWA flight crash in 1996 and the more recent September 11th attacks, countries such as the United States are pushing through increased measures to improve airport security and safety, launching airports into the public consciousness. Security itself has become an all-encompassing term, spreading into the current vocabulary. Widespread gun crime at schools has highlighted the 'airport-style security' measures of metal detectors and baggage checks that are now common place (BBC News Online, 1999). This paper explores several aspects of airport surveillance. In themselves, airports offer interesting case studies of how mobility may become watched and controlled. The heavy government presence in these spaces, the possibility for the invasion of privacy and the disregard of human rights must be questioned. We may also learn useful lessons at airports. For as recently discussed by Lyon (2003) airports may be seen as \"possible microcosms of wider societal surveillance trends\" (20) signposting possible surveillance developments in other spaces and places. In this paper, I shall therefore explore a number of the most recent surveillance techniques for monitoring and controlling passengers at the airport. These practices are argued to closely match designations of a 'panoptic sort' (Gandy, 1993) or surveillant sorting (Lyon, 2002b) from the surveillance studies literature. I discuss the implication of these techniques in terms of privacy issues and discrimination. In-turn, the second section to this paper examines the surveillance of non-human mobilities at airports. Informed by an actor network theory (ANT) approach and recent turns towards the agency of 'objects' and 'things', the paper provides a brief over-view of the surveillance of baggage, possessions and aircraft -indicative of a lack of research in this area and a trend towards the surveillance of non-humans. Firstly, let me contextualise this discussion within current turns towards mobility and security. I examine the theorisation of mobility within surveillance studies and trends towards surveillant sorting, before moving on to illustrate the increasing need for an ANT approach towards the subjects of surveillance.",
          "Recent progress towards the understanding of movement within surveillance studies probably reflects the current interest in mobility from disciplines such as sociology (Urry, 2002;2000), geography (Cresswell, 2001;Thrift, 1995) and cultural studies (Morris, 1988). This is no-doubt indicative of the increasingly mobile world in which we live, and must also owe its momentum to the popular fluid and mobile thought of philosophers such as Gilles Deleuze, Felix Guattari, Ian Chambers and Paul Virilio. This new mobility paradigm has moved beyond static idealizations of society towards theories that are marked by terms such as nomadism, displacement, speed and movement. Mobility has become as central as the tenets of society, space and power that fill much of our thinking. In many cases, mobility has often been understood to be an escape from a social order and as an act of resistance (Cresswell, 1993). De Certeau's (1984) tactics and Deleuze and Guattari's (1988) nomad-power transgress the fixity of the state. Gypsies and travellers cross physical boundaries and so become recognized as 'out of place' by authorities and local residents (Sibley, 1995). In many cases, physical mobility is often seen to signify the metaphorical movement from, and transgression of social norms. Mobility is therefore often viewed in terms of risk to the safe and static containers of space, territory and social order. Bauman's (1993) stranger and vagabond provide the perfect figures of risk to security as spatial and social boundaries are crossed. This is not to argue that mobility is always taken as deviant. Obviously, tourists and business travellers do not necessarily contradict social norms. Taking a relational (Cresswell, 1996) view of mobility allows us to see movement merely injected with meaning in different socio-spatial circumstances -it means different things, to different people, in different places and at different times. Therefore, where a tourist may be welcomed into a country a migrant may not be. However, the spaces at which mobility may be channelled provide the most obvious points for these social differentiations. Henceforth, it is at the borders, at airports -where movement and distinct spatial boundaries coexist -where undesirable mobilities may be distinguished from the desirable. This is increasingly achieved by surveillance. Surveillance is one method used to control mobility. As the growth of information and communication technologies allow data packets to be transferred between databases and other surveillant technologies, surveillance itself has become more mobile (Lyon, 2002b). For Lyon andothers (2002a, Phillips andCurry, 2002) however, surveillance may also struggle with mobility by stasis. What Lyon describes as the 'phenetic fix' characterizes surveillance techniques that attempt to capture the essence of movement -taking a snapshot of movement, bodies and identities. Information abstracted from moving bodies may be appropriated and fitted into neat categories and definitions. In this paper, I want to examine a particular form of the 'phenetic fix' labelled 'surveillant sorting' that is probably most illustrative of airport and border surveillance. Surveillant sorting is often used to describe the formation of those categories and profiles discussed above. Gandy's (1993) term the panoptic sort being another terminology. Issues surrounding sorting emphasise the possibility for discrimination -treating some better than others. Surveillant sorting is also able to distinguish the mobile. For example, studies have illustrated that the transfer of data packets may become shaped and prioritised, some given quicker speed and access than others (see: Graham and Wood, 2003). Mobile sorting may also work materially. Cars travelling through the Canadian-US border (Sparke, forthcoming) are put into categories of 'good' and 'bad' by the NEXUS system, the good are able to pass through quickly, where 'bad guys' are forced to wait in long queues for questioning. At airports, surveillance, rather than examination, is becoming the key method with which to distinguish passengers (Salter, 2003). In this paper, I examine a number of issues surrounding these procedures. And yet, I also want to explore a form of sorting that has gone on within the wider study of surveillance itself and which airports may be used to highlight. By this, I mean that there seems to be a tendency to limit our understanding of mobility to humans -to sort the human from the non-human. This paper could examine the mobility of passengers passing through airports and the associated methods of surveillance and control. However, to do this would be to ignore the orbiting movement of non-humans that are also subjected to the airport's controlling gaze. It is in this respect that we may turn to Actor Network Theory (ANT), the work of Bruno Latour, John Law and the recent veering of the social sciences towards the role of objects (Pels et al., 2002), things and their mobility (de Laet, 2000). These approaches are somewhat absent from surveillance studies as noted by Wood in a past issue of this journal (Wood, 2003). Wood observes Kirstie Ball's paper (2002) to be one of the only contributions to such a debate. Ball's approach pinpoints the elements of surveillance systems that \"are all socio-technically perpetuated through interactions of artefacts, technologies and people\" (586). Therefore, whilst humans and non-humans may form networks of the elements of surveillance, little emphasis has been placed upon these aspects of the subjects of surveillance. Examining how surveillance deals with mobility need not be limited to the mobile human, but extended to include things, and indeed their amalgamation with people (Haraway, 1991;Urry, 2002). Such a shift in focus is particularly pertinent when we turn to the increasing concentration upon the means of terrorism by governments and borders -shifting surveillance away from the perpetrators. Carter (2001) states that governments need better capabilities for the detection and prevention of terrorist activities. He argues that: This involves surveillance of persons and motives -a delicate matter -but also surveillance of potential means of destruction such as crop dusters, germ cultures and pilot instruction. Surveillance of means raises far fewer civil liberties issues than does surveillance of persons, and it might be much more effective. (15) The surveillance of the means of destruction is a much more attractive option for security. Governments take the view that a bag will not mind if it is x-rayed, CT scanned, and rummaged through. Examining the subjects of surveillance can enable our better understanding of the purpose, workings, and impacts of these systems. This approach is discussed in the final section. Firstly, it is useful to situate these practices within the development of air-travel, the growth of airports, and the rising need for airport security.",
          "The first airport resembling anything that we know today was built in 1922 at Königsberg in East Prussia (Zukowsky, 1996). Königsberg's terminal was a considerable advance from the windswept military landing fields and beaches. The new terminals became bridges for the transition between land and flight. Airports captured the imagination representing a symbol of progress and technological achievement in modernity. Indeed, the driving ambition of Hitler's Nazi regime was manifest in the rebuilding of Berlin's Templehof airport -the new airport reborn from the ashes of the failing past government and decaying economic system (Braun, 1995). From these early beginnings, airport space was a segregated and exclusionary one, being inhabited only by military personnel or the wealthy. Even before the First World War, aviation became an impossible dream fulfilled. Higher society frequented airfields to watch the displays of macho pilots and their flying machines (Wohl, 1994). Flights became social and cultural gatherings for the upper classes, excluding the lower. Soon, the development of the vacation and package holidays saw the growth in trips to Europe; to Spain and Portugal and the beginnings of mass transatlantic travel (Löfgren, 1999b). International air transport became less costly and connected to many destinations. Wide-body jets meant that 200 to 500 people could be carried by one flight eliminating the extortionate costs involved. The further deregulation of the American and British airlines saw airlines multiply, stimulating the competitiveness of seat prices, and also a greater choice of destination for many travellers. These changes also meant radical developments in the complexity of running airports as airlines rapidly grew and folded. Although, the growth of mass travel was still unable to fully democratize this space. VIP lounges developed for business and frequent flyers to enjoy the quiet luxury of a comfortable seat, the amenities of television, and most recently a shower or sauna. The progression of the airport culminated into the enormous multi-terminal, multi-runways sites that see millions pass through each year. They soon became airport-cities employing the work force of urban populations. Manuel Castells (1996) describes airports as a 'space of flows', nodes on the global networks of mobility. But the terminals are also built for a carefully controlled internal mobility. David Pascoe argues that the terminal was \"predicated on confluence, the precisely timed pumping of passengers into and out of [space]\", airlines then \"constructed specially designed terminals to control and direct the masses of passengers…\" (2001: 226). For as passengers increased so did the potential dangers and security risks. This was due partly to the function of airports as borders, but also to the nature of international terrorism that has developed over the past three decades. Perhaps Paul Virilio (1997) is correct where he writes that the border is now at the centre of our cities. Airports act as border zones to the vertical vectors of mobility that cross national and state boundaries. For this, Airports must function in a similar way to the borders that police the boundaries of our countries, regulating the movement of people that enter and leave. For Virilio, \"the doors to the city are its tollbooths and its custom posts are dams, filtering the fluidity of the masses, the penetrating power of the migrating hordes\" (1986: 7). Airports must therefore function as a screening or filter for the threats to a nation, but airports also become screens for quite different threats. Since the 1970's terrorism has grown, looking towards the aviation industry as a target. On the 22nd of July 1968, an El Al flight destined for Tel Aviv was hijacked. The hijacking was a bold political statement where passengers were to be traded for Palestinian terrorists imprisoned in Israel. By attacking the Israeli airline they were effectively attacking the Israeli state (Hoffman, 1998). In addition to this, the intense media coverage given to the hijacking saw the event publicized all over the world. Aviation became the perfect target for terror, which since 1968 has endured a rapid rise of these attacks. Planes were spaces that could be controlled easily; the fear of crashing subduing any passenger resistance. Airports also offered limited surveillance of the throngs of people that were travelling. The likelihood of a successful hijack has even been calculated, at 76% it is an obvious incentive for terrorists (Merari, 1998). Due to the increasing risks from terrorism and illegal immigration, airports had to find a way to take control of the airport space. Developments of airport security and surveillance were initially felt through actual changes to the space -to the architecture of the airport. John Zukowsky notes that, \"Intensified security measures changed the planning of airports, deliberately cutting up the open flow of space \" (1996: 15). The development of the sterile lounge concept (Wallis, 1998) saw baggage and security checks made before entrance to the gate. This removed the chance of threatening objects reaching the plane by person. Security checks could then be completed in a purpose built room far away from the gate to reduce queues disruptive to passengers. These techniques are still regarded as the preferred method, evident in the centralized security 'choke points' in most terminals (NRC, 1999). Surveillance has become therefore, one of the primary means of ensuring that airports are made safe and secure. 'Threat vectors' (NRC, 1999) that is, the paths by which threats may enter the airport and find their way to a plane are identified and monitored. In the next two sections, a number of these approaches and their implications are discussed.",
          "Monitoring passengers equally, proved to be far from the truth of running an international airport (Jenkins, 1998). Airport authorities needed a way of putting passengers under surveillance without having to examine every passenger rigorously. For this, methods of passenger screening and profiling have been developed to effectively sort the most probable threat to security from other passengers.",
          "The most recently publicized and perhaps most worrying form of surveillant sorting at airports has come through a surveillance technique known as profiling. Profiling is the ability for information or data about an individual to be built up. People may be sorted into profiles of particular consumer groups. The psychological profiles crime investigators use is an obvious example. Profiles are then used to predict a person's likely behaviour or the likely characteristics a criminal may embody. Profiling also usually relies upon vast quantities of information gathered about someone that are then stored and shared. In light of the growth of international terrorism, a new form of profiling has been developed called Computer Assisted Passenger Pre-Screening (CAPPS) in the United States. It is possible to discuss here some of the issues surrounding profiling and its potential impacts. Profiling was introduced in the US on the recommendations of the Al Gore led White House Commission for Aviation Safety and Security following the TWA flight crash of 1996. The Commission found that, \"passengers could be separated into a very large majority who present little or no risk, and a small minority who merit additional attention\" (1997). The rationale behind profiling is then to concentrate upon the minority -those likely to be a threat, rather than the majority, which requires much greater resources. The recommendations were based upon the practices of the airline El Al who used profiling techniques to fit the descriptions and likely behaviour of terrorists to passengers. CAPPS is reported to work by matching likely terrorist behaviour with present airline flight information. A flagged 'selectee' will then be subjected to personal checking of possessions and perhaps an interview and questioning. The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\" (1997). Problems are also clear over the secrecy of these profiles and the categories passengers become inadvertently pushed into. The lack of control passengers have over these profiles has led to concerns that innocent travellers may be unable to rid themselves of the 'selectee' status of their sorting. The repercussions of September 11th have seen the reorganisation of US airport security under the Transport Security Administration, itself under the newly formed Department for Homeland Security. Bolstered by this restructuring of governance, President Bush's developing and renamed Terrorist Information Awareness Program and rumours of the testing of CAPPS II have caused concern throughout privacy awareness groups. Posted to the US federal register on January 15th of this year, the TSA introduced a new system of records and amendment of the Privacy Act known as \"Aviation Security Screening Records\" (DOT, 2003) designed to facilitate the CAPPS II system. The proposal was open to comment and provoked a massive response from individuals and privacy awareness groups. The prime concern behind this system is again the discrimination and segregation of passengers, and also the sharing of passenger information between multiple government agencies. However, not only the sharing of Personal Name Records (PNR) are at stake (see Bennett, 1999), but the vague details of the report does not count out the possibility that more detailed banking records, tax histories and other sources of information may become easily passed between airports and other state departments. Fortunately, in Europe, different rules apply. In March 2003, the European Parliament moved to reverse an agreement made between the European Commission and the United States that would have forced PNR records to be transferred from European airlines to US airports. Another version of CAPPS II was posted to the Federal Register on August 1st with several revisions made upon the January version. At present, however, the majority of concerns lie in the current testing of CAPPS II, particularly with regard to the JetBlue controversy. The popular airline JetBlue illegally transferred a large number of passengers' records to an agency working for the US government (Singel, 2003). The information was to be used for the testing of passenger profiling on internal flights. Some of this information became public and was published to the Internet. A similar scheme has also been argued to be taking place through Galileo -the computer reservation system (CRS) that runs a large part of air travel reservation in the US.",
          "Other forms of surveillant sorting are occurring through biometric technologies. Biometry is the measurement of the body. Biometrics effectively treats the 'body as text'; identifying specific body parts such as the iris, face and palm signatures to identify the individual (see Ploeg, 1999Ploeg, , 2002)). The use of biometrics extends far beyond air travel, forensic evidence such as fingerprints are commonly used to identify suspects. Nevertheless, biometrics has also become popular at certain US and European airports. Biometrics work by containing detailed records of a passenger's body information (Delio, 2003), the most popular being retina patterns, but this may also include finger prints, DNA matching and face recognition (Agre, 2001) to name but a few. This information and passengers' identities are then stored on cards or a central database to be compared. Biometric systems are usually referred to as authentication systems, where a positive match to the stored information authenticates the identity (Clarke, 2003). However, other biometric systems, referred to as identification systems, compare the captured body data to large amounts of records kept on other databases. Here, passengers are not proving their identity; rather, airport authorities are identifying them. Biometric systems are of obvious concern to proponents of privacy rights, the critique and defence of biometrics coming from several different philosophical perspectives of technology and human agency (see van der Ploeg, 2003). One particular view is to critique biometrics for their treatment of passengers as objects. Much like a piece of baggage to be identified by a bar code, passengers' bodies offer similar bar codes to be read by authorities. Objections have also been raised over the penetration of bodies as biometric systems scan passenger identifiers. Others m ay analyse biometrics from the perspective of how the information is stored, particularly over the ease by which data may be shared with other 3rd parties. The possibility for data to be hacked and misused by external sources is another issue. Irma van der Ploeg (2003) discusses problems over the security of the American immigration service INSPASS authentication scheme at Los Angeles airport. Recorded hand geometry data was designed to be stored only on the card carried by the passenger. However, van der Ploeg illustrates that the card, if lost, would be quickly replaced by INSPASS thereby revealing the storage, somewhere, of this supposedly private information. But biometrics also has several surveillant sorting implications. For example, we can look to the effects upon the movement of passengers. Schiphol airport, Amsterdam has a fully working biometric system developed by the airport and the Dutch immigration service. The 'Privium' scheme is an 'authentication' scheme that, for 90 euros, allows enrolled passengers to bypass busy queues and check in delays. Passengers are then sorted into those enrolled by the scheme and those not. The logic behind these systems is similar to profiling in that it then gives more time for additional security measures to be placed upon those not enrolled in the biometric scheme. The systems therefore virtually sort passengers' information but also work to materially sort passenger's mobility within the terminal. The business 'kinetic elites' (Graham and Marvin, 2001;Andreu, 1998) may pass through to the VIP lounge at speed. And yet, the average traveller, forced to park long distances from the terminal, is excluded from member's club lounges and has to endure waiting in lengthy 'check-in' and security queues.",
          "Finally, the sorting of passengers has also begun with the introduction of what can be known as digital surveillance, or most commonly algorithmic surveillance (see Norris, 2002;Graham and Wood, 2003). Of course, CCTV has become one of the predominant modes of surveillance within airports as well as within cities, shops and indeed on my own university campus. The ability for CCTV to pick up deviant and threatening behaviour has been well documented, as well as its use in identifying suspects. Within airports, its continued and extended application has become a clear priority of airport security concerns. In the UK, the John Wheeler (2002) report on airport security suggested that valuable improvements have been made in this area. A technology now widely used at airports in the United States is a system called Exit Sentry developed by Cernium. Exit Sentry is able to monitor the direction of movement of passengers walking through the exit corridor of secured areas of an airport terminal such as arrivals. A passenger walking the wrong way, trying to enter the secured area through the exit corridor is warned with a flashing light. If the suspect then persists, a siren alerts security staff, and a recording of the suspects' movements is made. Systems such as Exit Sentry are exemplary of recent algorithmic surveillance technologies that in real time analyze CCTV footage of spaces. These systems understand the differences in movement of individual passengers, and may filter out static background information. Threats are not identified by a particular property of an object; rather, particular movements are inscribed with meanings of what is an allowed movement and what is considered suspicious and deviant. The uses for such technologies have been suggested to recognize the movements of car thieves and even people contemplating suicide at quiet train platforms (Norris, 2002). Indeed, research in progress at Southampton University has developed an approach called gait recognition that may be able to identify the identity of individuals by their distinctive walking styles. CCTV algorithmic surveillance effectively sorts and differentiates between mobility, in Exit Sentry's case, that is accepted (non-threatening) and that is unacceptable (threatening). The meanings are not essential to the mobility picked up by the cameras; rather the algorithms are imbued with these meanings. Sorting objects I now move beyond my discussion of airport surveillant sorting, towards a sorting of a different kind. Here I want to discuss the non-humans that have become placed under surveillance at the airport. As I have discussed airports are obviously places of human mobility. And yet, objects and things also intersect these spaces. Baggage flows through the baggage systems miraculously arriving at our destination. We carry on duty-free, cigarettes, and alcohol. But we are only aware of a tiny spectrum of the surveillance systems that place objects under this scrutiny. Initially, the sorting of objects and things at airports has occurred through the physical arrangement of airport spaces. For instance at the re-building of Templehof, the architect Ernst Sagebiel made the innovation of separating passengers, goods and baggage onto different levels of the airport (Braun, 1995). In addition, freight could arrive by an underground train opposite to the subway from which passengers arrived. This form of sorting is still used today, as automated baggage handling systems operate behind the holes in which our luggage disappears.",
          "The monitoring of our luggage at airports is probably one of the most visible methods of surveillance. Contaminating or illegal goods must be stopped. Airports are also nervous over the possibility of explosives and other weapons that may be used for terrorist activities. The identification of these objects is therefore one of the prime duties of airport security and immigration control as they concentrate upon 'threat vectors' (NRC, 1999). These objects may be identified and put under surveillance in a number of ways. If we first take hand luggage, the classic examples are probably the phase induction (PI) metal detection systems (Jenkins, 2002). Memorable scenes in films depict passengers unable to pass through these archways due to a belt or keys. At this point in a person's journey, hand luggage is usually x-rayed. X-ray machines are able to look inside a person's belongings, where operators search for suspicious looking objects, these include explosives, arms, and organic material. Although these x-ray systems, according to FAA research, could also be used upon passengers (NRC, 1996). X-ray operators could easily identify passengers carrying concealed weapons, without having to resort to personal body searches. Objects such as metals and sharp objects become a threat when held by a passenger. They have the potential to become a risk to the plane or the airport. The agency of a threat becomes possible because of the stable network of actors such as a knife, a passenger and indeed a plane. However, objects may take the shape of a threat independently of a person. For example, timer device explosives may be set in advance or barometric sensor devices respond to changes in pressure. Much like Latour's (1999) example of a sleeping policeman (speed bump), where the agency of a policeman to deter speeding becomes deferred onto the speed bump, here, the terrorists will or agency is given to the bomb. It is this deferred relationship that has led airports to implement baggage reconciliation: Positive Passenger Bag Matching (PPBM) recommended by the White House Commission (1997) in the United States. These systems are designed for the possibility that terrorists are not willing to blow themselves up when detonating a bomb. The terrorist would then check their bags onto a flight without actually boarding the plane. The PPBM system automatically flags up the bag that has been put onto the flight without an owner, the bag may then be pulled from the flight. Such a system relies upon the ability of the airport to know where a passenger's luggage is, through most recently, radio frequency (RFID) tagging and even individual trays that a bag is placed onto. Baggage handling systems most commonly use bar code technology however, so that baggage can be read to determine the identity of its owner and the flight it should be on. Surveillance technology has necessarily become more intense in an attempt to secure aviation from these objects. It is also much easier for airports to scan checked baggage than to actually interrogate a human subject. And yet, similarly to passenger screening, airports have not had the staff or equipment to check every person's bag. As such, the scanning of baggage has been linked to the CAPPS profiling systems so that a 'selectees' luggage is put under increased surveillance and identification for threats. For example, the Explosive Detection Systems (EDS) were initially used to scan 'selectee' passengers' bags for traces of explosives. Here, a 'selectee's' baggage is that scanned by the EDS machines. Since the events of 9/11 efforts have been made to increase the installation of EDS machines to ensure every passenger's bag is scanned, this has culminated in the International Civil Aviation Authority (ICAO) recommending that 100% hold scanning be made by 2006. Other systems use the computer tomography (CT) scanners used in hospitals. A slice or tomography of a bag can then be used to calculate the mass and density of materials that are then matched against explosive and hazardous materials.",
          "Perhaps the object we are most conscious of at airports is the passport. Although, considering the passport's importance to international travel there has been very little written about it in the social sciences (for an exception, see: Torpey, 2000;Caplan and Torpey, 2000;O'Bryne, 2001;Salter, 2003). And yet, the passport is one of the primary tools of states for the surveillance of their population's movements. For Torpey (2000), passports provide the means to govern a population's movement, 'penetrating' the individual to 'embrace' populations. Passports are used to identify a person, making a person legible to the state who may then enforce their authority over movement. Passports are also symbols of nation-states and our allegiance to them (O'Byrne, 2001). Paul Fussell's (1982) Abroad typically articulates the standardization that passports impose upon the traveller: where do people belong, where do they live, what is their hair colour, eye colour. All these aspects of our selves that we continually negotiate and question become squashed into the tight categories of the passport. For Löfgren (1999a): \"as a traveller you now had to live up to your passport identity to be able to prove your identity\" (19). This is nowhere more obvious than at the airport. Identification is then one of the primary means of airport surveillance; the display of the passport marks a 'reading' of the individual that occurs at multiple times during their journey. From supplying the ticket at check in, to security, to boarding the plane with your boarding pass and then landing and undergoing immigration control in the destination country, the passport must be displayed many times in the airport. It has become integral to the ritual of international travel. Airport staff and workers must also supply, not their passport, but a similar identification card to access areas of the airport. Indeed, airport vehicles must also be identifiable, clearly bearing company insignias and paint designs. In terms of mobility this is vital, passports and identity cards govern where a person, airport worker or object may go. Passports are a marker of our identity. Identities are read and given mobile and spatial limits by states and airports as they regulate mobility.",
          "We can also look outside the terminal to airspace for examples of non-human surveillance. Airspaces are territorial units but they also act as highways for the traffic of aircraft, these spaces must also become monitored and controlled for the purposes of security and safety. At the dawn of civil aviation, the organization of plane movement was described as \"decidedly sketchy in nature\" (quoted in Wegg, 1995: 115). The pilot had to rely upon skill and sight, and had no knowledge of local weather conditions. In order for the aviation industry to provide efficient and safe travel, it was essential that more could be known about environmental conditions. The first known aviation weather station in the UK was introduced at Croydon airport in 1921 (Wegg, 1995). This surveillance of weather and the general environment is perhaps an increasingly common trend for surveillance. Brazil's System for the Vigilance of the Amazon (SIVAM) developed by Raytheon provides a huge environmental monitoring system to show signs of illegal drug trafficking; mining and logging that have previously gone undetected in the rain forest. Still, the monitoring of weather is obviously not the only concern of air traffic control. As was quickly found in the 1920's crashes were most likely to occur around airports, where planes in the air and on land found it difficult to avoid each other. The need to orchestrate these movements proved paramount to safety. For this, Air Traffic Control (ATC) developed at airports, reinforcing the idea of an airspace that could be vigilantly monitored and controlled for the purposes of both safety and security. Again, at Croydon airport, the new terminal built in 1928 featured an ATC. The tower, or \"chart house\" scanned for incoming aircraft and maintained radiotelephony (RT) communications with planes. The towers therefore monitored all the positions of incoming and taking off aircraft. Un-identified or enemy planes could also be monitored from an ATC. More sophisticated radar technologies have since developed, the air traffic control radar beacon system (ATCRBS) being one. After takeoff, aircraft turn on their transponders that send signals recognized by equipment at air traffic control that may then monitor not merely the position of an aircraft but also the unique identity of the flight can be gained from the transponder signal. Air traffic control, can then manage and organize the airspace for the most efficient and safe flow of aircraft. A similar system has also developed on the ground at airports so that 'aircraft incursions' -planes wandering onto runways without permission -may be avoided. This has usually been completed using a pair of binoculars; however, problems arise during periods of low visibility in bad weather and particularly fog. Airport surface detection equipment (ASDE) is a radar system able to locate and monitor planes movement, combined with tower automated ground surveillance system (TAGS) aircraft and indeed, any other airport vehicle may also be identified on the ground surface (Wells, 1996). Although, it must be noted that newer airspace surveillance is moving considerably away from the control tower as power is given back to the planes. Instead, decentralised forms of surveillance are becoming evident in the form of intelligent on-board systems that allow each aircraft, and airport land vehicles to monitor each other's position.",
          "This paper has in part attempted to question the broad brushstrokes of 'mobility' and 'flow' that are often used to describe airports. Airports act to monitor these flows and filter out threats, sorting and categorizing mobile bodies. Whilst this may happen spatially in-terms of access to specific places of the airport, it also occurs in terms of movement. As I have shown, people may become discriminately separated into categories that also affect their passage and speed through the airport. These systems have been shown to be increasingly troubling in their invasion of privacy and the possibilities for expansive data sharing. In-turn this paper has examined a conceptual sorting within surveillance studies, using the airport to recognise how non-humans may become monitored and controlled -to explore the surveillance of not only human but also non-human mobility. This approach has been argued to be of particular significance given that governments are increasingly turning to non-human surveillance (Carter, 2001). Now the focus is on the means of terrorism. Objects such as explosives have become actants in the eyes of airport security, where barometric or timer explosives pose threats independent to the whereabouts of their makers. Indeed, the surveillance of weather and the location of the actual aircraft is also key to the safety concerns of flight. The mobility of non-humans therefore, pose new challenges for airport surveillance in a climate where 'terror', 'bomb' and 'explosives' are words on every passenger's minds. At its most basic level, I think an ANT approach can help scholars to acknowledge this monitoring of non-humans. Then we can go further than the mere description of these techniques. ANT's implication for surveillance studies is not the simple mobilisation of supposedly inaminate, non-sentient or nonhuman things that may be monitored. We can do more than merely \"follow the actors\" (Latour, 1993; see also : Hitchings, 2003). In particular, ANT may be used to give greater insight into the relationships or networks of objects that surveillance systems examine. But this exercise may seem somewhat pointless if the relationships between humans and nonhumans are not understood. Indeed, investigating the monitoring of a machine in a factory may seem interesting, but is this important for surveillance studies? Does this even fit our definition of what surveillance is? In my opinion, for now, our starting point must be human. This paper has separated its treatment of humans and non-humans in an attempt to illustrate a tendency for scholars to delimit their focus to the study of the surveillance of humans and its implications. It has not been my purpose to forget the complexity of relationships between people and things, the cyborg 'machinic ensembles' of the plane or car that transport us to our destination (Thrift, 1995;Lyon, 2001). Whilst the separations I have made have been deliberately artificial for ease of illustration, any examination of the surveillance of things, only really becomes important if we are to appreciate the impacts they may have upon the human and indeed the human associations with these things. Certainly, it seems as though the surveillance of non-humans, in the examples I have explored, are used precisely because of the difficulties posed by monitoring the human. Many of us do not wish to have a bar-code printed on us, or to be radio tagged. Therefore, non-humans are monitored instead of humans. In-turn non-human monitoring may also be used as indicators of human activity -as evidence. It is far easier to trace illegal weapons and explosives than to read the minds of passengers to test for the intent of their actions. In other cases, particularly remote monitoring, the material effects of human activity are easily seen by satellite photography. Illegal logging may therefore become recognised on vast areas of the Amazon by its initial impacts, as opposed to the individual actions and movements of a person. Of course, the network metaphor may not always fit. At airports, the relationship of passenger and baggage only really resembles a traditional network when passengers' are in contact with their luggage -when their relationship is static. More fluidic metaphors can be invoked to better describe the often shifting relationships of passenger to bag -a 'mutable mobile'. Furthermore, Law and Mol's (2001) article suggests that the metaphor of fire is useful to describe the oftenflickering presence of actors within a network. This metaphor can be seen in the instance of a passenger not boarding a flight and leaving their baggage behind. As discussed, baggage reconciliation ensures that this separation is not complete given the linkage created between passenger and bag. If a passenger fails to get on their flight, their 'absent presence' flickers into significance to ensure the removal of their luggage from the plane. At the same time, do we also limit our examination to humans, objects and things? Activists may argue that the surveillance of animals should be questioned. At airports in particular, animals are routinely transported illegally. But animals are in-fact still key to the surveillance and security operations at the airport. Animals are not only a subject of surveillance but are part of the surveillance system itself. Sniffer dogs or K9 teams can be trained as vapour detectors to monitor illegal goods such as narcotics, and explosive devices (Clutterbuck, 1994). These practices are not simply completed by people or animals but succeed because of their mutual relationship (Saunders, 1999; see also Haraway, 1991;Wolch, 2002). Questioning these relationships is long overdue in our fixation upon the high-tech. We must ask how do these interactions take place? What implications do they have? However, this is not to suggest that ANT is a magic recipe for success. There are obviously many shortcomings illustrated by critiques that query the pretension to inclusion -the universal enfranchisement of ANT (Lee and Brown, 1994), its traditional ignorance of spatiality (Hetherington and Law, 2000) and indeed the shortcomings of the network metaphor as illustrated above. That said we should not shy away from the many positives. Examining the networks of the subjects of surveillance sheds light upon the relationality of humans and things. An exploration of these networks exposes how an amalgamation of people and things may become illegitimate, worthy of surveillance. For example, a terrorist 'agency' or threat can be understood as the achievement of a person carrying scissors on a plane. Examining surveillance through this lens may reveal much about the very workings of surveillance systems, the norms and assumptions that frame them, and the implications for those they monitor. The control of the airport is in my view, a suitable starting point for such a discussion."
        ],
        "ground_truth_definitions": {
          "racial bias": {
            "definition": "biases being held towards ethnicity, and national origin - a person becoming sorted based on personal prejudice",
            "context": "The passenger's baggage will also be subject to increased surveillance, through additional scans and personal checks. Of obvious concern here is the possibility of discrimination towards particular passengers, biases being held towards ethnicity, and national origin -a person becoming sorted based on personal prejudice. Although, the Gore Commission report stated that it must be ensured that, \"selection is not impermissibly based on national origin, racial, ethnic, and religious or gender characteristics\"(1997).",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "3a03068318afdaa75a9dbdcbf960d63bf8035f0e",
        "sections": [
          "quality of information about a company, including, for example, company size, number of analysts following it, and the media coverage, it is reasonable to hypothesize that, ceteris paribus, investors may be able to gather value-relevant information about companies local to them (henceforth local companies) with greater ease and accuracy than they could about remote companies (henceforth non-local companies). Indeed, Coval and Moskowitz (2001) demonstrate that professional managers' local investments outperform their remote investments, a finding that both provides a richer characterization of professional managers' skill and, more importantly in the context of this paper, suggests that proximity to investment opportunities facilitates the acquisition of disproportionately accurate value-relevant information. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, 1 we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). 2 We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value. This figure is disproportionately high-nearly 20 percentage points higher than the average percent of all firms headquartered within 250 miles from the household (both equally and value-weighted). While it appears that the professional managers' preferences for, and especially success in, pursuing local investment opportunities stem from their ability to exploit the ensuing information asymmetries (Coval and Moskowitz, 2001), it is not clear whether the individual investors' local bias is primarily a result of their ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). That households would have a disproportionate preference for local stocks is not unexpected. There is evidence that people tend disproportionately to invest in the companies they are relatively familiar with, be it by virtue of their inclination to favor domestic investments over 1 To protect the confidentiality and anonymity of investor data and the retail broker, all researchers using the data set must sign and adhere to an appropriate non-disclosure agreement. 2 For existing studies of individual investors' investment see, e.g., Barber and Odean (2000, 2001), Dhar and Kumar  (2001), Dhar and Zhu (2002), Goetzmann and Kumar (2002), Hong and Kumar (2002), Kumar (2002), and Zhu  (2002). investments in foreign assets-the so-called home bias,3 or, in the domestic arena, invest disproportionately into own-company stock (Benartzi (2001) and Liang & Weisbenner (2002)). If individual investors' local investments stem primarily from non-information based reasons, as the evidence presented in Huberman (2001), Grinblatt and Kelohraju (2001), and Zhu (2002)   suggests, then investors' locality should be interpreted primarily as a behavioral phenomenon. If locality stems from household investing into the companies they are familiar with, though not necessarily particularly informed about, local investments will likely not earn superior returns on average. Indeed, Benartzi (2001) finds in the context of 401(k) plans that allocations to company stock, the most local of all investments, does not predict future returns. On the other hand, if the individual investors' local bias is information-driven, we should uncover evidence of superiority of their local investments, at least relative to their non-local investments. Such a finding would shed some light not only on the individual investors' ability to identify and exploit informational asymmetries, but also on the interpretation of the professional managers' investment skill. Indeed, if individual investors could generate superior returns on their local investments, then such an accomplishment by professional managers would seem less impressive and should be reinterpreted primarily as the skill first to realize that there is investment value in uncovering local information and then to collect it, rather than the skill to carry out particularly insightful analyses of such information. The central question we address is whether local investments are based on value-relevant information, in which case those investments should do well on average, or whether they stem primarily from households investing into the companies they are familiar with, though not necessarily particularly informed about, in which case the local investments will likely not earn superior returns on average. Thus, we next address the relation between location-driven information asymmetry and household portfolio returns. Simply put, if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios. In our initial inquiry, we find that local investments outperformed non-local ones by 3.7% per year over a one-year horizon. The excess return to investing locally is even larger among stocks not in the S&P 500 index (firms where informational asymmetries between local and non-local investors may be largest), while there is no excess return earned by households that invest in local S&P 500 stocks (where informational asymmetries are likely smallest). Moreover, the evidence is robust to the choice of the period of returns measurement, various adjustments for risk, the industry composition of the portfolio, the location of the household, as well as alternative explanations such as insider trading on own/former employers' stock. Having established that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks, we explore the benefits of pursuing locality by mimicking households' local investments and shying away from their non-local investments. To that end, we first dissect common stocks in each household portfolio according to two dimensions, locality of the stocks and their S&P 500 status (our proxy for information asymmetry), and then form aggregate position-weighted portfolios across all households. The results support the hypothesis that locally available information is valuerelevant. Specifically, the zero-cost portfolio that takes a long position in local investments and a short position in non-local investments has statistically and economically significant returns: monthly raw (risk-adjusted) returns are 19 (12) basis points. This effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and riskadjusted returns are substantially smaller for S&P 500 stocks (12 and 8 basis points per month, respectively) than for non-S&P 500 stocks (27 and 26 basis points per month, respectively). These results are striking, as the only portfolio formation criteria were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference. The final question we address is whether the effect of locally available value-relevant information can be compensated for by simply observing a firm's local ownership, that is, the collective opinion of the potential investors local to the firm. In other words, if the investor is not local, should he join the locals (i.e., follow the actions of the investors local to the firm)? The results suggest that merely favoring non-local non-S&P 500 stocks with high local ownership goes a long way toward reducing and almost eliminating the information asymmetry between the local and non-local non-S&P 500 stocks. Simply put, mimicking what locals do is about as good as being local. The remainder of this paper is organized as follows. Section I presents the data and summary statistics. Section II considers the characteristics and determinants of locality of investment from the perspective of the household. Section III focuses on the determinants of firm relative local ownership. Section IV seeks to characterize and quantify the information asymmetry related to locality of investment and provides a number of robustness checks. Finally, Section V concludes.",
          "",
          "We compile the data for this paper from several sources. The primary source is a large data set, obtained from a retail brokerage house, of individual investors' monthly positions and trades over a six-year period from 1991 to 1996. The data set covers all the investments 78,000 households made through the brokerage house, ranging from common stocks, mutual funds, government and corporate bonds, foreign securities, to derivatives. Each household could have as few as one account and as many as 21 accounts (the median is 2). The data set also provides some information about the households, such as their zip code, self-reported income, occupation, and age. For a detailed description of the data set see Barber and Odean (2000). In this paper we focus on the common stocks traded on the NYSE, AMEX, and NASDAQ exchanges. Common stock investments constitute nearly 2/3 of the total value of household investments in the sample. We use the Center for Research in Security Prices (CRSP) database to obtain information on stock prices and returns and COMPUSTAT to obtain several firm characteristics, including company headquarters location (identified by its state and county codes). We use the headquarters location as opposed to the state of incorporation because firms often do not have the majority of their operations in their state of incorporation. The sample of households used in this study is a subset of the entire collection of households for which we could ascertain their zip code and thus determine their location. We identify around 11,300 distinct zip codes in the data. We exclude from the sample households that were not located in the continental U.S. (that is, investors from Hawaii, Alaska, and Puerto Rico) and several zip codes corresponding to military installations. We obtained the latitude and longitude for each of the zip codes from the U.S. Census Bureau's Gazetteer Place and Zip Code Database. The corresponding company location comes from COMPUSTAT Annual Research Files, which contain the information regarding the company headquarters' county code. Finally, we identify the latitude and longitude for each county from the U.S. Census Bureau's Gazetteer Place and Zip Code Database as well. We use the standard formula for computing the distance d(a,b) in statutory miles between two points a and b as follows: d(a,b) = arccos{cos(a 1 )cos(b 1 )cos(a 2 )cos(b 2 )+cos(a 1 )sin(b 1 )cos(a 2 )sin(b 2 )+sin(a 1 )sin(a 2 )} 180 π r, (1) where a 1 and b 1 (a 2 and b 2 )are the latitudes (longitudes) of points a and b (expressed in degrees), respectively, and r denotes the radius of the Earth (approximately 3,963 statutory miles). We exclude the stocks we could not match with CRSP and COMPUSTAT; they were most likely listed on smaller exchanges. We also exclude stocks not headquartered in the continental U.S. The resulting \"market\"-the universe of stocks we could obtain the necessary characteristics and information about-consists of 4,827 stocks at the end of 1991, which cover 76% of the overall market capitalization at the end of 1991. In each sample year, we discard the households with the total portfolio value under $1,000 as of the end of the previous year. In sum, the resulting sample of households holding at least one common stock that we could identify and characterize in at least one of their account portfolios consists of 31,828 households at the end of 1991. This number tends to change somewhat over the sample period, reflecting primarily the liquidation of some accounts (the trades in the last month sell off the positions at the end of the previous month and there are no subsequent positions or trades recorded for the account).",
          "Basic household sample characteristics are presented in Table I. The table reports summary statistics of household income and household portfolio statistics at the end of 1991. 4 Panel A presents the income statistics. Households could report income in nine ranges (0-15,   15-20, 20-30, 30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars per year). One eighth of the households (13%) did not provide income information. We assume that the income level for the households in the first eight categories was the midpoint of the income range. Based on calculations using the 1992 Survey of Consumer Finances (SCF), the average household income in 1991 for households that had at least $125,000 of income was $250,000 (see Kennickell and Starr-McCluer (1994) for a description of the SCF data set). Thus, we assign the income level of $250,000 to the households that reported income greater than $125,000. The median household that actually reported its annual income reported income in the range from $50,000 to $75,000 (the midpoint presented in the table is $63 thousand) and the interquartile range of income range midpoints is $45-$113 thousand. Thus, households in the sample are clearly more affluent than the average household in the U.S. Panel B focuses on households portfolios of NYSE, AMEX, and NASDAQ common stocks as documented by the position files at the end of 1991. The mean value of common stocks in household portfolios was $29,643, while the median was $11,201. The average household held relatively few stocks (on average 3.0 stocks; the median was only 2 stocks per household). Around 38% of the households in the sample held assets other than common stocks (most often mutual funds). Among such households, around one-third of the average household portfolio was allocated to assets other than common stocks (35% average; 29% median). Thus, the commonstock portions of household portfolios in the sample typically consisted of a handful of stocks, which precludes the possibility of extensive diversification among common stocks. 5 We focus on these three household portfolio characteristics and on household income because we view them as (admittedly noisy) measures of two important facets of investment: propensity toward diversification (larger number of stocks, presence of asset classes other than common stocks) and availability of resources to collect information (larger dollar value of the portfolio and household income). Table I, Panel C further summarizes household portfolio statistics pertaining to the locality of their common stock investments according to two metrics. The first is a simple comparison of the distance from the household to its portfolio and the distance from the household to the market portfolio (the upper three rows in the panel). The second metric employs household proximity to company headquarters, where we set the threshold of locality at the distance of 250 miles (the lower three rows in the panel). Household portfolio distance measures 4 Portfolio summary statistics for subsequent years (not reported) are qualitatively similar to those presented in Table I. Details are available upon request. 5 While diversification could be more pronounced for the households holding other asset classes, the summary statistics suggest that nearly 2/3 of all households in the sample (62%) held only (typically very few) common stocks and were thus largely undiversified. Moreover, Goetzmann and Kumar (2002) show that the investors' portfolios were under-diversified relative to the extent of diversification possible when holding few stocks. were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position.",
          "portfolio is closer to the household than the market by about 300 miles. Put differently, average (median) households were closer to their portfolios than to the market portfolio by around 23%, although the nontrivial interquartile range suggests a broad range of locality, from households substantially closer to their portfolio than to the market (by around 700 miles or more) to those that are somewhat further away from their portfolio than to the market (by around 150 miles or more). These figures, though illustrative, do not necessarily treat investors in an equitable fashion, as investors in New York, for example, that hold the market portfolio would turn out to be far more local than investors in North Dakota that hold the same portfolio. In most subsequent analyses, we use a more intuitive and less biased approach. We regard 250 miles as a plausible upper bound on the span of local information and regard all investments into companies whose headquarters are within that distance as local. The distance of 250 miles is still reachable via a moderately strenuous daily roundtrip by car; local newspapers, radio, TV, and other media still occasionally provide some coverage of the local events within that distance. This cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. 6 Nevertheless, we identify strong locality effects that match economic intuition based on information asymmetry. In the sample of households at the end of 1991 the mean (median) percentage of the number of stocks in the household portfolio headquartered within 250 miles from the investor is 30.7% (0%). In fact, slightly less than one-half of the households (15,458 of 31,828) invest in at least one local firm. The interquartile range further suggests that at least 25% of the households in the sample have more local than non-local stocks in their portfolios (the actual number is 32%), a statistic that could be driven in part by the geographic distribution of investors and companies. As shown in Figure 1, New York and California boast disproportionately high numbers of both. We control for the geographic distribution of companies relative to investors by subtracting from this percentage another percentage-the fraction of all firms within 250 miles (presented in the fifth row of Table I, Panel C). The resulting measure of household locality, call 6 As a robustness check, we replicate the key analyses with the locality cutoff set at 100 kilometers (see Section IV.H) and the results are even stronger, which suggests that our choice of 250 miles as the cutoff is conservative. it H LOC,w , is summarized in the last row of Table I, Panel C. It is similar to the simple distribution of the percentage of portfolio stocks within 250 miles. The mean percentage of local firms that households invest into exceeds the percentage of all local firms by 18.1% on average. Before proceeding, we provide an illustration of the properties of H LOC,w . Its range is from -42% to 99.95% at the end of 1991. The first (low) extreme is typical of the households that chose to invest only non-locally, yet live in the area with many local investment opportunities (large percentage of the total market). A prominent example is a completely nonlocal investor living in New York City or Silicon Valley. The other extreme is typical of investors living in remote areas where there are very few local investment opportunities (and all are very small companies), yet their entire household portfolios are invested locally.",
          "",
          "Table II explores household portfolio locality in still more detail. The first column reports the distribution of household investments by locality that would prevail if investors invested in the most obvious benchmark-the market portfolio. As an alternative, given that the investors are not well-diversified (the mean number of stocks in a household's portfolio from our sample is three), in column (2) we also consider a different benchmark. 7 The portfolio shares listed for this benchmark represent the hypothetical locality breakdown if instead of holding its individual stocks each household invested in the appropriate value-weighted industry portfolio corresponding to each of its individual stocks. 8 The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio. The next five columns, columns (3) -( 7), present shares of portfolio value for all households, as well as the subsamples selected according to 7 We thank the referee for suggesting this robustness check. 8 Firms are assigned to one of the following 14 industry groups based on SIC code: mining, oil and gas, construction, food, basic materials, medical/biotech, manufacturing, transportation, telecommunications, utilities, retail/wholesale trade, finance, technology, and services. Using either the equally-weighted industry portfolio corresponding to each stock or a randomly-selected firm in the same industry as the benchmark yield very similar results. certain characteristics. Finally, the last column presents the fraction of households that invest only into one of the categories listed in the rows of the table in both panels. The first three columns illustrate how the portfolio shares deviate from the shares that would be found if every household invested in the market portfolio or the appropriate industry portfolio. For example, the average household invests 31% of its portfolio locally; however, the local share would be only 13% if each household invested in the market or the appropriate industry portfolio. Thus, relative to either benchmark, individuals' portfolios are subject to pronounced a local bias. The bulk of this local bias can be attributed to investments in local non-S&P 500 stocks. These stocks constitute on average 15% of the household portfolio, but would only make up 3% if the household invested in the market. Households also tend to place disproportionately more weight on local S&P 500 stocks (16% in actual portfolio versus 9% in the market portfolio on average), while placing substantially less weight on non-local S&P 500 stocks (41% in actual portfolio versus 61% in the market portfolio on average). Finally, the average household portfolio share allocated to S&P 500 stocks is 57%, 14% less than the share if investing in the market portfolio. Intuitively, it is this 14% under-weighting of the S&P 500 that allows for the 12% over-weighting of non-local non-S&P 500 stocks in the average household portfolio. These tendencies may be related to information asymmetry. Simply put, there are likely companies about which there is more information than the others. Potential determinants of such information asymmetry could include the overall size of the company (measured by market capitalization, number of employees, number of plants, etc.), the number of analysts following the stock, and membership in a major stock index. For example, the potential for sizeable information asymmetry is considerably smaller for S&P 500 than for non-S&P 500 companies. Thus, it is likely that information asymmetry is the most pronounced in the case of non-local vs. local investment into non-S&P 500 companies. Intuitively, investments into local S&P 500 stocks are unlikely to be driven by the hope to exploit value-relevant information available only to local individual investors. The third column of Table II suggests individual investors' portfolios exhibit greater disproportion between S&P 500 and non-S&P 500 non-local investments than between local ones. Across all households, 3/5 of non-local investments are in S&P 500 companies on average, while roughly half of local investments are in S&P 500 firms. Thus, investors tend to favor the remote familiarity of non-local S&P 500 stocks over the uncertainty of non-local non-S&P 500 stocks. The next four columns of Table II, columns (4) -( 7), test for robustness of the basic breakdown; they feature virtually identical breakdowns for all households exclusive of CA, CA households, as well as households with portfolio values of at least $10,000 and at least $50,000, respectively. Focusing on households from California, who constitute just over one quarter of the sample, is done to confirm that locality in investments is not driven by these CA households. The extent of locality of households with larger portfolios is an empirical issue. It could be higher than for smaller portfolios because larger portfolios signal the availability of resources to investigate and uncover value-relevant local information. On the other hand, it could be lower because larger portfolios are likely to consist of more stocks and also be combined with investments other than common stocks, both of which are suggestive of tendencies to achieve better diversification. As it turns out, with the increase in portfolio value the presence in local non-S&P 500 stocks is gradually substituted with non-local S&P 500 stocks. Finally, the last column of Table II illustrates that many households are either all local (one-sixth) or all non-local (one-half). This is not too surprising, given the small number of stocks in the average household portfolio. About one out of every twelve households only hold non-S&P 500 firms headquartered within 250 miles.",
          "Table III presents the results of fitting cross-sectional regressions of four household portfolio (HHP) locality measures, computed at the end of 1991 and summarized in Table I,  ",
          "In this section we focus on the determinants of local stock ownership. We define RLO j,t , the firm relative local ownership for firm j at the end of year t, as follows: denote the total value of the position in stock j by the households that had household portfolios of at least $1,000 and were local and non-local to firm j, respectively. As before, the threshold for locality is set to 250 miles. To eliminate the noise stemming from thin ownership in the sample, we include in the following analyses only the firms held by at least five investors in year t. Relative local ownership, RLO j,t , is defined as the fraction of household stock ownership in the firm held by households located within 250 miles of the firm (the first fraction) less the fraction of total nation-wide household portfolio wealth held by households located within 250 miles of the firm (the second fraction). A key result from Section II is that households on average invest disproportionately into local firms. In this section we consider the dual question of what characterizes firm relative local ownership. To that end, we form a panel of all firm-year observations in the sample and regress firm relative local ownership on several firm characteristics. The panel structure of the data prompts us to account for the unobserved year effects by adding year dummy variables into the specification and to allow for the correlation of the firm-specific error terms across time. Table IV presents regression results for all firms, S&P 500 firms, and non-S&P 500 firms. Aside from the regression approach outlined above (denoted as \"Mean\"), we also report the results of a robust median regression estimator (denoted as \"Median\"). Regressions fitted for all firms highlight that firm relative local ownership is related negatively to leverage, positively to the number of employees, and negatively to the firm's S&P 500 membership status, while it is largely unrelated to market value.10 From the perspective of information asymmetry, we interpret the number of employees as a noisy proxy of the opportunity to collect value-relevant information locally, or perhaps to simply become aware of the firm's existence (familiarity), which is gained through social interactions with the employees (i.e., word-of-mouth). There is also an alternative explanation for the positive effect of the number of employees on relative local ownership. Other studies have documented the effects of an underlying strong preference for company stock or equities, showing that those who hold a high proportion of equities in pension savings also hold a high fraction of non-pension assets in equities (Bodie and   Crane (1997) and Weisbenner (2002)), and that employees for firms that require the company match contributions to the 401(k) plan to be invested in company stock tend to invest more of their own 401(k) contributions in company stock (Benartzi (2001) and Liang & Weisbenner   (2002)). If employees of firms tend to invest part of their non-pension plan wealth in company stock, this could also explain why local ownership is higher for firms with more employees. However, these issues are unlikely to drive locality in the sample. The majority of the accounts in the database are non-retirement accounts. The accounts that are retirement accounts are either Individual Retirement Accounts (IRAs) or Keough plans, none are retirement plans sponsored by an investor's current employer. Thus, if a household is investing in its employer's stock through its 401(k) plan, this will not be contained in our data. The only investment in the stock of a company household members currently work for that would be reflected in our data would be investments in non-401(k) accounts. However, if a household member left an employer that offered a 401(k) plan, then the balance of that plan could have been rolled over into an IRA. Thus, while retirement account holdings in our data will not contain stock of the current employer, they may contain stock of a former employer. While a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data, households' potential ownership of a previous employer's stock through an IRA, or employees investing in their current firm's stock in non-retirement accounts potentially could cloud the conclusions drawn from our subsequent analyses, as the extent of locality and the returns based on locality might stem primarily from trading on inside information. 11 To address this concern, in Sections IV.B and IV.F we conduct appropriate robustness checks. We regard the S&P 500 status as a fairly accurate measure of both familiarity and the extent of availability of private, value-relevant information that would be more easily accessible to local investors. Simply put, S&P 500 companies are followed by many stock analysts, owned and researched by many institutional investors, held by numerous S&P 500 index funds and other mutual funds, and closely followed by the financial press and other media. Implications of such popularity of S&P 500 stocks are twofold. First, there generally is disproportionately little value-relevant private information regarding S&P 500 stocks to begin with and locality is unlikely to help (being from Seattle does not likely give a substantial advantage in acquiring information about Microsoft). Second, it does not require locality to be familiar with S&P 500 stocks. Thus, it is not surprising that the loading on the S&P 500 status is significantly negative. Moreover, the regressions fitted for S&P 500 firms did not yield any statistically significant determinants whose significance would be consistent across the two regressions (mean and median). It thus appears that relative local ownership of S&P 500 firms is not strongly related to firm characteristics. On the other hand, regressions fitted only for non-S&P 500 firms result in the statistical significance, same direction, and comparable magnitude of the impact of leverage and number of employees. This confirms the intuition that non-S&P 500 firms were the primary drivers of the results from Table IV for all firms. stem from the extraordinary performance of micro-cap stocks in the sample period. We thank Josef Lakonishok for pointing out this issue.",
          "",
          "This section examines the relation between location-driven information asymmetry and household portfolio returns. We seek to quantify the value of locally available information via a simple proposition that if individual investors succeed in collecting and processing locally available value-relevant information, then the value of that information should be reflected in the performance of their household portfolios.",
          "The initial point of inquiry is the relation between household portfolio performance and the allocation of household portfolio weights to local and non-local investments. We first regress raw household portfolio returns 11 We thank the referee for raising this concern and for suggesting a means of addressing it. 12 Local investment outperformed non-local ones by 4.7% (9.2% with fixed effects) over the 3-year horizon and by 15.8% (25.0% with fixed effects) over the 5-year horizon. To capture the notion that there should be more information asymmetry between local and nonlocal investors in non-S&P 500 stocks as opposed to local and non-local investors in widely followed and researched S&P 500 stocks, we also estimate the following regression: zip-code level fixed effects) per year on the one-year horizon, whereas the differences on longer horizons were not statistically different from zero. By contrast, the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments is substantially higher, that is, 6.1% (5.9% with zip-code level fixed effects) per year on the oneyear horizon and, generally, this gap grows with the investment horizon. 13 The third highlighted row in the panel features the difference between the two differences -this reflects the return to locality in non-S&P 500 companies relative to S&P 500 companies. Its interpretation is the extent to which the premium in investing locally increases with the increase in information asymmetry, in this case 4.5% per year (2.4% with fixed effects) on the one-year horizon and progressively more for longer investment horizons. Though statistical significance of this difference is sometimes dampened with the introduction of zip-code fixed effects, the results presented in this table are compelling. These analyses included all the households that held at least $1,000 in their portfolio at the end of 1991. We test whether larger household portfolios (portfolio values of at least $10,000 or $50,000 at the end of 1991) drive the results from Panels A and B because portfolio value may be correlated with the availability of resources and skill in uncovering locality information. The results, not reported for brevity, largely show that the effect is only somewhat stronger for larger portfolios. 14 To address the concern that the choice of the end of 1991 as the portfolio formation date could have driven the results, we replicate the above analyses for portfolio formation dates in other sample years-at the end of 1992, 1993, 1994, and 1995-for return horizons ranging from one year up to five years (as permitted by the end of the sample period). Another facet of this robustness check is fitting both the mean estimator (OLS) and the robust median regression estimator. In unreported analyses we find that the results are very robust along both dimensions: for both mean (OLS) and median estimators, the reported difference between differences estimators are mostly statistically significant, similar in magnitude at a given return horizon, and increasing monotonically with the return horizon. Another concern we address is the potential relation between the locality of investment and portfolio risk. For example, if local investments were concentrated in small technology stocks, then raw returns for local investments would exceed those for non-local investments on average. However, the additional return would reflect a difference in risk (small technology stocks are a riskier investment and thus should generate a higher return), as opposed to reflecting the return to the acquisition of local information. For each household we compute the average risk-adjusted monthly return by fitting time-series regressions of excess household portfolio returns on four factors (three Fama-French factors and the momentum factor): We estimate risk-adjusted household portfolio performance HHP α for all the households in the sample that had at least 36 monthly returns during the five-year sample period and then regress these estimates (expressed in basis points per month) on the locality breakdown as before: 14 We further checked whether the relation between locality and returns could be driven by omitted household characteristics that were shown in Table III to be associated with local investing (such as portfolio value and whether the household holds non-common stock assets in the portfolio). For example, perhaps higher household wealth could serve as a proxy for the availability of resources and skill to uncover value-relevant local information, and thus its inclusion in the return regression would render the locality effects insignificant. In unreported analyses we found the estimated effects of locality to be unchanged with the inclusion of household characteristics such as the size of portfolio.  The results, presented in the last two columns of Table V, are quite striking: after controlling for risk, across every specification the best performing investments are local non-S&P 500 stocks and the worst performing investments are non-local non-S&P 500 stocks. Two key statistics in Panel B-the difference between the regression coefficients on the shares of non-S&P 500 local and non-local investments and the difference between differences estimators (the last two highlighted rows in the table)-suggest that the effect of the information asymmetry of non-S&P 500 local and non-local investments in the base specification (without fixed-effects) is statistically and economically significant: 25.5 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 15 In terms of magnitude, this coefficient, when annualized, is comparable to those based on raw returns. The difference between the differences, which quantifies the incremental advantage attainable by local investment between S&P 500 and non-S&P 500 stocks, is 27.1 basis points per month for household portfolios that held at least $1,000 at the end of 1991. 16 This coefficient is again comparable to those based on raw returns. For example, 27.1 basis points per month translates approximately 3.25% per year, whereas the corresponding annual return backed out from raw returns ranges from 3.6% to 6.6%. Moreover, risk-adjusted resulted are more robust to the zip-code fixed effects. In sum, adjustment for risk did not affect the main conclusions derived from the earlier analyses based on raw returns.",
          "Given the time period and the geographic distribution of investors (see Figure 1), a potential concern is that the results may be influenced by certain localities and the success of certain industries. For example, around one-quarter of the households in our sample are from California. 15 The coefficients resulting from the fixed-effects regression differ substantially from the regression without fixedeffects only for portfolios holding at least $50,000, for which the coefficient is larger in magnitude, but not statistically significant. Note, however, that there are typically only one or two households with portfolios in excess of $50,000 per zip code, thus reducing the ability to identify more precisely the returns to locality in a fixed-effects regression. 16 Once again, the coefficients resulting from the fixed-effects regression differ substantially from the regression without fixed-effects only for portfolios holding at least $50,000, for which the coefficient is both larger in magnitude (120 basis points versus 71) and marginally statistically significant. For the California investors, their local investments could have been in technology companies, which tended to earn superior returns over the sample period. A first look suggests that this fact is unlikely to drive our results. Indeed, Table II suggests that the portions of portfolios invested locally for California and non-California households were virtually identical. Moreover, the inclusion of the household zip code fixed effects in our previous regressions provided control for potential biases in the geographic distribution of households and firms in the sample. Specifically, in the fixed-effects regressions the return to locality was estimated on the basis of the differences in portfolio returns within zip codes (rather than the average differences in returns across the zip codes). The importance of this issue merits additional robustness checks. non-local non-S&P 500 investments, and the difference between the two differences. The first column displays results from the regression of average excess monthly returns on the share of the portfolio invested locally, thus replicating the results from the second to last column in Table V. The second column displays the results when the share of the portfolio allocated across 14 industry groups is also included in the regression. After controlling for the industry composition of the portfolio, the difference between the non-S&P 500 local and non-local investments is 19.8 basis points per month, compared to the estimate of 25.5 without this control. The difference between the differences, which quantifies the incremental advantage attainable by local investments between S&P 500 and non-S&P 500 stocks, also falls only slightly, from 27.1 to 23.0 basis points, when portfolio industry shares are also included in the regression. This provides compelling evidence that the locality performance results are not driven by particular industries, as the return to locality is still sizeable after controlling for industry composition of the portfolio. The next four columns test whether the return to locality is driven by California households. Focusing on the non-California households first, the difference between the non-S&P 500 local and non-local investments is 19.7 basis points per month (19.6 when controlling for the industry composition of household portfolios). For California households, the difference between non-S&P 500 local and non-local investments is 38.6 basis points per month, substantially larger than that for households elsewhere. This is not that surprising, as at the end of 1991 the average California portfolio was tilted 27.8% towards technology stocks, compared to 19.2% for households outside of California. However, once we control for the industry composition of the portfolio, the difference between non-S&P 500 local and non-local investments falls to 17.0 basis points per month, in line with our estimates for investors in the rest of the country. The final two columns display results when households that had retirement accounts (IRA or Keough plans) are excluded from the sample. This analysis taps into the issue of insider information. Specifically, since households could be holding stock from a previous employer in a retirement account, assuming the 401(k) plan of the former employer was rolled over into an IRA, the locality of the household's investments could be attributed to holding the former employer's stock. In this were the case, the interpretation of the return to locality could be clouded because it could reflect insider information. Excluding the households with retirement accounts, however, does not diminish the returns to locality; if anything, they are slightly enhanced. Table VII provides further evidence on whether the return to locality is driven by particular geographic locations of the investors. We break the sample into nine regions, as Throughout the sample period, households enter and leave the sample. Of the 31,828 households with at least $1,000 in stock holdings at the end of 1991, around one third of the households reported an equity position in November 1996. Other households either appear in the sample after a certain number of months or disappear from the sample by liquidating their positions and closing the account. Thus, in a given period we get to observe the performance of a household if it \"survives,\" that is, does not get liquidated. A potential concern at this point might be survivorship bias (see, e.g., Brown, Goetzmann, Ibbotson, Ingersoll, and Ross (1992)). Indeed, in the domain of performance measurement of professional money managers, survivorship bias is a concern because money managers who disappear from the sample have often been underperformers, and their absence from the sample biases performance measures upward. In our sample, we find no evidence that individual investors systematically liquidated their positions because of their underperformance. 17 The final concern is that the portfolio breakdown with respect to locality, that is, the weights %L SP , %NL SP, %L NSP, and %NL NSP , are computed at the beginning of the returns measurement period. Although in unreported analyses we find that household locality is a highly persistent phenomenon, household portfolio locality may change over time, and relying on the portfolio locality breakdown at the beginning of the return measurement period introduces noise that may worsen as the returns period increases. While this consideration may have some bearing on the precision of the estimates pertaining to longer horizons, the one-year measures of information advantages of locality are unlikely to be substantially affected by the potentially stale portfolio locality breakdown. Moreover, it is not clear how to avoid stale information in this context, as regressing returns on some type of average portfolio locality breakdown computed during the return measurement period would raise concerns about endogeneity. We alleviate this 17 To test whether the liquidation of accounts was related to household portfolio performance, we calculated the portfolio returns of both the households that remained in the sample until the end (November 1996) and those who liquidated their account before the end of the sample. There were 24,901 households that appeared in the regression relating the locality of their portfolio at the end of 1991 to portfolio returns in 1992. Of these households, the 5,582 that maintained stock ownership in their accounts continuously through November 1996 had an average portfolio return of 10.9% in 1992. The remaining 19,319 households that liquidated their account some time before the end of the sample had an average portfolio return of 11.5% in 1992. Furthermore, there were 21,719 households for which average monthly excess (risk-adjusted) returns were calculated and related to household locality. The average excess monthly return for the households that maintained stock ownership in their accounts continuously through November 1996 was 10.8 basis points, compared to 13.7 basis points for the households that liquidated their account some time before the end of the sample. Finally, in unreported analyses, we regressed the probability of a household liquidating their account during the year to the household's prior one-year portfolio return separately for each year. concern in the subsequent analyses by forming portfolios that include a number of households with a certain profile of local investment and rebalance such portfolios annually.",
          "The previous section suggests that household portfolio performance is positively related with the extent and nature of locality in investors' portfolios and that the bulk of the performance differential stems from the investments in less widely known (non-S&P 500) stocks. In this and the next three sections we seek to quantify the economic value of local investment. We begin with a simple question: what are the benefits of pursuing locality by simply following the household portfolios aggregated according to varying degrees of their locality of investment. 18 At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Thus, these four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. The sample period is from February of 1991 to December of 1996. Table VIII summarizes the returns to the four portfolios ranked by household portfolio locality. Panel A presents average monthly raw returns and Panel B presents risk-adjusted returns, computed as excess monthly returns from the same four-factor model we used in the previous section (see Equation ( 6)). For both raw returns and risk-adjusted returns, the bottom three household locality quartile portfolios have similar performances and only the top quartile portfolio stands out. The performance differential relative to the top quartile, highlighted in the second column of each panel, is 19-25 basis points per month for raw returns and 15-18 basis In no year was there a significantly negative relation between the prior portfolio return and the probability of liquidating the account. In sum, we do not uncover evidence that poor household return led to account liquidation. 18 Aside from gauging the value of locality via a very simple portfolio strategy, these results, as well as those in the sections to follow, raise an interesting regulatory issue. Namely, while this and other portfolio strategies that follow are not feasible for most participants in financial markets because the information necessary to implement them is not available in a timely fashion, the discount broker handling the accounts itself has had full disclosure of all the positions and trades available in real time and the implementation of all the portfolios presented herein in principle points for risk-adjusted returns. This translates into the top household locality quartile outperforming the remaining three portfolios by 35-43% (28-33%) during the sample period according to raw (risk-adjusted) portfolio returns presented in the table. The evidence across both panels not only stresses the performance superiority of the portfolio of most local households, but also shows that this performance differential is largely not due to differences in risk exposures across portfolios.",
          "The analyses from the previous section relied on a simple portfolio strategy that gauged the broad impact of locality on household portfolio performance. In this section we proceed a step further by first dissecting each household portfolio according to two dimensions, locality of the investments and their S&P 500 status (our proxy for information asymmetry), and form six aggregate position-weighted portfolios across all households. 19 The results are presented in Table IX. The first section of the table (left three columns) is based on the portfolios characterized only by the locality of the securities relative to the households that held them, whereas the next two sections recognize not only locality, but also the S&P 500 status. Each section features the performance of portfolios of local and non-local stocks, drawn from the appropriate universe of stocks (all, S&P 500, and non-S&P 500), as well as the key portfolio of interest-the zero-cost portfolio formed by taking the long position in the portfolio of local stocks and the short position on the non-local stocks. The results support the hypothesis that locally available information is value-relevant. Specifically, the zero-cost portfolio based on all stocks has statistically and economically significant returns: monthly raw returns are 19 basis points (12 basis points after adjusting for risk). The next two sections of Table IX further demonstrate that this effect stems primarily from the stocks that exhibit more information asymmetry. Indeed, the corresponding raw returns and risk-adjusted returns in the next two sections are very different: they are substantially smaller for S&P 500 stocks, which most likely do not feature significant information asymmetries by investors' location, whereas they are considerably larger (27 and 26 basis points per month, would have been a straightforward exercise. It is not immediately clear whether, setting aside the ethical issues, mimicking the portfolios in such broad, aggregate fashion is in violation of any applicable securities regulations. 19 Our approach is similar in spirit to that followed by Coval and Moskowitz (2001). respectively) precisely where information asymmetries come into play-in the domain of non-S&P 500 stocks. These results are striking, as the only portfolio formation criteria we used were related to the geography and a basic stock index membership characteristic. Even these simple breakdowns yielded abnormal performance of 3.2-3.3% per year, and adjustment for risk characteristics of the portfolios made virtually no difference.",
          "The results from the previous section, compelling in their own right, do not take into account the fact that some households had very few local investments, whereas others had around one-half of their portfolio value invested in local stocks. 20 In Section IV.C we found that households that invest locally tend to outperform the rest, although we did not determine the source of their relative success. In this section we advance our analyses a step further by bringing the two inquiries together. The question at hand is simple: were the investors who invested more locally better able to uncover and exploit locally available value-relevant information than the investors who invested less locally. To address it, we split each of the six portfolios formed in the previous section into two subportfolios. The first (second) portfolio consists of all the portfolio positions held by households ranked below (above) the median of household locality distribution. 21 The results of the same analyses we carried out in Section IV.D on the twelve portfolios we form here are presented in Table X. At the top of the table we reproduce the key statistics highlighted near the bottom of Table IX, namely the raw and risk-adjusted average monthly returns for the zero-cost portfolios formed by taking the long (short) position in the respective local (non-local) portfolio. The next two rows summarize the analogous returns for the portfolios based on the positions held by non-local (below median) and local (above median) households. The results show that the ability to detect and exploit locally available value-relevant information is concentrated among the households whose portfolios tended to be more local. In each of the six columns, the second row, based on below-median locality households, features zero-cost portfolio average returns indistinguishable from zero. By contrast, zero-cost portfolio average returns in the third row, based on above-median locality households, in almost all cases are positive and both statistically and economically significant. In particular, the last two columns that pertain to the zero-cost portfolios most affected by the existence of locally available value-relevant information-those formed on the basis of non-S&P 500 stocks-showcase the highest average monthly returns: 0.38% for raw returns and 0.30% for risk-adjusted returns. These figures can be interpreted from the perspective of seeking to exploit the information contained in the investors' positions: paying attention not only to the stock characteristics, but also to the extent of household locality can enhance average zero-cost portfolio raw (abnormal) returns from 0.27% (0.26%) to 0.38% (0.30%). Moreover, this improvement, though moderate, is attainable without any incremental cost except for screening by household locality.",
          "Tables XI and XII provide additional evidence of the robustness of our results. Recall that a key vehicle for own-firm stock ownership, 401(k) plans, is not included in our data. A household's potential ownership of a previous employer's stock through an IRA or an employee investing in their current firm's stock in non-retirement accounts, however, could cloud our conclusions, as the results could be driven by the (former) employees trading on inside information. To address this concern, in Section IV.B we excluded households with retirement accounts and found that such exclusion did not affect our results. In Table XI, we explore the performance of a household's largest local investment relative to its other, if any, local investments. The motivation for this analysis is intuitive: if the household were holding its ownemployer's stock, it would likely be its largest holding. 22 Thus, if households purchase their own firm's stock based on inside information, the return on the portfolio of these biggest local investments, aggregated across households, should outperform the aggregated portfolio of all of the other local holdings across households. 23 Across all stocks, across S&P 500 stocks only, as well as across non-S&P 500 stocks only, we cannot reject the hypothesis that there is no difference in performance between a household's largest local investment and its other local investments. This is true both of raw and risk-adjusted returns. Also, in unreported analyses, we find that, consistent with Table IX, the difference in returns in the zero-cost portfolio that takes a long position in the local investments exclusive of the biggest local investment and a short position in the non-local investments offers a significantly positive return, a finding that is even stronger when the portfolios contain only non-S&P 500 investments. In sum, these results provide compelling evidence that the performance of households' local investments is not driven by insider trading, as there is no difference in the performance of the largest local holding relative to other local investments. ",
          "Our choice of 250 miles as the cutoff between local and non-local investments was governed by an assumption that the distance of 250 miles is a plausible upper bound on the span of local information. As discussed earlier, this cutoff is admittedly arbitrary and it could be argued that it is on the conservative side. As a robustness check, in unreported analyses we replicated the basic analyses from Table V with the locality cutoff set at 100 kilometers (around 62 miles). 27 Changing the locality threshold has several implications. First, the breakdown into local and non-local portfolio shares changes from 31% vs. 69% to 20% vs. 80%. 28 Second, the results based on the 100 kilometer-cutoff are very consistent, and are even stronger by 30% to 50%, which suggests that our choice of 250 miles as the cutoff indeed was conservative. 27 A table with detailed results based on the 100-kilometer cutoff is available from the authors upon request. 28 Moreover, the breakdown with respect to both locality and S&P 500 status changes along the same lines: from 16%, 15%, 41%, and 28% portfolio shares invested into local S&P 500, local non-S&P 500, non-local S&P 500, and non-local non-S&P 500 stocks at the 250 mile cutoff to the respective portfolio shares of 10%, 11%, 47%, and 32% at the 100 kilometer cutoff. Finally, the 100-kilometer cutoff allows for some direct comparisons between the results pertaining to individual investors and those pertaining to mutual fund managers (Coval and   Moskowitz, 2001). In terms of portfolio locality shares, the average fraction of the mutual fund portfolio that is local is only 7%, whereas the average household portfolio share of local investment in our sample is 20%. In terms of performance of local investments, Coval and   Moskowitz (2001) find that the difference in raw returns from investing locally vs. non-locally is 2.7% over the one-year horizon (1.2% difference in risk-adjusted returns). Our estimates, backed out from the unreported analyses using the 100-kilometer cutoff, suggest that the difference in raw returns from investing locally is 4.8% for individual investors (1.7% difference in riskadjusted returns). This result should be interpreted with due caution. It does not necessarily suggest that individual investors in some sense have outperformed the professional money managers. Rather, it suggests that there indeed is locally available value-relevant information to be discovered, and that the primary source of mutual fund managers' success along this dimension likely is their awareness of the existence of such information and their diligence in collecting it, rather than some particularly insightful and sophisticated way of interpreting it once it is available to them.",
          "This study provides a detailed insight into the availability of asymmetric information in financial markets, particularly in the context of geography of investment by individual investors. Using a detailed data set on the investments 78,000 U.S. retail investors made through a large retail discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). As for the preference for local stocks, we sought to discern whether such local bias is primarily a result of the individual investors' ability to exploit asymmetric information or their inclination simply to invest into the companies they are familiar with (though not necessarily particularly informed about). We find remarkably robust evidence of superiority of individual investors' local investments relative to their non-local investments. This evidence is particularly strong where information asymmetries are likely to play the most pronounced role-among the non-local, less widely known stocks. The significance of the results presented herein stretches beyond the concerns related to individual investors; it helps us refine our understanding of the professional managers ability to exploit local information (Coval and Moskowitz, 2001). Specifically, two very different groups of investors posted similar returns to locality (differential performance between local and nonlocal stocks). Thus, both individual households and professional money managers seem to have been able to process and exploit locally available information to earn excess returns.",
          "The sample consists of 31,828 households that held at least $1,000 of stocks in their portfolio at the end of 1991. Households could report annual income in nine ranges (0-15, 15-20, 20-30,  30-40, 40-50, 50-75, 75-100, 100-125, and 125+ thousands of dollars). One eighth of the households did not provide income information. The income level for the households in the first eight categories was assumed to be the midpoint of the income range. Households that reported income greater than $125,000 were assigned an income level of $250,000 because calculations using the 1992 Survey of Consumer Finances suggest that the average household income in 1991 for households that had at least $125,000 of income was $250,000. Household portfolio distance measures were computed by value-weighting the distance measures across individual stocks in the portfolio according to their household portfolio equity position. Panel Percent of all firms within 250 miles 12.6% 5.7% 3.8% -14.7% Percent of portfolio within 250 miles minus percent of all firms within 250 miles 18.1% -0.9% -5.7% -47.6% * Slightly less than one-half of the households (15,458 of 31,828) invest in at least one firm that is headquartered within 250 miles.",
          "The average local and non-local portfolio share across 31,828 households with at least $1,000 in stock holdings at the end of 1991 is reported. A stock is defined as \"local\" for a given household if the firm is headquartered within 250 miles of the household. The \"If invested in Industry Portfolio\" shares in column (2) represent the hypothetical locality breakdown wherein for each stock the measure of locality is the ratio of aggregate market capitalization of all local firms belonging to the same industry and aggregate market capitalization all firms nationwide belonging to the same industry, and all the individual stock measures are then aggregated according to the weights of the respective stocks in the household portfolio.  The table presents regressions of household portfolio (HHP) locality measures on portfolio and household characteristics. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. For those households that did not report income, log (income) was set to zero and the indicator variable \"Did not report income\" was set to one. Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Actual household returns (calculated based on monthly rebalancing of their portfolio) are calculated over the next year, next 3 years, and next 5 years and are expressed in percentages. This return is then related to the portfolio shares at the end of 1991:  Standard errors (shown in parentheses) allow for heteroskedasticity.   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.",
          "The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. The average excess monthly household return α (calculated from a four-factor model and expressed in basis points) is then related to the portfolio shares at the end of 1991:   , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively.  , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles) and then assigned to locality quartiles. Average household relative locality cutoffs across all years are: -6.7% for quartile one, -0.8% for the second quartile, and 45.8% for the third quartile. The return for each quartile represents the weighted return for all the households in that quartile (the weight is the value of each household's portfolio). Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-to-market, and momentum effects. These four locality portfolios are rebalanced at the beginning of each year based on households' locality relative to the market portfolio. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (All Stocks, Local) is the return on local investments weighted across all household portfolios. Portfolio 2 (All Stocks, Non-Local) is the return on non-local investments weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various zero-cost portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The left panel focuses on all stocks held by households, and breaks those holdings into local and non-local investments. The return on the zero cost portfolio that is long local stocks and is short non-local stocks weighted across all household portfolios is displayed in the top row. This calculation is then repeated restricting the sample to households whose portfolio locality share is below the median (row 2) and above the median (row 3). At the beginning of each year (end of January for 1991), households are ranked based on the locality of their portfolio relative to the market portfolio (fraction of household portfolio within 250 miles less fraction of market portfolio within 250 miles). The average household relative locality median cutoff across all years is -0.8% for the median. The three rows of the middle panel portfolio are defined analogously, but just focus on S&P 500 stocks held by households. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the nine separate portfolios. Portfolio 1 (Biggest Local Stock) is the return on the biggest local investment weighted across all household portfolios. Portfolio 2 (Other Local Stocks) is the return on local investments, other than the household's largest, weighted across all household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long the biggest local stock (portfolio 1) and is short the other local stocks (portfolio 2), again weighted across all household portfolios. The middle three portfolios are defined analogously, but just focus on S&P 500 stocks held by households. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by households. Biggest    The average raw return per month and the excess return per month (alpha) from various portfolios over the period 2/91 through 12/96 are expressed in percentage points. Excess monthly returns are calculated from a four-factor model, which accounts for the market, size, book-tomarket, and momentum effects. The loadings on each of the four factors are displayed above for the six separate portfolios. Portfolio 1 (Non-S&P 500 Stocks, Local) is the return on non-S&P 500 local investments weighted across all non-California household portfolios. Portfolio 2 (All Non-S&P 500, Non-Local) is the return on non-local, non-S&P 500 investments weighted across all non-California household portfolios. Portfolio 3 is the return on the zero-cost portfolio that is long non-S&P 500 local stocks (portfolio 1) and is short non-local stocks (portfolio 2), again weighted across all non-California household portfolios. The right three portfolios are defined analogously, but just focus on non-S&P 500 stocks held by California households. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Standard errors reported are Newey-West standard errors with autocorrelation up to three lags. , ** , * Significance at the 1 percent, 5 percent, and 10 percent levels, respectively. The sample consists of 31,828 households with at least $1,000 in stock holdings at the end of 1991. A stock is \"local\" for a given household if headquartered within 250 miles of the household. Relative local ownership is defined as the fraction of stock ownership in the firm that is located within 250 miles of the firm less the fraction of total nation-wide household portfolio value that is located within 250 miles of the firm. The median RLO cutoff is 10.0%. On average, non-S&P 500 non-local stocks constitute 28.2% of the value of a household's portfolio (14.2% has high RLO, 14.0 has low RLO). Actual household returns, based on monthly rebalancing of their portfolio, are calculated over the next year, next 3 years, and next 5 years. This return is then related to the portfolio shares at the end of 1991:   Sources: Compustat for firms' location and author's calculations for retail investors' location."
        ],
        "ground_truth_definitions": {
          "local bias": {
            "definition": "disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do.",
            "context": "Using a detailed data set on the investments 78,000 U.S. retail investors made through a large discount broker over the six-year period from 1991 to 1996, we find that individual investors exhibit local bias, that is, disproportionate preference for local stocks, to an even larger degree than U.S. mutual fund managers do (see Coval and Moskowitz, 1999). We find that the average share of local investments (defined as the investments into companies headquartered within 250 miles from the investor) is around 30%, both in terms of the number of stocks in the household portfolio and their value.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "2d1201d181da2166ccbf1e922634b8b74a58f141",
        "sections": [
          "The Power of Political Rhetoric RONALD R. KREBS University of Minnesota, Minneapolis, USA PATRICK THADDEUS JACKSON American University, Washington, DC, USA While scholars of International Relations and comparative politics have usually treated rhetoric as epiphenomenal, one strand of constructivism has recently returned rhetoric to the heart of political analysis, especially through the mechanism of persuasion. We too maintain that rhetoric is central to political processes and outcomes, but we argue that persuasion is theoretically and methodologically problematic. We aver that rhetoric's role may be more usefully conceptualized in the context of coercion, and we advance a stylized model that illustrates how rhetorical coercion operates, explains why it works, and identifies key scope conditions. We subsequently illustrate our model's relevance through a detailed examination of a 'hard' case. This article's agenda is twofold. First, it advises scholars in these fields to avoid focusing on unanswerable questions about actors' motives and to examine instead what actors say, in what contexts, and to what audiences. Second, it lays the groundwork for a 'coercive constructivism', complementing the liberal version so prevalent today. KEY WORDS ♦ coercion ♦ constructivism ♦ discourse ♦ Habermas ♦ Israel ♦ military service ♦ persuasion ♦ rhetoric Talk is often thought to belong to the realm of diplomacy, war to the realm of action. Yet, during the 2003 Iraq War, the Bush administration was nearly as preoccupied with how the combat was portrayed as with the combat itself. Its foreign policy team invariably spoke of coalition forces rather than American forces, the war to liberate Iraq rather than the invasion of Iraq, Saddam's death squads rather than Saddam's fedayeen. Rhetoric is central to politics, even when politics takes the form of war. Yet rhetoric is curiously not central to much scholarship in comparative politics and International Relations. Politics is typically marked by rhetorical competition, but our theoretical frameworks are generally hard-pressed to make sense of its dynamics and outcomes. Whereas the dominant materialist tradition treats rhetoric as epiphenomenal, we argue, following recent constructivist work, that the rhetorical interplay itself provides leverage in explaining outcomes. We are less comfortable, however, with the argument advanced by some constructivists that political actors deploy resonant rhetorical forms and thereby persuade their interlocutors of the correctness of their preferred course of action. Although persuasion undoubtedly does occur in the political arena, it is also rare. Moreover, such mechanisms rest on a strong specification of the subjective motivations of individuals and thus are methodologically intractable. Recent mainstream constructivist research has prompted a refreshing debate on and advanced our understanding of political deliberation and argumentation. By focusing on internalized norms as the driving forces of behavior and policy, however, such research has ultimately diverted attention from the dynamics of rhetoric. Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics. 1  Employing an expansive definition of rhetoric that includes all speech actswhether they are oral or written, whether they take place under conditions approximating Habermas' 'ideal speech situation' or not -we argue that rhetorical maneuver can prove critical to success in political contests even when one's opponents have not internalized the promoted values. While claimants may deploy arguments in the hope that they will eventually persuade, their more immediate task is, through skillful framing, to leave their opponents without access to the rhetorical materials needed to craft a socially sustainable rebuttal. Rhetorical coercion occurs when this strategy proves successful: when the claimant's opponents have been talked into a corner, compelled to endorse a stance they would otherwise reject. Most narrowly conceived, this article suggests and explores a language-focused mechanism of political influence complementing recent scholarship, but it also has a broader agenda. At the level of methodology, it calls on mainstream scholarship to avoid centering causal accounts on unanswerable questions about actors' true motives and to focus instead on what actors say, in what contexts, and to what audiences. At the level of substantive theory, it joins European Journal of International Relations 13(1) other recent work in returning rhetoric to the heart of political analysis. But rhetoric is often both the medium and the locus of political contest, and thus focusing on its role in the context of deliberation can take us only so far. Rhetoric matters, we argue, when human beings power as well as when they puzzle. This article represents a step away from constructivism with a liberal flavor, focused on the transformation of values, toward constructivism with coercive characteristics, focused on the exercise of power. This article proceeds in four main sections. First, we critically review existing approaches to rhetoric in empirical political science, examining in particular the limitations of (liberal or deliberative) constructivist approaches premised on persuasion. Second, we set forth our generalizable mechanism of rhetorical coercion and explicate both its logic and its limits. Third, we illustrate this mechanism's operation by exploring in detail a single 'hard' case: the relatively successful efforts of Druze Arabs in Israel to garner greater citizenship rights by framing them as the just deserts of their military sacrifice. This outcome is not entirely explicable without close attention to the dynamics of competing rhetorical claims, and our model of rhetorical coercion helps make sense of this puzzling and important case. Fourth, we briefly discuss the particular possibilities and limits of rhetorical coercion within international politics.",
          "Most scholars of international and comparative politics disparage rhetoric as epiphenomenal. The very phrase 'mere rhetoric' captures the view that what counts is not the language people use but the material power resources upon which they can draw. This perspective is shared by 'realist' (and especially rationalist) writers, and it accords with a well-established understanding of political power. Disenchanted with narrow materialist approaches, some have in recent years sought to harness the power of ideas. Many have invoked beliefs, ideas, and culture to supply inputs and to serve as coordination devices in games with multiple equilibria (Goldstein and Keohane, 1993;Laitin, 1998). Political psychologists have long argued that ideas influence how actors interpret evidence and sift through information (Jervis, 1976). Students of collective action have suggested that principled commitments can motivate participants and render them unusually insensitive to the costs of protest (Cohen, 1985). However, scholars with an ideational bent have typically relegated rhetoric to the margins. For 'idealists', words matter only insofar as they reflect actors' true beliefs, with private statements seen as more revealing than public pronouncements. Public rhetoric is of causal consequence only from the top down, as leaders of states and social movements deploy resonant language in an effort to mobilize support (Edelman, 1964;Snow et al., 1986;Snow and Krebs and Jackson: Twisting Tongues and Twisting Arms Benford, 1988). While devotees of 'interests' and 'ideas' vigorously debate the relative causal power of their favored variables, scholars in both traditions have little use for public rhetorical contestation. But to treat rhetoric as epiphenomenal is to render much of politics puzzling and to do violence to politics as lived by its participants. Rhetoric is certainly a weapon of the weak, but those holding the reins of power can and must deploy it as well. The acquisition and maintenance of rule ultimately hinge as much on legitimacy as on physical coercion, and such legitimacy can be established only through rhetorical action (Weber, 1968). While political contestants unquestionably further their agenda by exploiting material resources, they generally at the same time 'frame' their political activity, explaining the purposes to which their material power is put (Gamson, 1992). Such contests among state leaders and societal activists should be at the center of the study of politics. Rationalists and political psychologists might both protest that they have done precisely this. Rationalist scholars have pointed out that talk is not always cheap. State leaders who renege on their public rhetorical commitments may bear substantial domestic and international costs, giving them incentives both to tell the truth and to strategically manipulate audience costs (Fearon, 1994;Sartori, 2002). Though important, this insight flattens rhetoric into a purely informational tool, a way of efficiently revealing whether one is a high-cost or low-cost actor. As such, it cannot shed light on the framing competitions that often lie at the heart of politics. Political psychologists working within the area of political communication have demonstrated that speakers who succeed in defining the relevant considerations can shape mass opinion and ultimately policy outcomes. 2 But nearly all framing experiments have abstracted far from reality in exposing subjects to just a single issue frame (Druckman, 2001). Even the exceptions have failed to interrogate fully the dynamics of framing. Recent studies have concluded that the framing effect disappears when targets are exposed to competing frames from equally credible sources and that targets then respond in accord with supposedly invariant preferences (Druckman, 2004;Sniderman and Theriault, 2004). By this account, the intense framing contests endemic to politics are peripheral in that their only effect (given the caveat of equal credibility) is to render frame competition inconsequential, and baffling in that actors continue to expend copious resources on their rhetorical efforts. Moreover, this research program has ironically diverted attention from rhetorical interplay and instead directed it towards attributes of the target (e.g. political knowledge, awareness, expertise) and the source (e.g. credibility, trustworthiness, expert status). Studies of 'message effects' are comparatively rare. 3 Our debt to the psychological literature on framing is clear, but we wish to bring rhetoric itself back into the heart of the analysis.",
          "Most approaches fail to accord public rhetoric per se causal status, but one major exception has been the vibrant constructivist literature in international and comparative politics (Checkel, 1998;Finnemore and Sikkink, 2001). A central problem for constructivists has been explaining how and why new norms emerge and why actors might obey norms despite contrary material pressures. 4 Among the universe of mechanisms through which political actors develop 'shared understandings', and arguably occupying pride of place, is persuasion: 'normative claims ', Finnemore (1996: 141) asserts, 'become powerful and prevail by being persuasive' (see also Payne, 2001;Johnston, 2001). The targets of persuasive rhetorical moves do not grudgingly comply, but rather sincerely internalize new beliefs and consequently adopt new identities and preferences. Through persuasion, 'agent action becomes social structure, ideas become norms, and the subjective becomes the intersubjective' (Finnemore and Sikkink, 1998: 914). Through persuasive rhetoric, national and transnational social movements prompt not merely compliance with but consent to existing norms, and norm entrepreneurs need not resort to coercion to effect change. Persuasion -and public rhetoric as its medium -have emerged as the coins of the constructivist realm (see, among many others, Checkel, 2001;Crawford, 2002;Keck and Sikkink, 1998;Risse et al., 1999;Risse, 2000). Though sometimes only implicitly, these scholars have often drawn on Jürgen Habermas' logic of 'communicative action' to explain the power of rhetoric (Johnson, 1993;Lynch, 1999Lynch, , 2002;;Müller, 2001;Risse, 2000). 5  Habermas acknowledges that political actors often engage in narrowly goaldirected (teleological) action, but what makes his political vision distinctive is its affirmation of the possibility and prevalence of rational dialogue, of openminded deliberation in the quest for truth. Every legitimate use of language, Habermas argues, is premised on mutual understandings, on 'idealizations that transcend any particular language game', and these 'give rise to the perspective of an agreement that is open to criticism on the basis of validity claims' (Habermas, 1990: 199). For Habermas, politics (at least in its ideal form) is less about contest than consensus, less about powering than puzzling, and deliberative exchange consequently takes center stage (Habermas, 1984). Ideally, actors leave power and rank at the door, and they seek to persuade others and are themselves open to persuasion. Experimental studies of deliberation in fact conclude that the key to sustaining reflective discussion lies in the willingness of participants to keep an open mind (Barabas, 2004). We are very much in sympathy with this emphasis on public rhetorical interchange, but we are skeptical as to the utility of privileging persuasion as a causal mechanism. As Habermas himself recognizes, the unconstrained dialogue of 'communicative action' is not an empirically accurate portrait of politics, for power and rank are omnipresent in the political sphere. Actors do not leave their identities at the door when entering into dialogue, and they do not employ language unadulterated by earlier political contestation. Discourse is therefore always structured -and not by reason alone. Rules of exclusion and employment dictate what arguments can be proffered, under what conditions, and by whom (Foucault, 1972). Insofar as empirically oriented constructivists invoke 'communicative action' to explain real-world processes, it is fair to ask whether rational deliberation characterizes actual political debate. Only on the rare occasions when it does can Habermasian discourse ethics yield empirical insight into political contestation. It would, therefore, seem more helpful to adopt a theoretical framework that explicates the power of rhetoric even when politics is not truth-seeking and truth-generating, and a conception of rhetoric that includes all speech, no matter how interlaced with power relations. We are also skeptical that analysts can overcome the methodological hurdles standing in the way of demonstrating that persuasion has occurred. Such an explanation presumes that one can pin down the 'real motives' driving individual choice, so that one can distinguish beliefs sincerely embraced from those adopted for strategic or other reasons (Payne, 2001: 39-41). But conclusive proof of persuasion is elusive, for it requires unmediated access to people's minds (Wittgenstein, 1953: §150-5). Did Gorbachev accede to the reunification of Germany because he had been persuaded by the trump card of self-determination (Risse, 2000) or because he had calculated that the net costs of resisting the United States were prohibitive? Did Europeans abandon their overseas colonies because they had been persuaded that formal empire was illegitimate (Crawford, 2002) or because they had calculated that the net expected costs of retaining the colonies were excessive? It is impossible to say based on the available evidence, nor is it clear what evidence could even in principle clinch the case. 6 Relying on statements delivered in private settings does not escape the problem, since these articulations may be as strategic as their public counterparts (Scott, 1990). Many of these questions have received attention elsewhere. Scholars of transnational activism have drawn attention to the 'mobilization of shame', and they too have concluded that ways of framing claims cannot be fabricated out of whole cloth (Crawford, 2002;Price, 1998Price, , 2003;;Risse and Sikkink, 1999). Elements of William Riker's suggestive work on 'heresthetics ' (1996) are apropos, as is Frank Schimmelfenig's research on 'rhetorical action ' (2004). We seek to build on and systematize these important contributions, but we also depart from them. First, while many constructivists recognize that the targets of activist efforts may comply at first for strategic reasons, they claim that lasting normative change requires internalization. 7 This formulation is problematic because it relies on incompatible microfoundations in stage one (instrumental adaptation) and stage two (internalization). On methodological grounds, we prefer a mechanism of political influence highlighting strategic European Journal of International Relations 13(1) action throughout the entire episode. We agree with Weber that consistency, not completeness, should be the hallmark of sound work in the social sciences (Weber, 1949). Further, we are reluctant to characterize any normative change as 'lasting' in the sense of forestalling normative contestation over the long run. We proceed from the premise that norms are inherently subject to challenge and that the rhetorical arrangements sustaining norms are never fully stabilized and are at best relatively stable. Rhetorical contestation is thus always possible in theory, although it may not always be manifest in practice. Consequently, we focus on specific bounded episodes of contestation (McAdam et al., 2001: 29-30). Finally, existing accounts focusing on the strategic uses of public language -both constructivist (e.g. Risse, 2000) and rationalist (e.g. Riker, 1996)continue to be crafted around assertions about the 'interests' served by various rhetorical deployments and the 'motives' driving both speakers and targets. We seek to place the mechanism of rhetorical coercion on firmer methodological footing and, more broadly, to demonstrate the viability of a non-purposive social science. 8 Our skepticism regarding explanations rooted in the identification of motives is not novel. Hans Morgenthau, for one, declared the search for statesmen's motives 'futile' because 'motives are the most illusive of psychological data' (Morgenthau, 1993(Morgenthau, [1948]]: 5). Scholars in the rationalchoice tradition have offered similar observations (Frieden, 1999). Our 'solution', however, is more controversial. We are not prepared to follow our rational-choice colleagues down the path of assuming and deducing preferences. Rather, we take a less-traveled road and argue that social scientists should not begin by specifying the intentions of actors. Let us be clear: we are not arguing that actors do not possess motives, that those motives do not shape actors' behavior, or that such behavior is irrelevant to political outcomes. But we do aver that purposive accounts are analytically less useful than models in which preferences need not be specified or ranked. We therefore seek to minimize the place of motives as driving forces in our accounts of political processes and outcomes. We do not recommend this course lightly, for it flies in the face of deeply held epistemological commitments among social scientists in general and political scientists in particular. Purging empirical narratives of the language of subjective motivation is consequently difficult indeed. 9 Moreover, we recognize that all methodological choices are problematic, since the modeling of complex social processes requires abstracting from reality. Yet while this path is less traveled, it is by no means virgin ground. Many social scientists have distanced themselves from subjectivist understandings of culture as a coherent system of beliefs that might be ascertained through sophisticated survey techniques. We, like they, conceptualize culture as the always contested and often contradictory intersubjective semiotic practices through which social actors seek to generate meaning (Ross, 1997;Wedeen, 2002). Public semiotic codes constrain social actors regardless of whether they truly 'believe' in these codes' content, and thus the question of 'actual' belief is of little relevance (Weber, 1968;Jackson, 2002). Relational sociologists, most notably Charles Tilly, have maintained that individualist approaches commit the sin of reductionism: even if we did know people's motives, that would not help us explain social processes and outcomes. We follow these scholars in avoiding individual consciousness as a causal factor and in shifting 'the emphasis from consciousness to conversation, from action to interaction, from selves to sociabilities' (Tilly, 1998: 400; see also Emirbayer, 1997;and Shotter, 1993). These foundational theoretical moves serve as the point of departure for our model of rhetorical coercion. 10 We cannot observe directly what people think, but we can observe what they say and how they respond to claims and counter-claims. In our view, it does not matter whether actors believe what they say, whether they are motivated by crass material interests or sincere commitment. What is important is that they can be rhetorically maneuvered into a corner, trapped into publicly endorsing positions they may, or may not, find anathema. Rhetoric affects political outcomes even when all actors are cynical operators with little interest in genuine deliberation. The resolution of political issues through public debate need not imply any significant level of intersubjective consensus. Thinking about public rhetoric in this fashion avoids the crudeness of vulgar materialism, the reductionism of ideational approaches, and the heroic optimism of persuasion. Our perspective, which one might term 'coercive constructivism', both relates to and is different from existing approaches. Unlike both materialist and ideational accounts, we argue that language has a real causal impact on political outcomes. Unlike ideational approaches and liberal constructivism, we are skeptical that politics is more often characterized by puzzling than powering; we are equally skeptical that actors can transcend coercion and participate in fully rational deliberation. 11",
          "Rhetoric lies at the heart of politics. But many accounts of politics fail to accord it much weight, and many models fail to appreciate its explanatory leverage. This section seeks to explain how and why skillful rhetorical maneuvering can underpin a successful political campaign -not by persuading one's opponents of the rectitude of one's stance, but by denying them the rhetorical materials out of which to craft a socially sustainable rebuttal. Rhetorical coercion is a political strategy that seeks to twist arms by twisting tongues. This section proceeds by clarifying what rhetorical coercion is, how it operates, and the conditions under which it can be effective.",
          "We begin with a stylized account of an episode of rhetorical contestation. Seeking to effect change in some policy or ongoing course of action, a claimant (C) directs an argument toward the opposition (O) in view of a public (P). For the purposes of the model, it does not matter whether O has exclusive control over the actions at issue (if O is, for example, the government) or whether C and O share responsibility (if they are, for example, political parties in a legislative assembly). All that matters is that O's accession or resistance is critical to the outcome. Any argument that C puts forward contains two analytically separable parts: a frame (or set of terms) that characterizes the issue at hand and a set of implications that C suggests follows from that frame. For example, C might be a group of students protesting a war; their argument, advanced against the national government O through speeches and signs at a rally, portrays the war both to O and to P as unjust and unnecessary (frame) and calls for the end of hostilities and the withdrawal of troops (implications). In responding to C, O may accept or reject either or both the frame and the implications of C's argument. These alternatives yield four different potential outcomes of this episode of political contestation, reflected in Table 1. In case 1, O accepts both the frame and the implications of C's argument, and policy changes accordingly. 12 For C, this outcome represents an unmitigated triumph. Case 2 is more mixed. In case 2, O accepts the proposed implications and changes the policy, but it rejects C's reasoning. An example would be if an environmental movement (C) urged the government (O) to restrict logging for ecological reasons, and the government agreed to restrict logging but justified the policy shift in economic terms. Though the movement would have won the substantive battle, it might reasonably perceive its victory as less than complete; the government might in the future expand logging rights on economic grounds as easily as it had earlier restricted them on that very basis. It is possible that the environmentalists' rhetorical tactics drove the government's concessions, but determining that would require Krebs and Jackson: Twisting Tongues and  access to the authorities' true motives -something we earlier argued one can never ascertain. Both case 1 and case 2 mark (at least temporary) terminuses; contestation might resume at a later stage, but these cells represent relatively stable outcomes in the short to medium run. In contrast, cases 3 and 4 are cases of continued contestation. In case 3, O accepts the terms of C's arguments, but rejects the implications that C draws. For example, a lobbying group (C) may argue that high taxes are an unfair burden on the citizenry and should be reduced through an across-the-board reduction in tax rates; a political party (O) may agree that high taxes are problematic but suggest instead targeted tax cuts. As this relatively narrow policy debate occurs within a common issue frame, it might be termed an implication contest. In case 4, the two parties disagree about the very terms of debate as well as the policies that follow, and their rhetorical efforts consequently focus on advancing their preferred issue frame in the hope that their political opponents will accept it (along with the concomitant implications). Without a common frame bounding the debate, such rhetorical interchange -a framing contestis far more fluid, wide-ranging, and fundamental than in an implication contest. Politics is replete with such framing contests. Advocates of bilingual instruction invoke diversity and inter-cultural respect, while critics charge that instruction in anything other than the dominant language would threaten national ideals and students' eventual success in the marketplace. Opponents of abortion call the fetus an unborn child, portray the act as murder, and label themselves defenders of the right to life; supporters of abortion rights depict the fetus as an insentient being, portray the act as a medical procedure, and dub themselves the defenders of a woman's right to choose. Engaged in either an implication contest or a framing contest, C seeks through its claims and counter-claims to alter the rhetorical environment within which political battle is waged. While C might ideally prefer to persuade O, it must design a political strategy that can attain its desired ends even if persuasion proves impossible. The public plays a crucial role: both C and O must craft their appeals with an eye to some audience which sits in judgment of their rhetorical moves. If C can shift the rhetorical basis on which O justifies its stance to P, C can limit the range of O's possible responses and policy options, transcending the erstwhile implication or framing contest. Put simply, rhetorical coercion has taken place when this strategy proves successful -when O, regardless of its private beliefs, can no longer sustain its public opposition.",
          "Rhetorical contestation consists of parties attempting to maneuver each other onto more favorable rhetorical terrain and thereby to close off routes of European Journal of International Relations 13(1) acceptable rebuttal. Rhetorical coercion is successful when C's rhetorical moves deprive O of materials out of which to craft a reply that falls within the bounds of what P would accept. In the end, O finds itself, against its better judgment, endorsing (or at least acquiescing in) C's stance regardless of whether O has been persuaded or believes the words it utters. The alternatives -enduring punishment at the hands of P or investing significant resources in creating new terms of debate -would be prohibitively costly and time-consuming. In our model of rhetorical coercion, neither the motives nor the sincerity of the parties is particularly relevant. 13  C can succeed in rhetorically coercing O because of two key facts of social life. First, political actors can rarely take tangible steps or advance policy positions without justifying those stances and behaviors -in short, without framing. Politics may entail coercion or distribution, but at the same time it involves the struggle over meanings. Meanings, however, cannot be imposed unilaterally or through the exercise of material power alone. They are, by their very nature, intersubjective (Laffey and Weldes, 1997), and the effort to forge shared meaning implicates some audience in the process. The debate between C and O does not transpire in a vacuum: both are continually striving to legitimate their positions in P's eyes (Perelman, 1982). Second, speakers may not say just anything they would like in the public arena: rhetoric is not infinitely elastic but is structured. 14 Every community of discourse shares a number of topoi, or rhetorical commonplaces, that both enable and constrain speakers' rhetorical possibilities. These commonplaces are not 'fully predetermined, already decided distinctions', but weakly shared notions that can be 'expressed or formulated in different ways in different, concrete circumstances' (Shotter, 1993: 170-1). While C and O are free to weave together these commonplaces in creative ways, they are not free to deploy utterly alien formulations in the course of contestation: such arguments would fall, almost literally, on deaf ears. The available space for rhetorical contestation is, therefore, locally bounded, and the parties to a contentious episode cannot introduce entirely novel arguments. They must configure their appeals utilizing rhetorical tools drawn from a chest that is, in the short term, effectively fixed (Swidler, 1986). Rhetorical innovation, while possible and even inevitable in the long run, is far less likely in the short run for three reasons. First, while structures of discourse are never fully hegemonic and are continually being reworked, coherent political action would be impossible if rhetorical universes were in a state of continuous deep flux. Relative rhetorical stabilities must emerge to permit the construction of political strategies, and thus, at any given time, 'the terrain of dominant discourse is the only plausible arena of struggle' (Scott, 1990: 102). Second, imagining, formulating, and disseminating a brand-new commonplace requires an unusual level of commitment on the part of a claimant in terms of material resources, time, and effort. Deploying existing commonplaces -which have already been imagined, formulated, and disseminated -is far less costly (McAdam et al., 2001: 47-50). The process of transforming an unusual rhetorical form into a commonplace is necessarily lengthy and uncertain, but political actors normally possess far shorter time horizons. Arguments can prove powerful only when the commonplaces on which they draw are already present in the rhetorical field, which is shaped both by the unintended consequences of prior episodes of contestation and/or by campaigns undertaken in advance with the express purpose of reconfiguring the rhetorical terrain. Third, and related, novel rhetorical resources are likely to be drowned out by existing arguments and frames. Dense networks of communication carry and regularly reinforce established commonplaces, overwhelming proffered alternatives. Rhetorical innovations consequently tend to arise at the margins, where communication networks are less dense. Overcoming such barriers is possible in principle, but it is in reality so improbable that C and O must, in a given contentious episode, play within the rhetorical arena that presents itself to them. And that arena privileges particular frames and places others beyond the pale, regardless of the material resources at the speaker's disposal. If C and O were the only two actors in our stylized story, they might introduce novel arguments costlessly and endlessly, and rhetorical coercion would then be impossible. Their rhetorical maneuvers are constrained, however, because rhetorical contestation is public in nature (Bennett, 1980: 805-6). The continual competition for public legitimation ensures that P occupies a critical position, setting limits to the policy stances that the contestants can in practice advance. The relevant audience sets the contours of rhetorical contestation, and stepping beyond or reshaping them requires an investment not attractive or even feasible in the short run. This focus on the public's role constitutes a point of intersection with scholars in the rational-choice tradition who have ascribed great importance to 'audience costs' (Fearon, 1994). In such models, the actors and the audience have clear and consistent preferences that precede the contentious episode; public pronouncements serve to signal the content and/or intensity of those preferences. By contrast, our model does not begin by identifying and ordering the parties' desires, and the outcome of a particular episode does not depend on these preferences' content or ranking. Instead, the importance of the public P lies in the imperative for appeal frames to draw on rhetorical commonplaces present in the public's everyday deliberations (Shotter, 1993: 65-9; see also Bennett, 1980). Our model presumes not that European Journal of International Relations 13(1) P has a well-thought-out position in advance on the issue being contested, but rather that there is only a limited set of arguments that P would, in principle, find minimally acceptable. 15  In sum, one argument 'wins' not because its grounds are 'valid' in the sense of satisfying the demands of universal reason or because it accords with the audience's prior normative commitments or material interests, but because its grounds are socially sustainable -because the audience deems certain rhetorical deployments acceptable and others impermissible. One cannot, however, fashion strict covering laws regarding the audience's response. Students of conversation and argumentation from across several disciplines (anthropology, communication, sociology, philosophy, psychology) have argued that context is unusually crucial to their field of study (Billig, 1996;Gumperz, 1982;Perelman, 1982;Sawyer, 2001;Shotter, 1993;Toulmin, 1958). Classifying structures of discourse, reconstructing the rhetorical possibilities, exploring the rhetorical interplay: these critical challenges require one to formulate arguments limited in time and space and limited by the arena of rhetorical competition. Thus, for example, we do not ask below what sorts of appeals are most effective in general at compelling authorities to grant oppressed minorities first-class citizenship. Rather, through an interrogation of the Israeli case, we suggest that a specific claim (framed around military sacrifice and the equation of rights and obligations) is more likely to be successful in a particular discursive context (when talk about citizenship is narrowly republican). Such an explanation is modest, yet powerful, and potentially generalizable.",
          "The logic of rhetorical coercion suggests that two factors are most important in explaining when the model will have explanatory value. First, is P a relevant party to C's claims-making? Recall that C's power is rooted in its credible threat, implicit or explicit, to bring P in on its side; C's ability to do so deprives O of the option of refusing to respond to or even acknowledge C's claims. If C cannot issue its threat credibly, then O can safely ignore C's claims, no matter how they are framed. We can readily imagine conditions under which P would not be relevant to the interaction between the protagonists in our stylized story. It may be that P simply cannot hear C -perhaps because C is being actively repressed; perhaps because C lacks sufficient resources to publicize its message; or perhaps because C's claims regard alleged misdeeds that transpire far from the public eye. 16 Moreover, in some social contexts, decisions need not be justified. 17 The prevalence of justificatory behavior in the political arena suggests, however, that we are correct to maintain that the scope of the model's application is quite large.",
          "Second, how limited is the universe of rhetorical commonplaces? We have argued that actors must, in any particular claims-making episode, forge their appeals by drawing upon existing commonplaces that represent the boundaries of legitimate framing. When these boundaries are loose, numerous rhetorical strands are available, ensuring that O can almost always put forward a meaningful rebuttal to C. In such circumstances, the terms of debate are hardly constraining. When these boundaries are relatively restricted, rhetorical coercion is more easily achieved, as O has less rhetorical 'wiggle room' -that is, fewer socially plausible ways of rebutting C's claim. We recognize that verbal appeals are multivocal, that advocates and opponents of particular policies may advance their arguments employing similar terms. But public language, in our view, is never infinitely flexible, and even so capacious a rhetorical formulation as that of 'rights' in the American context disciplines the speaker. Such disciplining is more common and thus rhetorical coercion is more likely when speakers have fewer commonplaces on which to draw. 18",
          "One of the most venerable norms in the West is that linking military service and citizenship. 'Whoever puts himself in the service of the state', observed Otto Hintze, 'must logically and fairly be granted the regular rights of citizenship' (Hintze, 1975: 211), and the call to the colors is widely considered the supreme example of state service (Janowitz, 1976). Thus the leaders of those relegated to second-class citizenship have often counseled in wartime against draft evasion and have even urged voluntary enlistment. Afterwards they have shrewdly contrasted their people's record of loyalty and sacrifice to the reality of entrenched political and social inequity. For instance, African-Americans volunteered in droves for the Union Army in the US Civil War and for the American Expeditionary Force in World War I, wrapping their postwar demands in the bloody flag. The rhetoric of military sacrifice has at times proven effective. Mobilized veterans have most obviously exploited their status to gain both political rights and distributional benefits. Oppressed and marginalized ethnic, racial, and religious groups have, based upon their collective sacrifice, also advanced claims for citizenship rights, albeit with varying degrees of success. African-Americans, for example, have repeatedly found this path to citizenship blocked. But their lack of postwar progress is hardly surprising given the high stakes of citizenship battles. The demands of subordinate groups for first-class citizenship challenge the core of the existing social order, and the opposition to meaningful change is consequently often fierce. It is not African-Americans' repeated failures that require explanation as much as the successes of others -such as American women after World War I, Japanese-Americans European Journal of International Relations 13(1) after World War II, and Sikhs in post-independence India -in deploying the rhetoric of sacrifice to win improvements in their status. The opponents of reform typically occupy powerful positions in political, social, and economic structures, and few would expect them to give way before words alone. The politics of citizenship is thus a 'hard case' for the power of rhetoric, for material interests and resources would appear to provide a ready explanation for which groups' citizenship claims are granted and which are denied. We will argue, however, that the particular rhetorical configurations advanced by Israel's various Arab minorities help explain these groups' differential rates of success in overcoming the barriers to first-class citizenship in this 'ethnic democracy' (Smooha, 1997). Drawing on their record of military service, Druze Arabs argued that equal obligations demand equal rights. Jewish politicians found themselves unable to craft a culturally meaningful rebuttal to Druze demands, and they were compelled to concede Druze claims. In contrast, Christian and Muslim Arabs, who have never been drafted into the Israel Defense Forces (IDF), insisted that Israel abide by its public commitment to liberal democracy. While this frame also drew on Israeli commonplaces, it permitted Jewish politicians to justify discrimination against Christians and Muslims by arguing that those who do not perform civic duties cannot lay claim to equivalent public rights and benefits. Although the Druze were poorer than their fellow Arabs and smaller in number, for many years they were more effective in placing their concerns on the national agenda and in garnering a favorable response from the authorities. Consequently, material factors cannot account for the success of the Druze relative to other Arabs, and Jewish politicians' continual resistance to Druze demands suggests that persuasion was not at work. Israel's relationship with its Arab minorities nicely illustrates the power and limits of rhetorical coercion. 19   Background Israel's Arab population -today well over a million strongis divided by religion, with the Muslims, the Christians, and the Druze constituting the three largest groups. Possessing basic political rights, the entire Arab community has enjoyed great socioeconomic progress since Israel's founding in 1948, particularly relative to Palestinians in the West Bank and the Gaza Strip and to Arabs throughout the Middle East. Yet they have also been openly labeled a fifth column, and they have, often without regard to religious affiliation, been the victims of variegated forms of discrimination on the part of the state and the majority Jewish population. The new Israeli government subjected Arabs to military rule, conducted pervasive surveillance within the Arab communities, limited Arabs' capacity for travel and an independent economy, expropriated the bulk of Arab-controlled land, excluded Arabs from the giant labor Krebs and Jackson: Twisting Tongues and Twisting Arms federation known as the Histadrut, and so on. Jewish leaders typically invoked the language of security to justify these policies, but its broad definition encompassed ideological, economic, and even partisan interests as well as security more narrowly conceived. Much overt discrimination came to an end by the late 1960s, but substantial discrimination persisted in more subtle forms (Kretzmer, 1988;Lustick, 1980;Shafir and Peled, 2002). Starting from this common baseline, however, the paths of Israel's Arab minorities have diverged. While all continue today to endure second-class status relative to Jewish Israelis, the Druze have made greater headway than the larger and wealthier Christian and Muslim communities. Since 1956 the Druze have been able to deploy a set of rhetorical commonplaces unavailable to their Christian and Muslim neighbors. That year, male Druze became subject to the military draft, reversing a policy of exemption that had been applied to all Arabs since the passage of Israel's universal conscription law in 1950 (Peled, 1998). Some have speculated plausibly that the Israeli authorities hoped that Druze service in the IDF would provide proof of Israeli liberalism and thereby neutralize Arab nationalists and satisfy Western critics (Firro, 1999;Lustick, 1980), but even full declassification of the relevant documents would not definitively establish the government's motives. We argue that, regardless of who wanted to draft the Druze and why, conscription helped the Druze prosper in Israeli politics.",
          "In the mid-1960s, the Druze began to mobilize, and they regularly sought a public forum in which to air their grievances, which they consistently framed around their people's military service and the equation of rights and obligations. In 1967 Druze activists circulated widely an appeal insisting that 'since we have done our duties … and see in the lack of equality … an infringement and denial of our rights, we urgently demand correction of this wrong' (Firro, 1999: 187). Surveying the history of Druze claims-making, one scholar noted that, among both Jews and the Druze, 'injustices to individual Druzes are usually attacked as ingratitude toward a man who was willing to shed his blood for his country, but now his country turns against him' (Ben Dor, 1979: 134-5). Even in recent years, the Druze have continued to gravitate to this rhetorical mode. During a series of public protests in the 1990s, mayors of Druze villages and towns 'usually denounce [ed] the Israeli government for its broken promises, and stress[ed] the price paid by the community in the form of hundreds of Druze soldiers killed during their service in the Israeli army' (Yiftachel and Segal, 1998: 487). Even the minority of Druze who have objected to the draft have accepted these basic terms. As a Communist Druze Knesset representative argued in the early 1990s, 'If this is a blood covenant, then give us the feeling that we are living in our state, in our homeland, with full rights like all citizens . . . And if there is to be no equality European Journal of International Relations 13(1) of rights, then do not demand equality of obligations, and allow Druze to choose -to serve or not to serve in the IDF' (Zohar, 1991). In constructing their claims frame, the Druze have drawn on rhetorical commonplaces deeply rooted in Israel's rhetorical traditions. Although participation in the pre-state Jewish community, the yishuv, was necessarily voluntary, it prized individual and group commitment to the public good, defined as the historical mission of the Jewish people to rebuild the land of Israel and epitomized by the kibbutz movement. The new state preserved this emphasis on civic commitment as the basis for membership, grafting a powerful statist ideology onto the earlier discourse (Liebman and Don Yihya, 1983). In an early Knesset debate, Prime Minister David Ben Gurion insisted that women too be subject to the draft, bluntly declaring, 'There are no rights without obligations. And above all is the obligation of security, the obligation of defending our existence, our freedom, our independence, and our growth' (Divre Ha-Knesset, 1950: 3:537). Even dissidents have protested within the terms of this dominant definition of citizenship: as one conscientious objector from the Lebanon War put it, 'military service, paying taxes, and obeying the law . . . That is what makes you a citizen and makes you eligible to enjoy the defense and the fruits that [the state] equally distributes' (Helman, 2000: 324). 20  This civic republican tradition has long sat uncomfortably alongside an ascriptive component in the definition of the Israeli polity. The 1948 Declaration of Independence proclaimed the new country 'the Jewish state' even as it promised that Israel would promote the welfare and protect the basic freedoms of all its inhabitants. The country's formal name, the State of Israel, suggests an abiding commitment to pluralistic secularism, but its historical-religious title, the Land of Israel, remains in popular use (Kimmerling, 1985). Some have concluded that 'Jewish ethnicity is a necessary condition for membership in the political community, while the contribution to the process of Jewish national redemption is a measure of one's civic virtue' (Peled, 1992: 435). Yet this ignores the potential contradictions between these discourses. Such tensions are muted when only Jewish citizens are drafted, for then Israeli leaders can publicly embrace civic republicanism while preserving ethnoreligious priorities. But Druze claims-making framed around collective military sacrifice challenged the coherence of Israel's 'ethnorepublican' citizenship. Confronted with such a challenge, Jewish Israeli leaders had two alternatives: either they could acknowledge their exclusive definition of the political community, or they could open its doors. While the Druze squeezed concessions out of Jewish politicians beginning in the late 1960s, there is no evidence that their appeals were persuasive: both the limited and incremental nature of Israeli concessions as well as the grudging fashion in which they were offered are not consistent with a narrative centered Krebs and Jackson: Twisting Tongues and Twisting Arms around persuasion. Rather, our model of rhetorical coercion can help explain why the Druze made headway in this ethnic democracy. Faced with Druze claims, Jewish leaders could have launched a framing contest by (a) admitting openly the discriminatory nature of the state. Alternatively, they could have conceded the Druze frame but challenged its implications by (b) claiming that the Druze were not relatively disadvantaged or (c) arguing that the Druze community's problems were of its own making. The first alternative frame was unappealing -not just because of the likely international repercussions, but also because it would have contradicted both Israel's self-proclaimed status as the sole democracy in the Middle East and its dominant way of speaking about citizenship. The second put forward a claim widely known to be false, and the third possible response, while perhaps plausible with regard to questions of economic development, could not justify administrative segregation, which was, whether cleverly or fortunately, among Druze activists' first targets. There was, however, a fourth option: Jewish politicians could have refused to reply -as long as relevant audiences were unaware of the Druze's plight. The Druze's capacity to engage in rhetorical coercion was consequently dependent on garnering attention from the Hebrew-language media. As a general rule, the Hebrew press rarely covered the Arab communities, for its readership was almost entirely Jewish. But the Druze arguments were resonant with republican rhetorical practice, producing a narrative that Jewish publics comprehended and embraced. As a result, when the Druze made noise, the press amplified it, compensating for the Druze's lack of political clout and ensuring a hearing in the halls of power (Asa'ad, 2000;Toledano, 2000). In short, the Druze trapped Jewish leaders in a rhetorical cul-de-sac in which an implication or framing contest was unsustainable. They maneuvered their Jewish opponents onto a rhetorical playing field on which the Druze could not lose, for no rebuttal would have been acceptable to key audiences, both domestic and international. Wary of calling punishment upon themselves, Jewish leaders had little choice but to concede. Consequently, 'when the Druze demanded something, the ears were much more open' (Toledano, 2000). Despite the Druze community's electoral insignificance, cases of alleged discrimination against individual Druze received attention even in the Knesset (Ben Dor, 1979: 134-6). The Druze demand in the late 1960s for full membership in the Labor Party was irrefutable: as a prominent Labor functionary wrote in a major newspaper, 'How can we explain to that Druze … that he is good enough to endanger his life for the state but that he is not good enough to be a member of the Labor Party?' (Lin, 1970). Confident that their rhetorical resources have bequeathed disproportionate influence, the Druze have resisted an alliance with their numerous fellow Arabs, with whom they share many grievances. Even those Druze otherwise critical of Israel's policies have often supported continued conscription so that European Journal of International Relations 13(1) the military sacrifice frame would remain available. Lacking abundant political resources, the Druze adopted a strategy aimed at rhetorical coercion, and they demonstrated that it could work. Jewish politicians were compelled to say things they (likely) did not believe and ultimately even to follow through on those promises. Like their Druze counterparts, Christian and Muslim Arabs drew on Israeli commonplaces, arguing that Israel needed to eliminate discriminatory measures to live up to its democratic promise. Yet, in comparison to the Druze, they have encountered far greater public resistance, for Jewish politicians had a rhetorical escape hatch. Arab rights, they often argued, were not sacrosanct, for those who did not fulfill their civic obligations (that is, military service) deserved less protection than those who had made the supreme sacrifice. In seeking to convince a wavering Knesset member to support the military administration in 1962, Ben Gurion protested, 'I have recognized all these years that rights are dependent on obligations . . . I do not understand why I may limit the freedom and steal the time of young Jewish men and women when they are in the army, but why it is forbidden to limit much less the freedom of those who do not serve in the army.' 21 That same Labor functionary who advocated offering the Druze membership in the party justified the exclusion of other Arabs on similar grounds: 'I asked them [close Arab friends] how they could sit in the party central committee … next to a bereaved father or mother, next to a party member who had just returned from the front, next to a member who stands ready to leave the next day for reserve service, while they still do not fulfill this obligation' (Lin, 1970). Faced with claims framed in liberal terms, Jewish leaders had a response at the ready, evading the rhetorical dead-end into which Christians and Muslims might have hoped to maneuver them. For decades, this rhetorical move has frustrated Arab efforts to attain first-class citizenship. As one Arab activist recently complained, 'They are trying to link our rights as citizens to performing military service. That's not how a real democracy works' (quoted in Schechter, 2003). Christian and Muslim claimants have long remained mired in a framing contest, unable to persuade or rhetorically coerce Jewish politicians. 22   Alternative Explanations How else might one explain the relative success of the Druze in attaining increments of Israeli effective citizenship? The most intuitive answer would attribute this outcome to the distribution of material resources. But the Druze's potential voter base has been tiny, their per capita income has remained relatively low, they have historically eschewed violent confrontation with the authorities, and they have often had difficulty creating and sustaining effective lobbying organizations (Firro, 1999). By all standard measures of political power, the Druze should have been less successful than other Arabs, not more so.",
          "A second alternative would invert the first. Precisely because the Druze were so small, so weak, and so moderate, Israel could countenance their inclusion in the IDF and grant them some modicum of effective citizenship without threatening the state's Jewish identity. Had the Druze's population been as large as Israel's Christian or Muslim communities, concessions would have been unthinkable. The Druze's small size made them an attractive target for cooptation (Lustick, 1980). But if this argument were correct, presumably the state should have flung its doors wide open, for the costs would have been negligible and the propaganda advantages substantial. In fact, however, while Jewish decision-makers readily offered rhetorical concessions to the Druze, they resisted implementation: 'To say that if the ears were open, things were done -there is a difference' (Toledano, 2000). The Druze made tangible progress, but they fought hard for those increments of meaningful citizenship. Nor did the Druze act like a model coopted minority. They were not content with petty favors, but rather became deeply dissatisfied with their status and continually presented Jewish leaders with difficult and uncomfortable choices. A third alternative would posit that Israel's conquest of the West Bank and Gaza Strip in 1967 and the addition of a large, politicized Palestinian population gave Jewish politicians incentives to grant Druze citizens' demands so as to prevent the Palestinization of the Druze community. Presumably, however, they faced far greater incentives to accommodate the much larger Christian and Muslim citizen populations, whose members had not yet openly allied with the Palestinian national cause. Nevertheless, the Israeli establishment remained highly resistant to Christian and Muslim claims-making. This argument thus has difficulty accounting for the divergent patterns in Israel's relations with its various Arab communities after 1967 -a difficulty not faced by the mechanism of rhetorical coercion. A fourth alternative account would attribute the Druze anomaly to their predilection for taqiyya, or the art of concealment. 23 Long viewed as heretics by their Muslim neighbors, the Druze have historically sought to shield themselves from persecution by hiding their religious observances. With regard to politics, this ultra-pragmatic doctrine prescribes neutrality when possible and 'bandwagoning' -allying with the likely victor -when necessary (Layish, 1985). One would thus have expected the Druze to avoid challenging stronger parties, but in fact the Druze displayed a rebellious streak during Ottoman and European colonial domination of the region. Despite a long history of communal tension, they made common cause with their fellow Arabs in the struggle for Lebanese autonomy early in the 20th century. And, frustrated that one foreign ruler had simply replaced another, their leadership initiated a nationalist revolt against the French in the mid-1920s -with disastrous consequences (Tauber, 1993a(Tauber, , 1993b;;Firro, 1999). The Druze were not proto-nationalists, but they were at times willing to set aside intra-Arab differences, compelling one to ask why they European Journal of International Relations 13(1) did not cast their lot with other Arabs in Israel in the decades after 1948. Taqiyya is obviously of little help in answering this question. Equally important, it lacks analytical bite. The doctrine is so flexible that it is less an explanation for Druze behavior than a tidy description of it.",
          "The Druze's success in compelling Jewish Israeli politicians to grant them a modicum of first-class citizenship illustrates nicely the operation of rhetorical coercion. Although this single case study cannot of course prove that the mechanism is generalizable, we view rhetorical coercion as among the set of 'widely operating explanatory mechanisms that combine differently and therefore produce different outcomes in one setting or another ' (McAdam et al., 2001: 13-14). The Druze case suggests the applicability of rhetorical coercion to domestic politics, but is the mechanism operative in the international arena? Extending the earlier theoretical discussion leads to the conclusion that rhetorical coercion is more likely as social ties in political communities, domestic and even international, grow increasingly extensive and dense. On the whole, however, rhetorical coercion will operate less effectively and less frequently in international settings. Key factors highlighted by the model are often present in international relations: state leaders speak to international audiences, they are sensitive to those audiences' reactions, and justificatory speech acts are prevalent. Indeed, while not systematically elaborating the mechanism's dynamics, others have shown rhetorical coercion to be of critical explanatory importance in such divergent contexts as Cold War crisis bargaining (Bially Mattern, 2004), intra-Arab alliance dynamics (Barnett, 1998), eastward enlargement of the European Union and NATO (Fierke and Wiener, 1999;Schimmelfenig, 2004), and Soviet premier Mikhail Gorbachev's proposals for nuclear disarmament (Evangelista, 2001). Yet even these examples suggest the limits of rhetorical coercion in international politics. Rhetorical coercion, like other forms of political communication, is premised on a political community that shares at least some understandings of the boundaries of acceptable discourse. The more tightly linked the community, the greater the possibilities for rhetorical coercion. Thus it is not surprising to find this mechanism primarily operative among regional groupings with relatively strong requirements for legitimation. The Soviet Union had long legitimated its leadership of the communist bloc by casting itself as the true force for global peace, and its routine calls for nuclear disarmament sustained that claim; Soviet generals could not publicly dispute Gorbachev's proposal without simultaneously undercutting Soviet authority among communist regimes. Similarly, Egypt had, under Nasser, positioned itself as the leader of the Arab world Krebs and Jackson: Twisting Tongues and Twisting Arms based on a pan-Arab discourse that represented Israel as its inherently opposed Other; Nasser reluctantly accepted the risk of war with Israel in 1967 because deviating from the norms of Arab nationalist discourse would have imperiled Egypt's status as the spokesman for Arab nationalism and might have jeopardized his regime's stability. Finally, the European Union, the most tightly bound regional arrangement on the globe today, could not have denied membership to consolidated democracies in Eastern Europe without contradicting the community's legitimating discourse which had intertwined the European concept with liberal democracy; to have vetoed or even vocally opposed eastward enlargement would have challenged the basis for community and threatened individual members' credibility. Such dynamics thus depend on the strength and density of the ties binding the community's members, and such ties vary substantially across the international system. 24 Where such ties are strong and dense -that is, within such regional groupings of states, where international anarchy (as even realists might concede) is most attenuated -rhetorical coercion remains distinctly possible. The cosmopolitan dream of a 'global public' remains today just that, although there are emerging global standards of legitimacy in several arenas and increasingly robust mechanisms through which norm violators might be held accountable (Grant and Keohane, 2005). Where such ties are weak and sparse -as in relations across such regional groupings and as in the many arenas lacking shared bases for legitimation -rhetorical coercion will be far more difficult. As the metaphor of the two-level game implies, political elites in such circumstances are likely to be engaged with and responsive to multiple and different audiences simultaneously, and these audiences are unlikely to share a single conception of what rhetorical moves are acceptable and appropriate. The more complex the social environment, the greater the opportunities for actors to evade the consequences of rhetorical interchange, and thus the more imposing the obstacles to rhetorical coercion. Where such ties have never existed or have been ruptured -perhaps as a consequence of the limits of communications technologies or perhaps as a result of enduring interstate rivalry -communication (including rhetorical coercion) is impossible, and diplomacy is a dialogue of the deaf. As a consequence, it is not the imposing capacity of foreign publics, let alone some elusive global public, to sanction state action that makes rhetorical coercion potentially successful on the international scene. When rhetorical coercion is operative internationally, the critical punishing agent is more often a domestic audience prepared to exact costs for violations of standards of rhetorical appropriateness. While foreigners have disapproved of American unilateralism in recent years, it is not clear that this opposition has driven their governments' policies (Brooks and Wohlforth, 2005), and while anti-Americanism has unquestionably been prevalent, its political consequences European Journal of International Relations 13(1) appear to have often been exaggerated (Keohane and Katzenstein, 2006). The George W. Bush administration could dismiss foreign opposition to the Iraq War, particularly because cooperation on matters of joint concern such as trade and terrorism (narrowly construed) continued seemingly unabated. But the American public's increasing disenchantment with the war has been a far greater cause for concern for the administration, as it is this public that might be directly brought into a political contest over the war -a political contest within which rhetorical coercion is far more likely to operate. 25",
          "Machiavelli famously advised that 'a ruler need not have all the positive qualities …but he must seem to have them… He must seem, to those who listen to him and watch him, entirely pious, truthful, reliable, sympathetic, and religious' (Machiavelli, 1994: 55). Realists, not inaccurately, read Machiavelli as warning against allowing moral considerations to impede actions necessary for the pursuit of political power. But, as coercive constructivists, we would add that these lines imply that social identity has an effect on social and political outcomes, regardless of whether or not the actor internalizes the components constituting this identity. A ruler pursuing policies incapable of public justification would find her path strewn with numerous practical obstacles. This lack of rhetorical resources might even compel her to alter her course of action. In accord with other constructivists, we aver that social norms and social identities matter. 26 In accord with political realists, we argue that whether or not a social actor has internalized a particular set of normative principles is not causally relevant. In accord with the broader cultural turn in the social sciences, we combine these positions by focusing on public language, on how normative principles are deployed in public debate, and by exploring the causal impact this deployment can have on what people say and consequently on what they do -not on the impossibly elusive question of what they believe. Our model of rhetorical coercion can helpfully capture the dynamics of framing contests, when there is controversy about how an issue should be characterized, and of implication contests, when narrower (yet no less important) battles are waged over the consequences of some accepted framing. Our analysis draws attention to two key conditions affecting the applicability of the model. First, rhetorical coercion can occur only when the public is a party to the debate or when the claimant can credibly threaten to involve the audience. Second, not all structures of discourse are equally conducive to rhetorical coercion. This mechanism is more likely to operate when these structures are relatively restricted (and constraining) than when they are relatively loose (and permissive). Furthermore, our model cannot illuminate the process of rhetorical contest in two circumstances: first, when an opponent accepts a claim without argument, and, second, when an opponent accepts the implications of an argument but not its terms. We contend, however, that much social and political life lies within our model's scope. 27  In short, this article seeks to invigorate the study of political rhetoricbut in a fashion relatively unfamiliar to recent debates in international and comparative politics. We argue that rhetorical contestation shapes policy outcomes and that the approaches most common in these subfields miss a significant portion of political and social reality. We further suggest that it is possible to make causal claims without trespassing into the murky waters of subjective motivation and without relying on problematic mechanisms like persuasion. Many IR constructivists would be comfortable with the first of these claims, many realists and students of political culture with the second, few from either camp with their combination. This article has sought to demonstrate that their conjunction is both logically sustainable and potentially productive.",
          "material interests' problematic and have instead embraced a linguistic focus that is not dissimilar to our own. 5. Among those drawing on social psychology are Checkel (2001); Crawford (2002: 26-7);and Johnston (2001). Others have remained agnostic as to how persuasion operates (Finnemore and Sikkink, 1998). We limit ourselves here to discussion of the Habermasian version of the persuasion argument, since in our judgment it is the most clearly articulated mechanism. For an alternative treatment of Habermas in international relations, see Mitzen (2005). 6. On the lack of evidence of changed minds in Crawford (2002), see Dessler and Owen (2005: 602). 7. We are grateful to Kathryn Sikkink for helpful discussion on this point. 8. For treatments closer to our own, see, among others, Barnett (1998); Bially Mattern (2004); Fierke (1998); Goddard (2006). 9. Such language even slips on occasion into our own case study, for the conventions of historical writing insist on subjects who possess agency, articulate goals, and pursue motives. The Foucauldian alternative -interrogating the central discursive formations -might help us escape this trap, but it would come at the cost of the narrative form, which better captures the tensions and surprising reversals of actual political contest. Such traces of motivational language are consequently inevitable, but they should not cloud or draw attention from the rhetorical dynamics themselves. We are grateful to an anonymous reviewer for pressing us on this point. 10. The distinction between persuasion and coercion as modes of political influence is by no means original, but our conceptualization of rhetorical contestation as potentially coercive is more unusual. Scholars, especially those working within a psychological framework, sometimes treat persuasion as synonymous with all attitude change, citing even nuclear deterrence as an example. In contrast, we define coercion as the imposition of constraint on choice, whereas normative persuasion seeks ideally to bypass choice. Both coercion and persuasion may lead the target to assent, but the coerced target merely complies, while the persuaded target consents. Put differently, persuasion speaks to change in preferences, while coercion seeks to alter the social environment confronting an actor while leaving preferences untouched. 11. For a related discussion of 'realist constructivism', see Jackson and Nexon (2004). 12. Note that 'acceptance' here does not imply agreement, but refers only to O's public reaction to C's argument. 13. While we make no strong assumptions regarding the content of the actors' motives, the model does rely on the thin, almost trivial, assumption that actors are fundamentally strategic -that they pursue policies that they believe will further their goals (McAdam et al., 2001). 14. For related arguments, see Cruz (2000); Spillman (1995); Steinberg (1999). 15. In this stylized model, we presume that both actors must legitimate their stance in the eyes of the same audience, but the reality of both international and domestic politics is often more complicated. Speakers in both arenas may be playing to entirely different audiences or may be competing to define which audience is in fact relevant. The dynamics of such contestation are not well addressed by this paper and will be taken up in future articles. We are grateful to anonymous reviewers for raising this question. 16. This is not infrequently the case when O is a government whose commands are implemented by a secretive bureaucracy. A potential weakness of rhetorical coercion is its implication that rhetorical concessions translate smoothly into policy change. O may renege on its commitments, particularly during implementation when there may be little need for public justification. While such breakdowns are likely, we maintain that violations will prove difficult to sustain should C hold O's feet to the public fire in a new episode of rhetorical contestation. 17. Superiors often do not explain their logic to subordinates, and exasperated parents often tell children, 'Just do as I say!' In such circumstances, rhetorical contestation is not likely to be effective. We are grateful to Robert Jervis for pointing this out. 18. For more on the logic of rhetorical coercion, see Jackson (2006); Krebs (2006). 19. For a more extensive treatment of these cases, see Krebs (2006: Chs 3-5). 20. Military service occupies so prominent a site in the Israeli cultural complex that 'civic virtue has been constructed in terms of and identified with military virtue' (Helman, 2000: 320). 21. Ben Gurion to M.K. Yizhar, 12 February 1962, State Archives (Jerusalem, Israel), Office of the Prime Minister Files, C 6304/1086. 22. Christians and Muslims could not control the rhetorical playing field, but they nevertheless made significant gains in the mid-1980s thanks to their raw political power. After the 1977 breakdown of Labor Zionism's stranglehold over the political system, the electoral competition tightened, and the Arab population was actively wooed. Candidates from across the political spectrum stumped for the first time in Arab villages, and the major parties committed themselves to improving the Arabs' lot. 23. A different version of the cultural argument, which Druze and Jews alike have relished, argues that the two peoples are natural allies as fellow minorities in the Muslim Middle East. But the assertion of a natural Jewish-Druze affinity is propagandistic rhetoric that has, since the founding of Israel, served the interests of both Israeli officials and Druze leaders. Druze religious texts are typically dismissive of other religions, and while particularly disparaging of Islam -the dominant regional religious tradition responsible for their oppression -they are hardly admiring of Jews. See Parsons (2001: 70-6); and Dana (2003: 43-51). 24. As this formulation suggests, we believe that network analysis provides a more useful set of tools for grasping such variation than do metaphors regarding, for example, the 'thickness' of the 'normative environment' (Barnett, 1998: 39). For an insightful application of network logic to imperial dynamics, see Nexon and Wright (forthcoming). 25. Consider also the examples discussed earlier. That audiences in the communist bloc outside the USSR might have questioned Soviet leadership or that Arab elites might have challenged Egypt's did matter, but primarily indirectly. Had the Soviet generals or Nasser anticipated that domestic audiences would respond to the prospective loss of regional or ideological authority with collective indifference, they would have perceived themselves to be operating with a far freer hand. That domestic audiences were sensitive to their country's standing in the relevant community of nations, and were believed to be prepared to act on that basis, was crucial in these cases. 26. The language of identity has not figured explicitly in this article, but it is our view that rhetoric and identity are inseparable. Identity is not the mental property of an individual, nor can collective identity be apprehended by aggregating the attitudes of the constituent individuals. Rather, identities are necessarily relational, and thus they are forged through the process of their articulation. Political communities are defined by a common rhetorical lexicon and coalesce around particular rhetorical configurations. 27. Our model cannot, however, capture the long-term processes through which new commonplaces enter the rhetorical space."
        ],
        "ground_truth_definitions": {
          "rhetorical coercion": {
            "definition": "an alternative means of conceptualizing the place of rhetoric in politics",
            "context": "Persuasion does not exhaust the ways through which rhetoric might shape political contest. In particular, we would call attention to culturalist approaches that concentrate on observable rhetorical contests, on narrative and language games. Drawing on this tradition as well as the broad insights of the field of political communication, we propose a model of 'rhetorical coercion' as an alternative means of conceptualizing the place of rhetoric in politics.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4232b05abe413bf3294a560082b0b6bb72dc5a27",
        "sections": [
          "The impact of absolute material deprivationgrossly inadequate food, clothing, shelter, water, and sanitation-on health has been recognized for centuries (93); until relatively recently, discussions of socioeconomic influences on health in the United States focused primarily on links between poverty and health. Over the past 15-20 years, however, a new discourse on social factors and health-with wider relevance to the general population-has emerged in the United States, building on earlier work in Europe and Canada. Figure 1 illustrates the rapidly growing literature on the social (including economic) determinants of health (SDOH) in the United States and elsewhere. The concept is becoming far less marginal in the U.S. public health realm in general, not only in academia; the SDOH have received increasing attention from public health and nonprofit agencies (21,29,88,90,113). This growing momentum reflects a confluence of several phenomena: First, an accumulating critical mass of knowledge in social and biomedical sciences from the United ",
          "",
          "Increasing number of studies of social determinants of health in the United States and Europe. Source: Results of a PubMed search for \"social determinants.\" Literature related to health outcomes, indicators, or promotion was included; health-care literature was not included. States and other countries (1, 10, 123) has led to increased understanding of how social factors influence health and has enhanced the scientific credibility of relevant efforts. Notable recent initiatives include the World Health Organization (WHO) Commission on the Social Determinants of Health (122), the MacArthur Foundation Network on Socioeconomic Status and Health (111), and the Robert Wood Johnson Foundation (RWJF) Commission to Build a Healthier America (91). Incremental improvements in health with increasing social advantage have now been observed in the United States (14) as well as in Europe (69,92), indicating the relevance of SDOH for middle-class as well as the most disadvantaged Americans. Systematic efforts have disseminated this knowledge and made it compelling for broader U.S. audiences (17,91). An increasing focus among U.S. researchers, health agencies, and advocates on the concept of health equity has also contributed, encompassing the spectrum of causesincluding social determinants-of racial/ ethnic and other social disparities in health that raise concerns about justice (8, 15, 79,88,113). Finally, U.S. public health leaders and researchers have increasingly recognized that the dramatic health problems we face cannot be successfully addressed by medical care alone. The low U.S. ranking on key health indicators internationally has continued to fall as our medical expenditures skyrocket, far outstripping those of healthier nations.",
          "The term social determinant of health is often used to refer broadly to any nonmedical factors influencing health, including health-related knowledge, attitudes, beliefs, or behaviors (such as smoking). These factors, however, represent only the most downstream determinants in the causal pathways influencing health; they are shaped by more upstream determinants. To illustrate the upstream/downstream metaphor, consider people living near a river who become ill from drinking water contaminated by toxic chemicals originating from a factory located upstream. Although drinking the contaminated water is the most proximate or downstream cause of illness, the more fundamental (yet potentially less evident, given its temporal and physical distance from those affected) cause is the upstream dumping of chemicals. A downstream remedy might recommend that individuals buy filters to treat the contaminated water before drinking; because more affluent individuals could better afford the filters or bottled water, socioeconomic disparities in illness would be expected. The upstream solution, focused on the source of contamination, would end the factory's dumping. Although these concepts may make intuitive sense, the causal pathways linking upstream determinants with downstream determinants, and ultimately with health, are typically long and complex, often involving multiple intervening and potentially interacting factors along the way. This complexity generally makes it easier to studyand address-downstream determinants, at the risk of failing to address fundamental causes. This article focuses on the more upstream social determinants of health-the factors that play a more fundamental causal role and represent the most important opportunities for improving health and reducing health disparities. Figure 2 illustrates the conceptual framework for the RWJF Commission's work. Although the relationships are more complex, this simplified schema highlights several important concepts. First, it shows that healthrelated behaviors and receipt of recommended medical care (key downstream determinants of an individual's health) do not occur in a vacuum. Rather, these factors are shaped by more upstream determinants related to the living and working conditions that can influence health both directly (e.g., through toxic exposures or stressful experiences) and indirectly (by shaping",
          "",
          "Living and working conditions in homes and communities",
          "",
          "",
          "What influences health? Upstream and downstream determinants.",
          "): years or level of completed schooling, which does not reflect the quality of education the health-related choices that individuals have and make for themselves and their families). The diagram highlights how health is shaped not only by living and working conditions, but also by even more upstream determinants that reflect the economic and social resources and opportunities that influence an individual's access to health-promoting living and working conditions and to healthy choices.",
          "Evidence from decades of research examining associations between key social factorsprimarily educational attainment and income in the United States and occupational grade (ranking) in Europe-and health outcomes throughout the life course overwhelmingly links greater social disadvantage with poorer health (1, 10, 27,46,66,67). The striking and pervasive-albeit not universal-patterns are informative. Researchers have observed stepwise socioeconomic gradients in Europe, particularly the United Kingdom, for 30 years (69, 92) and more recently in the United States (14,73,82). As seen in Figure 3, using U.S. data, overall and in multiple racial/ethnic groups, the improvements in health with increasing social advantage-measured here by income-generally follow a stepwise, incremental gradient pattern: Although the most disadvantaged-those with belowpoverty-level incomes or without high-school completion-typically experience the worst health, even those with intermediate income or education levels appear less healthy than the most affluent/educated (14). The social gradients in health provide clues to understanding the SDOH. Although other research is needed to clarify the underlying pathways, the dose-response relationship suggested by the gradient patterns supports the biological plausibility of a fundamental causal role for one or more upstream SDOH. Gradients by income, education, or occupational grade could reflect relatively direct health benefits of having more economic resources (e.g., healthier nutrition, housing, or neighborhood conditions, or less stress due to more resources to cope with daily challenges), unmeasured socioeconomic factors, and/or associated psychosocial/ behavioral factors, such as health-related behaviors (109), self-perceived social status (121), or perceived control (68). Reverse causation as an alternative explanation is discussed below.  ",
          "Following the framework depicted in Figure 2, we briefly review current knowledge of how several important upstream social factors influence health.",
          "Neighborhoods can influence health through their physical characteristics, such as air and water quality and proximity to facilities that produce or store hazardous substances; exposures to lead paint, mold, dust, or pest infestation in housing; access to nutritious foods and safe places to exercise; or risk of pedestrian accidents (6, 23,48,49,51,77,97). The availability and quality of neighborhood servicesincluding schools, transportation, medical care, and employment resources-can also influence health, e.g., by shaping residents' opportunities to earn a living (43,83,117). Neighborhoods' physical and service characteristics can create and reinforce socioeconomic and racial/ethnic disparities in health. Health is also shaped by social relationships. For example, neighborhoods where residents express mutual trust and are willing to intervene for the public good have been linked with lower homicide rates (76, 98); conversely, less closely knit neighborhoods and more social disorder have been related to anxiety and depression (32,84,94). Many-but not all-studies have found that neighborhood features are associated with health even after considering residents' individual-level characteristics (37). Surprisingly, some researchers-albeit not manyhave found poorer health among disadvantaged individuals living in relatively advantaged neighborhoods (85,89,120), possibly because of adverse psychological effects of feeling worse off than one's neighbors and/or stronger social ties or reduced exposure to discrimination associated with a greater geographic concentration of one's own group (119). Working conditions and health. The physical aspects of work-the traditional domain of occupational health and safety-represent an obvious pathway through which work influences health. For example, jobs requiring repetitive movements and/or high physical workload put workers at higher risk for musculoskeletal injuries and disorders (81), whereas physically inactive workers in sedentary jobs are at increased risk of obesity and chronic diseases such as diabetes and heart disease (115); physical conditions in the workplace such as inadequate ventilation, high noise levels, and hazardous chemical exposures can also harm health. Psychosocial aspects of work represent another pathway to health. For example, working overtime has been associated with injury, illness, and mortality (20). Workers in jobs characterized by high demands coupled with low control or by perceived imbalance of efforts and rewards are at higher risk of poor health (34,56); control at work may be a major contributor to socioeconomic differences in health among employed persons (56,68). Social support at work has also been linked with health (104-107); environments facilitating mutual support among coworkers may buffer against physical and mental health stressors (60). Work-related opportunities and resources can also influence health. Employment-related earnings represent most Americans' primary economic resource, shaping health-related decisions made for themselves and their families; work-related benefits-including medical insurance, paid leave, schedule flexibility, workplace wellness programs, child-and elder-care resources and retirement benefits-could also be important. Well-paying jobs are more likely to provide benefits, greater financial security, and ability to afford healthier living conditions. In contrast, the working poor-estimated at 7.4 million U.S. workers in 2006 (112)-generally do not earn enough to cover basic necessities and are less likely to have health-related benefits (27,54). Different pathways linking work and health may interact to exacerbate social disparities in health: Socially disadvantaged groups Multiple pathways linking education to health. are more likely to have health-harming physical and psychosocial working conditions, along with disadvantaged living conditions associated with lower pay (38). Education and health. Figure 4 depicts three interrelated pathways through which educational attainment (completed schooling) is linked with health. It is widely recognized that education can lead to improved health by increasing health knowledge and healthy behaviors. This may be explained in part by literacy, allowing more-educated individuals to make better-informed, healthrelated decisions-including about receipt and management of medical care-for themselves and their families (36,99). Greater educational attainment has been associated with health-promoting behaviors (3) and earlier adoption of health-related recommendations (31). Education also plays an important role in health by shaping employment opportunities, which are major determinants of economic resources. More-educated individuals experience lower rates of unemployment, which is strongly associated with worse health and higher mortality (4); they are more likely to have jobs with healthier physical and psychosocial working conditions, better health-related benefits (44), and higher compensation (30) (which determines affordability of health-promoting living conditions). Education may also affect health by influencing social and psychological factors. More education has been associated with greater perceived personal control (74), which has frequently been linked with better health and health-related behaviors (63,74,75). Greater educational attainment is generally associated with higher relative social standing; subjective social status (an individual's perception of his or her ranking in a social hierarchy) may predict health even after controlling for more objective indicators of social status (35). More education also has been linked with increased social support (72), which is associated with better physical and mental health (5); social support may buffer the health-damaging effects of stress (110), influence health-related behaviors (24), and if one's social networks are socially advantaged, enhance access to employment, housing, and other opportunities and resources that can influence health (19). The role of educational quality-e.g., the employment opportunities, prestige, social networks, and other advantages accompanying a degree from an elite institution-is rarely considered in health studies. Educational attainment thus can underestimate health-related differences related to education (75,95). Income, wealth, and health. Economic resources reflect access to material goods and services, including income (monetary earnings during a specified time period) and wealth (accumulated material assets, such as the value of one's home, household possessions, vehicles and other property, bank accounts, and investments). Theoretically, wealth may better reflect economic resources overall, but it is more difficult to measure than income and hence less frequently measured in health studies. Among studies that have included both, many (but not all) have found links between wealth and health after considering income (87). Racial/ethnic differences in income markedly underestimate differences in wealth (13). Reverse causation (income loss due to poor health) occurs but does not fully account for the observed associations of income/wealth and health (58,78). Many longitudinal studies show that economic resources predict health or its proximate determinants, even after adjustment for education (2, 33, 52) [although education is a stronger predictor for other outcomes (52) and both are likely to matter (13, 58)]. Health effects of increasing income have been observed in randomized and natural experiments (58). Several researchers have observed health effects of income/wealth even after adjusting for many other relevant factors (33,58,62). Particularly when other socioeconomic factors are inadequately measured, however, observed associations between income/wealth and health may reflect effects of other socioeconomically linked factors such as educational attainment and quality, childhood socioeconomic circumstances, neighborhood characteristics, physical and psychosocial working conditions, and subjective social status. The health effects of low economic resources may be ameliorated by access to other resources and opportunities; for example, some relatively low-income countries/states (e.g., Cuba, Costa Rica, and Kerala, India) have favorable health indicators that may be explained by long-standing societal investments in education, social safety nets, and/or prevention-oriented medical care (41). Income inequality (measured at an aggregate level) has often been linked with health (116), although a causal link is debated (65,116). Income inequality could affect health by eroding social cohesion (59). The link could also be explained by other factors strongly associated with both income inequality and health, such as lack of social solidarity, which could be both a cause and an effect of income inequality. Race, racism, and health. In the United States and many other societies, race or ethnic group is another important social factor that influences health, primarily because of racism. Racism refers not only to overt, intentionally discriminatory actions and attitudes, but also to deep-seated societal structures that-even without intent to discriminate-systematically constrain some individuals' opportunities and resources on the basis of their race or ethnic group. Racial residential segregation is a key mechanism through which racism produces and perpetuates social disadvantage (22,117). Blacks and Latinos are more likely to reside in disadvantaged neighborhoods with inadequately resourced schools and hence to have lower educational attainment and quality Racial discrimination, racism: includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate (96), with resultant health effects through pathways discussed above. Racism may also affect health more directly through pathways involving stress; chronic stress related to experiences of racial/ethnic bias, including relatively subtle experiences arising even without consciously prejudicial intent, may contribute to racial/ethnic disparities in health, regardless of one's neighborhood, income, or education (80,118). More education or income may paradoxically expose blacks or Latinos to more discrimination because of more contact with (non-Latino) whites. Race-health links could also be shaped by perceptions of how one's race-and its associations with social influence, prestige, and acceptance-affects one's relative place in social hierarchies. Associations between discrimination and health similar to those observed in the United States are being found in other countries (118). The pervasive role of stress. Coping with daily challenges can be particularly stressful when one's financial and social resources are limited. Recent evidence implicates chronic stress in the causal pathways linking multiple upstream social determinants with health, through neuroendocrine, inflammatory, immune, and/or vascular mechanisms (71,108). Stressful experiences-such as those associated with social disadvantage, including economic hardship (12, 40) and racial discrimination (118)-may trigger the release of cortisol, cytokines, and other substances that can damage immune defenses, vital organs, and physiologic systems (71,101). This mechanism can lead to more rapid onset or progression of chronic illnesses, including cardiovascular disease (108), and the bodily wear and tear associated with chronic stress may accelerate aging (70,102,103). The accumulated strain from trying, with inadequate resources, to cope with daily challenges may, over time, lead to more physiological damage than would a single dramatically stressful event (70). A recent collection of papers summarizes current knowledge of pathways and biological mechanisms likely to be involved in the health effects of stress and other psychosocial factorsincluding perceived control, subjective social status, and social support (1).",
          "The importance of early childhood experiences. Among the strongest bodies of SDOH evidence is work considering adverse health effects of early childhood experiences associated with family social disadvantage. Many studies have shown that early experiences affect children's cognitive, behavioral, and physical development (7, 25, 53, 55), which predicts health; developmental differences have been associated with socioeconomically linked differences in children's home environments, including differences in stimulation from parents/caregivers (7, 39,50,114,123). Biological changes due to adverse socioeconomic conditions in infancy and toddler years appear to become \"embedded\" in children's bodies, determining their developmental capacity (53). Several longitudinal studies following children from early childhood through young adulthood have linked childhood developmental outcomes with subsequent educational attainment (18,47,100), which is strongly associated with adult health (discussed above). Substantial evidence indicates that pathways initiated by childhood adversity can be interrupted. Studies show that high-quality early childhood development interventionsincluding center-based programs to nurture and stimulate children and to support and educate parents-greatly ameliorate the effects of social disadvantage on children's cognitive, emotional/behavioral, and physical development (57); the first five years of life appear to be most crucial (55), although opportunities for intervention continue throughout childhood (55) and adolescence (42). over lifetimes and across generations (9, 26,40,45,46,53,55,64,66,102,108,124). As illustrated in Figure 5, developed for the RWJF Commission, upstream social factors influence health at each life stage, with accumulating social advantage/disadvantage and health advantage/disadvantage over time. Children of socially disadvantaged parents are less healthy and have more limited educational opportunities, both of which diminish their chances for good health and social advantage in adulthood. Emerging research on geneenvironment interactions suggests that the intergenerational transmission of social advantage and health may be partially explained by epigenetic changes in gene expression, which in turn are passed on to subsequent generations (61).",
          "",
          "",
          "A large body of evidence from observational research strongly and repeatedly links multiple upstream social (including economic) factors with a wide array of health outcomes, and understanding-albeit incomplete-of underlying pathways and biological mechanisms has been growing. With notable exceptions, however, we know little about effective ways to address social factors to improve health and reduce health disparities-about when, where, and how to intervene. The gaps in knowledge reflect several challenges. More often than not, the relationships between upstream social factors and health are complex and play out over long periods of time, involving multiple intermediate outcomes subject to effect modification by characteristics of people and settings along the causal chain. This complexity makes it difficult to learn about the specific pathways through which upstream social factors shape health and to identify priorities for intervention. Addressing the knowledge gaps is also complicated by our limited ability to measure upstream social factors. Current measures do not fully capture-or tease out the distinct effects of-relevant aspects of income, wealth, education, or occupational rank. For example, the observed effects of race/ethnicity on adult health after adjustment for available socioeconomic measures suggest a potential role for unmeasured social influences (13)e.g., childhood circumstances, neighborhood characteristics, accumulated wealth, racial discrimination. Development of better measures of these influences is in its infancy (13,37,80,118). Research funding is also an issue. Most U.S. research funding supports studies of single diseases rather than causal or contributory factors with effects that manifest across multiple diseases, putting SDOH research at a disadvantage. The health effects of upstream social factors-or interventions to address themmay not manifest for decades or generations; longitudinal studies are expensive and access to longitudinal databases is particularly limited in the United States (9). Conducting randomized trials, the gold standard for establishing effectiveness in health sciences, is particularly challenging for upstream interventions.",
          "Investment at this time would be particularly strategic in several areas. Research to improve the measurement of social factors is an important requisite for effective efforts in all of the following areas. Descriptive studies and monitoring. Ongoing descriptive research is needed to monitor changes over time both in the distributions of key upstream social factors (e.g., income, wealth, and education) across groups defined by race/ethnicity, geography, and gender, and in their associations with health outcomes in specific populations and settings. Interpreted in light of relevant policies, these findings can indicate the extent to which social and health disadvantage affects different groups and gauge progress toward improving health and reducing health disparities. Longitudinal research. We need more life-course research, including longitudinal studies to build public-use databases with comprehensive information on both social factors and health, collected over time frames long enough-ideally multiple generations-for health consequences of early childhood experiences to manifest. A more reasonable balance is needed between investments in studying adult disease and examining children's trajectories of health and social advantage across the life course. Connecting the dots: linking knowledge to elucidate pathways and assess interventions. Even robust longitudinal data are unlikely to provide sufficient information for tracing the effects of an upstream determinant (A) through relevant pathways to its ultimate health outcomes (Z), particularly if exposure to A occurs in childhood and outcome Z occurs much later. Attempting to document and quantify the effects of A on Z in a single study represents an important obstacle to understanding how social factors influence health-and how to intervene. Considering the potential for effect modification by characteristics of people and contexts at each step of multiple complex causal pathways, the consistency of existing findings linking upstream social determinants with distal health outcomes seems remarkable. To strengthen our understanding of how upstream social factors shape health, we need to connect the dots by building the knowledge base incrementally through linking a series of distinct studies (perhaps spanning multiple disciplines) that examine specific segments of the pathways connecting A to Z. For example, one study could test the effects of an upstream determinant on an intermediate outcome, which then could be the independent variable in subsequent studies of increasingly downstream intermediate outcomes; no single study would be expected to span all steps from A to Z. Once the links in the causal chain are documented, a similar incremental approach could be applied to study the effectiveness of interventions, e.g., testing the effects of an upstream intervention on an intermediate outcome with established links to health. This approach to advancing knowledge is not new: Medicine and public health often rely on evidence from studies of intermediate outcomes (e.g., obesity) with demonstrated links to other outcomes (e.g., diabetes or cardiovascular disease) (11). Although not definitive, the knowledge gained from connecting the dots can be compelling when confirmed in multiple studies; furthermore, policy makers must recognize that the limited generalizability of findings from randomized experiments introduces uncertainty as well (11).",
          "We need research to inform translation of existing knowledge about the SDOH into effective and efficient policies. Often, the rate-limiting step may not be insufficient knowledge of pathways but rather lack of solid evidence about what, specifically and concretely, works best in different settings to reduce social inequalities in health. For example, although we have convincing evidence that educational quality and attainment powerfully influence health through multiple pathways, lack of consensus about interventions is often invoked to justify inaction. Knowledge of pathways can point to promising or at least plausible approaches but generally cannot indicate which actions will be effective and efficient under different conditions; that knowledge can come only from well-designed intervention research, including both randomized experiments (when possible and appropriate) and nonrandomized studies with rigorous attention to comparability and bias. Intervention research often seeks to identify the magic bullet that will yield results on its own, a stand-alone intervention with independent effects after adjusting for other factors. This notion may be reasonable when considering surgery, but the complex pathways linking social disadvantage to health suggest that seeking a single magic bullet is unrealistic. Interventions with individuals may require simultaneous efforts with families and communities. Recognizing the expense and methodologic challenges, we need multifaceted approaches that operate simultaneously across domains to interrupt damaging (and activate favorable) pathways at multiple points at which the underlying differences in social advantage and the consequent health inequalities are produced, exacerbated, and perpetuated. Other issues must also be addressed. Research funding must be expanded beyond a focus on single diseases and/or biomedical factors exclusively. The time frame for evaluating program or policy effectiveness should be extended (11). Researchers must be trained in the concepts, measures, and methods needed both to study SDOH and their interactions with biomedical factors (e.g., gene-environment interactions) and to consider social factors in clinical and laboratory studies focused on other questions. Political barriers to translating knowledge to action. The field of SDOH is coming of age in many ways, with respect to increased attention within and beyond academia; documentation of strong and pervasive links between social and economic factors and health; and the accumulation of knowledge of pathways and biological mechanisms that provide a scientific foundation for appreciating the role of social factors in health. Although associations between social factors and health are no longer in question, we have much to learn, both about the underlying processes linking upstream social determinants and most health outcomes and about effective ways to intervene. Lack of evidence, however, is not always the major barrier to action. Often, the chief obstacle is lack of political will; particularly in the United States, our deeply embedded culture of individualism can impede actions that require a sense of social solidarity. For example, as noted in an Institute of Medicine report, \"whether early childhood programs can make a difference has been asked and answered in the affirmative innumerable times\"; the remaining questions are about the most effective and efficient interventions (55). Even after major business groups have advocated universal highquality preschool as essential for achieving a productive-i.e., healthy and educated-future workforce (16,28,86), this goal remains elusive. Descriptive, explanatory, and interventional research can play a supportive role in building consensus about the need for action by increasing public and policy-maker awareness of unacceptable conditions such as racial and socioeconomic disparities in health; by making the links between social factors and health meaningful and plausible to the public and policy makers; and by suggesting, testing, and helping to estimate the costs of promising science-based approaches. Information about the pathways and mechanisms through which social advantage influences health can provide an important counterweight to victim-blaming, which too often impedes policies focused on upstream social and economic factors. Based on current understanding of the role of stress in the links between multiple social factors and health outcomes, studies of pathways involving stressful circumstances and physiological responses to stress may yield knowledge needed to help achieve consensus for action. Research on the SDOH can provide practical guidance for policies and add meaning and credibility to ethical and economic arguments for the need to act, not only to ameliorate the adverse health consequences but also to reduce social disadvantage itself.",
          "1. A critical mass of knowledge related to the social determinants of health has accumulated in the United States and elsewhere, documenting associations, exploring pathways and biological mechanisms, and providing a previously unavailable scientific foundation for appreciating the fundamental role of social factors in health. 2. The questions are no longer about whether social factors are important influences on health, but rather about how social factors operate and how we can most effectively intervene to activate health-promoting pathways and interrupt health-damaging ones. 3. Too little attention has been given to the upstream social determinants of health, such as economic resources, education, and racial discrimination. Although most research has focused on the more easily studied and addressed downstream factors, these upstream determinants represent the fundamental causes in pathways that influence downstream factors and ultimately lead to health effects. 4. One barrier to expanding our understanding of how upstream social determinants influence health is a widespread expectation that a single research study can encompass an entire pathway from upstream factor to downstream health effects. Such studies are unlikely to be achieved, however, given the complex causal chains and long time periods involved. Rather, we should focus on advancing knowledge of pathways incrementally by linking results from studies of specific pathway segments. 5. More research on pathways and biological mechanisms is needed, but this will not necessarily yield sufficient information for identifying the most effective and efficient interventions. Well-designed studies of interventions are essential. 6. On the basis of experience and awareness of the complexity of pathways, isolated interventions focused on single discrete (upstream or downstream) social factors may not be effective. The challenge is to design and adequately study multidimensional interventions that address multiple factors simultaneously. 7. Lack of evidence is not always the major barrier to action on the social determinants of health. Particularly in the United States, the crucial obstacle is often lack of political will. A strategic research agenda on the social determinants of health should also address the factors that can enhance or impede political will to translate knowledge into effective action.",
          "The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review. ",
          ""
        ],
        "ground_truth_definitions": {
          "racial discrimination/racism": {
            "definition": "includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate",
            "context": "Racial discrimination, racism: includes societal structures, such as residential segregation and social networks, that systematically perpetuate social disadvantage along racial or ethnic lines, even without conscious intent to discriminate",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "24589ef86242ff182e7bae495b94e7c11931f20c",
        "sections": [
          "Whitehouse: Listening to Birds in the Anthropocene / 55 The post-war Great Acceleration had its symbol in the loss of something meaningful and beautiful, but also common enough to have once been taken for granted. Listening to birds, Carson implied, was no longer something conceivable as straightforward or inevitable. Carson exemplified this shift by quoting an Alabama resident writing about the effects of a massive spraying programme to control fire ants: Our place has been a veritable bird sanctuary for over half a century. Last July we all remarked, \"There are more birds than ever.\" Then, suddenly, in the second week of August, they all disappeared … There was not a sound of the song of a bird. It was eerie, terrifying. What was man doing to our perfect and beautiful world? Finally, five months later a blue jay appeared and a wren. 8 Life in such a perfect world was thus accompanied by the presence of birds, a presence made most readily and delightfully manifest in their songs. When once familiar companions no longer accompanied local residents, the shock was unexpected and unnerving. The Anthropocene is still a new and largely academic term but one that relates to concerns that have been more widely felt, particularly since they were so clearly articulated by Carson. First, there is the concern that everyday experiences of birds and other wildlife are under threat from everyday human activities. Second, there is a belief that local changes and local activities can have global causes and effects. Finally, there is the concern that while things might seem okay in the present, nothing can be taken for granted about the future-not even the near future and what now seems commonplace. The Anthropocene is thus both a term that relates to real and observable changes in the local worlds people perceive around them and to semiotic elaborations on those perceptions that draw together local and global, human and non-human, present and future, into anxiety-laden narratives. There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"9 Anxious semiotics is central to listening to birds in the Anthropocene not only directly in the face of loss but in the tensions of daily experiences that might seem, on the face of it, to be positive. Fluctuations and changes in what can be heard lead back to possible human causes, perhaps even our own activities. Anxious semiotics can even emerge in response to an increase in some species, perhaps spreading in the wake of anthropogenic changes, or to birds singing earlier in the season, perhaps because of climate change. Anxiety points towards potential, as well as actual, loss. It also points towards concerns about human culpability for change, both locally and more generally. Conservationists monitor the fortunes of birds and their habitats assiduously, enjoying the support of much of the public as they do so. But growing desires to attend to and to care for birds and their ecology seem to be continually outstripped by our capacity to disrupt and endanger. The environmentalism spawned by Silent Spring has constantly drawn attention to the ways that ecosystems, both local and global, are affected by human activity. What prospers and what disappears are causally bound together with human actions in ways that are sometimes readily apparent and sometimes barely perceptible. I explore the associations that people perceive as emerging between bird sounds and environmental changes. Following the semiotic approach of Kohn,10 I argue that the symbolic and moral connotations of listening to birds in the Anthropocene follow from their iconic and indexical grounding in places, producing an anxious semiotics in which even positive associations can have portentous or uncertain implications. My argument progresses from recent claims made by Bernie Krause about the evolution of soundscapes and their disruption by humans to a series of narratives contributed to the Listening to Birds project: an anthropological study of people's relations to birds through sound. Finally, I invite the reader to listen to four recordings and consider the sounds they hear as a means of sensing life in the Anthropocene.",
          "Harmony and balance was a state that Carson considered to be under threat in the modern world. She began Silent Spring with the nostalgic statement, \"There was once a town in the heart of America where all life seemed to live in harmony with its surroundings.\"11 This harmony was not just something that could be seen or measured but that could also be heard in the varied mix of sounds in the environment. This point finds elaboration in the work of musician and scientist Bernie Krause, most notably in his book The Great Animal Orchestra. 12 Krause's argument is that places have evolved their own acoustic ecology and that in a natural environment the sounds will tend to fit together rather than disrupting or competing with one another. He begins The Great Animal Orchestra by speculatively describing the soundscape of the American plains, 16,000 years ago and prior to the arrival of humans in North America. An array of now extinct mammals, along with more familiar birds, insects and amphibians, fill the air with their sounds. But despite this complex mix of sounds, each animal is able to make itself heard. Inspired by Murray Schafer's soundscape approach, Krause argues that this is because each species has a sound that occupies a distinct bandwidth and so can exist in harmony with the other sounds that tend to occur around it, be they from other species or from physical factors such as running water. \"This is,\" he writes, \"the tuning of the great animal orchestra.\" 13 This tuning can be rendered visible through the use of spectrograms, which translate sound into a graph plotting time against frequency. As Mundy has pointed out, spectrograms have played a significant role in creating an \"image of evolution,\" but she argues that, more typically, this takes the form of a stable, visible object that is readily comparable and can thus be more easily incorporated into larger taxonomies.14 In Krause's argument, each spectrogram is an image of evolution because it is assumed that the sound of each species is indexically related to the other sounds in the same soundscape, i.e. there is a causal relation between the sounds in an environment and the \"bandwidth\" occupied by the sounds made by each species. This set of inter-relating sounds occupying a distinct sonic niche has emerged, according to Krause, through evolutionary processes. The nesting of these sounds is rendered as a visible object in the spectrogram. Krause divides up sound into three categories. First is the geophony, sounds made by the physical environment such as waves, wind or rain. Second is the biophony, which consists of sounds made by animals, plants and other organisms. Finally is the anthrophony or humangenerated sound. While this classification is ostensibly based on the origins of the sounds rather than what they actually sound like, Krause emphasises the evolutionary emergence of the different types of sound, arguing that: The sounds of geophony were the first sounds on Earth-and this element of the soundscape is the context in which animal voices, and even important aspects of human sonic culture, emerged. 15 So here we have a sense of the sounds of the world developing in relation through the geological eras and into the present day. The different types of sound that Krause outlines might themselves be regarded as sonic epochs in which a particular category of sound emerged or tended to predominate. As such, Krause is implying that we have now entered the epoch of anthrophony and that human sounds are drowning out the biophony and geophony in many parts of the world. The Anthropocene has also ushered in a new kind of anthrophony, with the sounds of industry, machinery, combustion engines and electronic amplification being a rather different and more disruptive type of human-induced sounds than those that would have predominated in the pre-industrial era. As such, I would argue that one can differentiate between pre-and post-Anthropocene anthrophony, the former tending to integrate more closely with other sounds and the latter often disrupting or dominating them. Indeed, Krause's main concern is not with the effects of anthrophony in general but the effects of what he calls electromechanical sounds. Problematic sounds don't so much originate in human bodies themselves but in Anthropocene technologies used by humans. Humans, Krause argues, are not just having an effect on the biophony by drowning out the sounds of other species with their own but also by disrupting ecosystems. Here his concerns are clearly aligned with those of Carson. He is troubled by the loss of sounds but also by the disruption to the harmonious sonic ecosystem. It is not simply a spring of silence that is worrying but one of disharmony. Humans have effects on other sounds not just through their own sounds but through their effects on habitats. These changes reduce the quantity of sound but also cause disruption to the relative harmony of the soundscape. When a biome is compromised, spectrograms will lose both density and diversity, along with the clear bandwidth discrimination among voices that is otherwise visible in nonstressed-habitat graphic displays. Biophonies from stressed, endangered, or altered biomes tend to show little organisational structure. 16 This emphasis on considering sounds within the context of the environments in which they emerge is a point sometimes forgotten in the analysis of bird sounds by scientists and other scholars, who take recorded sounds as discrete, decontextualized objects. Biophonic sounds are thus indexical of the conditions within which organisms live and have evolved. Krause also defines noise in relational terms: I think of noise as an acoustic event that clashes with expectation-loud heavy metal music in an intimate restaurant is noise … A straight-piping motorcycle gunning its way through the delicate landscape of Yosemite Valley shatters the numinous experience for both visitors and animals.17  Noise is thus a conflict that emerges from perceived disjunctions of one set of sounds and another, or between one aspect of the world as experienced and another. It is dissonant rather than resonant, and Krause's argument is that dissonance and noise have increased greatly to the detriment of the biophony and to human well-being. As sounds are the products of activities, this also points to increasing dissonance between human activities more generally and the activities of other species in our environment. These conclusions lead Krause to lament that, \"In order to hear the wild biophonic world, we need to get to places free from human noise.\" 18 The world as it should sound, for Krause, is not just pre-Anthropocene, like the rural idyll Carson draws on, but pre-human. It is a world not of companions but of strangers. How then should one listen to birds in the Anthropocene-in a world in which humans are not just present but dominant and noisy and in which our activities are often dissonant with those of other species? What signs are people listening for and how are they listening? The Anxious Semiotics of Humans and Other Species Krause's work provides support for what many have long realised: that the sounds of birds and other animals are importantly sounds in place. Sound-making happens in relation to other sounds and to the acoustic conditions. For birds, sound-making is also place-making; it is an act of territorialising space, of making relations with other birds and continually re-weaving the context of their lives. In order to do this, as Krause argues, birds make sound in relation to other sounds and both their lives and their evolution enable them to be responsive to signs within their environment. This emphasis on sound and communication as emerging relationally and across species boundaries can be aligned with the recent proliferation of \"more-than-human\" approaches in the social sciences. 19 These new ways of thinking about life have created fresh impetus in the environmental humanities and social sciences, but few authors in this field have explicitly addressed how to analyse the sorts of meanings and communication that emerge in more-than-human relations. A recent exception has been Eduardo Kohn, who has argued that the grounding for how humans and non-humans perceive and understand their world is similar across species. 20 Drawing on the varied works of Peirce, Bateson and von Uexküll, Kohn argues that the semiosis of life is grounded in iconic and indexical signs. The tendency of anthropology and other social sciences has been to emphasise symbolic signs, particularly language, as the primary means of communication. The apparent absence of symbolic communication in non-humans has fuelled an emphasis on human communicative exceptionalism that appears to forge a gulf in human and non-human semiotics. Kohn argues that symbols should instead be seen as nested within a semiotic field of icons and indexes from which their meaning emerges. In this respect, the ways that humans find meaning in the world is not so different to the ways that other organisms do: they are all grounded in the iconic and indexical. 21 Birds perceive the representations of other organisms, including those of their own kind, and their sound-making emerges in association while, for humans, the symbolic only \"works\" because it is emergent from the same sorts of associations. As Kohn puts it, \"The semiosis of life is iconic and indexical. Symbolic reference ... is an emergent dynamic that is nested within this broader semiosis of life from which it stems and on which it depends.\" 22 What follows from this grounded and emergent concept of the symbolic is morality. \"The moral is also distinctively human, because to think morally and to act ethically requires symbolic reference. It requires the ability to momentarily distance ourselves from the world and our actions in it to reflect on our possible modes of future conduct … This distancing is achieved through symbolic reference.\" 23 While I am more hesitant than Kohn to exclude ethical consciousness from non-humans, I agree that both the symbolic and the ethical emerge from the iconic and indexical semiosis of being-in-the-world 24 and that the symbolic also provides the potential for a conceptual, and usually partial, distancing from this immersion. Being-in-the-world is, for humans at least, double-edged because it entails a constant oscillation along a spectrum of mindfulness 25 that runs between engagement and detachment (see also the work on scientists and naturalists by Candea, MacDonald, and Ellis), 26 between Dwelling and Skill (London: Routledge, 2000); S. Eben Kirksey and Stefan Helmreich, \"The Emergence of Multispecies Ethnography,\" Cultural Anthropology 25, no. 4 (2010): 545-576. 20  Kohn,How Forests Think. 21 Ibid.,[38][39] Ibid., 55. 23 Ibid., 133. 24 By \"being-in-the-world\" I wish to convey a relational sense of life that is derived from thinking of the organism-in-its-environment as indivisible and emergent. \"Being\" cannot be separated from \"the world.\" the real and the symbolic, between is and ought. Focusing on this oscillation is, I argue, essential to understanding how people listen to birds in the Anthropocene. In order to clarify my argument, I first need to describe the semiotics of being-in-theworld that I have in mind. Developing Kohn's argument, I bring his semiotic approach into conversation with Tim Ingold's claim that life is best realised as a meshwork. By doing so, I aim to add some flesh to Kohn's semiotics and give a sense of how meanings emerge from the entanglements of the meshwork. Ingold introduces his understanding of meshwork thus: Every… trail [of movement or growth] discloses a relation. But the relation is not between one thing and another-between the organism 'here' and the environment \"there.\" It is rather a trail along which life is lived. Neither beginning here and ending there, nor vice versa, the trail winds through or amidst like the root of a plant or a stream between its banks. Each such trail is but one strand in a tissue of trails that together comprise the texture of the lifeworld. This texture is what I mean when I speak of organisms being constituted within a relational field. It is a field not of interconnected points but of interwoven lines; not a network but a meshwork. 27 As such, any organism is a tangle of interconnecting life lines and thus is emergent from these relations and associations. \"Organisms and persons … are not so much nodes in a network as knots in a tissue of knots, whose constituent strands, as they become tied up with other strands, in other knots, comprise the meshwork.\" 28 For Ingold then, life is characterised by movement and constant birth; it is a process out of which forms emerge but are never fixed. A question that arises from such a view of life is how, in such a fluid and entangled world, can any phenomenon be knowable enough to be dealt with or represented? How can living beings make a good enough \"guess\" at what is emerging to keep on living in a form that is iconic of what it has been before? It is in answer to these questions that Kohn's employment of Peirce's semiotics can be helpful. According to Kohn, Peirce \"strove to situate his entire philosophical project … within a special kind of realism that could encompass actual existence within a broader framework that would account for its relationship to spontaneity, growth, and the life of signs in human and nonhuman worlds.\" 29 This \"broader framework\" might be better thought of as a meshwork-a way of thinking about the world as emergent, fluid and entangled that chimes with both Kohn's and Peirce's ontological claims. The lines, knots and texture of the meshwork are themselves semiotic. Iconic forms are continually emerging in the continuous rebirth of the world; materialised indexical associations are what \"things\" are made of. The meshwork is semiosis made flesh. Kohn develops these themes by drawing on Peirce's concepts of \"firstness,\" \"secondness\" and \"thirdness.\" Firstness is that which is spontaneous and ephemeral-a glimpse of possibilities that are as yet unrealised and unconnected. Kohn describes the tantalising whiff of cinnamon from a freshly opened passion fruit as an example of firstness. Secondness is more startling and sometimes shocking; secondness can be unpredictable or a jolt to habits. For Kohn, a crashing palm tree exemplifies secondness, as a sudden disruption in the meshwork that might realign its relations and draw attention to troubling indexical causation. Thirdness, finally, is more general, regular and habitual. It can take the form of behaviour but can also be seen in terms of certain general tendencies of life such as selforganisation and resistance to entropy. As Kohn argues, \"This tendency is what makes the world potentially predictable and what makes life as a semiotic process, which is ultimately inferential, possible. For it is only because the world has some semblance of regularity that it can be represented. Signs are habits about habits.\"30 Thirdness is the sort of regularity that Krause hears represented in a relatively uncompromised ecosystem in which each bird responds to the regularities of other sounds and acoustic conditions, and it is essential to semiosis more generally. All signs contain aspects of firstness, secondness and thirdness but tend to amplify one of these more than the others. They are all relational. Humans and other beings can sometimes be shocked (secondness) by what is encountered, but being shocked draws deeper attention to habits and regularities. We know the world and our movements within its meshwork differently through shock, and listening to birds in the Anthropocene is habitually shocking. In examining perceptual practices in the meshwork, Ingold writes not of shock but instead marks a distinction between astonishment and surprise. 31 Astonishment, he argues, is a response to a world that is open and continually being remade. In this, even the mundane can be astonishing. Surprise is a response to a world conceived of as closed and predictable, which thus creates the possibility for surprising deviations. Astonishment involves \"riding the crest of the world's continued birth\"32 whereas surprise involves grasping the world \"within a grid of concepts and categories.\" 33 The former Ingold associates with animism and the latter with the sciences. However, I do not see the two sorts of response as being wholly inimical. Kohn's argument that the symbolic realm, in which the world might be conceived of as closed, predictable and thus potentially surprising, is itself emergent from an openness to the world, in which entanglement, continuous birth and astonishment are endemic. Moving too far into the closed, symbolic world can be pathological. As Kohn points out when describing his own temporary breakdown, symbolic semiosis that is divorced from an iconic and indexical grounding can easily induce panic: Panic and its dissipation reveal these properties of symbolic semiosis. They point both to the real dangers of unfettered symbolic thought and to how such thought can be regrounded. Watching birds regrounded my thoughts, and by extension my emerging self, by recreating the semiotic environment in which symbolic reference is itself nested. Through the artifice of my binoculars I became indexically aligned with a bird, thanks to the fact that I was able to appreciate its image now coming into sharp focus right there in front of me. 34 In his account, Kohn describes a movement from being deeply alienated and ungrounded in his semiosis while travelling on a bus to the dissipation of this panic when he takes time to look at a tanager while out walking. This move marked a 're-astonishment' with the world, while the panic was a shock that revealed more clearly his regular habits of being. The move was not so much between engagement and detachment but between immersion and alienation. This is what can happen when symbolic thoughts drift free from their anchoring in the indexical and iconic conditions of being-in-the-world. As Candea argues in relation to his work with scientists studying meerkats, detachment is not the opposite of engagement but a particular manifestation of engagement, one that enables scientists to understand the world of the meerkats more deeply and sensitively. 35 It is, in fact, a manifestation of the oscillation I described earlier between the real grounding of being-in-the-world and the symbolic and ethical. The surprise of a scientist is not so much a product of disengagement but of the oscillation towards an emergent symbolic semiosis in which the world is conceptualised as a closed model. However, in producing such a model the scientist needs to be fully engaged with a world of complex movement and constant form generation. They need to have consciously dwelt in the meshwork in order to build the model. In order to be surprised, one first needs to be astonished. The shocking surprises of the Anthropocene are most readily perceived by those who find the world constantly astonishing. My reason for discussing these relational semiotic approaches is to emphasise that the symbolic and ethical associations and interpretations of listening to birds in the Anthropocene are grounded in an experience of being-in-the-world; they are not simply cultural models that are imposed onto what is perceived. Any such models or symbolic associations are emergent from being-in-the-world rather than prior to it. Likewise moral ideas are emergent from the iconic and indexical grounding of the meshwork. Following Ingold and Kohn I argue that it is not possible to have symbolic and moral ideas without first being instantiated in the world in a meshwork of relations and associations that are at once both material and semiotic. As such, the ways that humans listen to their surroundings are not fundamentally different in their grounding to the ways that birds listen to theirs. In the narratives that follow a range of moral and symbolic ideas are represented, but these draw on the indexical associations of place and temporality. While the approach that I have outlined above is more broadly about how humans and other species perceive the world, there are some particularities to listening to birds in the Anthropocene that require elaboration. I argue that the semiotic context of the Anthropocene is peculiarly anxious because, as I pointed out earlier, it emphasises our separation from the rest of life just at the moment we connect with it. As Aldo Leopold knew, \"One of the penalties of an ecological education is that one lives alone in a world of wounds.\" 36 The Anthropocene world is increasingly not a world of reassuring thirdness and regularity but of a shocking secondness-such as Carson wrote about in Silent Spring-in which the causes of deleterious change seem to lead us back to our own actions. As the indexical and iconic grounding of our being-in-the-world shifts, those wounds become more apparent to those that are the least alienated from other forms of life. The moral and symbolic fallout is the anxious semiotics of the Anthropocene.",
          "I derive three principles from the above discussion that together provide insights into the particularities of listening to birds in the Anthropocene: 1. The semiotic grounding of perception is an indexical and iconic meshwork, both for birds and for humans. Although the meshwork is constantly emerging, there is still potential for regularities (thirdness) that are responded to by organisms in their actions and that provide a basis for the continuance of their lives. 2. The symbolic emerges from this grounding and provides potential for an oscillation between different kinds of mindfulness e.g. engagement and detachment, real and ideal etc. Human perception often involves this kind of oscillation. 3. The semiotic grounding of the Anthropocene is particularly unstable and the causes of this instability can often be traced to human action. The perception of instability and uncertainty thus provokes a distinctly anxious semiotics that can shock and unsettle senses of place and time, as well as provoking moral disquiet. To add some flesh to the bones of these principles, I turn now to the Listening to Birds project, an anthropological study of people's relations with birds through sound. Through this research I received many stories from people narrating their own experiences of listening. 37 Most came from respondents in the UK but many were also contributed from other parts of the Englishspeaking world, including Australia and New Zealand. I take these narratives as a means by which people highlight what sounds they notice, why those sounds are significant and how they respond to changes in what they hear. They also illustrate broader moral and symbolic dimensions of people's relations with their environment and the role that the sound-making of birds plays in this. What I find most striking about many of the stories is that they concern listening to birds in familiar places around where people live. They are about birds that live alongside people and inhabit their gardens and towns. These are not stories of distant soundscapes in remote and wild places but of companion species that make their own places through sound in the places that people make. What also becomes clear in these narratives is the importance to people of a sense of resonance with birds and with their environment more generally. By this I mean a kind of iconic resemblance or indexical association that arises between human activities and those of birds, as represented in their sound-making. The activities of birds are widely understood to resonate with the rhythms of time and season and, following from this understanding, many 37 These were received via the project website, which invited contributions of people's experiences of listening to birds. The website was widely publicised when the project was covered by the BBC News website. respondents expressed feelings of well-being in response to hearing birds. This seems most apparent when people's lives also resonate with these rhythms,38 as Rob from Droitwich wrote: As a farmer I'm up well before dawn at this time of year but even now there are birds singing in the morning. I heard my first dunnock yesterday. They have already started pairing up and the little owls have been amusing us with their bright chirrups every evening and morning. They certainly brighten up the dark mornings and evenings. As we move into spring I'm lucky enough to hear the dawn chorus every day and it does give me a real lift. Lambing time can be magical when you see new-born lambs taking their first steps to a sometimes deafening suite of blackcaps, robins, chiffchaffs, willow warblers and the rest. If you can't appreciate that then there's something seriously wrong with you. When people like Rob attend to the activities of birds as they go about their lives they are given, as he puts it, \"a real lift.\" This lift, I argue, stems from a sense of resonance that comes from this sympathetic attention to the activities of other beings around us. The sort of attunement of activities that Rob describes resonates, but it also requires that he be sensitive to the lives of those around him. Rob is, according to his account, astonished on a daily basis by listening to birds. His engagement with his surroundings enables him to notice the daily and seasonal changes and regularities. The signs of new and burgeoning life that he perceives serve to exemplify the stability of the place where he lives and the life he leads in it. This resonant grounding even draws Rob into making a moral point about the need for others to appreciate such an experience, perhaps reflecting an anxiety that many do not. His sensing of place and time and of his own resonance with birds is grounded in his being-in-the-world but also draws forth this generalised reflection. As Ingold argues, resonance is analogous to the rhythmic interplay of musicians, who may be playing different sounds but who are following the patterns of sounds around them in ways that are complementary and harmonious. 39 As Krause implies, resonance is also integral to the listening and sound-making of birds and other animals, as they make themselves heard through resonating with the acoustics of their own worlds. The apparent regularity of home, as signalled to Rob by the sound-making of birds, is important in forming many people's sense of the temporality of days and seasons. Rachel Carson clearly understood this connection and its power. Indeed, her book might better have been called Silenced Spring. Spring as a period on the calendar might not be silent without the birds, but for many it would scarcely have been spring either. Bird song does more than simply herald the spring; in an important sense it is spring. As Carson realised, the Anthropocene was making home seem less stable and unproblematic. Many narratives describe the effects of this instability in how home sounds through its birds. A respondent from northern England wrote: I grew up in a suburban area of a northern industrial town and was used to seeing lapwings flitting away across the fields, pee-witting as they did so. I haven't seen a lapwing anywhere near the area for years now and it saddens me to think that my daughters won't get that simple joy of seeing and hearing something otherworldly on their doorstep. Whitehouse: Listening to Birds in the Anthropocene / 65 In this example, an integral sight and sound of the rural-urban fringe has disappeared, probably because of the changes wrought by modern agriculture and our attempts to produce more food more cheaply. The silencing of lapwings is felt anxiously by a father who wants his daughters to experience some continuity with his own sense of place. Again, the grounding of sights and sounds comes to have symbolic connotations of home and of this respondent's own biography, but the differences between his own experience and that of his daughters provokes shock. Other narratives describe new birds that have moved in, like the Asian ring-necked parakeets that now add colour and sometimes discordant sound to the south-eastern suburbs of England. A respondent from Surrey writes: Over the last year or so I started hearing strange squawking in my garden. Normally the bird songs are of the usual garden variety, so this was interesting. It turns out that we have ring-necked parakeets living in the area and it was them creating the squawks and screeches. Now I listen out for them as it's lovely to see the bright green parakeets flying around and \"terrorizing\" the local woodpigeons. The ordinary suburban lives of the English Home Counties are enlivened by these exotic birds, terrorising the woodpigeons and contrasting with bird songs \"of the usual garden variety.\" Sometimes the apparent stability and quietude of suburbia needs something to shake it up; the shock of the new serves to emphasise familiar regularities. But whether these stories concern disappearing species or incoming exotics they speak of the sorts of connections with place that people make through bird sounds and of the acoustic ecologies of places. This grounding leads on to moral pronouncements about how people would like places and times to sound and of the sort of resonance they seek. In this final example, a respondent from Cumbria articulates memories of the place where he grew up before raising concerns over the increasing dissonance between farming and the activities of the birds, a dissonance that has led to the silencing of one particular bird: I grew up in Ireland and lived about three miles from Foxford, County Mayo. In the summer, and especially in the late evenings and at night, one could hear the corncrake call-lots of them-some near, and others far away. The call told me that summer was at last here. I would dearly love to hear that sound again or for my children and grandchildren to hear it too. Sadly the corncrake went into decline because of silage making-farmers cutting grass while the corncrake and other wild birds are nesting on the ground. I would love to see a comeback. The resonance that my respondents described in relation to the sound-making of birds is, I argue, an emergent ethic that draws lives and activities together. It is integral to how they experienced their surroundings but also to how they idealised experience and how they wanted their relations with a place to be. The desire for corncrakes to make comeback in County Mayo derives from this sort of idealisation of how a place should be experienced, and this ethical concern is thrown into sharp relief by their disappearance since the advent of Anthropocene agricultural intensification. This desire for resonance is not just about how things sound but about fitting in with the activities of birds in a way that is sometimes iconic and sometimes complementary. Listening to birds, for my respondents, is not simply about apprehending the sound but about trying to understand how sounds relate to their own activities and the places and temporalities they inhabit. When people move to new areas or other countries they often find the different bird sounds they encounter to be unsettling, to precipitate a sense that they don't belong and that their activities no longer resonate with the right kind of bird sounds. Even without the abruptness of moving to a new area, changes in local bird populations and the loss of species and their sounds can foster a sense of anxiety and loss. But silence, as Carson and Krause both realised, is more unsettling still. My respondents are, I think, revelling in perceiving the activities of other species as they become manifested in sound. Life makes sound and movement and their lack can easily come to be understood as a kind of death. The silence and stillness of birds killed by pesticides or motor vehicles, or habitats depleted by logging or drainage, promote an anxious semiotics of death and loss typical of the Anthropocene, when change and loss is readily attributed to disruptive human actions, sometimes perhaps even the listener's own. Silence and discordance are symptomatic of the Anthropocene.",
          "To explore these issues further, I now invite the reader to listen to four sound recordings of birds that were made in four different countries. These span a range of situations from urban gardens, to islands, to a celebrated rural location for birds, to a rainforest remnant where an enigmatic bird lingers on the edge of extinction. I offer a description of my own listening as a starting point for the reader and as a guide to what is being heard. These are not intended as exhaustive interpretations but to facilitate closer listening. I aim to assist the reader in understanding why these places sound the way they do and to encourage them to listen more closely to what they hear as they go about their lives more generally. The anxious semiotics that is never far away only emerges through active listening. Finally, I add as a proviso that listening to these recordings is not the same experience as \"being there.\" Listening is not simply a process in which sound is heard but is a whole bodily experience of being in place in which sound is a focal point. Recordings may \"transport\" one somewhere but they do not furnish the entire semiotic context and sense of being-in-the-world. Capertee Valley, New South Wales: June 2013 40 I spent a week writing a draft of this paper in the, to me, unfamiliar surroundings of the Australian bush. The Capertee Valley is a renowned area for birds a couple of hours from Sydney: a mix of woodland, fields, hills and valley. Certainly there is abundant life here: a constant twitter of fairy-wrens, thornbills and red-browed finches; the chirrups of a Jacky winter; a garrulous cry from a kookaburra or a currawong; the eerie calls of Australian magpies and ravens; the distant squawks of a flock of cockatoos and the agitated bleating of masked lapwings. What do these sounds tell me about the place where I am? At times it seems chaotic, with a whole range of disparate and similar sounds competing with one another. But I am, unusually, in a place where birds, together with a few insects and amphibians, are almost all I can hear. If this place sounds of anything it sounds of its birds. It is far from silent, or even quiet, but does seem to have what Krause calls tranquility. 41 Silence, he argues, is not a desirable state but quite disorienting. In silence, as Carson probably realised, it is hard to find a sense of place. Tranquility, on the other hand, is a state between noise and silence in which a kind of endorphin-rich serenity is attained. The only significant disruption to the sounds of birds comes when a plane flies high overhead. I likely flew over this area on my own journey to Australia and so the sound of aircraft is, in part, enabling me to hear this place. But air travel also seems to be contributing to climate change and the increasingly severe summer heat that regularly leads to devastating fires in this part of Australia. The valley is, to use Haraway's phrase, \"full of bumptious life\" but the strains of Anthropocene anxieties and the ironies of global travel still encroach. 42 Mata da Balbina, Brazil: August 2008 43 The Atlantic forest of eastern Brazil has fragmented enormously over the last century. Many of its bird species have disappeared or become extremely scarce and localised. Some are so rare they are almost like ghosts. One such bird is Stresemann's bristlefront, a thrush-sized bird with a long tail and mass of bristles around the base of its bill from which it takes its name. Until the mid-1990s this was a species only known from two museum specimens. It seemed like a shadowy myth of the past but was then discovered fleetingly in the coastal forests before disappearing as soon as it had appeared. Then in the early 2000s it was found again in a fragment of hilltop forest called Mata da Balbina close to the border of the Bahia and Minas Gerais provinces. Here the small population lingered and I was able to visit in 2008, together with another British birder. We arrived at the forest knowing little about the bird and our knowledge of its singing came from a single online recording made the previous year. The field guides that we used had no description of the sound at all, presumably because it was unknown when the guides were produced. After some time searching, a liquid, cascading song rose up from the thick undergrowth. It was soon close enough to make a recording, which we carefully played back, quietly and at long intervals, to try to entice the bird to show itself. 44 The bird we could hear in the forest sounded slightly different to the online recording because it added in an alternating ending to the song. Curiously, it seemed to respond more enthusiastically to the recording we had downloaded rather than the playback of its own song. It overlapped with the recording almost as if in a duet. The lack of scientific information on the singing of Stresemann's bristlefront rather liberated my thinking about what I was listening to; the singing had no \"facts\" to conform to. Almost an hour passed in the midst of the dense tangle of branches before we saw a female creep nervously across the forest floor, its singing encircling us enigmatically. It was a 41 Krause, The Great Animal Orchestra, 216. 42 Donna Haraway, The Companion Species Manifesto: Dogs, People and Significant Otherness (Chicago: Prickly Paradigm Press, 2003), 32. 43 See audio clip #2 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 44 The ethical issues surrounding playback of bird sounds are complex. For further discussion see David Sibley, \"The Proper Use of Playback in Birding,\" accessed 27 February 2015 http://www.sibleyguides.com/2011/04/the-proper-use-of-playback-in-birding/. beautiful but disconcerting experience; I wondered if the recording I made of its song might outlive the species itself. Stewart Island, New Zealand: November 2008 45 The small harbour of Oban is named after a port in the west of Scotland but this Oban is the main settlement on Stewart Island, an hour or two's boat ride from the southern tip of New Zealand's South Island. It's only my second day in the country, after a lengthy flight from the UK. Jet-lagged and weary, I had already had the strange experience of finding myself on the other side of the world, switching from early winter to early summer as I did so. And yet the birds were much the same as at home in Britain, thanks to a range of familiar garden birds that had been brought to New Zealand by acclimatisation societies in the late 19 th century. The impoverished avifauna of the main islands is swamped by these settler birds, most of which are well-regarded songbirds back in Europe. The sense of similarity to home is oddly disorienting. I am, in part, shocked by the familiarity of what I hear. I arrived by boat in Oban and began to hear some of the \"real\" New Zealand birds: the voluptuous warbles of tuis and the raucous gabble of kakas. These exotic sounds contrasted with the more prosaic chatter of house sparrows and the descending chirruping of chaffinches. This mix of the recognisable sounds of home and the new sounds of the Antipodes jarred, both in terms of familiarity and in the quality of sound. A duck flying over was again familiar, although most ducks in the area were a hybrid mix of introduced European mallards and native grey ducks. An oystercatcher and a gull were both familiar sounding too, much like those that careen about the coastlines of Scotland, but these were subtly different native species rather than introduced birds. Human history and the sounds of birds are inextricably bound together in New Zealand. The relatively short period of human settlement has shaped the avifauna and ecology in dramatic and readily perceptible ways. As much as the presence of the birds I hear, I sense the absence of those that have disappeared since Europeans arrived: bush wren, piopio, huia and many others. In Krause's terms it sounded like an ecosystem that has yet to become sonically integrated, with disparate types of sound, meant for different acoustic and ecological conditions, in uneasy coexistence. Aberdeen, Scotland: March 2014 46 A bright morning, just as winter is turning to spring. The winter has been the wettest on record, perhaps an effect of global warming; the switch to mild, dry weather seems to encourage a rush of activity in the botanical gardens. I'm reminded of the hubbub of sounds from the Capertee Valley: a rush of twittering finches, tits, dunnocks, treecreepers, goldcrests and robins. A great spotted woodpecker taps loudly at a trunk and, being near the coast, the wail of a herring gull cuts through the air. While the gardens seem like a green oasis in the city, the 45 See audio clip #3 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with a Fostex FR2-LE and Sennheiser ME66. 46 See audio clip #4 at http://environmentalhumanities.org/archives/listening-to-birds/. Recorded with an Olympus LS11. Whitehouse: Listening to Birds in the Anthropocene / 69 distant thrum of traffic is ever-present and soon the stream of birdsong is engulfed by a passing helicopter heading out to serve the offshore oil rigs. These garden and suburban sounds seem homely enough and the vibrancy of early spring exudes from the tumble of sounds. The clashes and contrasts are most obvious with the rumble of traffic, both aerial and terrestrial. I wonder if the birds are adapting their soundings to the ambient conditions, just as great tits are purported to have done in noisy urban areas. 47 Is the loud exuberance of singing a response to the noisy environment as much as the onset of spring warmth? The above descriptions reveal something about the sounds on the recordings and the context of their making. They also reveal a lot about how I listen to birds and how my own Anthropocene anxieties inform how I do this. My listening practices are somewhat specialised, given that listening to birds has been a central part of my life, but there are certain aspects of my own process of listening that I consider to be exemplary of how many people listen to birds in the Anthropocene. Much of what might be deemed distinctive about my own practices derives from a fine-grained knowledge of what kinds of birds I am hearing. Although in some situations I was not entirely familiar with the local avifauna, I could, with a little practice, put a name to many of the sounds. This was a result of careful listening and comparison to recordings but also through visual identification, facilitated by field guides illustrated with systematically arranged iconic images and descriptive texts. 48 But, while I was doing this to a rather precise degree, the naming of what is being heard is common to any narrative of listening. Even if the listener only thinks of the sound as being a bird then they are entering into the symbolic process of relating a perceptual encounter to a linguistic marker. This process of naming enters the experience into other sets of relations. Some named sounds might be redolent of place and time; others might be out of place or exotic. Some will be familiar and expected; others could be strange and unexpected. Like many people, I am astonished by the liveliness of the birds I listen to, but I can also be shocked, both by the strange case of a ghostly bird clinging tenuously to existence and by the familiarity of the birds I hear in another hemisphere. When people name aspects of what they directly encounter, they enter those phenomena into a whole range of other narratives, relations and reflections, which are in turn re-entered into the way one listens to one's surroundings. In the Anthropocene those new relations often lead us back to ourselves and the actions of humans, leading us into listening for discordance, disruption and absence. Even in hearing familiar and comforting sounds there is an anxiety in knowing that these can shift abruptly, or are competing with ever encroaching noise and habitat destruction.",
          "The more we care about our world and the more we pay attention to it, the worse things seem to get. This is a powerful anxiety for many in the Anthropocene. Engaging with and reflecting upon the world is essential to our own sense of well-being, but it brings with it the realisation of our own destructiveness. The more we listen to birds the more we notice the loss of birds from pesticides, the destruction of habitat, the encroaching dominance of Anthrophonic sounds, the sounds that are out of place and the ecosystems that are dissonant. There is nothing new about the way that the semiotics of our listening oscillates between our being-inthe-world and our reflection upon it. The semiotics of the Anthropocene, however, destabilises the groundings of our perceptions and draws our reflections anxiously towards our own disruptiveness. Silence and loss hang heavy in the Anthropocene, but this does not mean that our ongoing relations with birds and their sound-making are not fruitful or that we should desist attending to them to avoid the inevitable unease. These lives are ones that people often hope to resonate with, that is, to attend to them in a way that is ethical, that is aesthetic. As Donna Haraway puts it: \"All ethical relating, within or between species, is knit from the silk-strong thread of ongoing alertness to otherness-in-relation.\" 49 The \"ongoing alertness\" to the sounds of birds that my respondents told me about is just this kind of ethical relating, but it is a relating that is importantly grounded in the same kinds of semiotic processes through which birds listen to their own world. I argue that listening to birds in the Anthropocene should not encourage a separation of human activities from those of birds but should instead ground the development of relations of companionship. It elicits not simply a narrative of encroaching loss and the ever present threat that humans pose to non-humans, but one of enskillment, of how we learn to listen to birds and to the rest of our world and how we learn to make ourselves, however uneasily, at home in it. Andrew Whitehouse is an anthropologist and birder teaching at the University of Aberdeen, Scotland. His work has explored conservation conflicts in western Scotland, landscape and anthropology and, more recently, people's relations with birds through sound. He is co-editor of the volume Landscapes Beyond Land: Routes, Aesthetics, Narratives published by Berghahn in 2012."
        ],
        "ground_truth_definitions": {
          "anxious semiotics": {
            "definition": "Sense of uncertainty and concern over potential human culpability in loss.",
            "context": "There is nothing inherently new about the experience of anxiety in relation to environmental conditions, but the Anthropocene brings with it particular configurations that underpin many experiences of listening to birds. I use the term anxious semiotics to convey this sense of uncertainty and concern over potential human culpability in loss. Listening to birds in the Anthropocene is not simply a process of grieving for what is lost, although as van Dooren has argued, grief and loss are profound elements of modern relations with birds on \"the dull edge of extinction.\"",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.13",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
