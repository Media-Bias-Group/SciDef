{
  "extract.step.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The section provides explicit definitions for \"racial discrimination\" and \"structural racism\".\n\nFor \"racial discrimination\", the definition is directly stated and then further elaborated with two components: \"disparate treatment\" and \"disparate impact\". These components are also explicitly defined within the context of racial discrimination.\n\nFor \"structural racism\", the definition is directly quoted from another source.\n\nThe context for each term includes the sentence defining the term and the surrounding sentences. I made sure to extract the definition verbatim from the text.",
        "extracted_terms": [
          {
            "term": "racial discrimination",
            "definition": "the unequal treatment of persons or groups on the basis of their race or ethnicity.",
            "context": "The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact).",
            "type": "explicit"
          },
          {
            "term": "disparate treatment",
            "definition": "\"differential treatment on the basis of race that disadvantages a racial group\"",
            "context": "24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity.",
            "type": "explicit"
          },
          {
            "term": "disparate impact",
            "definition": "\"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\"",
            "context": "(1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity.",
            "type": "explicit"
          },
          {
            "term": "structural racism",
            "definition": "\"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\"",
            "context": "A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems.",
            "type": "explicit"
          }
        ]
      },
      {
        "augmented": true,
        "section": "In order to understand how NFD rules influence the CSR activities of firms, we conducted statistical analyses that compared firms in three 'early adopter' countries (France, the UK and Denmark) with other OECD countries, as well as taking into account the changes within these countries over time. These three countries were first among the OECD to introduce NFD regulation with an explicit focus on the social dimension. In this context, mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately-owned stock exchange-listed corporations. We excluded cases where regulation applied only to state-owned companies, as in Sweden since 2007. We also excluded disclosure regulation limited to hazardous environmental risks, as found in the Netherlands or the US. Spain is an ambiguous case, since its 2011 government regulation only recommends (rather than mandates) disclosure (González and Vílchez, 2015). Similar requirements were also introduced by Norway in 2013, but this only covers one year during our observation period. In the EU, Directive 2014/95/EU on non-financial disclosure required all member states to transpose the Directive into their national legislation before December 2016. This development will change the picture of CSR disclosure and makes our analysis timely. Our statistical analysis makes use of the Thomson Reuters Asset4 ESG database, which measures firm-level corporate responsibility activities. This database has already been validated in the CSR literature (Eccles et al., 2014;Cheng et al., 2014;Hawn and Ioannou, 2016;Rathert, 2016). Recently, it has increasingly been used in studies published in the Journal of Business Ethics (Al-Shaer and Zaman, 2017; Aouadi and Marsat, 2016;Benlemlih et al., 2018). Asset4 functions as a financial intermediary providing investment information related to environmental, social, and governance (ESG) issues; it systematically collects information from company reports, company websites, and other sources, such as newspapers and nongovernmental organizations. In addition, firm-level financial data were obtained through Thomson Reuters DataStream. Our sample of firms consisted of stock exchange-listed corporations during the period 2002-2014. This covers the time before and after the implementation of non-financial disclosure rules in Denmark and the UK. However, the very early introduction of policies in France during the 1980s predates the existence of any source of comparable international CSR data. We reported our results for 24 OECD countries that had a sample size of 10 firms or more per year during the period of observation. Moreover, and to make the countries more comparable, we restricted our sample to companies responsible for 90% of total market capitalisation per year in countries with more than 100 companies, namely Australia, Canada, Japan, the UK, and the US. Table 2 shows the countries and average annual number of firms included in our analysis. The annual number of companies used in the analysis varied slightly from year to year owing to missing values. In total, our sample contains 19,709 firm-year observations. For our analysis, and as suggested by (Fransen, 2013), we examined CSR activities in a disaggregated fashion, focusing solely on those related to social issues. We devised a CSR measurement based on 36 items published by Asset4 that measure the presence or absence of policies related to social issues, as well as whether or not companies have taken specific action to implement these policies. 1 We included corporate activities across seven sub-dimensions of the social pillar: prod- Moreover, we created a second index based on a coding of corporate controversies and scandals across a similar range of issues (see Error! Reference source not found.). Examples of social pillar controversies include irresponsible activities linked to business ethics in general, political contributions, bribery, and corruption. Whereas CSR data are largely based on self-reporting by companies and tend to be almost complete, the Asset4 ESG Controversy Score measures companies' exposure to irresponsible activities and negative events (and their progress) as reflected in global media (e.g. scandals, lawsuits, ongoing legislative disputes or fines). Thus, though all ethical questions are discursive, some are already discursively more settled and might be involved in court cases, whereas others are still 'only' being discussed in the media. Consequently, nearly 50% of firms scored zero as regards CSiR, reflecting a potential underreporting bias. The resulting variables for CSR and CSiR ranged from 0 to 100, expressing the percentage of measured activities in relation to the possible maximum. Missing data was excluded from the analysis. We tested H1a and H1b using the CSR index, and H2 using the CSiR index. We also calculated one additional dependent variable. To test H3a and H3b, we created a measure of variation in the CSR activities between firms. A ratio of the spread of CSR activities reflects the distance between each firm and the median CSR activity level in each country in each year. The main independent variable was a country-level indicator of NFD policies developed by using information published by the Hauser Institute for Civil Society. The institute has collected cross-national information on NFD regulation going back to the 1970s. We used this to build a binary NFD variable indicating whether a country had implemented mandatory non-financial disclosure requirements (1) or not (0). As mentioned earlier, we focused explicitly on the impact of NFD legislation on the social activities of a wide range of stock exchange-listed companies. In our analyses, we included several firm-level control variables (Jackson and Apostolakou, 2010;Koos, 2012;Padgett and Galan, 2010). First, we expected that CSR activities would increase with firm size, measured here by the logged number of employees. Second, financial performance might positively influence CSR through the availability of resources, so we measured financial performance according to return on assets (pretax income as a percentage of total assets, winsorized values in the 1 st and 99 th percentile). We also included a ratio of total debt to total assets to capture financial constraints on CSR activities owing to indebtedness. Third, CSR might also be influenced by the degree of innovation, in view of greater future business opportunities or the need to justify new products in the eyes of customers. Thus, we included the level of R&D spending as a percentage of total assets. To acknowledge possible reverse causality of firm characteristics, these variables were lagged by one year. In general, the coefficients of the control variables were consistent with previous studies. Moreover, we included industry fixed effects based on a supersector industry classification benchmark (ICB) to control for sectoral differences that affect CSR activities and year dummies to account for the observed trend towards increasing levels of CSR. Finally, we added country dummies to control for wider institutional differences between countries. All firm-level control variables were obtained from financial data collected by Thomson Reuters DataStream. A key consideration in our study was whether our CSR index would be suitable to capture the effects of NFD regulation. Broad requirements for disclosure do not necessarily lead to the adoption of specific policies or implementation activities. However, a com-pany might previously have adopted a CSR-related policy, but only begin to provide explicit information to the public about this after disclosure has become mandatory. In this sense, the measurement would capture the shift from an implicit to a more explicit form of CSR, but not necessarily a change in corporate activities. In order to control for this type of confounding, we controlled for firm-level adoption of GRI (Global Reporting Initiative) standards as an indicator of the quality of reporting. Consequently, we argue that it is unrealistic to reduce differences in our measured CSR index to differences in reporting, but we have interpreted these to reflect changes in actual CSR activities. To model the impact of NFD regulation between the years 2002 and 2014, we adopted different general linear squares (GLS) random effects models; these enabled us to test our hypotheses using robust standard errors clustered by company. Random effects estimations are appropriate since our key explanatory variable for NFD regulation does not vary much over time (Wooldridge, 2010). All estimations were carried out using STATA 12.1 and 15.0.",
        "reasoning": "The user wants to extract terms and their definitions from the provided text. I will go through the text sentence by sentence, looking for direct definitions.\n\n1.  **\"mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately-owned stock exchange-listed corporations.\"** - This is a clear, explicit definition for \"mandatory disclosure\".\n\n2.  **\"Asset4 functions as a financial intermediary providing investment information related to environmental, social, and governance (ESG) issues; it systematically collects information from company reports, company websites, and other sources, such as newspapers and nongovernmental organizations.\"** - This describes what Asset4 is and what it does, but isn't a concise, formal definition of the term itself. It's more of a description of its function.\n\n3.  **\"The Asset4 ESG Controversy Score measures companies' exposure to irresponsible activities and negative events (and their progress) as reflected in global media (e.g. scandals, lawsuits, ongoing legislative disputes or fines).\"** - This describes what the score measures, but doesn't formally define the score itself as a *concept*. It's a description of the metric.\n\n4.  **\"Random effects estimations are appropriate since our key explanatory variable for NFD regulation does not vary much over time (Wooldridge, 2010).\"** - This explains *why* random effects estimations are appropriate, but does not define \"Random effects estimations\".\n\nTherefore, only \"mandatory disclosure\" has a clear and explicit definition in the text.[[ ## reasoning ## ]]\nThe text explicitly defines \"mandatory disclosure\". Other terms like \"Asset4\" and \"Asset4 ESG Controversy Score\" are described in terms of their function or what they measure, but not given a formal definition that fits the \"Term is defined as X\" structure. \"Random effects estimations\" is also only explained in the context of its appropriateness, not defined.",
        "extracted_terms": [
          {
            "term": "mandatory disclosure",
            "definition": "regulation concerning the disclosure of social activities that applies to a broad segment of privately-owned stock exchange-listed corporations.",
            "context": "These three countries were first among the OECD to introduce NFD regulation with an explicit focus on the social dimension. In this context, mandatory disclosure is defined as regulation concerning the disclosure of social activities that applies to a broad segment of privately-owned stock exchange-listed corporations. We excluded cases where regulation applied only to state-owned companies, as in Sweden since 2007.",
            "type": "explicit"
          }
        ]
      },
      {
        "paper_id": "167f1d5e64b874bea86acdef6d5476da904fec6a",
        "sections": [
          "(e.g., Snyder, 1981;Snyder & Campbell, 1980;Snyder & Swann, 1978). The investigation of hypothesis testing has been concerned with both descriptive and prescriptive issues. On the one hand, researchers have been interested in understanding the processes by which people form, test, and revise hypotheses in social judgment, logical reasoning, scientific investigation, and other domains. On the other hand, there has also been a strong implication that people are doing things the wrong way and that efforts should be made to correct or compensate for the failings of human hypothesis testing. This concern has been expressed with regard to everyday reasoning (e.g., see Bruner, 1951;Nisbett & Ross, 1980) as well as professional scientific endeavor (e.g., Mahoney, 1979;Platt, 1964). In this article, we focus on hypotheses about the factors that predict, explain, or describe the occurrence of some event or property of interest. We mean this broadly, to include hypotheses about causation (\"Cloud seeding increases rainfall\"), categorization (\"John is an extrovert\"), prediction (\"The major risk factors for schizophrenia are . . .\"), and diagnosis (\"The most diagnostic signs of malignancy are. . .\"). We consider both descriptive and prescriptive issues concerning information gathering in hypothesis-testing tasks. We include under this rubric tasks that require the acquisition of evidence to determine whether or not a hypothesis is correct The task may require the subject to determine the truth value of a given hypothesis (e.g., Jenkins & Ward, 1965;Snyder & Campbell, 1980;Wason, 1966), or to find the one true hypothesis among a set or universe of possibilities (e.g., Bruner, Goodnow, & Austin, 1956;Mynatt, Doherty,& Tweney, 1977,1978;Wason, 1960Wason, ,1968)). The task known as rule discovery (Wason, 1960) serves as the basis for the development of our analyses, which we later extend to other kinds of hypothesis testing. We first examine what \"confirmation\" means in hypothesis testing. Different senses of confirmation have been poorly distinguished in the literature, contributing to misinterpretations of both empirical findings and theoretical prescriptions. We propose that many phenomena of human hypothesis testing can be understood in terms of a general positive test strategy. According to this strategy, you test a hypothesis by examining instances in which the property or event is expected to occur (to see if it does occur), or by examining instances in which it is known to have occurred (to see if the hypothesized conditions prevail). This basic strategy subsumes a number of strategies or tendencies that have been suggested for particular tasks, such as confirmation strategy, verification strategy, matching bias, and illicit conversion. As some of these names imply, this approach is not theoretically proper. We show, however, that the positive test strategy is actually a good all-purpose heuristic across a range of hypothesis-testing situations, including situations in which rules and feedback are probabilistic. Under commonly occurring conditions, this strategy can be well suited to the basic goal of determining whether or not a hypothesis is correct. Next, we show how the positive test strategy provides an integrative frame for understanding behavior in a variety of seemingly disparate domains, including concept identification, logical reasoning, intuitive personality testing, learning from outcome feedback, and judgment of contingency or correlation. Our thesis is that when concrete, task-specific information is lacking, or cognitive demands are high, people rely on the positive test strategy as a general default heuristic. Like any all-purpose strategy, this may lead to a variety of problems when applied to particular situations, and many of the biases and errors described in the literature can be understood in this light. On the other hand, this general heuristic is often quite adequate, and people do seem to be capable of more sophisticated strategies when task conditions are favorable. Finally, we discuss some ways in which our task analysis can be extended to a wider range of situations and how it can contribute to further investigation of hypothesis-testing processes.",
          "The Rule Discovery Task Briefly, the rule discovery task can be described as follows: There is a class of objects with which you are concerned; some of the objects have a particular property of interest and others do not. The task of rule discovery is to determine the set of characteristics that differentiate those with this target property from those without it. The concept identification paradigm in learning studies is a familiar example of a laboratory rule-discovery task (e.g. Bruner, Goodnow, & Austin, 1956;Levine, 1966;Trabasso & Bower, 1968). Here, the objects may be, for example, visual stimuli in different shapes, colors, and locations. Some choices of stimuli are reinforced, others are not. The learner's goal is to discover the rule or \"concept\" (e.g., red circles) that determines reinforcement. Wason (1960) was the first to use this type of task to study people's understanding of the logic of confirmation and disconfirmation. He saw the rule-discovery task as representative of an important aspect of scientific reasoning (see also Mahoney, 1976Mahoney, , 1979;;Mynatt et al., 1977Mynatt et al., , 1978;;Simon, 1973). To illustrate the parallel between rule discovery and scientific investigation, consider the following hypothetical case. You are an astro-physicist, and you have a hypothesis about what kinds of stars develop planetary systems. This hypothesis might be derived from a larger theory of astrophysics or may have been induced from past observation. The hypothesis can be expressed as a rule, such that those stars that have the features specified in the rule are hypothesized to have planets and those not fitting the rule are hypothesized to have no planets. We will use the symbol R H for the hypothesized rule, H for the set of instances that fit that hypothesis, and H for the set that do not fit it. There is a domain or \"universe\" to which the rule is meant to apply (e.g., all stars in our galaxy), and in that domain there is a target set (those stars that really do have planets). You would like to find the rule that exactly specifies which members of the domain are in the target set (the rule that describes exactly what type of stars have planets). We will use T for the target set, and R T for the \"correct\" rule, which specifies the target set exactly. Let us assume for now that such a perfect rule exists. (Alternate versions of the rule might exist, but for our purposes, rules can be considered identical if they specify exactly the same set T.) The correct rule may be extremely complex, including conjunctions, disjunctions, and trade-offs among features. Your goal as a scientist, though, is to bring the hypothesized rule R H in line with the correct rule R T and thus to have the hypothesized set H match the target set T. \\ou could then predict exactly which stars do and do not have planets. Similarly, a psychologist might wish to differentiate those who are at risk for schizophrenia from those who are not, or an epidemiologist might wish to understand who does and does not contract AIDS. The same structure can also be applied in a diagnostic context. For example, a diagnostician might seek to know the combination of signs that differentiates benign from malignant tumors. In each case, an important component of the investigative process is the testing of hypotheses. That is, the investigator wants to know if the hypothesized rule R H is identical to the correct rule R T and if not, how they differ. This is accomplished through the collection of evidence, that is, the examination of instances. For example, you might choose a star hypothesized to have planets and train your telescope on it to see if it does indeed have planets, or you might examine tumors expected to be benign, to see if any are in fact malignant. Wason (1960Wason ( , 1968) developed a laboratory version of rule discovery to study people's hypothesis-testing strategies (in particular, their use of confirmation and disconfirmation), in a task that \"simulates a miniature scientific problem\" (1960, p. 139). In Wason's task, the universe was made up of all possible sets of three numbers (\"triples\"). Some of these triples fit the rule, in other words, conformed to a rule the experimenter had in mind. In our terms, fitting the experimenter's rule is the target property that subjects must learn to predict. The triples that fit the rule, then, constitute the target set, T. Subjects were provided with one target triple (2,4,6), and could ask the experimenter about any others they cared to. For each triple the subject proposed, the experimenter responded yes (fits the rule) or no (does not fit). Although subjects might start with only a vague guess, they quickly formed an initial hypothesis about the rule (R H ). For example, they might guess that the rule was \"three consecutive even numbers.\" They could then perform one of two types of hypothesis tests (Htests): they could propose a triple they expected to be a target (e.g., 6, 8, 10), or a triple they expected not to be {e.g., 2,4,7). In this paper, we will refer to these as a positive hypothesis test (+Htest) and a negative hypothesis test (-Htest), respectively. Wason found that people made much more use of +Htests than -Htests. The subject whose hypothesis was \"consecutive evens,\" for example, would try many examples of consecutive-even triples and relatively few others. Subjects often became quite confident of their hypotheses after a series of+Htests only. In Wason's (1960) task this confidence was usually unfounded, for reasons we discuss later. Wason described the hypothesis testers as \"seeking confirmation\" because they looked predominantly at cases that fit their hypothesized rule for targets (e.g., different sets of consecutive even numbers). We think it more appropriate to view this \"confirmation bias\" as a manifestation of the general hypothesis-testing strategy we call the positive test (+ test) strategy. In rule discovery, the +test strategy leads to the predominant use of +Htests, in other words, a tendency to test cases you think will have the target property. The general tendency toward -(-testing has been widely replicated. In a variety of different rule-discovery tasks (KJayman & Ha, 1985;Mahoney, 1976Mahoney, , 1979;;Mynattetal., 1977Mynattetal., , 1978;;Taplin, 1975;Tweney et al., 1980;Wason & Johnson-Laird, 1972) people look predominantly at cases they expect will have the target property, rather than cases they expect will not As with nearly all strategies, people do not seem to adhere strictly to -(-testing, however. For instance, given an adequate number of test opportunities and a lack of pressure for a quick evaluation, people seem willing to test more widely (Gorman & Gorman, 1984;Klayman & Ha, 1985). Of particular interest is one manipulation that greatly improved success at Wason's 2,4,6 task. Tweney et al. (1980) used a task structurally identical to Wason's but modified the presentation of feedback. Triples were classified as either DAX or MED, rather than yes (fits the rule) or no (does not fit). The rule for DAX was Wason's original ascending-order rule, and all other triples were MED. Subjects in the DAX/MED version used even fewer -Htests than usual. However, they treated the DAX rule and the MED rule as two separate hypotheses, and tested each with -(-Htests, thereby facilitating a solution. The thrust of this work has been more than just descriptive, however. There has been a strong emphasis on the notion that a -Hest strategy (or something like it) will lead to serious errors or inefficiencies in the testing of hypotheses. We begin by taking a closer look at this assumption. We examine what philosophers of science such as Popper and Platt have been arguing, and how that translates to prescriptions for information gathering in different hypothesis-testing situations. We then examine the task characteristics that control the extent to which a -Hest strategy deviates from those prescriptions. We begin with rule discovery as described above, and then consider what happens if additional information is available (examples of known targets and nontargets), and if an element of probabilistic error is introduced. The basic question is, if you are trying to determine the truth or falsity of a hypothesis, when is a 4-test strategy unwise and when is it not? The Logic of Ambiguous Versus Conclusive Events As a class, laboratory rule-discovery tasks share three simplifying assumptions. First, feedback is deterministically accurate. The experimenter provides the hypothesis tester with error-free feedback in accordance with an underlying rule. Second, the goal is to determine the one correct rule (R T ). All other rules are classified as incorrect, without regard to how wrong R H may be, although the tester may be concerned with where it is wrong in order to form a new hypothesis. Third, correctness requires both sufficiency and necessity: A rule is incorrect if it predicts an instance will be in the target set when it is not (false positive), or predicts it will not be in the target set when it is (false negative). We discuss later the extent to which each of these assumptions restricts generalization to other tasks. Consider again Wason's original task. Given the triple (2, 4, 6), the hypotheses that occur to most people are \"consecutive even numbers,\" \"increasing by 2,\" and the like. The correct rule, however, is much broader, \"increasing numbers.\" Consider subjects whose hypothesized rule is \"increasing by 2.\" Those who use only +Htests (triples that increase by 2, such as 6, 8,10) can never discover that their rule is incorrect, because all examples of \"increasing by 2\" also fit the rule of \"increasing.\" Thus, it is crucial to try -Htests (triples that do not increase by 2, such as 2,4,7), This situation is depicted in Figure 1. Here, U represents the universe of instances, all possible triples of numbers. T represents the target set, triples that fit the experimenter's rule (\"increasing\"). H represents the hypothesized set, triples that fit the tester's hypothesized rule (say, \"increasing by 2\"). There are in principle four classes of instances, although they do not all exist in this particular example: 1. HOT: instances correctly hypothesized to be in the target set (positive hits). 2. HOT: instances incorrectly hypothesized to be in the target set (false positives). 3. HOT: instances correctly hypothesized to be outside the target set (negative hits). 4. HOT: instances incorrectly hypothesized to be outside the target set (false negatives). Instances of the types HnT and H n T falsify the hypothesis. That is, the occurrence of either shows conclusively that H ¥= T, thus RH i 1 RT? the hypothesized rule is not the correct one. Instances of the types H n T and H H f verify the hypothesis, in the sense of providing favorable evidence. However, these instances are ambiguous: The hypothesis may be correct, but these instances can occur even if the hypothesis is not correct. Note that there are only conclusive falsifications, no conclusive verifications. This logical condition is the backbone of philosophies of science that urge investigators to seek falsification rather than verification of their hypotheses (e.g., Popper, 1959). Put somewhat simplistically, a lifetime of verifications can be countered by a single conclusive falsification, so it makes sense for scientists to make the discovery of falsifications their primary goal. Suppose, then, that you are the tester in Wason's task, with the hypothesis of \"increasing by 2.\" If you try a -f Htest (e.g., 6, 8,10) you will get either a yes response, which is an ambiguous verification of the type H Pi T, or a no, which is a conclusive falsification of the type H Pi f. The falsification H n T would show that meeting the conditions of your rule is not sufficient to guarantee membership in T. Thus, +Htests can be said to be tests of the rule's sufficiency. However, unknown to the subjects in the 2, 4, 6, task (Figure 1) there are no instances of H n T, because the hypothesized rule is sufficient: Any instance following R H (\"increasing by 2\") will in fact be in the target set T (\"increasing\"). Thus, +Htests will never produce falsification. If you instead try a -Htest (e.g., 2,4,7) you will get_either a no answer which is an ambiguous verification (H fl T) or a yes answer which is a conclusive falsification (HOT). The falsification H fl T shows that your conditions are not necessary for membership in T. Thus, -Htests test a rule's necessity. In the 2, 4, 6 task, -Htests can result in conclusive falsification because R H is sufficient but not necessary (i.e., there are some target triples that do not increase by 2). In the above situation, the Popperian exhortation to seek falsification can be fulfilled only by -Htesting, and those who rely on +Htests are likely to be misled by the abundant verification they receive. Indeed, Wason deliberately designed his task so that this would be the case, in order to show the pitfalls of \"confirmation bias\" (Wason, 1962), The hypothesis-tester's situation is not always like this, however. Consider the situation in which the hypothesized set merely overlaps the target set, as shown in Figure 2, rather than being embedded within it, as  shown in Figure 1. This would be the case if, for example, the correct rule were \"three even numbers.\" There would be some members of H n T, instances that were \"increasing by 2\" but not \"three evens\" (e.g., 1, 3, 5), and some members of H fl T, \"three evens\" but not \"increasing by 2\" (e.g., 4, 6, 2). Thus, conclusive falsification could occur with either +Htests or -Htests. Indeed, it is possible to be in a situation just the opposite of Wason's, shown in Figure 3. Here, the hypothesis is too broad and \"surrounds\" the target set. This would be the case if the correct rule were, say, \"consecutive even numbers.\" Now a tester who did only -Htests could be sorely misled, because there are no falsifications of the type HOT; any instance that violates \"increasing by 2\" also violates \"consecutive evens.\" Only +Htests can reveal conclusive falsifications (HOT instances such as 1,3,5). Aside from these three situations, there are two other possible relationships between H and T. When H and T are disjoint (Figure 4), any +Htest will produce conclusive falsification, because nothing in H is in T; -Htests could produce either verification or falsification. This is not likely in the 2,4, 6 task, because you are given one known target instance to begin with. In the last case (Figure 5), you have finally found the correct rule, and H coincides with T. Here, every test produces ambiguous information; a final proof is possible only if there is a finite universe of instances and every case is searched. In naturally occurring situations, as in Wason's (1960) task, one could find oneself in any of the conditions depicted, usually with no way of knowing which. Suppose, for example, that you are a manufacturer trying to determine the best way to advertise your line of products, and your current hypothesis is that televi-sion commercials are the method of choice. For you, the universe, U, is the set of possible advertising methods; the target set, T, is the set of methods that are effective, and the hypothesized set, H, is television commercials. Suppose that in fact the set of effective advertising methods for these products is much broader: any visual medium (magazine ads, etc.) will work. This is the situation depicted in Figure 1. If you try +Htests (i.e., try instances in your hypothesized set, television commercials) you will never discover that your rule is wrong, because television commercials will be effective. Only by trying things you think will not work (-Htests) can you obtain falsification. \\bu might then discover an instance of the type HOT nontelevision advertising that is effective. Suppose instead that the correct rule for effectively advertising these products is to use humor. This is the situation in Figure 2. You could find a (serious) television commercial that you thought would work, but does not (H D T), or a (humorous) npntelevision ad that you thought would not work, but does (H Pi T). Thus, conclusive falsification could occur with either a +Htest or a -Htest. If instead the correct rule for these products is more restricted, say, \"prime-time television only,\" you would have an overly broad hypothesis, as shown in Figure 3. In that case, you will never obtain falsification if you use -Htests (i.e., if you experiment with methods you think will not work), because anything that is not on television is also not on prime time. Only + Htests can reveal conclusive falsifications, by finding instances of H n T (instances of television commercials that are not effective). What is critical, then, is not the testing of cases that do not fit your hypothesis, but the testing of cases that are most likely to prove you wrong. In Wason's task these two actions are identical, but as shown in Figures 2 through 5, this is not generally so. Thus, it is very important to distinguish between two different senses of \"seeking disconfirmation.\" One sense is to examine instances that you predict will not have the target property. The other sense is to examine instances you most expect to falsify, rather than verify, your hypothesis. This distinction has not been well recognized in past analyses, and confusion between the two senses of disconfirmation has figured in at least two published debates, one involving Wason (1960Wason ( , 1962) ) and Wetherick (1962), the other involving Mahoney (1979Mahoney ( , 1980)), Hardin (1980), andTweney, Doherty, andMynatt (1982). The prescriptions of Popper and Platt emphasize the importance of falsification of the hypothesis, whereas empirical investigations have focused more on the testing of instances outside the hypothesized set.",
          "Where's the Information? The distinction between -testing and seeking falsification leads to an important question for hypothesis testers: Given the choice between +tests and -tests, which is more likely to yield critical falsification? As is illustrated in Figures 1 through 5, the answer depends on the relation between your hypothesized set and the target set. This, of course, is impossible to know without first knowing what the target set is. Even without prescience of the truth, however, it is possible for a tester to make a reasoned judgment about which kind of test to perform. Prescriptions can be based on (at least) two considerations: (a) What type of errors are of most concern, and (b) Which test could be expected, probabilistically, to yield conclusive falsification more often. The first point hinges on the fact that +Htests and -Htests reveal different kinds of errors (false positives and false negatives, respectively). A tester might care more about one than the other and might be advised to test accordingly. Although there is almost always some cost to either type of error, one cost may be much higher than the other. For example, a personnel director may be much more concerned about hiring an incompetent person (H flT) than about passing over some potentially competent ones (H PI T). Someone in this position should favor +Htests (examining applicants judged competent, to find any failures) because they reveal potential false positives. On the other hand, some situations require greater concern with false negatives than false positives. For example, when dealing with a major communicable disease, it is more_serious to allow a true case to go undiagnosed and_untreated (H n T) than it is to mistakenly treat someone (H n T). Here the emphasis should be on -Htests (examining people who test negative, to find any missed cases), because they reveal potential false negatives. It could be, then, that a preference for +Htests merely reflects a greater concern with sufficiency than necessity. That is, the tester may simply be more concerned that all chosen cases are true than that all true cases are chosen. For example, experiments by Vogel and Annau (1973), Tschirgi (1980), and Schwartz (1981, 1982) suggest that an emphasis on the sufficiency of one's actions is enhanced when one is rewarded for each individual success rather than only for the final rule discovery. Certainly, in many real situations (choosing an employee, a job, a spouse, or a car) people must similarly live with their mistakes. Thus, people may be naturally inclined to focus more on false positives than on false negatives in many situations. A tendency toward +Htesting would be entirely consistent with such an emphasis. However, it is still possible that people retain an emphasis on sufficiency when it is inappropriate (as in Wason's task). Suppose that you are a tester who cares about both sufficiency and necessity: your goal is simply to determine whether or not you have found the correct rule. It is still possible to analyze the situation on the basis of reasonable expectations about the world. If you accept the reasoning of Popper and Platt, the goal of your testing should be to uncover conclusive falsifications. Which kind of test, then, should you expect to be more likely to do so? Assume that you do not know in advance whether your hypothesized set is embedded in, overlaps, or surrounds the target. The general case can be characterized by four quantities': The overall base-rate probability that a member of the domain is in the target set. This would be, for example, the proportion of stars in the galaxy that have planets. p(h) The overall probability that a member of the domain is in the hypothesized set. This would be the proportion of stars that fit your hypothesized criteria for having planets. The overall probability that a positive prediction will prove false, for example, that a star hypothesized to have planets will turn out not to. z\" = p(t|h) The overall probability that a negative prediction will prove false, for example, that a star hypothesized not to have planets will turn out in fact to have them. The quantities z + and z~ are indexes of the errors made by the hypothesis. They correspond to the false-positive rate and falsenegative rate for the hypothesized rule R H (cf. Einhorn & Hogarth, 1978). In our analyses, all four of the above probabilities are assumed to be greater than zero but less than one. 2 This corresponds to the case of overlapping target and hypothesis sets, as shown in Figure 2. However, other situations can be regarded as boundary conditions to this general case. For example, the embedded, surrounding, and coincident situations (Figures 1, 3, and5) are cases in which z + = p(t|h) = 0, z~ = p(t|h) = 0. or both, respectively, and in the disjoint situation (Figure 4), z + = 1. _ Recall that there are two sets of conclusive falsifications: H Q T (your hypothesis predicts planets, but there are none), and H n T (your hypothesis predicts no planets, but there are some ",
          "According to Equation 1, even if you have no information about z + and z~, you can estimate their relationship from estimates of the target and hypothesis base rates, p(t) and p(h). It is not necessarily the case that the tester knows these quantities exactly. However, there is usually some evidence available for forming estimates on which to base a judgment. In any case, it is usually easier to estimate, say, how many people suffer from schizophrenia than it is to determine the conditions that produce it. It seems reasonable to assume that in many cases the tester's hypothesis is at least about the right size. People are not likely to put much stock in a hypothesis that they believe greatly overpredicts or underpredicts the target phenomenon. Let us assume, then, that you believe that p(h) « p(t). Under these circumstances, Equation 1 can be approximated as (2) Thus, if p(t) < .5, then z* > z , which means that p(Fn|+Htest) > p(Fn|-Htest). In other words, if you are attempting to predict a minority phenomenon, you are more likely to receive falsification using +Htests than -Htests. We would argue that, in fact, real-world hypothesis testing most often concerns minority phenomena. For example, a recent estimate for the proportion of stars with planets is Vt (Sagan, 1980, p. 300), for the prevalence of schizophrenia, less than 1% (American Psychiatric Association, 1980), and for the incidence of AIDS in the United States, something between 10'' and 10\" 5 (Centers for Disease Control, 1986). Even in Wason's original task (1960), the rule that seemed so broad (any increasing) has a p(t) of only '/ 6 , assuming one chooses from a large range of numbers. Indeed, if p(t) were greater than .5, the perception of target and nontarget would likely reverse. If 80% of the population had some disease, immunity would be the target property, and p(t) would then be .2 (cf. Bourne & Guy, 1968;Einhorn& Hogarth, 1986). Thus, under some very common conditions, the probability of receiving falsification with +Htests could be much greater than with -Htests. Intuitively, this makes sense. When you are investigating a relatively rare phenomenon, p(i) is low and the set H is large. Finding a t in H (obtaining falsification with -Htests) can be likened to the proverbial search for a needle in a haystack. Imagine, for example, looking for AIDS victims among people believed not at risk for AIDS. On the other hand, these same conditions_also mean thatp(t) is high, and set H is small. Thus, finding a t in H (with +Htests) is likely to be much easier. Here, you would be examining people with the hypothesized risk factors. If you have a fairly good hypothesis, p(t|h) is appreciably lower than p(T), but you are still likely to find healthy people in the hypothesized risk group, and these cases are informative. (You might also follow a strategy based on examining known victims; we discuss this kind of testing later.) The conditions we assume above (a minority phenomenon, and a hypothesis of about the right size) seem to apply to many naturally occurring situations. However, these assumptions may not always hold. There may be cases in which a majority phenomenon is the target (e.g., because it was unexpected); then p(t) > .5. There may also be situations in which a hypothesis is tested even though it is not believed to be the right size, so that p(h) ^ p(t). For example, you may not be confident of your estimate for either p(t) or p(h), so you are not willing to reject a theoretically appealing hypothesis on the basis of those estimates. Or you may simply not know what to add to or subtract from your hypothesis, so that a search for falsification is necessary to suggest where to make the necessary change. In any case, a tester with some sense of the base rate of the phenomenon can make a reasoned guess as to which kind of test is more powerful, in the sense of being more likely to find critical falsification. The conditions under which -f Htests or -Htests are favored are summarized in Table 1. There are two main conclusions to be drawn from this analysis. First, it is important to distinguish between two possible senses of \"seeking disconfirmation\": (a) testing cases your hy- pothesis predicts to be non targets, and (b) testing cases that are most likely to falsify the hypothesis. It is the latter that is generally prescribed as optimal. Second, the relation between these two actions depends on the structure of the environment. Under some seemingly common conditions, the two actions can, in fact, conflict. The upshot is that, despite its shortcomings, the +test strategy may be a reasonable way to test a hypothesis in many situations. This is not to say that human hypothesis testers are actually aware of the task conditions that favor or disfavor the use of a +test strategy. Indeed, people may not be aware of these factors precisely because the general heuristic they use often works well.",
          "The 2,4,6 task involves only one-half of the proposed +test strategy, that is, the testing of cases hypothesized to have the target property (+Htesting). In some tasks, however, the tester may also have an opportunity to examine cases in which the target property is known to be present (or absent) and to receive feedback about whether the instance fits the hypothesis. For example, suppose that you hypothesize that a certain combination of home environment, genetic conditions, and physical health distinguishes schizophrenic individuals from others. It would be natural to select someone diagnosed as schizophrenic and check whether the hypothesized conditions were present. We will call this a positive target test (+Ttest), because you select an instance known to be in the target set. Similarly, you could examine the history of someone judged not to be schizophrenic to see if the hypothesized conditions were present. We call this a negative target test (-Ttest). Generally, Ttests may be more natural in cases involving diagnostic or epidemiological questions, when one is faced with known effects for which the causes and correlates must be determined. Ttests behave in a manner quite parallel to the Htests described above. A +Ttest results in verification (T f~l H) if the known target turns out to fit the hypothesized rule (e.g., someone diagnosed as schizophrenic turns out to have the history hypothesized to be distinctive to schizophrenia). A +Ttest results in falsification if a known target fails jo have the features hypothesized to distinguish targets (T l~l H). The jwobability of falsification with a +Test, designated x + , is p(h|t). This is equivalent to the miss rate of signal detection theory (Green & Swets, 1966). The_falsifying instances revealed by +Ttests (missed targets, T_n H) are the same kind revealed by -Htests (false negatives, H D T). Note, though, that the miss rate of +Ttests is calculated differently than the false negative rate of -Htests [x+ = p(h]t); z~ = p(t|h)]. Both +Ttests and -Htests assess whether the conditions in R H are necessary for schizophrenia. With -Ttests, verifications are of the type T n H (nonschizophrenks who do not have the history hypothesized for schizophrenics), and falsifications are of the type T n H (nonschizophrenics who do have that history). The probability of falsification with -Ttests, designated x~, is p(h|t). This is equivalent to the false alarm rate in signal detection theory. -Ttests and +Htests reveal the same kinds of falsifying instances (false alarms or false positives). The rate of falsification with -Ttests is x~ = p(hft) compared to 2* = />(t)h) for +Htests. Both -Ttests and +Htests assess whether the conditions in R H are sufficient. We can compare the two types of Ttests in a manner parallel to that used to compare Htests. The values x.* and jc\" (the miss rate and false alarm rate, respectively) can be related following the same logic used in Equation 1: (3) If we again assume thatp(t) < .5 andp(h) = p(l), then x + > x~. This means that +Ttests are more likely to result in falsification than are -Ttests. The full set of conditions favoring one type of Ttest over the other are shown in Table 2. Under common circumstances, it can be normatively appropriate to have a second kind of \"confirmation bias,\" namely, a tendency to test cases known to be targets rather than those known to be nontargets. It is also interesting to consider the relations between Ttests and Htests. In some situations, it may be more natural to think about one or the other. In an epidemiological study, for example, cases often come presorted as T or T (e.g., diagnosed victims of disease vs. normal individuals). In an experimental study, on the other hand, the investigator usually determines the presence or absence of hypothesized factors and thus membership in H or H (e.g., treatment vs. control group). Suppose, though, that you are in a situation where all four types of test are feasible. There are then two tests that reveal falsifications of the type H D T (false positives or false alarms), namely +Htests and -Ttests. These falsifications indicate that the hypothesized conditions are not sufficient for the target phenomenon. For example, suppose a team of meteorologists wants to test whether certain weather conditions are sufficient to produce tornadoes. The team can look for tornadoes where the hypothesized conditions exist (+Htests) or they can test for the conditions where tornadoes have not occurred (-Ttests). The probability of discovering falsification with each kind of test is as follows: p(Fn|+Htest)",
          "Thus, if we assume, as before, that p(t) < .5, and p(h) = p(t), then_z + > x~: the probability of finding a falsifying instance (h n T) is higher with +Htests than with -Ttests. _ There are also two tests that reveal falsifications of the type Thus, the probability of finding HOT falsifications is higher with +Ttests than with -Htests. These relationships reinforce the idea that it may well be advantageous in many situations to have two kinds of \"confirmation bias\" in choosing tests: a tendency to examine cases hypothesized to be targets (+Htests) and a tendency to examine cases known to be targets (+Ttests). Taken together, these two tendencies compose the general +test strategy. Under the usual assumptions [p(t) < .5 and p(t) <= p(h)], +Htests are favored over -Htests, and +Ttests over -Ttests, as more likely to find falsifications. Moreover, if you wish to test your rule's sufficiency, +Htests are better than -Ttests; if you wish to test the rule's necessity, +Ttests are better than -Htests. Thus, it may be advantageous for the meteorologists to focus their field research on areas with hypothesized tornado conditions and areas of actual tornado sighting (which, in fact, they seem to do; see Lucas & Whittemore, 1985). Like many other cognitive heuristics, however, this +test heuristic may prove maladaptive in particular situations, and people may continue to use the strategy in those situations nonetheless (cf. Hogarth, 1981;Tversky & Kahneman, 1974).",
          "Laboratory versions of rule discovery usually take place in a deterministic environment: There is a correct rule that makes absolutely no errors, and feedback about predictions is completely error-free (see Kern, 1983, andGorman, 1986, for interesting exceptions). In real inquiry, however, one does not expect to find a rule that predicts every schizophrenic individual or planetary system without error, and one recognizes that the ability to detect psychological disorders or celestial phenomena is imperfect. What, then, is the normative status of the +test heuristic in a probabilistic setting? Irreducible error. In a probabilistic environment, it is somewhat of a misnomer to call any hypothesis correct, because even the best possible hypothesis will make some false-positive and false-negative predictions. These irreducible errors might actually be due to imperfect feedback, but from the tester's point of view they look like false positives or false negatives. Alterna-tively, the world may have a truly random component, or the problem may be so complex that in practice perfect prediction would be beyond human reach. In any case, the set T can be defined as the set of instances that the feedback indicates are targets. A best possible rule, R B , can be postulated that defines the set B. B matches T as closely as possible, but not exactly. Because of probabilistic error, even the best rule makes falsepositive and false-negative prediction errors (i.e., p(t|b) > 0 and p(t|b) > 0). The probabilities of these errors, designated e + and t~, represent theoretical or practical minimum error rates. 3   Qualitatively, the most important difference between deterministic and probabilistic environments is that both verification and falsification are of finite value and subject to some degree of probabilistic error. Thus, falsifications are not conclusive but merely constitute some evidence against the hypothesis, and verifications must also be considered informative, despite their logical ambiguity. Ultimately, it can never be known with certainty that any given hypothesis is or is not the best possible. One can only form a belief about the probability that a given hypothesis is correct, in light of the collected evidence. Despite these new considerations, it can be shown that the basic findings of our earlier analyses still apply. Although the relationship is more complicated, the relative value of +tests and -tests is still a function of estimable task characteristics. In general, it is still the case that +tests are favored when p(t) is small and p(h) = p(t), as suggested earlier. Although we discuss only Htests here, a parallel analysis can be performed for Ttests as well.",
          "Assume that your goal is to obtain the most evidence you can about whether or not your current hypothesis is the best possible. Which type of test will, on average, be more informative? This kind of problem calls for an analysis of the expected value of information (e.g., see Edwards, 1965;Raiffa, 1968). Such analyses are based on Bayes's equation, which provides a normative statistical method for assessing the extent to which a subjective degree of belief should be revised in light of new data. To perform a full-fledged Bayesian analysis of value of information, it would be necessary to represent the complete reward structure of the particular task and compute the tester's subjective expected utility of each possible action. Such an analysis would be very complex or would require a great many simplifying assumptions. It is possible, though, to use a simple, general measure of \"impact,\" such as the expected change in belief (EAP). Suppose you think that there is some chance your hypothesis is the best possible, p(Rn = RB). Then, you perform a +Htest, and receive a verification (Vn). You would now have a somewhat higher estimate of the chance that your hypothesis is the best onep(R H = R B |Vn, +H). Call the impact of this test AP Vn , +H , the absolute magnitude of change in degree of belief. Of course, you might have received a falsification (Fn) instead, in which case your belief that R H = RB would be reduced by some amount, AP Fn ,+H-The expected change in belief for a +Htest, 3 For simplicity, we ignore the possibility that a rule might produce, say, fewer false positives but more false negatives than the best rule. We assume that the minimum c* and t~ can both be achieved at the same time. The more general case could be analyzed by defining a joint function olV and r which is to be minimized. given  RB). ( 10) This probabilistic analysis looks different from its deterministic counterpart in one respect. Before, the emphasis was strictly on falsification. Here, verification can sometimes be more informative than falsification. Using +Htests to illustrate, Equations 7 and 8 imply that if z* > .5, then AP V n,+H > AP Fn ,+H -A hypothesis with z* > .5 is a weak hypothesis; you believe the majority of predicted targets will prove wrong. Perhaps this is an old hypothesis that is now out of favor, or a new shot-in-thedark guess. The AP measure captures the intuition that surprise verification of a longshot hypothesis has more impact than the anticipated falsification. In considering the expected impact of a test, you must balance the greater impact of unexpected results against the fact that you do not think such results are likely to happen. With the EAP measure, the net result is that verifications and falsifications are expected to make equal contributions to changes in belief, overall (as shown in Equations 9 and 10). Verifications and falsifications have equal expected impact even in a deterministic environment, according to this definition of impact. The deterministic environment is merely a special case in which ( + = t~ = 0. Given this probabilistic view of the value of verification and falsification, where should one look for information? The answer to this question, based on the comparison between -t-Htests and -Htests, changes very little from the deterministic case. It would be a rational policy for a tester to choose the type of Htest associated with the greatest expected change in belief. In that case, according to Equations 1 1 and 1 2, you want to choose the test for which z -e is greatest: +Htests if (z + -« + ) > (z\" -e~). In other words, choose the test for which you believe the probability of falsification (z) is most above the level of irreducible error (c). This prescription is obviously very similar to the conditions specified for the deterministic environment. Indeed, if the two «s are equal (even if nonzero) the rule is identical: Choose the test with the higher z. Thus, the prescriptions shown in Table 1 hold in a probabilistic environment, as long as irreducible error is also taken into account. In the Appendix we also present an alternative measure of informativeness (a measure of \"diagnosticity\" often used in Bayesian analyses); the basic premises of our comparison remain intact. Qualitatively similar results obtain even when using a non-Bayesian analysis, based on statistical information theory (see Klayman, 1986).",
          "The foundation of our analysis is the separation of disconfirmation as a goal from discontinuation as a search strategy. It is a widely accepted prescription that an investigator should seek falsification of hypotheses. Our analyses show, though, that there is no correspondingly simple prescription for the search strategy best suited to that goal. The optimal strategy is a function of a variety of task variables such as the base rates of the target phenomenon and the hypothesized conditions. Indeed, even attempting falsification is not necessarily the path to maximum information (see also Klayman, 1986). We do not assume that people are aware of the task variables that determine the best test strategies. Rather, we suggest that people use a general, all-purpose heuristic, the positive test strategy, which is applied across a broad range of hypothesistesting tasks. Like any all-purpose heuristic, this +test strategy is not always optimal and can lead to serious difficulties in certain situations (as i n Wason's 2,4,6 task). However, our analyses show that +testing is not a bad approach in general. Under commonly occurring conditions, the +test strategy leads people to perform tests of both sufficiency and necessity (+Htests and +Ttests), using the types of tests most likely to discover violations of either.",
          "",
          "The main point of our analysis is not that people are better hypothesis testers than previously thought (although that may be so). Rather, the +test strategy can provide a basis for understanding the successes and failures of human hypothesis testing in a variety of situations. In this section, we apply our approach to several different hypothesis-testing situations. Each of the tasks we discuss has an extensive research literature of its own. However, there has been little cross-task generality beyond the use of the common \"confirmation bias\" label. We show how these diverse tasks can be given an integrative interpretation based on the general +test strategy. Each task has its unique requirements, and ideally, people should adapt their strategies to the characteristics of the specific task at hand. People may indeed respond appropriately to some of these characteristics under favorable conditions (when there is concrete task-specific information, light memory load, adequate time, extensive experience, etc.). We propose that, under less friendly conditions, hypothesis testers rely on a generally applicable default approach based on the +test strategy.",
          "At the beginning of this paper, we described the concept-identification task (Bruner et al., 1956) as a forerunner of Wason's rule-discovery task (Wason, 1960). In both tasks, the subject's goal is to identify the rule or concept that determines which of a subset of stimuli are designated as correct. In concept identification, however, the set of possible instances and possible rules is highly restricted. For example, the stimuli may consist of all combinations of four binary cues (letter X or T, large or small, black or white, on the right or left), with instructions to consider only simple (one-feature) rules (e.g., Levine, 1966). The hypothesis set, then, is restricted to only eight possibilities. Even when conjunctions or disjunctions of features are allowed (e.g., Bourne, 1974;Bruneret al., 1956), the hypothesis set remains circumscribed. A number of studies of concept identification have documented a basic win-stay, tose-shift strategy (e.g., see Levine, 1966Levine, ,1970;;Trabasso&Bower, 1968). That is, the learner forms an initial hypothesis about which stimuli are reinforced (e.g., \"Xs on the left\") and responds in accordance with that hypothesis as long as correct choices are produced. If an incorrect choice occurs, the learner shifts to a new hypothesis and responds in accordance with that, and so on. In our terms, this is +Htesting. It is what we would expect to see, especially since total success requires a rule that is sufficient for reward, only. In the concept-identification task + Htesting alone could lead to a successful solution. However, because there are only a finite number of instances (cue combinations), and a finite number of hypotheses, +testing is not the most effective strategy. A more efficient strategy is to partition the hypotheses into classes and perform a test that will eliminate an entire class of hypotheses in a single trial. For example, if a small, black X on the left is correct on one trial, the rules \"large,\" \"white,\" \"T,\" and \"right\" can all be eliminated at once. If on the next trial a large, black X on the right is correct, only \"black\" and \"X\" remain as possibilities, ignoring combinations. This \"focusing\" strategy (Bnmer et al.,19S6) is mathematically optimal but requires two things from subjects. First, they must recognize that having a circumscribed hypothesis set means it is possible to use a special efficient strategy not otherwise available. Second, focusing requires considerable cognitive effort to design an efficient sequence of tests and considerable memory demands to keep track of eliminated sets of hypotheses. Subjects sometimes do eliminate more than one hypothesis at a time, but considering the mental effort and memory capacity required by the normative strategy, it is not surprising that a basic +test heuristic predominates instead (Levine, 1966(Levine, , 1970;;Millward & Spoehr, 1973;Taplin, 1975).",
          "As suggested earlier, the +test strategy applies to both Htests and T tests. Thus, tasks that allow both are of particular interest. One example is the four-card problem (Wason, 1966(Wason, ,1968;;Wason & Johnson-Laird, 1972) and its descendants (e.g., Cox & Griggs, 1982;Evans & Lynch, 1973;Griggs, 1983;Griggs & Cox, 1982, 1983;Hoch & Tschirgi, 1983, 1985;Yachanin & Tweney, 1982). In these tasks, subjects are asked to determine the truth-value of the proposition \"if P then Q\" (P -• Q). For example, they may be asked to judge the truth of the following statement: \"If a card has a vowel on the front, it has an even number on the back\" (Wason, 1966(Wason, ,1968)). They are then given the opportunity to examine known cases of P, P, Q, and Q. For example, they can look at a card face-up with the letter E showing, face-up with the letter K, face-down with the number 4 showing, or face-down with the number 7. In our terms, this is a hypothesis-testing task in which \"has an even number on the back\" is the target property, and \"has a vowel on the front\" is the hypothesized rule that determines the target set. However, the implication P -* Q is not logically equivalent to the if-andonly-if relation tested in rule discovery: P is required only to be sufficient for Q, not also necessary. Subjects nevertheless use the same basic +test approach. From our point of view, to look at a vowel is to do a +Htest. The card with the consonant is a -Htest, the even number a +Ttest, and the odd number a -Ttest. If the +test heuristic is applied to problems of the form P -» Q, we would expect to find a tendency-to select the +Htest and the +Ttest (P and Q), or the -I-Htest only (P). Indeed, these choice patterns (P and Q, or P only) are the most commonly observed in a number of replications (Evans & Lynch, 1973;Griggs & Cox, 1982;Wason, 1966Wason, , 1968;;Wason & Johnson-Laird, 1972). However, there is a critical difference between the rule to be evaluated in the four-card problem and those in rule discovery. The implication P -»Q is subject to only one kind of falsification, P n Q. As a result, the +test strategy is inappropriate in this task. The only relevant tests are those that find false positives: +Htests and -Ttests (P and Q, e.g., E and 7). Earlier, we proposed that people would be able to move beyond the basic +test strategy under favorable conditions, and research on the four-card problem has demonstrated this. In particular, a number of follow-up studies have shown that a concrete context can point the way for subjects. Consider, for example, the casting of the problem at a campus pub serving beer and cola, with the proposition \"if a person is drinking beer, then the person must be over 19\" (Griggs & Cox, 1982). Here the real-world context alerts subjects to a critical feature of this specific task: The error of interest is \"beer-drinking and not-over-19\" (P n Q). The presence of people over 19 drinking cola (PflQ) is immaterial. In this version, people are much more likely to examine the appropriate cases, P and Q (beer drinkers and those under 19). Hoch andTschirgi (1983, 1985) have shown similar effects for more subtle and general contextual cues as well. Although there have been many explanations for the presence and absence of the P and Q choice pattern, a consensus seems to be emerging. The if/then construction is quite ambiguous in natural language; it often approximates a biconditional or other combination of implications (e.g., see Legrenzi, 1970;Politzer, 1986;Rumain, Connell, & Braine, 1983;Tweney & Doherty, 1983). A meaningful context disambiguates the task by indicating the practical logic of the situation. Some investigators have suggested that in an abstract or ambiguous task, people resort to a degenerate strategy of merely matching whatever is mentioned in the proposition, in other words, P and Q (Evans & Lynch, 1973;Hoch &Tschirgi, 1985;Tweney & Doherty, 1983). We suggest, however, that this heuristic of last resort is not a primitive refuge resulting from confusion or misunderstanding, but a manifestation of a more general default strategy (+testing) that turns out to be effective in many natural situations. People seem to require contextual or \"extra logical\" information (Hoch & Tschirgi, 1983) to help them see when this all-purpose heuristic is not appropriate to the task at hand.",
          "Snyder, Swann, and colleagues have conducted a series of studies demonstrating that people tend to seek confirmation of a hypothesis they hold about the personality of a target person (Snyder, 1981;Snyder & Campbell, 1980;Snyder & Swann, 1978;Swann & Giuliano, in press). For example, in some studies (Snydei;1981;Snyder & Swann, 1978), one group of subjects was asked to judge whether another person was an extrovert, and a second group was asked to determine whether that person was an introvert. Given a list of possible interview questions, both groups tended to choose \"questions that one typically asks of people already known to have the hypothesized trait\" (Snyder, 1981, p. 280). For example, subjects testing the extrovert hypothesis often chose the question \"What would you do if you wanted to liven things up at a party?\" This behavior is quite consistent with the +test heuristic. Someone's personality can be thought of as a set of behaviors or characteristics. To understand person A's personality is, then, to identify which characteristics in the universe of possible human characteristics belong to person A and which do not. That is, the target set (T) is the set of characteristics that are true of person A. The hypothesis \"A is an extrovert\" establishes a hypothesized set of characteristics (H), namely those that are true of extroverts. The goal of the hypothesis tester is, as usual, to determine if the hypothesized set coincides well with the target set. In other words, to say \"A is an extrovert\" is to say: \"If it is characteristic of extroverts, it is likely to be true of A, and if it is not characteristic of extroverts, it is likely not true of A.\" Following the +test strategy, you test this by examining extrovert characteristics to see if they are true of the target person (+Htests). The -t test strategy fails in these tasks because it does not take into account an important task characteristic: Some of the available questions are nondiagnostic. The question above, for example, is not very conducive to an answer such as \"Don't ask me, I never try to liven things up.\" Both introverts and extroverts accept the premise of the question and give similar answers (Swann, Giuliano, & Wegner, 1982). Subjects would better have chosen neutral questions (e.g., \"What are your career goals?\") that could be more diagnostic. However, it is not +Htesting that causes problems here; it is the mistaking of nondiagnostic questions for diagnostic ones (Fischhoff & Beyth-Marom, 1983;Swann, 1984). All the same, it is not optimal for testers to allow a general preference for +Htests to override the need for diagnostic information. A series of recent studies suggest that, given the opportunity, people do choose to ask questions that are reasonably diagnostic; however, they still tend to choose questions for which the answer is yes if the hypothesized trait is correct (Skov & Sherman, 1986;Strohmer& Newman, 1983;Swann & Giuliano, in press;Trope & Bassok, 1982, 1983;Trope, Bassok, & Alon, 1984). For example, people tend to ask a hypothesized introvert questions such as \"Are you shy?\" Indeed, people may favor +Htesting in part because they believe +Htests to be more diagnostic in general (cf. Skov & Sherman, 1986;Swann & Giuliano, in press). Interestingly, Trope and Bassok (1983) found this +Htesting tendency only when the hypothesized traits were described as extreme (e.g., extremely polite vs. on the polite side). If an extreme personality trait implies a narrower set of behaviors and characteristics, then this is consistent with our normative analysis of +Htesting: As p(t) becomes smaller, the advantage of +Htesting over -Htesting becomes greater (see Equations 1 and 2). Although only suggestive, the Trope and Bassok results may indicate that people have some salutary intuitions about how situational factors affect the +test heuristic (see also Swann & Giuliano, in press).",
          "So far we have only considered tasks in which the cost of information gathering and the availability of information are the same for +tests and -tests. However, several studies have looked at hypothesis testing in situations where tests are costly. Of particular ecological relevance are those tasks in which one must learn from the outcomes of one's actions. As mentioned earlier, studies by Tschirgi (1980) and Schwartz (1982) suggest that when test outcomes determine rewards as well as information, people attempt to replicate good results (reinforcement) and avoid bad results (nonreinforcement or punishment). This encourages +Htesting, because cases consistent with the best current hypothesis are believed more likely to produce the desired result. Einhorn and Hogarth (1978; see also Einhorn, 1980) provide a good analysis of how this can lead to a conflict between two important goals: (a) acquiring useful information to revise one's hypothesis and improve long-term success, and (b) maximizing current success by acting the way you think works best. Consider the case of a university admissions panel that must select or reject candidates for admission to graduate school. Typically, they admit only those who fit their hypothesis for success in school (i.e., those who meet the selection criteria). From the point of view of hypothesis testing, the admissions panel can check on selected candidates to see if they prove worthy (+Htests). It is much more difficult to check on rejected candidates (-Htests) because they are not conveniently collected at your institution and may not care to cooperate. Furthermore, you would really have to admit them to test them, because their outcome is affected by the fact that they were rejected (Einhorn & Hogarth, 1978). In other words, -Htests would require admitting some students hypothesized to be unworthy. However, if there is any validity to the admissions committee's judgment, this would have the immediate effect of reducing the average quality of admitted students. Furthermore, it would be difficult to perform either kind of Ttest in these situations. -l-Ttests and -Ttests would require checking known successes and known failures, respectively, to see whether you had accepted or rejected them. As before, information about people you rejected is bard to come by and is affected by the fact that you rejected them. The net result of these situational factors is that people are strongly encouraged to do only one kind of tests: +Htests. This limitation is deleterious to learning, because +Htests reveal only false positives, never false negatives. As in Wason's 2,4,6 task, this can lead to an overly restrictive rule for acceptance as you attempt to eliminate false-positive errors without knowing about the rate of false negatives. On the other hand, our analyses suggest that there are situations in which reliance on + Htesting may not be such a serious mistake. First, it might be the case that you care more about false positives than false negatives (as suggested earlier). You may not be too troubled by the line you insert in rejection letters stating that \"Regrettably, many qualified applicants must be denied admission.\" In this case, +Htesls are adequate because they reveal the more important errors, false positives. Even where both types of errors are important, there are many circumstances in which +Htests may be useful because false positives are more likely than false negatives (see Table 1). When P(t) = P(h) andp(t) < .5, for example, the false-positive rate is always greater than the false-negative rate. In other words, if only a minority of applicants is capable of success in your program, and you select about the right proportion of applicants, you are more likely to be wrong about an acceptance than a rejection. As always, the effectiveness of a +test strategy depends on the nature of the task. Learning from +Htests alone is not an optimal approach, but it may often be useful given the constraints of the situation.",
          "There has been considerable recent interest in how people make judgments of contingency or covariation between factors (e.g., see Alloy & Tabachnik, 1984;Arkes & Harkness, 1983;Crocker, 1981;Nisbett & Ross, 1980;Schustack & Steinberg, 1981;Shaklee & Mims, 1982), and one often-studied class of contingency tasks is readily described by the theoretical framework proposed in the present paper. These are tasks that require the subject to estimate the degree of contingency (or its presence or absence) between two dichotomous variables, on the basis of the presentation of a number of specific instances. For example, Ward and Jenkins (1965) presented subjects with the task of determining whether there was a contingency between the seeding of clouds and the occurrence of rainfall on that day. Subjects based their judgments on a series of slides, each of which indicated the state of affairs on a different day: (a) seeding + rain, (b) seeding + no rain, (c) no seeding + rain, or (d) no seeding + no rain. In our terms, the dichotomous-contingency task can be characterized as follows: The subject is presented with a target property or event and a set of conditions that are hypothesized to distinguish occurrences of the target from nonoccurrences. In the Ward and Jenkins (1965) example, the target event is rain, and the condition of having seeded the clouds is hypothesized to distinguish rainy from nonrainy days. This task is different from rule discovery in two ways. First, the hypothesized rule is not compared to a standard of \"best possible\" prediction, but rather to a standard of \"better than nothing.\" Second, the information search takes place in memory; the tester determines which information to attend to or keep track of rather than controlling its presentation. (A similar characterization is presented by Crocker, 1981.) Despite these differences, we propose that the basic +test strategy is manifested in covariation judgment much as it is in other, more external tasks. The event types listed above can be mapped onto our division of instances into H and H, T and T (see Table 3). The labels given the cells, A, B, C, and D, correspond to the terminology commonly used in studies of contingency. One possible evaluation strategy in such a problem is to think of cases in which the conditions were met (days with cloud seeding), and estimate how often those cases possessed the target property (rain). This is +Htesting: examining instances that Similarly, one could think of cases in which the target property occurred (it rained) to see whether the hypothesized conditions were met (clouds had been seeded). This is equivalent to +Ttesting, based on instances in cells A and C. We expect, as usual, that people will favor +Htests and +Ttests over -Htests and -Ttests. We also expect that there may be a tendency toward +Htesting in particular, because of greater attention to the sufficiency of rules than to their necessity (e.g., you do not mind if it rains sometimes without seeding). Also, many contingency tasks are framed in terms of the relation between causes and effects. Htests may be more natural then, because they are consistent with the temporal order of causation, moving from known causes to possible results (cf. Tversky & Kahneman, 1980). These hypotheses lead to some specific predictions about people's judgments of contingency. On a group level, judgments The data from a variety of studies support these predictions. Schustack and Sternberg (1981), for example, found that the contingency judgments of subjects taken as a group were best modeled as a linear combination of the number of instances of each of the four types, with the greatest emphasis placed on Acell, B-cell, C-cell, and D-cell data, in that order. Similar results were reported in an experiment by Arkes and Harkness (1983, Experiment 7), and in a meta-analysis of contingency-judgment tasks by Lipe (1982). A number of studies have also examined data from individual subjects. Although some studies indicate that people are influenced almost entirely by A-cell data (Jenkins & Ward, 1965;Nisbett & Ross, 1980;Smedslund, 1963), there is now considerable evidence for the prevalence of an A -B strategy (Arkes & Harkness, 1983;Shaklee&Mims, 1981,1982;Ward & Jenkins, 1965). This label has been applied to strategies that compare the number of H n T instances with the number of H n T (Cell A vs. CeUB) as well as strategies that compare T n H (Cell A) with T n H {Cell Q. The first comparison is consistent with our idea of +Htesting, the second with +Ttesting. These two kinds of comparison have not been clearly distinguished in the literature. For example, Arkesand Harkness (1983) sometimes label the condition-but-no-event cell as B, and sometimes the eventhut-no-condition cell as B. However, in one study, Shaklee and Mims (! 981) were able to distinguish A -B and A -C patterns in their data and found evidence of both. Further evidence of a +test approach is found in a recent study by Doherty and Falgout (1985). They presented the Ward and Jenkins (1965) cloud-seeding task on a computer screen and enabled subjects to save instances in computer memory for later reference. Although there were large individual differences, the most common pattern was to save a record of instances in cells A and B (the results of +Htests). The second most common pattern was to save A-, B-, and C-ceB instances (+Htests and +Ttests), and the third most common pattern was B and C (the falsifications from +Htests and +Ttests). Together, these 3 patterns accounted for 32 of 40 data-saving patterns in two experiments. In contingency judgment as in rule discovery, the +test strategy can often work well as a heuristic for hypothesis testing. However, this approach can deviate appreciably from statistical standards under some circumstances. Most statistical indexes (e.g., chi-square or correlation coefficient) put equal weight on all four cells, which -f testing does not Are people capable of more sophisticated strategies? Shaklee andMims (1981,1982) and Arkes and Harkness (1983) describe a sum-of-diagonals strategy that generally fares well as a rough estimate of statistical contingency. However, a simple combination of+Htests and +Ttests would result in a pattern of judgments very similar to the sum-of-diagonals strategy. A stimulus set could be carefully constructed to discriminate the two, but in the absence of such studies, we suspect that many sum-of-diagonals subjects may actually be using a combination of A versus B (+Htests) and A versus C (+Ttests). This may explain why individual analyses indicate frequent use of sum-of-diagonals strategies whereas group analyses often indicate that D-cell data is given little weight. On the other hand, we would expect that subjects might use more sophisticated strategies under favorable circumstances. There is some evidence that reduced memory demands have such an effect. Contingency judgments are more sophisticated when data are presented in summary form, rather than case by case (Arkes & Harkness, 1983;Shaklee & Mims, 1981, 1982;Shaklee & Tucko;1980;Ward & Jenkins, 1965). Also, the problem context and the wording of the question may direct attention to relevant sources of data (Arkes & Harkness, 1983;Crockei;1982;Einhorn& Hogarth, 1986).",
          "The concept of a general +test strategy provides an integrative interpretation for phenomena in a wide variety of hypothesis-testing tasks. This interpretation also prompts a number of new theoretical and empirical questions. There are several ways our analyses can be extended to explore further the nature of hypothesis-testing tasks and the strategies people use to accomplish them. We present a few examples here. In this article we discuss tasks in which the goal is to determine the correctness of a single hypothesis. This is a common situation, since people (including scientists) tend to view hypothesis testing in terms of verifying or falsifying one particular hypothesis (Mftroff, l974;Tweney, 1984Tweney, ,1985;;Tweney& Doherty, 1983;Tweney et al., 1980). On the other hand, it would be interesting to analyze the use of simultaneous alternate hypotheses in obtaining informative tests of hypotheses (see Figure 6). The importance of specific alternatives has been emphasized in laboratory hypothesis-testing studies (e.g., Wason & Johnson-Laird, 1972, chap. 16) and in philosophical discussions (e.g., Platt, 1964). An analysis like ours could be used to examine how alternate hypotheses can increase the expected information from tests, under what circumstances an alternative is not useful (e.g., with a straw-man hypothesis), and when it would be better to simultaneously verify or falsify two alternatives rather than perform a test that favors one over the other. From a theoretical perspective, it might also be interesting to examine a situation in which a larger set of alternate hypotheses are evaluated simultaneously. This may not be representative of ordinary scientific thought, but could provide an interesting normative standard (cf. Edwards, 1965;Raiffa, 1968). It is also akin to problems commonly faced by artificial intelligence researchers in designing expert systems to perform diagnostic tasks (see, e.g., Duda&Shortliffe, 1983;Fox, 1980). Another possible extension of these analyses is to consider standards of comparison other than \"correct\" or \"best possible.\" la many situations, it may be more appropriate to ask whether or not your hypothesis is \"pretty good,\" or \"good enough,\" or even \"better than nothing.\" Then, instead of comparing error rates to irreducible minima (t + and O, you are comparing them to other standards (s* and s~). Similarly, it would be possible to consider the testing of a rule for estimating a continuous variable rather than for predicting the presence or absence of a property. What you want to know then is the expected amount of error, rather than just the probability of error. Our theoretical analyses also suggest a number of interesting empirical questions concerning toe ways in which people adapt their strategies to the task at hand. For example, we indicate that certain task variables have a significant impact on how effective the -Hest strategy is in different situations. We do not know the extent to which people respond to these variables, or whether they respond appropriately. For example, do people use -Htests more when the target set is large? Will they do so if the cost of false negative guesses is made clear? Our review of existing research suggests that people may vary their approach appropriately under favorable conditions. However, there is still much to learn about how factors such as cognitive bad and taskspecific information affect hypothesis-testing strategies. Finally, there is a broader context of hypothesis formation and revision that should be considered as well. We have focused on the process of finding information to test a hypothesis. The broader context also includes questions about how to interpret yourfindings(e.g., seeDarley&Gross, 1983;Hoch&Ha, 1986;Lord et al., 1979). The astrophysicist must decide if the blur in the picture is really a planer, the interviewer must judge whether the respondent has given an extroverted answer. Moreover, questions about how hypotheses are tested are inevitably linked to questions about how hypotheses are generated. The latter sort of questions have received much less attention, however, possibly because they are harder to answer (but see, e.g., Gettys, 1983;Gettys & Fisher, 1979). Obtaining falsification is only a first step. The investigator must use that information to build a new hypothesis and must then do further testing. Thus, analyses of hypothesis testing and hypothesis generation will be mutually informative.",
          "Over the past 30 years, there have been scores of studies on the nature of hypothesis testing in scientific investigation and in everyday reasoning. Many investigators talk about confirmation bias, bat this term has been applied to many different phenomena in a variety of contexts. In our review of the literature, we find that different kinds of \"confirmation bias\" can be understood as resulting from a basic hypothesis-testing heuristic, which we call the positive test strategy. That is, people tend to test hypotheses by looking at instances where the target property is hypothesized to be present or is known to be present. This +test strategy, in its various manifestations, has generally been regarded as incompatible with the prescription to seek disconfirmation. The central idea of this prescription is that the hypothesis tester should make a deliberate attempt to find any evidence that would falsify the current hypothesis. As we show, however, +testing does not necessarily contradict the goal of seeking falsification. Indeed, under some circumstances, +testing may be the only way to discover falsifying instances (see Fig- ure 3). Furthermore, in probabilistic environments, it is not even necessarily the case that falsification provides more information than verification. What is best depends on the characteristics of the specific task at hand. Our review suggests that people use the +test strategy as a general default heuristic. That is, this strategy is one that people use in the absence of specific information that identifies some tests as more relevant than others, or when the cognitive demands of the task preclude a more carefully designed strategy. Our theoretical analyses indicate that, as an all-purpose heuristic, +testing often serves the hypothesis tester well. That is probably why it persists, despite its shortcomings. For example, if the target phenomenon is relatively rare, and the hypothesis roughly matches this base rate, you are probably better off testing where you do expect the phenomenon to occur or where you know the phenomenon occurred rather than the opposite. This situation characterizes many real-world problems. Moreover, +tests may be less costly or less risky than -tests when realworld consequences are involved (Einhorn & Hogarth, 1978;Tschirgi, 1980). Like most general-purpose heuristics, however, +testing can lead to problems when applied inappropriately. In rule discovery, it can produce misleading feedback by failing to reveal a whole class of important falsifications (violations of necessity). In propositional reasoning (e.g., the four-card problem), +testing leads to superfluous tests of necessity (+Ttests) and neglect of some relevant tests of sufficiency (-Ttests). In a variety of tasks, including concept indentification, intuitive personality testing, and contingency judgment, a +test strategy can lead to inefficiency or inaccuracy by overweighting some data and underweighting others. The consequences of using a +test strategy vary with the characteristics of the task. Our task analyses serve two major functions. First, they nighlight some of the structural similarities among diverse tasks in the broad domain of hypothesis testing. This permits integration of findings from different subareas that have so far been fairly isolated from each other. Second, our approach provides a framework for analyzing what each task requires of the subject, why people make the mistakes they do, and why changes in the structure and content of tasks sometimes produce significant changes in performance. These questions are central to understanding human hypothesis testing in the larger context of practical and scientific reasoning."
        ],
        "ground_truth_definitions": {
          "confirmation bias": {
            "type": "implicit",
            "definition": "a tendency to test cases known to be targets rather than those known to be nontargets",
            "context": "Under common circumstances, it can be normatively appropriate to have a second kind of \"confirmation bias,\" namely, a tendency to test cases known to be targets rather than those known to be nontargets. It is also interesting to consider the relations between Ttests and Htests"
          }
        }
      },
      {
        "paper_id": "09c660d438e8b372f3e77504c069255b85c00256",
        "sections": [
          "According to CLT, then, people traverse different psychological distances by using similar mental construal processes. Because the various distances have the same egocentric reference point, they should all be cognitively related to each other and similarly affect and be affected by level of construal. As psychological distance increases, construals would become more abstract, and as level of abstraction increases, so too would the psychological distances people envisage. Construal levels thus expand and contract one's mental horizon. The different distances should also similarly influence prediction, evaluation, and action, inasmuch as these outcomes are mediated by construal. The present article builds upon our earlier work on temporal construal theory, which focused in particular on the way that temporal distance from future events influences representation and judgment (Liberman & Trope, 1998;Trope & Liberman, 2003). Going beyond this earlier theory, we now treat temporal construal theory as a special case of a general theory of psychological distance. At the core of the proposed theory is a functional approach to construal levels, according to which mental construal processes serve to traverse psychological distances and switch between proximal and distal perspectives on objects. We describe the properties of the mental construal processes that enable them to fulfill this function and further explicate our approach by relating it to extant theories of how people respond to distant objects. Some of these ideas were presented in earlier literature reviews and book chapters (Liberman & Trope, 2008;Liberman, Trope, & Stephan, 2007;Trope, Liberman, & Wakslak, 2007). The present article presents a more advanced and comprehensive formulation of the theory and examination of related theories and research. The article consists of three main parts. In the first part, we present the basic assumptions of CLT. We explain what we mean by construal levels and why they are related to psychological distance (Section I) and examine the cognitive relationships among the four dimensions of psychological distance (Section II) and their bidirectional relationship to level of construal (Section III). In the second part, we turn to the construal-mediated consequences of psychological distance for prediction, preference, and self-regulation (Sections IV-VI). In the third part, we address open questions about psychological distance and discuss new directions for future research (Sections VII-IX).",
          "In the following three sections, we examine the basic assumptions of CLT regarding psychological distance, construal level, and the relationship between the two.",
          "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental. For example, by moving from representing an object as a \"cellular phone\" to representing it as \"a communication device,\" we omit information about size; moving from representing an activity as \"playing ball\" to representing it as \"having fun,\" we omit the ball. Concrete representations typically lend themselves to multiple abstractions. For example, a cellular phone could be construed also as a \"small object,\" and \"playing ball\" could be construed as \"exercising.\" An abstract representation is selected according to its relevance to one's goals. Thus, if one's goal is to contact a friend, then \"a communication device\" is relevant, but size is not. From the perspective of that goal, the cell phone could be replaced by (i.e., would be conceptually close to) a desktop computer with an Internet connection. If, however, one's goal is to pickpocket a valuable object, then size is a relevant attribute, and function is of less relevance. Given this goal, the cell phone could be replaced by (i.e., would be conceptually close to) a wallet. Like irrelevant details, details that are inconsistent with the chosen abstract representation are omitted from the representation or assimilated into it. For example, the detail that the ball game is slow would be omitted or modified once the activity is represented as \"exercising.\" Because abstract representations necessarily impose one of many alternative interpretations, and because irrelevant or inconsistent details are omitted or assimilated to it, these representations tend to be simpler, less ambiguous, more coherent, more schematic, and more prototypical than concrete representations (Fiske & Taylor, 1991, p. 98; E. R. Smith, 1998). It is important to note, however, that higher level construals are not simply more impoverished or vague than lower level construals. They often convey additional information about the value of the stimulus and its relations to other stimuli. For example, \"having fun\" entails many characteristics that are not an obvious part of \"playing ball outside\" (e.g., valence) and places the activity within a broader context by specifying its relations to other concepts (e.g., a party). Thus, the process of abstraction involves not only a loss of specific, idiosyncratic, and incidental information, but also ascription of new meaning deduced from stored knowledge and organized in structured representations. There are multiple levels of abstractness, as one could construct increasingly more inclusive categories of objects (e.g., poodle, dog, a mammal). Actions form hierarchies, too, as goals could be translated into more abstract, superordinate goals (Carver & Scheier, 2000;Miller, Galanter, & Pribram, 1960;Vallacher & Wegner, 1987). In such hierarchies, each action (e.g., study for an exam) has a superordinate, abstract level, which answers the question of why the action is performed (e.g., do well) and a subordinate, concrete level, which provides the details of how the action is to be performed (e.g., read a textbook). Higher levels of abstractness contain less concrete details about the specific type of action performed, the objects it involves, and its immediate context and more information about the general meaning and the valence of the action (Semin & Fiedler, 1988;Trope, 1986Trope, , 1989)). On the basis of this conceptualization, we posit two related criteria that can be used to distinguish which features of an item or an event are more high level and which are more low level (i.e., which features will be increasingly captured at a higher vs. lower level of construal). The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is. The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. The topic of the lecture, on the other hand, would be important regardless of the location's convenience. In this sense, details about location are subordinated to details about topic, and thus constitute a lower level of construal. Construal and distance-CLT contends that people use increasingly higher levels of construal to represent an object as the psychological distance from the object increases. This is because high-level construals are more likely than low-level construals to remain unchanged as one gets closer to an object or farther away from it. For example, the higher level goal to contact a friend is more stable over time than the more concrete goal to send her an e-mail, because an Internet connection might be unavailable when one is actually trying to contact the friend. From a temporally distant perspective, it is therefore more useful to construe this action in terms of the high-level goal rather than the low-level goal. The same holds true for the other distances. Thus, abstract categories tend to change less across social distance. For example, more people use communication devices than cell phones, and therefore, the former construal is more useful for relating to socially distant individuals. It is critical that, although we believe that these functional relationships underlie the association between construal and distance, we propose that the effects of distance on construal and of construal on distance are overgeneralized, making them persist even when the initial reasons that gave rise to the association are no longer present. That is, a large distance from an object would activate a high-level construal of the object even when lowlevel details are not particularly likely to change over distance, and high level of construal of an object would produce a sense of distance from the object even when such construal does not afford retrieval of particularly distal exemplars of the object. For example, students may know the room in which a guest lecture will take place well in advance and be quite certain that it will not change. Nevertheless, CLT predicts that these students will ignore the quality of the lecture room (a low-level detail) when it is temporally distant, when it takes place in a remote location, when it is planned for another person, and when it is unlikely to take place. Finally, it is worth emphasizing that although psychological distance and construal levels are related, they are not the same. Psychological distance refers to the perception of when an event occurs, where it occurs, to whom it occurs, and whether it occurs. Construal levels refer to the perception of what will occur: the processes that give rise to the representation of the event itself. Thus, psychological distance from an event should be more closely related to the spatiotemporal distance of the event from the self than to its inherent properties, whereas the construal of the event should be more closely related to its inherent properties than to its spatiotemporal distance from the self. Below, we develop more fully the assumption that the different psychological distances are interrelated (Section II) and that each distance affects and is affected by level of construal (Section III).",
          "We contend that the propensity to complete the sentence \"a long time ago, in a ____ place\" with \"far away\" rather than with \"nearby\" reflects not only a literary convention but also an automatic tendency of the mind. Indeed, people use spatial metaphors to represent time in everyday language and reasoning (Boroditsky, 2007). In social psychology, spatial distance is often used to measure social distance. For example, choosing a more distant seat from another person is taken to reflect social distancing from that person (e.g., Macrae, Bodenhausen, Milne, & Jetten, 1994;Mooney, Cohn, & Swift, 1992). More generally, if a common dimension of psychological distance underlies the various distance dimensions we have enumerated, then these distance dimensions should be mentally associated. For example, remote locations should bring to mind the distant rather than the near future, other people rather than oneself, and unlikely rather than likely events. In an investigation of this hypothesis, Bar-Anan, Liberman, Trope, and Algom (2007) used a picture-word Stroop task (Stroop, 1935) to examine the cognitive interrelations among psychological distances. Participants viewed landscape photographs containing an arrow that was pointing to either a proximal or a distal point in the landscape (see Figure 1). Each arrow contained a word denoting either psychological proximity (e.g., \"tomorrow,\" \"we,\" \"sure\") or psychological distance (e.g., \"year,\" \"others,\" \"maybe\"). Participants' task was to respond by pressing one of two keys as quickly and as accurately as possible. In one version of the task, they had to indicate whether the arrow pointed to a proximal or distal location. In another version, they had to identify the word printed in the arrow. In both versions, participants responded faster to distance-congruent stimuli (in which a spatially distant arrow contained a word that denoted temporal distance, social distance, or low likelihood, or a spatially proximal arrow contained a word that denoted temporal proximity, social proximity, or high likelihood) than to distance-incongruent stimuli (in which a spatially distal arrow contained a word denoting proximity, or a spatially proximal arrow contained a word denoting distance). These findings suggest that spatial distance, temporal distance, social distance, and hypotheticality have a common meaning and that people access this common meaning automatically, even when it is not directly related to their current goals. We think that the common meaning is psychological distance and that it is automatically assessed because of its important implications for the perceiver. For example, on a rainy day, it matters whether an umbrella one notices belongs to a friend or to a stranger (social distance); in the jungle, it is important whether a tiger is real or imaginary (hypotheticality); in making a financial investment, it is important whether a recession is anticipated in the near or distant future (temporal distance), here or somewhere else (spatial distance). Yet, unlike valence, distance is not an inherent aspect of the semantic meaning of objects. Umbrellas, tigers, and recessions are inherently good or bad, but they are not inherently proximal or distal. It is perhaps for this reason that distance has not been included among the basic dimensions of meaning: evaluation, potency, and activity (Osgood & Suci, 1955). The idea that distance dimensions are automatically associated further suggests that the distance of a stimulus on one dimension may affect its perceived distance on other dimensions. In this spirit, research by Stephan, Liberman, and Trope (2010) has investigated how social distance affects and is affected by spatial distance and temporal distance. This line of research was based on past work showing that polite language signifies and creates interpersonal distance: People address strangers more politely than they address friends, and the use of polite, formal language creates a sense of distance (Brown & Levinson, 1987). Consistent with this analysis, Stephan et al. found that the use of normative, polite language rather than colloquial, less polite language led participants to believe that the target of the communication was spatially and temporally more distant. For example, using normative rather than colloquial language to address a person (e.g., \"My brother is taking our family car, so the rest of us will stay at home\" vs. \"… will be stuck at home\") led participants to infer that the addressee was in a more remote location and that the conversation referred to an event in the more distant future. Another set of studies by Stephan et al. (2010) found evidence for the reverse direction of influence, namely, an effect of spatial and temporal distance from the target of communication on the use of polite language. In one study, participants wrote instructions for a person who was expected to read them either in the near future or the distant future (e.g., participants wrote sightseeing suggestions for a tourist that was supposed to arrive on the following day or a year later). The results showed that participants preferred to phrase instructions more politely when they were addressed to the distant future than to near future tourist. Similar findings were found with addressing a spatially proximal versus distal stranger. Williams and Bargh (2008, Study 4) showed, in a similar vein, that participants who were primed with spatial distance (relative to proximity) by marking close (vs. distant) points on a Cartesian plan subsequently reported greater social distances between themselves and members of their family and their hometown. Finally, Wakslak and Trope (2008) showed that hypotheticality also affects the perception of other distances, such that people expect unlikely events (which, in terms of CLT are distant on the dimension of hypotheticality), compared with likely events, to occur in situations that are relatively more distant in time, space, and social distance. For example, a rare cat blood type was expected to be found in cats in spatially remote rather than nearby places, whereas a common cat blood type was expected in a near rather than a remote location. Presumably, people map probability onto other distance dimensions and therefore expect improbable events to happen to distant others in distant places and times. Discussion-Recent research shows that the different psychological distance dimensions are associated and suggests that psychological distance is an aspect of meaning, common to spatial distance, temporal distance, social distance, and hypotheticality. This research further shows that cues of distance on one dimension affect the perceived distance of objects on other dimensions. For example, the spatial distance from an event may depend not only on its location relative to that of the perceiver but also on whether it is expected in the near or distant future, whether it occurred recently or a long time ago, whether it is expected to be experienced by oneself or another person, and whether it is probable or improbable. In this respect, the different psychological distances may be to some extent interchangeable (see Pronin, Olivola, & Kennedy, 2008). Our research has only started to explore the relationships among the various distance dimensions. Below we consider further questions about more complex relationships among the psychological distances than those we have identified thus far. The psychophysics of distance: How do objective distances in time, space, social target, and hypotheticality map onto generalized psychological distance? It seems reasonable to expect a relatively high sensitivity to changes in objective distance from the self here and now at the proximal end of the continuum. As distance increases, changes in objective distance might produce smaller corresponding changes in psychological distance. As vividly illustrated by Steinberg's painting View of the World from the 9th Avenue, the distance between 9th Avenue and 10th Avenue in Manhattan seems to Steinberg greater than the distance between New York City and Chicago. In other words, consistent with the Weber-Fechner law, psychological distance from an object might follow a concave, logarithmic function rather than a linear function. Recent research by Zauberman, Kim, Malkoc, and Bettman (2009) provides evidence showing that such a function fits subjective estimates of temporal distance from future events. Likewise, research on estimates of spatial distance uncovered increased sensitivity around an egocentric reference point (e.g., Holyoak & Mah, 1982). Investigating the functions that relate temporal distance, prospective and retrospective, as well as other distances, to psychological distance remains an important direction for future research. Another important but unexplored question is how different distances combine to affect psychological distance. One interesting possibility is that the Weber-Fechner law may apply across different psychological distance dimensions. That is, changes in distance of an object from the self on one dimension would have a greater impact on psychological distance when that object is proximal on another dimension than when it is distant on that other dimension. For example, the same changes in spatial distance of an object from the self might have a stronger impact on the psychological distance of that object when the changes are expected in the near future rather than in the distant future. To use Steinberg's drawing, this artist might think of a meeting as coming up sooner if he plans to have the meeting in Manhattan than in Chicago.",
          "According to CLT, different objects are mentally associated to the extent that they are psychologically equidistant from the self. In principle, the associations among proximal objects could be as strong as those among distal objects. However, because psychological distance is egocentric, anchored at the self here and now, objects at the proximal end may have more in common than those that are removed from that end. There is only one me and many others, and therefore, the things that are me and mine are more similar to each other than the things that are not me and mine. Only one place in space is \"here,\" but there are many places that are far away, and therefore, things that are here are more related to each other than things that are far away. Similarly, there is likely less variation among real events than possible events. The greater commonality among proximal objects than distal objects might give rise to stronger interrelationships among the former than the latter.",
          "Although we suggest that different types of distance are related, we do not wish to suggest that they are the same. It is possible that some distance dimensions are more basic or influential than others. For example, Boroditsky has recently proposed that spatial distance is primary and that temporal distance is understood as an analogy to it (Boroditsky, 2000(Boroditsky, , 2001;;Boroditsky & Ramscar, 2002). In support of this proposal, she showed that thinking about space before answering questions about time influenced the participants' responses, but thinking about time before answering questions about space did not affect participants' responses (Boroditsky, 2000, Experiments 2 and 3). In another series of studies, she presented participants with lines on a computer screen and showed that judgments of exposure duration were affected by the lines' length whereas judged length was not affected by duration of exposure (Casasanto & Boroditsky, 2008). Spatial distance may be more basic than temporal distance or other distances in that it is learned earlier, more clearly detected, less ambiguous, or easier to communicate about. It is possible, however, that the distinction between self and not-self, which underlies social distance and perspective taking (e.g., Pronin, 2008), might be a core psychological distance that exerts a particularly strong influence on the activation and perception of other distances. Perhaps hypotheticality, the distinction between real and imagined objects and between probable and improbable events, is least prominent and acquired at an older age, compared to other distance dimensions. The various distance dimensions may differ in other respects. Time is unidimensional and uncontrollable. We incessantly travel from the past to the future and have no control over time. Spatial distance has three dimensions, is relatively stable, and can be controlled by moving closer or farther away from things at wish. Social distance is only partly controllable (e.g., we can try to get closer to a person but success is not entirely dependent on our actions). Hypotheticality is also only partly controllable, because we cannot always change the probabilities of events. Another important difference among the distances is their relation to valence. Whereas social distance decreases positivity (e.g., in groups are perceived as more positive than out groups), temporal distance typically increases positivity (people are more positive about the more distant future). Exploring the psychological consequences of these differences in controllability, dimensionality, and valence is a fruitful direction for future research.",
          "The basic premise of CLT is that distance is linked to level of mental construal, such that more distant objects will be construed at a higher level, and high-level construal will bring to mind more distant objects. It may seem intuitive that from a distance we see the forest, and as we get closer, we see the trees. It may also seem intuitive that, to see the forest rather than individual trees, we need to step back. These effects should apply, however, not only to spatial distance but also to other distances, and not only to visual input, where it might seem a natural constraint of our perceptual system, but also to conceptual abstraction. We do not literally see either tomorrow or next year. Yet we may think about tomorrow in terms of trees and about next year in terms of the forest. We do not literally take a step back to forget the daily hassles and consider our life in general. Yet, when thinking of the general aspects of our life rather than about daily details, we may find ourselves looking farther into space. Evidence for these associations between distance and construal has been found at the level of both implicit associations and explicit judgments and decisions. Bar-Anan, Liberman, and Trope (2006), for example, examined the associations between level of construal and psychological distance using an Implicit Association Test. Similar to other assessments of implicit associations using this test (see Greenwald, McGhee, & Schwartz, 1998), participants in these studies were presented with stimuli from four categories: stimuli pertaining to high-level construal (e.g., category names such as \"drinks\"), stimuli pertaining to low-level construal (e.g., exemplar names such as \"coke\"), stimuli pertaining to low psychological distance (e.g., the word \"ours\" or the word \"friend\" for the social distance), and stimuli pertaining to high psychological distance (e.g., the word \"theirs\" or the word \"stranger\"). In CLT-congruent trials, high-level stimuli were paired with distant stimuli and low-level stimuli were paired with proximal stimuli, whereas on CLT-incongruent trials, high-level stimuli were paired with proximal stimuli and low-level stimuli were paired with distal stimuli. With all four dimensions of psychological distance-temporal distance, spatial distance, social distance, and hypotheticality-the participants were faster with congruent than with incongruent pairings, indicating that participants implicitly associate psychological distance with high-level construal and psychological proximity with low-level construal. This set of studies suggests that the association between psychological distance and construal level can be activated automatically without conscious deliberation. Beyond mere association, the reciprocal effects of distance on construal and of construal on distance have important implications for perception, categorization, action identification, and person perception. In the following, we discuss those implications. Visual Perception-The distinction between the (high-level) gestalt and (low-level) constituent details, such as the proverbial distinction between the trees and the forest, readily applies to visual perception. In a widely used task (Navon, 1977), participants are presented with global letters that are made of local letters (e.g., a large L made of 20 small Hs), and are asked to indicate whether a target letter appears on the screen (e.g., \"Is there an H on the screen?\"). The target letter can match a global letter or a local letter. Would psychological distance facilitate attending to global letters (e.g., the \"forest\") and impair attending to local letters (e.g., the \"trees\")? Would attending to global letters, as opposed to local letters, enhance perceived distance? Liberman and Förster's (2009 Liberman and Förster's (in press) studies suggest affirmative answers to both questions. In one of the studies (Liberman & Förster, in press), they primed participants with temporal distance or temporal proximity by making them write essays about their lives tomorrow or their lives on a day a year later and imagine taking an attention test on that day. Navon's (1977) task was then presented. Relative to a control group, in which participants did not write an essay, temporal distance facilitated processing of global letters and impaired processing of local letters, whereas temporal proximity produced the opposite effect. Similar effects were found for priming of spatial distance and social distance. To examine the reverse direction of influence, namely, that of global perception on estimated psychological distance, Liberman and Förster (2009) procedurally primed participants with either global or local perceptual processing, using a variation of Navon's (1977) task. In the global priming condition, the target letters were always global, and in the local priming condition, the target letters were always local, whereas in the control condition, the targets were global in half of the trials and local in the other half. Relative to the control condition, global processing led to greater estimates of temporal distance, spatial distance, social distance, and hypotheticality. Local processing had the opposite effect. For example, participants who were primed with global processing estimated temporal distance to a dental visit as longer and spatial distance between themselves and a designated point in the room as larger than participants primed with local processing. In a related vein, Wakslak and Trope (2009) found that priming global processing (vs. local processing) through the Navon task led participants to assign lower probability to a variety of everyday life occurrences. It is important to note that, given that psychological distance is egocentric, as CLT contends, if the distance between two objects is not mentally represented as social distance from oneself, temporal distance from now, or spatial distance from here, it should not necessarily depend on level of construal. Consistent with this prediction, in Liberman and Förster's (2009) studies, primed level of construal affected estimated spatial distance between the participant and a sticker in the room, but it did not affect the estimated spatial distance between the experimenter and a marked desk in the room. Likewise, level of construal did not affect estimates of temporal distance that were not anchored on now (e.g., \"How much time after receiving an invitation would you go to the dentist?\"). High-level construal of visual information often entails abstraction of coherent images from fragmented visual input. An example is the Gestalt Completion Task (see Figure 2; Street, 1931; see also Ekstrom, French, Harman, & Dermen, 1976), where performance depends on detecting the global pattern and where attending to details interferes with performance. In a series of studies, participants completed what they believed to be sample items of the Gestalt Completion Task, supposedly as a practice version before they perform the actual task. Participants' performance improved when they anticipated working on the actual task in the more distant future (Förster, Friedman, & Liberman, 2004), when they thought the actual task was less likely to take place (Wakslak, Trope, Liberman, & Alony, 2006), and when social distance was enhanced by priming of high social status (P. K. Smith & Trope, 2006). A psychologically distant perspective thus seems to enable people to better see the gestalt. Whereas distance improves the ability to perceive the gestalt in a visual array, it should have the opposite effect when the task requires attention to details. Distance should therefore have a detrimental effect on the ability to identify a missing low-level, local element within a coherent whole (e.g., a missing hand on a watch, a missing handle on a drawer chest). Wakslak et al. (2006) used the picture completion subtest of the Wechsler Intelligence Scale for Children (Wechsler, 1991) to test this prediction. As expected, participants did worse on sample items of this task when they believed they were less likely to later complete it. Pictures and Words-Pictures are concrete representations that bear physical resemblance to the referent objects, whereas words are abstract representations that carry the essence of that object (Amit, Algom, & Trope, 2009;Amit, Algom, Trope, & Liberman, 2008). Words therefore comprise a higher level of construal than do pictures. Supporting this assumption, Amit, Algom, and Trope (2009) found that participants classified items into more groups when the items were presented in pictures than when they were presented in words. In another study, they presented spatially, temporally, or socially near or distant items in either a pictorial or verbal format (see Figure 3). Participants' task was speeded identification of the object. For example, in an experiment on spatial distance, words or pictures were presented against a background that created an illusion of depth. As expected, participants responded faster to pictures of objects when they were made to seem spatially near than spatially distant, but they responded faster to words denoting those objects when they were made to seem spatially distant than spatially near. To manipulate temporal distance, they used words and pictures of modern objects and ancient objects (e.g., a cart and a car), and to manipulate social distance they used words and pictures of domestic and foreign objects (e.g., a coin of domestic and foreign currency). Responses were faster when pictures represented psychologically proximal objects and words represented psychologically distal objects than vice versa. It seems, then, that processing is most efficient when there is a congruency between the portrayed distance and the presentation medium. Moreover, using similar stimuli in free recall experiments, Amit, Trope, and Algom (2009) demonstrated better memory for proximal than for distal stimuli when the targets were represented in pictures but that it was better for distal than for proximal stimuli when the targets were represented in words. These findings were obtained with temporal, spatial, and social distances. Categorization-If the distant future is represented more abstractly, then individuals should use broader categories to classify objects for distant situations than for proximal situations. In support of this prediction, research has shown that people group objects into fewer, broader categories when they imagine the objects in a distant future scenario than a near future scenario (Liberman, Sagristano, & Trope, 2002, Study 1) or in an unlikely scenario than a likely scenario (Wakslak et al., 2006). Consistent with research on categorization, research on segmentation of ongoing events has found that people chunk behavior sequences into broader segments when the behavior is distal rather than proximal. For example, Henderson, Fujita, Trope, and Liberman (2006) asked New York University participants to partition an ongoing behavioral sequence into as many sections as they deemed appropriate. Participants viewed an animated film developed by Heider and Simmel (1944) that shows two triangles and a circle moving against and around each other and were told that the film depicted the action of three teenagers around a cabin at a well-known summer camp. In the spatially near condition, the camp was said to be located on the East Coast; in the spatially distant condition, the camp was said to be located on the West Coast. As expected, participants created fewer, broader sections out of the video when they believed the campers it depicted were in a spatially distant, as opposed to a spatially near, location. Similar effects were found for events that were described as less likely versus more likely (Wakslak et al., 2006). Do narrow, specific categories promote a sense of psychological proximity? A recent series of studies by Wakslak and Trope (2009) manipulated level of categorization in several distinct ways and found the predicted effect on event likelihoods. In one study, construal level was primed by asking participants to generate either superordinate categories or subordinate exemplars of 40 objects (e.g., table, sport, book). Next, participants completed a supposedly unrelated questionnaire where they made a series of probability judgments. As expected, participants who had been primed to have a high-level construal mindset indicated that the events were less likely to occur compared to those who had been primed to have a low-level construal mindset. In another study, participants compared either alignable or unalignable objects. In the alignable condition, participants compared two digital cameras, with information provided about the same seven features for each camera (e.g., battery life, digital zoom). In the nonalignable condition, participants were provided with information about seven different features for each camera (e.g., battery life for the digital camera; focus for the traditional camera). It was assumed that individuals who are asked to compare alternatives with nonalignable features would seek to make the attributes comparable by representing the alternatives at increasingly higher levels of abstraction (see M. D. Johnson, 1984;Malkoc, Zauberman, & Bettman, 2008). As expected, participants cued to think more abstractly (by having compared products with nonalignable attributes) judged an event involving the products as being less likely to occur than those cued to think more concretely (by having compared products with alignable attributes). Action Identification-Actions, like objects, may be construed in high-level terms, which link them to a superordinate purpose (why one performs them), or in low-level terms, which link them to subordinate means (how one performs them). Here, too, greater psychological distance promotes higher levels of construal (Liberman & Trope, 1998). In one of the studies, participants tended to describe more distant future activities (e.g., studying) in highlevel terms (e.g., \"doing well in school\") rather than in low-level terms (e.g., \"reading a textbook\"). Similar effects emerged when actions were to take place in a spatially distant location (Fujita, Henderson, Eng, Trope, & Liberman, 2006), when the actions were framed as unlikely to actually take place (Wakslak et al., 2006), and when the actor was dissimilar to the perceiver (Liviatan, Trope, & Liberman, 2008). If high-level construals serve to represent psychologically distant events, then activating high-level construals should lead people to think of events in psychologically more distant situations. Indeed, research shows that thinking about an activity in high-level, \"why\" terms rather than low-level, \"how\" terms leads people to think of the activity as taking place in more distant points in time (Liberman, Trope, Macrae, & Sherman, 2007;McCrae, Liberman, Trope, & Sherman, 2008) and of the actor as more socially distant (Stephan et al., 2010). Action-identification theory (Vallacher & Wegner, 1989) specifies how to determine the superordinate and subordinate levels of categorizing a given action. Semin and Fiedler's (1988) Linguistic Categorization Model assesses the abstractness of the action itself. At the most concrete level are descriptive action verbs (\"lift,\" \"take\"), which are typically directly observable. Interpretative action verbs (\"pretend,\" \"help\") are more abstract, as they involve interpretation and require some knowledge of a context larger than the immediately perceived. State verbs (\"prefer,\" \"understand\") are still more abstract, and adjectives (\"helpful,\" \"aggressive\") are the most abstract category. The Linguistic Categorization Model is therefore a useful tool for examining the relationships between psychological distance and abstractness of action construal. Indeed, various kinds of distance have been found to affect abstractness of language. For example, people were found to use more abstract language when describing another person's actions than their own actions (Semin & Fiedler, 1989; see also Fiedler, Semin, Finkenauer, & Berkel, 1995), when describing spatially distant interactions than spatially near interactions (Fujita, Henderson, et al., 2006), and when instructed to address another person politely than in colloquial language (Stephan et al., 2010). Would the reverse direction of influence hold? That is, would linguistic abstractness affect perceived psychological distance? Semin and Smith (1999, Studies 2 and 3) studied the effect of linguistic abstractness on the temporal distance of recalled events. They provided participants with retrieval cues of varying abstractness and examined the temporal distance of the events they recalled. For example, participants were asked to recall either an occasion on which they helped somebody (i.e., concrete retrieval cue) or an occasion on which they displayed a trait of helpfulness (i.e., abstract retrieval cue). As predicted, an abstract retrieval cue prompted older memories than memories that were prompted by a concrete retrieval cue. Person Perception-A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995;E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995;Semin & Fiedler, 1988;Semin & Smith, 1999). Distance should therefore enhance the correspondence bias. Furthermore, representing a person abstractly in terms of his or her dispositions would create a sense of distance. Naturally, social psychologists researched extensively the relations between social distance and dispositional inference. We first briefly review this literature and then describe comparable effects of other psychological distances.",
          "Considerable evidence for differences in construal between self and other comes from research on the actor-observer effect in attribution (E. E. Jones & Nisbett, 1972; for a review, see Gilbert, 1998). This research shows that a person's view of his or her behavior emphasizes the role of concrete situational factors that operate at the moment of action (e.g., \"I stepped on your toe because the bus was crowded\"), whereas his or her view of other people emphasizes the causal role of stable, general dispositional properties of the actor (\"he stepped on my toe because he is clumsy\"). Self-other differences might be explained as reflecting differences in knowledge (people know more about themselves and the variability of their behavior over situations than about others) and differences in the salience of behaviors versus situations (the latter is more salient from one's own perspective, the former from the observer's perspective). Would similar differences in construal emerge when knowledge is identical for near and distal social targets? Research relating abstractness of memories to the perspective in which they are recalled seems to offer an affirmative answer. It has been shown, for example, that personal memories of behaviors that were recalled from a third-person perspective (e.g., \"try to remember your first day at school, as if you are now watching the kid you were\") rather than from a first-person perspective (\"try to remember your first day at school, as if you are a kid again\") tended to use dispositional (as opposed to situational) terms (Frank & Gilovich, 1989;Nigro & Neisser, 1983). In a similar vein, Libby and Eibach (2002, Study 4) found that imagining performing an activity (e.g., rock climbing, playing drums) from a third-person perspective produced less vivid and rich reports of the activity than imagining the same activity from a first-person perspective. In terms of CLT, this means that a third-person perspective, which imposes more distance than a first-person perspective, induces a higher level of construal. Indeed, Pronin and Ross (2006) showed that taking a third person perspective rather a first-person perspective on one's own behavior promoted attribution of the behavior to personality traits rather than to specific situational factors. Temporal distance and spatial distance: Correspondent inference, the tendency to use high-level dispositional attributions, seems to increase not only with social distance but also with temporal distance. Over time, observers' attributions of an actor's behaviors become more dispositional and less situational (Frank & Gilovich, 1989; see also Funder & Van Ness, 1983;Moore, Sherrod, Liu, & Underwood, 1979;Nigro & Neisser, 1983;Peterson, 1980; but see Burger, 1986). These findings are consistent with memory consolidation, namely, the tendency for concrete details to fade more rapidly than general abstractions, thus making memories of the distant past more abstract than memories of the recent past (Bartlett, 1932;Wyer & Srull, 1986). However, assuming that memory is an active, reconstructive process, CLT proposes that increasing past temporal distance is associated not only with a loss of specific details but also with an increased use of high-level, schematic construals (Ross, 1989). Consistent with this proposal, McDonald and Hirt (1997) showed that, over time, student participants not only forgot a fellow student's grades but also were more likely to infer the grades from that student's general ability, resulting in a more coherent (and dispositional) impression of the target person over time. More recently, research has examined dispositional inferences from near future and distant future behavior (Nussbaum, Trope, & Liberman, 2003, Study 1) and from spatially close or spatially distant behavior (Henderson, Fujita, et al., 2006, Study 2). These studies found that student participants drew stronger corresponding attitude inferences from situationally constrained behavior when the behavior was expected in the distant future (vs. near future) or when it was believed to have occurred in a spatially remote location (vs. a near location). These findings demonstrate that the correspondence bias, the tendency to underweight lowlevel, situational constraints on observed behavior, increases with psychological distance from the behavior. Past research has shown that perceivers often form spontaneous trait inferences (STIs) when they observe trait-implying behaviors of other people. For example, upon reading the sentence, \"The secretary solved the mystery halfway through the book,\" people spontaneously inferred the trait \"clever\" (Winter & Uleman, 1984). Can temporal or spatial distance affect the extent to which abstract traits are spontaneously inferred at an early stage of information processing? Rim, Uleman, and Trope (2008) addressed this question in a series of studies that used rate of false recognition of implied (but not present) traits as a measure of STI. They found that New York University participants led to believe that actors were in a distant location (Florence, Italy), compared with a proximal location (Manhattan, New York), formed more STIs, even though behavioral information about targets was held constant across the two conditions. The same effect was found using temporal distance: Participants who were led to believe that actors were from the relatively distant past (year 1997), compared with the more recent past (year 2007), formed more STIs. Self-inferences: Distance may affect not only the inferences we draw about others but also the inferences we draw about ourselves. Pronin and Ross's (2006) research shows that people more likely view their future and past selves than their present selves in terms of general personality traits. In a related study, Wakslak, Nussbaum, Liberman, and Trope (2008) asked participants to imagine themselves in different situations either in the near future or in the distant future and to indicate the extent to which their behavior in those situations would reflect each of the Big Five personality traits. It was found that in the distant future, compared with the near future, participants expected to exhibit their traits more consistently across situations. Another study by Wakslak et al. showed that participants were faster in judging whether general personality traits describe the way they would be in the distant future than the near future. These findings suggest that the distant future self is represented more in terms of general, decontextualized traits than the near future self. People seem to have a clearer view of their personality characteristics when taking a temporally distant perspective rather than a temporally proximal perspective on the self. Indeed, using Linville's (1985Linville's ( , 1987) ) and Donahue, Robins, Roberts, and John's (1993) measures of self-complexity, Wakslak et al. found that self-descriptions were more structured and less complex when they referred to a distant future self than to a near future self. It would be interesting to examine in future research the effects of other distances on the self-concept. For example, would thinking of oneself in an unlikely situation or in a remote spatial location make one see oneself in a more coherent, schematic way? Is it possible that this effect also occurs while traveling (or even while planning traveling)? Does imagining the perspective of another person, especially an unfamiliar person, increase perceived selfintegrity? In general, the construal of the self, which is often more concrete than the construal of others, may be highly abstract and high level when the self is viewed from a distanced perspective, in remote times, places, imaginary situations, and from a third-person perspective. Discussion-Extensive research has verified that as psychological distance increases, construals become more abstract, and as level of construal increases, so too do perceptions of psychological distance. Much of past research has studied individual distance dimensions and types of construal within disparate theories. Without denying the uniqueness of the different distances and types of construal, CLT provides a unifying framework that integrates these disparate lines of research. It suggests that high-level construals and lowlevel construals serve different cognitive functions. High-level construals have evolved to represent distal objects because, with distance, one needs to conserve the essential, invariant properties of the referent object. In contrast, low-level construals preserve the object in minute detail for immediate use. High-level construals serve to transcend the here and now, whereas low-level construals instantiate the present. In the following, we expand our examination of construal levels by relating them to prospection and mental simulation, inferences of distance, and to heuristic low-effort processing.",
          "The present view is consistent with recent functional approaches to memory and cognition (Barsalou, 1999;Buckner & Carroll, 2007;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). These approaches suggest that a major function of episodic memory is prospection, namely, anticipating the future rather than merely reproducing the past. The theories of constructive simulation (Schacter & Addis, 2007) and embodied cognition (Barsalou, 1999;Niedenthal, Barsalou, Winkielman, Krauth-Gruber, & Ric, 2005;Winkielman, Niedenthal, & Oberman, 2008) specifically suggest that episodic memory enables prospection through a constructive process of mentally simulating future episodes. Such simulations are analogical and multimodal and serve to guide choice and action with respect to the future (Barsalou, 2008). CLT adds to this view in several respects. First, it is possible that simulations vary in level of construal, ranging from multimodal simulations that are rich in contextual detail and resemble the kind of analogical representation identified by embodied cognition researchers to general simulations that retain common elements and omit incidental detail. CLT predicts that the latter, higher level simulations are more likely to be used with increasing distance of the past episodes from which simulations are constructed and of the future target situation to which the simulation is applied. For example, a simulation of a future meeting with a friend is likely to be more abstract (i.e., contain less detail on the tone of her voice and the look of the room in which the meeting would take place) to the extent that it is constructed from meetings held with that friend in the relatively distant past or distant locations and to the extent that the meeting with the friend is expected in the relatively distant future or location. Second, it is possible that as distance increases, prospection is increasingly more likely to be based on amodal symbolic representations. For example, a representation of a more distant future meeting with a friend that works in a high-tech company may refer more to semantic knowledge about layoffs in the high-tech industry and include less detail related to perceptual properties, such as the tone of her voice. The research showing that people switch from pictorial to linguistic representations of objects as distance from the object increases is consistent with this possibility (Amit, Algom, & Trope, 2009). Third, symbolic representations might also differ in abstraction, ranging from broad abstractions (she is a woman) to more narrowly applied knowledge (she is a single mother to four and a high-tech manager who is afraid to lose her job). Fourth, as the research reviewed here suggests, these changes in the nature of mental representation of objects might be induced not only by prospective temporal distance from the object but also by spatial, social, and hypotheticality distances. In sum, according to CLT both analogical simulations and symbolic representations might vary in level of construal. Distance may determine whether an analogical or symbolic representation is constructed and the level of abstractness at which it will be constructed. Inferring distance from construal level: We argued that because high-level construals are broad, they bring to mind more distant instantiations of objects, and because low-level construals are narrow, they bring to mind more proximal instantiations of objects. It is also possible for construal level to affect the psychological distance of objects through metacognitive inferences (N. Schwartz & Clore, 1996). People may interpret their low-level construal of an object as indicating that the object is close and their high-level construal of an object as indicating that the object is distant. This metacognitive inference of distance from construal level might involve a more complex attributional calculus when one or more other distances are known. Specifically, the construal-based inference that an object is distant on any given dimension will be discounted when the object is known to be distant on another dimension. Correspondingly, the construal-based inference that an object is distant on any given dimension will be augmented when the object is known to be proximal on another dimension. For example, one would attribute a detailed construal of a meeting with a friend to a relatively close relationship with that friend when the meeting is known to take place in the distant future rather than the near future. Thus, direct implicit associations among different distance dimensions generally result in positive relationships among those dimensions. However, when inferring distance from construal, adjusting the inference of distance on one dimension for distance on other dimensions may result in a negative relationship among those distances.",
          "Could the relationship between distance and construal be accounted for by assuming that people are less motivated to make an accurate judgment regarding relatively distant objects? Does the shift toward higher level construals with increasing distance reflect a shift toward low-effort heuristic processing (see Petty & Cacioppo, 1984)? We think that extracting the general meaning and invariant characteristics of objects is not necessarily more or less effortful than fleshing out the minute details of the objects. For example, generating a superordinate category for the concept soda (e.g., drinks) does not necessarily differ in these respects from generating exemplars of the concept (e.g., Diet Coke). Indeed, our research has not found any systematic relationship between the effects of distance on construal and measures of involvement or accuracy motivation (e.g., P. K. Smith & Trope, 2006;Wakslak et al., 2006). Three additional sets of findings argue against the idea that distance-driven high-level construals necessarily reflect uneffortful, heuristic processing. First, this idea cannot account for performance findings showing that judgments about distal objects (compared with judgments about proximal objects) are more accurate when a correct response requires high-level construals of the objects (e.g., identifying a pattern) and are less accurate only when the correct response requires low-level construals (e.g., detecting a missing detail; e.g., Liberman & Förster, in press;Wakslak et al., 2006). Second, low involvement and uneffortful processing of distant (vs. proximal) objects may account for the underutilization of low-level information about distant objects, but it cannot account for the underutilization of high-level information about proximal objects (e.g., Liberman & Trope, 1998;Nussbaum, Liberman, & Trope, 2006). Third, Fujita, Eyal, Chaiken, Trope, and Liberman (2008) directly addressed the issue of heuristic processing by examining sensitivity to argument strength, a commonly used test of heuristic processing (Chaiken, Giner-Sorolla, & Chen, 1996;Petty & Cacioppo, 1984). Fujita et al. presented participants with either strong or weak arguments from a wildlife conservation organization that either referred to a superordinate category (e.g., orcas in Puget Sound) of an attitude object or subordinate, specific exemplar (e.g., Simon, an orca in Puget Sound) of the attitude object. The results showed that when the arguments referred to a high-level category, attitudes were more sensitive to argument strength when the attitude object was temporally distant than near. However, when the arguments referred to a subordinate exemplar, the reverse was true; That is, attitudes were more sensitive to argument strength in the near than distal condition. Involvement might account for the findings obtained with arguments referring to low-level construal of the attitude object but not those obtained with arguments referring to high-level construal of the attitude object. Finally, using the number of thoughts generated about an attitude object as a measure of systematic processing, Ledgerwood, Trope, and Chaiken (2008) found that the number of thoughts was unaffected by temporal distance from the attitude object. We return to the issue of depth of processing in the sections on prediction and evaluation. Construal-Mediated Effects of Psychological Distance-What are the implications of the link between distance and construal for prediction, evaluation, and self regulation? In the following three sections, we address this question.",
          "As argued above, the very function of high-level construals is to enable people to mentally transcend the here and now by forming a representation consisting of the invariant features of the available information and projecting those representations onto distal situations. Predictions of future experiences would therefore be more schematic than the actual experiences, giving rise to a variety of prediction biases that stem from underweighting contextual and incidental features (Gilbert & Wilson, 2007;Kahneman, Krueger, Schkade, Schwarz, & Stone, 2006;Wilson & Gilbert, 2003). For example, people tend to predict that they will react more extremely to positive events (e.g., getting tenure) and negative events (e.g., a rejected paper) than they actually do. This misprediction stems from underweighting the effect of diluting low-level contextual circumstances (Wilson, Wheatley, Meyers, Gilbert, & Axsom, 2000). According to CLT, these biases should be accentuated by distance, because predicting more distal events should direct attention to even more invariant, schematic features of those events and away from low-level situational details. In the following, we examine the implications of this idea for lay prediction of research findings, predictions about one's own performance, and the effect of global trends and local deviations on extrapolation.1 Theories Versus Noise in Scientific Prediction-In scientific prediction, high-level construals reflect the theory, which fosters confidence, whereas lower level construals include noise, which might undermine confidence. A study by Nussbaum et al. (2006, Study 1) examined the confidence of advanced psychology students in replicating classic findings in psychology in either the near future or the distant future. For example, participants imagined entering a class at the university, either the next day or a year later (depending on the experimental condition), handing the students a list of words to memorize, and then testing how well they remember it after moving some of the students to a different room. Participants estimated how likely it is that those tested in the same room would outperform, on average, those that were moved to a different room, thus replicating the encoding specificity effect. Participants were more confident that they would replicate this effect when they imagined conducting the experiment in the distant future than in the near future, especially when reminded of the theory underlying prediction. The same pattern of results was obtained with other classic findings in social, cognitive, and developmental psychology. Knowledge Versus Task Format in Predicting Performance- Nussbaum et al. (2006) have also examined confidence in predictions about one's own performance. One of the studies assessed participants' confidence in predicting their performance on a general knowledge quiz expected to take place either on the same day or 2 months later (Nussbaum et al., 2006, Studies 3 and 4). The questions were the same, but in either a relatively easy or hard question format, which we assumed is a low-level aspect of the quiz, compared with the content the quiz focuses on. Specifically, the quiz consisted of either multiple-choice questions (relatively easy format) or open-ended questions (relatively hard format). In another study, the quiz consisted of questions with either two response alternatives (relatively easy) or four response alternatives (relatively hard). The results showed that the more difficult question format appropriately reduced confidence in near future performance but failed to reduce confidence in distant future performance. We think that this was the case because question format was a low-level aspect of the task. The results also showed that participants' beliefs about their knowledge in each domain predicted how confident they were in predicting their performance in that domain in the distant future better than in the near future. We think that this was the case because such beliefs concern the essence of the quiz as a test of knowledge and thus constitute a high-level construal of the quiz.",
          "Local Deviations-In a study that investigated the effect of spatial distance on the tendency to base predictions on global rather than local information (Henderson et al., 2006), New York University participants viewed a series of graphs depicting information from the years 1999-2004 (e.g., average number of photocopies per student). The information was said to pertain to the New York University campus in Manhattan (spatially near condition) or to the New York University campus in Florence, Italy (spatially distant condition). Each graph showed either an upward or downward trend, with the final year (2004) always deviating from that global trend. Participants estimated the likelihood that the year 2005 would be consistent with the general trend or with the more recent local deviation. In terms of CLT, global trends convey a highlevel construal, whereas deviations, being local exceptions, should receive more weight in low-level construals. As expected, spatial distance enhanced the tendency to predict on the basis of the global trend rather than the local deviation. Discussion-Spatial and temporal distances increase the impact of high-level information (e.g., theories, self-beliefs, general trends) and decrease the impact of low-level information (e.g., irregular outcomes, specific situational and task characteristics) on prediction. Thus, two complementary processes may contribute to the unwarrantedly higher confidence levels associated with predictions about psychologically distant events: underweighting of the uncertainty associated with low-level information and overweighting of the certainty associated with high-level information. Although we may know less about distant than near situations, our greater reliance on high-level construals in predicting the more distant situations may lead us to make more confident predictions about distant than near situations. Past research has shown that individuals tend to be more optimistic about distant future than near future outcomes (Gilovich, Kerr, & Medvec, 1993;T. R. Mitchell, Thompson, Peterson, & Cronc, 1997;Nisan, 1972). According to CLT, greater optimism is a likely but not necessary result of temporal distance. That is, greater temporal distance promotes optimism only when high-level construals imply a relatively high likelihood of desired outcomes. When low-level construals imply a high likelihood of desired outcomes, optimism may not increase and may actually decrease with greater temporal distance. Indeed, Nussbaum et al.'s (2006) studies on predicting one's performance showed that temporal distance led to more optimistic predictions when the question format was difficult but not when it was easy. Thus, underweighting a low-level aspect of outcomes (question format) increased optimism or decreased optimism depending on whether that aspect implied a high likelihood or low likelihood of success. Distance from an outcome might also reduce personal involvement, thus giving rise to shallow processing and less cautious predictions (see e.g., Petty & Cacioppo, 1984). Again, the critical difference between this view and CLT is that the latter assumes that distance reduces the use of low-level information, not high-level information. Accordingly, confidence increases with distance from an outcome only when the confidence stems from high-level construal of the outcome. The Henderson et al. (2006) and Nussbaum et al. (2006) studies described previously support this prediction.",
          "We make choices and set preferences with respect to our construals of objects rather than the objects themselves. When we choose a car, we do not decide on the car itself but rather on our construal of the car. When we decide on a diet, we do so because the construal of its outcomes seems attractive to us. We argue that construals depend not only on the actual attributes of the objects but also on the object's psychological distance. We therefore predict that all distances should similarly affect evaluation and preferences inasmuch as they all promote higher level construals. A common assumption in psychology and behavioral economics is that the value of an outcome is discounted as temporal distance from the outcome increases (see, e.g., Ainslie, 1975;Loewenstein & Prelec, 1992;Rachlin, Brown, & Cross, 2000). CLT proposes, however, that temporal distance shifts the overall attractiveness of an outcome closer to its high-level construal value than to its low-level construal value. When the low-level value of an outcome is more positive than its high-level value, the outcome should be more attractive in the near future, thus showing time discounting. However, when the high-level value of an outcome is more positive, the outcome should be more positive in the distant future, thus showing time augmentation (see Liberman &Trope, 2003, andTrope &Liberman, 2003, for more extensive discussions). In the following, we apply this principle to central versus peripheral features, feasibility versus desirability concerns, arguments in favor versus against an action, alignable versus unalignable attributes as sources of value, and attributes versus alternatives in choice matrices. Central Versus Peripheral Features-According to CLT, central, goal-related features of outcomes constitute high-level construal of outcomes, whereas peripheral, relatively goalirrelevant features of outcomes constitute low-level construal of outcomes. Distancing an outcome should therefore increase the weight of central features relative to peripheral features. Trope and Liberman (2000) found support for this prediction in studies on evaluations of objects and events containing both a primary and secondary aspect. In one study, for instance, participants imagined buying a radio set either the next day or 1 year later, to listen to morning programs. In one version, participants read that the sound quality of the radio set was good, but that the clock that was incidentally included was relatively useless. In another version, participants read that the sound quality of the radio set was poor, but that the clock aspect was quite useful. As expected, thinking about the radio set in the more distant future increased satisfaction when the sound quality was good and the clock poor, but decreased satisfaction when the sound quality was poor and the clock good, indicating that time delay increased the weight of central features and decreased the weight of peripheral features (for related findings in persuasion contexts, see Fujita et al., 2008). Conceptually similar findings were obtained with social distance, operationalized as interpersonal similarity (Liviatan, Trope, & Liberman, 2008) and social power (P. K. Smith & Trope, 2006). For example, in one study (Liviatan et al., 2008, Study 4), participants evaluated an essay of either high or low quality (a central, high-level aspect of value) allegedly written by a student that performed well or poorly on an unrelated physics exam (a peripheral, low-level aspect of value). Some participants were led to believe that the writer was similar to themselves, whereas others were led to believe that the writer was dissimilar to them. The weight of the peripheral information about the physics ability (relative to that of the essay quality) was greater in participants' evaluations of the essay written by the similar student than by the dissimilar student. It is interesting that research which compared the decisions people make for themselves to the advice they give to others obtained similar findings. Kray and Gonzalez (1999) and Kray (2000) compared participants' own choices with the advice they gave to socially close and distant others. They found that in advising others, especially socially remote others, participants tended to give more weight to a single attribute which they designated as the most important and less weight to other, more peripheral attributes. For example, when advising another person about choosing between two jobs, participants gave more weight to personal satisfaction (which they viewed as the most important dimension) and less weight to salary and location (the less important dimensions) than when choosing for themselves (Kray, 2000, Study 2). In two other studies, Kray found that this preferential weighting of important attributes was stronger in advising a distant social target (a student in another department) than a closer target (a student in one's own class). Moreover, as advisers, participants rated central attributes as highly important and peripheral ones as unimportant, whereas as deciders they rated the various attributes as relatively similar in importance. It is worth noting that participants reported greater responsibility and potential regret, and generated a larger number of decision-relevant attributes when making decisions for others than for themselves. As Kray pointed out, it therefore seems unlikely that participants simply invested less effort in advising others than in deciding for themselves. In our terms, these findings demonstrate choosing according to more central, high-level aspects for socially distant than social close others. Feasibility Versus Desirability-Desirability concerns involve the value of the action's end state (a high-level construal feature), whereas feasibility concerns involve the means used to reach the end state (a low-level construal feature). Therefore, desirability concerns should receive greater weight over feasibility concerns as psychological distance increases. Consistent with this prediction, it was found that as temporal distance from an activity (e.g., attending a guest lecture) increased, the attractiveness of the activity depended more on its desirability (e.g., how interesting the lecture was) and less on its feasibility (e.g., how convenient the timing of the lecture was; Liberman & Trope, 1998). Similar results have been found for other distance dimensions, including hypotheticality and social distance (e.g., Liviatan et al., 2008;Todorov, Goren, & Trope, 2007; see review by Liberman et al., 2007). Arguments in Favor of and Against an Action-In deciding whether to undertake an action, cons are subordinate to pros. This is because the subjective importance of cons depends on whether or not pros are present more than the subjective importance of pros depends on whether or not cons are present. For example, if we know that a medical treatment has some health benefit, we would inquire about its potential side effects before making a decision. But if the treatment has no benefits, we would decide against taking it without further inquiry about its side effects. In contrast, we would inquire whether a medical treatment has health benefits whether or not it has side effects. Thus, the importance of side effects depends on whether the treatment is known to have benefits, but the importance of benefits is independent of whether the treatment is known to have side effects. If cons are subordinate to pros, then pros should become more salient as temporal distance from the action increases, whereas cons should become less salient as temporal distance from the action increases. To test this prediction, Eyal, Liberman, Trope, and Walther (2004) asked participants to generate arguments in favor and against new (i.e., nonroutine) near future or distant future actions, such as introducing a new exam procedure (e.g., switching to open-ended questions instead of multiple-choice questions; Study 2), social policies (e.g., restricting private cars in the city center; Study 3), and a variety of personal and interpersonal behaviors (e.g., approaching a fellow student and offering to write an assignment together; Studies 4-6). As predicted, in all the studies, participants generated more pros and fewer cons as temporal distance from the actions increased. In an extension of these findings, Herzog, Hansen, and Wänke (2007) suggested that if pros are more salient as temporal distance increases and cons are more salient as temporal distance decreases, then an increase in temporal distance should make it easier to generate pros and more difficult to generate cons. Further, because attitudes tend to be more in line with content when the retrieval is experienced as easy (Wänke & Bless, 2000), the ease of retrieval associated with generating pros and cons of near and distant future activities should influence attitudes toward those activities, even when the number of arguments is held constant. In a test of these ideas, participants read about a proposed action that was to happen in the near or distant future and were instructed to write down either four pros or four cons regarding the activity. As expected, participants (a) found it easier to generate pros and more difficult to generate cons when the issue concerned the distant rather than near future and (b) had more favorable attitudes toward the action when it was to occur in the distant future. Alternatives and Attributes in Choice Matrices-Choice and enactment of a course of action is often based on information people search for about the available alternatives. Decision theoretic work has distinguished between searching within attributes, across alternatives and searching within alternatives, across attributes (Tversky, 1972). In a typical study, participants are presented with a matrix of information in which rows represent alternatives (e.g., different apartments), columns represent attributes (e.g., price, location, noise), and cells include the standing of each alternative on the corresponding attribute. Participants search this matrix by exposing the information in each cell, one at a time (see Payne, Bettman, & Johnson, 1988, for a review of this paradigm). We think that searching attributes within alternatives reflects a lower level construal of the choice situation than searching alternatives within attributes. This is because attributes are instantiated in specific objects and when detached from specific objects they become abstractions. Attributes are general, whereas objects are unique and variable manifestations of specific combinations of attributes. We thus expect that within-alternative search would characterize processing of proximal decision situations, whereas within-attribute search would characterize processing of distal decision situations. Borovoi, Liberman, and Trope (2010) recently tested this prediction. In a study on temporal distance, participants considered a choice for either the near future (e.g., choosing an apartment to rent in the next 2 weeks) or the distant future (e.g., choosing an apartment to rent a year later). In a study on social distance, participants considered a choice either for themselves or for another student. As expected, there were more within-alternative steps and less within-attribute steps when making decisions for psychologically proximal situations than for psychologically distal situations. It is important to note that in both studies, participants opened an equal number of cells and invested a similar amount of time in both distance conditions, indicating that they were not less motivated in the distal condition than in the near condition. It seems then that when making choices for proximal rather than distal situations, people are more likely to organize information within alternatives rather than within attributes. Alignable Versus Nonalignable Attributes-As noted earlier, comparing nonalignable objects (with information provided about different attributes for each object) requires higher level construal than comparing alignable objects (with information provided about the same attributes for each object), because making nonalignable objects comparable requires representing them in terms of more abstract attributes. On the basis of this argument, Malkoc, Zauberman, and Ulu (2005) reasoned that decisions that have distant future consequences (relative to near future consequences) would involve an increased consideration of nonalignable attributes. In a test of this prediction, participants evaluated two brands of potato chips and selected one brand to receive either at the end of the session (near future) or at the end of the semester (distant future). The two brands were designed based on pretesting to be equally attractive overall; one of the options, however, was designed to be better on its alignable attributes, whereas the other brand was better on its nonalignable attributes. As expected, temporal distance shifted both evaluations and choice toward the nonalignable better option over the alignable better option, indicating an increased reliance on nonalignable attributes when making decisions with distant future consequences. Discussion-Considerable evidence suggests that the value of outcomes is discounted or augmented over psychological distance depending on how it is distributed between highlevel and low-level aspects of the outcomes. People seem to appreciate proximal outcomes for their low-level aspects and distal outcomes for their high-level aspects. Although they often know less about distant than near outcomes, their evaluation of the distant outcomes might be more extreme when high-level construal of the outcomes have strong evaluative implications. These findings are consistent with CLT and the more general view that valuation is a constructive process (see e.g., Payne, Bettman, & Johnson, 1993;Shafir, Simonson, & Tversky, 1993;Weber et al., 2007). It is important to point out that the effect of distance on the value of outcomes is distinct from its effect on the controllability of the outcomes (Liberman & Trope, 2003). Psychological distance from outcomes often reduces the extent to which the outcomes are contingent upon one's action. For example, people typically have less control over others than themselves, over strangers than friends or relatives, and over events that occur in spatially remote than near locations. Moreover, because the number of ways of attaining an outcome often diminishes with proximity to the outcome, one's immediate action is less critical for a psychologically distant outcome to occur. For example, a long time before an exam, failing to study may be compensated by studying more intensely later, but a short time before the exam, when only a few hours remain, such possibility no longer exists. Similarly, failing to help a stranger may be compensated by help from another stranger, but failing to help a close friend is less likely to be compensated by someone else, because people typically have fewer close friends than strangers. By making outcome less contingent on one's action, greater psychological distance may decrease the motivation to engage in that action. Students may be less motivated to study for a distant exam than for a close exam because failure to study is easier to compensate for at a distance, or people may be less motivated to help a stranger than a close friend, because in the former case, their failure to help is more likely to be compensated by others. These changes in motivation do not reflect changes in value (e.g., the value of success on an exam may not increase closer to it). However, in many situations, measures of value might be contaminated with motivation and therefore show discounting over psychological distance. This could be one of the reasons for the widespread belief in economics and the behavioral sciences that value is discounted over time and psychological distance.",
          "Like predictions and evaluations, behavioral intentions and self-regulation should be increasingly based on high-level construal aspects as psychological distance increases. In the following, we examine the implications of this idea for values and ideologies, self-control, and negotiation. Values as High-Level Behavioral Guides-Values are commonly viewed as abstract structures that provide continuity and meaning under changing environmental circumstances (Feather, 1995), as stable meaning-producing superordinate cognitive structures (Rohan, 2000), and as trans-situational guides for action (S. H. Schwartz & Bilsky, 1987). On the basis of CLT, we propose that values, because of their relatively abstract and decontextualized nature, will be more readily applied to and guide intentions for psychologically distant situations. Evidence for this proposal has been recently obtained by Eyal, Sagristano, Trope, Liberman, and Chaiken (2009). One study used S. H. Schwartz's (1992) value questionnaire to assess the importance participants assigned to a wide range of values (e.g., power, benevolence, hedonism) and then asked participants to imagine 30 behaviors (e.g., \"rest as much as I can\") and to indicate the likelihood of performing each behavior either in the near future or in the distant future. Eyal et al. correlated the rated importance of each value and the mean likelihood of performing the behaviors corresponding to that value. As expected, these correlations were higher when the behaviors were planned for the distant rather than the near future, suggesting that people's values are better reflected in their intentions for the distant future than in their intentions for the immediate future or their actual behavior. For example, being high (vs. low) in hedonism might mean planning hedonic activities for the distant future, but not necessarily for the upcoming week. It is interesting that Eyal et al. (2009) also found that although values predicted participants' intentions for the distant future, feasibility concerns were more predictive of their intentions for the near future. For example, the number of hours participants volunteered in the distant future condition was predicted by their benevolence values but not by the convenience of the timing. In contrast, the number of hours participants volunteered in the near future condition was not predicted by their benevolence values and instead depended on the convenience of the timing. Extending this line of thought, Eyal, Liberman, and Trope (2008) argued that people judge immoral acts as more offensive and moral acts as more virtuous when the acts are psychologically distant than near. They showed that transgressions against core values that are deemed harmless because of extenuating circumstances (e.g., eating one's dead dog) were judged more severely when imagined from a more distant temporal or social perspective. Conversely, moral acts which might have had ulterior motives (e.g., adopting a disabled child when a government pays high adoption pensions) are judged more positively from temporal distance. The findings suggest that moral criteria are more likely to guide people's judgments of distant rather than proximal behaviors. Ideology and Incidental Social Influence-Our attitudes shift, often outside of our awareness, in response to other people in our local social context, including communication partners, significant others, and even total strangers (Baldwin & Holmes, 1987;Davis & Rusbult, 2001;Higgins & Rholes, 1978;Kawakami, Dovidio, & Dijksterhuis, 2003;Lowery, Hardin, & Sinclair, 2001). CLT predicts that when an attitude object is psychologically near, evaluations will be attuned to a particular social context and therefore more likely to be affected by incidental attitudes of others in the social situation rather than by one's ideology. Conversely, when the attitude object is psychologically distant, it will be abstracted away from its local context, and evaluation will therefore be less affected by the incidental attitudes of salient others and, instead, reflect one's ideology. A series of studies by Ledgerwood et al. (2008) tested the hypothesis that attitudes will align with those of another person in the local social context more when psychological distance is low (vs. high). Using an anticipated interaction paradigm, participants read about a policy that would increase the deportation of illegal immigrants starting either next week (near future) or next year (distant future) and learned that their discussion partner was either in favor of or against deporting illegal immigrants. They then privately reported how likely they would be to vote in favor of the policy. Participants' voting intentions shifted toward the interaction partner's attitude when the policy was set to be implemented in the near future but not when it was to be implemented in the distant future. However, voting intentions more strongly reflected participants' previously assessed ideological values when the policy was to be implemented in the distant (vs. near) future. Specifically, the more participants valued preserving the societal status quo, the more they supported a distant future policy that would enforce the deportation of illegal immigrants. Self-Control-Situations that require self-control involve a conflict between two opposing motivations (e.g., a desire to go out with friends and a need to study for an exam). Fujita, Trope, Liberman, and Levin-Sagi (2006) proposed an analysis of self-control conflicts as conflicts between behavioral implications of high-level construal (i.e., valence attached to primary, central, goal-relevant, superordinate considerations) and behavioral implications of low-level construal (i.e., valence attached to secondary, incidental, goal-irrelevant, subordinated, features). Failure of self-control, according to this proposal, is succumbing to the motivation implied by the low-level value. For example, if studying for an exam is related to more superordinate goals than going out with friends, then the latter behavior would represent a failure of self-control. Consistent with this analysis, a series of studies by Fujita et al. demonstrated that higher level construal increased self-control. In one study, participants first completed a task in which they indicated either why or how they would maintain good physical health. This task was designed to induce a high-level or low-level construal mindset, respectively (see Freitas, Gollwitzer, & Trope, 2004). Participants were then asked to hold a handgrip while connected to bogus electrodes, ostensibly as part of a psychophysiological assessment of personality. Participants were told that the longer they held the handgrip, the more diagnostic was the information obtained from the apparatus. Thus, the situation presented a conflict between a desire to get diagnostic, self-relevant information (high-level value) and the inconvenience of holding the handgrip (low-level value). The results indicated, as predicted, that participants in the high-level construal condition held the handgrip longer than those in the low-level construal condition. Just like higher level of construal, greater temporal, spatial, or social distance from a situation that poses a self-control dilemma should also enhance self-control. Indeed, research on intertemporal preference reversals has shown that people are better able to commit to self-control a long time in advance than a short time in advance (e.g., Ainslie & Haslam, 1992). A study conducted by Freitas, Salovey, and Liberman (2001) specifically showed that people were better able to forgo flattering but useless feedback for the sake of unpleasant but useful feedback when the feedback was delayed than when it was imminent. Similarly, research on children's delay of gratification has shown that greater temporal and spatial distance from a tempting object enhances self-control (Metcalfe & Mischel, 1999;Mischel, Shoda, & Rodriguez, 1989). CLT would further predict that we are more likely to advise another person to exercise self-control than to make a similar decision for ourselves, and that deciding on self-control is easier when it is premised on less likely events. For example, we could predict that deciding to start a diet upon getting an A in an exam would be facilitated to the extent that getting an A is less likely. In general, psychologically distancing oneself from a situation might promote people's ability to exercise self-control in that situation (see Kross & Mischel, in press). Negotiation-As with values, issues within an interpersonal negotiation can differ in their centrality and worth. If a pair of negotiators can trade off their lowest and highest priority issues (e.g., give in on secondary issues in exchange for getting what they want on highpriority issues, a process called logrolling), they are more likely to succeed in \"expanding the pie,\" maximizing both individual and joint outcomes. Because negotiators should be expected to focus more on central concerns and less on peripheral concerns as distance increases, we would expect to see more logrolling agreements in a distant future than near future context. Examining this idea within the context of a live negotiation, Henderson, Trope, and Carnevale (2006) found that whereas 91% of dyads with a temporally distant perspective reached a full logrolling agreement, only 50% of dyads with a temporally near perspective did so. The enhanced reciprocal concessions made by dyads with the temporally distant perspective culminated in better negotiated individual and joint outcomes. Moreover, research on the role of construal levels in the negotiation process has shown that negotiators who construed issues abstractly rather than concretely were more likely to discover integrative agreements (Henderson & Trope, 2009). For example, having negotiators think abstractly about issues (by having them generate general descriptions) rather than concretely about issues (by having them generate specific descriptions) increased negotiators' logrolling and individual and joint outcomes. These findings have implications for how negotiators handle conflicts over minor versus major issues and specific interests versus broad values and ideological differences in situations that do not allow for tradeoffs (Druckman & Broome, 1991;Harinck & De Dreu, 2004). Specifically, CLT suggests that the resolution of minor issues and specific interests should be hindered when individuals adopt a psychological proximal perspective or a lower level construal and facilitated by having a more distal perspective and abstract construal. Conversely, the resolution of conflict over major issues, values, and ideological differences should be hindered when individuals adopt a psychologically distant perspective or a higher level construal and facilitated by a less abstract construal and a psychologically proximal perspective. Discussion-The research reviewed in this section suggests that adopting a distal versus proximal perspective changes the way people make behavioral plans, resolve value conflicts, negotiate with others, and cope with self-control problems. The choices people make for psychologically distant situations are guided by their general attitudes, core values, and ideologies. As people get psychologically closer to the situation, their choices are increasingly influenced by more specific attitudes, secondary values, and incidental social influences. It seems, then, that from a distant perspective, global concerns are prioritized and unequivocally pursued, whereas from a proximal perspective, those priorities are weakened and even reversed as local concerns become more prominent. People may have less information about distant situations. Nevertheless, they often act as if they know better what to do or what they should have done when they remove themselves from the situation and take a distant rather than proximal perspective on it. Implications and Extensions-To recapitulate, the research we have reviewed thus far suggests that psychological distance triggers high-level construals, which focus on the central and enduring features of an object while screening out incidental and peripheral details. These high-level construals promote attunement to what is consistent about an object across multiple contexts, allowing individuals to transcend the particularities of the present situation and act according to their global concerns. Conversely, psychological proximity triggers low-level construals, which include the concrete and contextualized aspects of an object. These in turn are attuned to the current context, immersing individuals in the immediate situation and enabling them to flexibly respond to local concerns. In this final part of the article, we explore new directions for research on more speculative implications and extensions of CLT. These include psychological phenomena that might map onto different levels of construal (Section VII) and phenomena that might map onto different psychological distances (Section VIII). We conclude with a discussion of psychological distance in the brain (Section IX).",
          "We have shown that the distinction between different levels of construal is applicable to many psychological phenomena (e.g., gestalts vs. details, trait vs. situation attributions, pros vs. cons). In this section, we suggest that two more distinctions might map onto high-versus low-level of construal: assimilation versus contrast and high-versus low-level emotions. Assimilation and Contrast-When perceiving two stimuli in relation to each other (compared with perceiving the same stimuli separately), a perceiver may assimilate the two stimuli to each other, thereby perceiving each of the stimuli as more similar to the other stimulus, or contrast them away from each other, thereby perceiving each of the stimuli as more distinct from the other stimulus (Parducci, Perrett, & Marsh, 1969;Schwarz & Bless, 1992, 2007). For example, when considering a paper of a student in relation to the best paper in class (vs. considering it in isolation), the focal paper may be assimilated to the excellent paper and thus seem better, or it may be contrasted away and thus seem worse. In view of the opposite effects of assimilation versus contrast, the question of what makes each of them more likely becomes crucial (Mussweiler, 2001;Schwarz & Bless, 2007;Stapel, 2007). Most germane to the present framework are Schwarz andBless's (1992, 2007) inclusion/ exclusion model and Förster, Liberman, and Kuschel's (2008) global/local model of assimilation versus contrast. According to these models, because global, high-level construals are more inclusive, using those construals is likely to result in including two stimuli in the same category and an assimilation effect. Using low-level construals, however, is likely to result in categorizing the two stimuli in separate categories and a contrast effect. Consistent with this view, it has been found that priming of high-level construal and greater temporal distance enhances assimilation and reduces contrast. For example, in one of the studies, (Förster et al., 2008, Study 4), participants compared their athletic skills with either a moderately high standard or a moderately low standard and then rated their expected athletic performance in an athletic competition that would take place the next day (proximal temporal perspective) or a year from then (a distal temporal perspective). In the control condition, time was not specified. The results showed that a distant time perspective enhanced assimilation (i.e., produced a high self-rating after comparison to a high standard and a low self-rating after comparison to a low standard), whereas a proximal time perspective enhanced contrast (i.e., produced a low self-rating after comparison to a high standard and a high self-rating after comparison to a low standard). Borovoi et al. (2008) reasoned that if distance enhances assimilation, then it would also enhance the positive effect that attractive but unattainable alternatives might have on evaluations of the choice set. Consistent with this prediction, they showed that adding a very attractive job that was no longer available to a set of job offers increased the attractiveness of the entire set of job offers when the jobs were expected in the distant future rather than the near future. It seems that the attractiveness of available options is assimilated to that of attractive but unattainable possibilities when thinking about the relatively distant future. Affect-It is commonly assumed that psychological distance from an object diminishes the intensity of affective responses to that object. People typically react more strongly to events that are closer to them in time and space, to events that happen to themselves than to others, and to events that are real rather than hypothetical. However, building on appraisal theories of emotion (see, e.g., Beer & Keltner, 2004;Clore, Ortony, & Foss, 1987;Ortony & Turner, 1990;Tangney & Fischer, 1995;Tracy & Robins, 2007), CLT suggests that it might be useful to distinguish between emotions that may ensue from a low-level construal of an emotion-eliciting event and emotions that ensue from a high-level construal of the event. For example, love might be based on a more general and abstract representation of the target person than lust (Förster, Özelsel, & Epstude, in press). It is interesting that some emotions involve, by their very definition, taking a distant perspective. For example, guilt and shame require taking a socially distant perspective, as they include considering the situation from the perspective of another social agent (Beer & Keltner, 2004), whereas sadness does not require taking such a socially distant perspective. Anxiety involves consideration of a potential, distal danger (for mice, anxiety is often induced by a smell of a cat), whereas fear is a reaction to an immediately present danger (e.g., for mice, the presence of a cat; Gray, 2000). Likewise, counterfactual emotions (e.g., regret) involve consideration of hypothetical alternatives to reality. According to CLT, emotions that involve taking a distant perspective require relatively abstract, high-level construal of the emotion-eliciting situation. It would be interesting to examine whether, indeed, some emotional experiences are more abstract than others. For example, would thinking of guilt and anxiety prime people to think in higher level terms in a subsequent, unrelated task? Once the classification of emotions to high level versus low level is established, CLT would predict that higher level emotions would decay less steeply and may even intensify over distance. Eyal and Fishbach (2008) recently accrued evidence in support of this hypothesis. They showed, for example, that pride decays over time less than happiness. Because of their distance transcending qualities, abstract emotions seem to assist in self-control more than concrete emotions. For example, Eyal and Fishbach showed that among dieters, priming participants with pride rather than happiness resulted in higher choice of a carrot (a self-control choice) rather than a chocolate bar (a self-control failure). CLT would further suggest that even the same emotion may be high or low-level, depending on whether it reflects high or low-level construals of the object. For example, in a funeral, sadness about the transience of life and compassion are central and thus constitute high-level emotions, but happiness upon seeing old friends is peripheral and thus constitutes a lowlevel, peripheral emotion. In a birthday party, however, the centrality of these emotions reverses. We would predict that in this respect too, secondary emotions would diminish over distance more than primary emotions. For example, the positive value that may be added to a funeral by the prospect of meeting friends would be more apparent when the funeral is closer in time and space, when we anticipate going ourselves rather than think about somebody else and when attending the funeral is likely rather than unlikely.",
          "",
          "Social power: Social power may engender a sense of distance from others. Indeed, individuals who have power see themselves as less similar to and thus more distant from other people than individuals who have less power (e.g., Hogg, 2001;Hogg & Reid, 2001;Lee & Tiedens, 2001;Snyder & Fromkin, 1980). This perception might be due to the fact that groups, organizations, and societies ordinarily have a pyramidal structure with fewer individuals occupying high-power positions than low-power positions. There is therefore greater similarity in the positions held by individuals with low power than by individuals with high power. If social power makes people feel distant from others, then it should also predispose them to construe information abstractly, focus on the central aspects of situations, disregard secondary aspects, and establish clear priorities. Several lines of research are consistent with these predictions. For example, Overbeck and Park (2001) observed that high-power participants were better at distinguishing between central and peripheral information, a hallmark of abstract processing. Guinote, Judd, and Brauer (2002) found that assigning participants to high-power positions promoted the use of abstract language. A series of studies by P. K. Smith and Trope (2006) examined the effect of power priming on a range of construal-level phenomena. Their findings show that power primed participants (a) were better able to abstract visual stimuli in an embedded figures task and Gestalt completion task, (b) were better at detecting covariation in a series of data, and (c) made more superordinate categorizations. Finally, Magee, Milliken, and Lurie (in press) have recently found that the level of abstraction in verbatim reactions to the September 11, 2001 attack was higher for individuals with high social power, compared to those with low social power. These findings suggest that the distal perspective activated by the possession of social power promotes going beyond the information given, detecting the underlying structure, and abstracting from it superordinate, central features. Power-related construal may expand people's mental horizons, enabling them to transcend the immediate circumstances and take into account the past, future, a broad range of people, and unlikely possibilities. Recent research by P. K. Smith, Wigboldus, and Dijksterhuis (2008) suggests that the relationship between power and construal level might be bidirectional. Their findings show that thinking abstractly enhances individuals' sense of their own power. Are there analogous effects of level of construal on the way we perceive others? Do we think of individuals as suitable for power positions, or as actually holding such positions, when they articulate and enact highlevel construals of the situation? Do our inferences and preferences as to the power individuals possess depend on whether they can articulate plans that transcend the present and extend to the distant future, remote places, diverse groups, and unusual circumstances? These questions await future research.",
          "The endowment effect suggests that when individuals are asked to sell an object they own, they ask for a significantly higher price than buyers tend to offer (Thaler, 1980). For example, in a typical endowment study, half of the participants receive a mug and are asked to set a price at which they would be willing to sell the mug to the other half of the participants, who in the meantime set the price at which they would be willing to buy the mug (e.g., Kahneman, Knetsch, & Thaler, 1990). Results of numerous such experiments and field studies show that average selling prices are two to three times higher than average buying prices. Given that the benefit of having the mug and, hence, its objective value, should not change with random ownership, these results present a perplexing phenomenon. The most widely accepted explanation of the endowment effect is loss aversion (Bar-Hillel & Neter, 1996;Kahneman et al., 1990; but see also Liberman, Idson, Camacho, & Higgins, 1999) that goes back to prospect theory (Kahneman & Tversky, 1979). Prospect theory suggests that individuals' perception of losses and gains differ, such that the subjective pain of a loss is more intense than the subjective pleasure of a gain of similar objective value. According to a prospect theory approach to the endowment effect, selling an object is perceived as a loss, compared with the seller's reference point of having the object; buying, in contrast, is perceived as a gain, compared with the buyer's reference point of not having an object. Because individuals are loss averse, they therefore tend to evaluate objects they consider selling more positively than they do objects they consider buying. Adopting a CLT approach to the endowment effect, Irmak, Wakslak, and Trope (2009) argued that sellers adopt a distant perspective on an object they are considering selling, thinking about the object from the perspective of the distant other to which the object will be moving; whereas buyers consider the object from a more proximal perspective, considering how they personally view this nearing object. As a result, sellers will construe the object on a high level, and selling prices will largely be influenced by the object's high-level features (and less by its low-level features). In contrast, buyers will construe the object on a lower level, and buying prices will therefore be influenced by the object's low-level features. In other words, we argue that sellers and buyers think about the object differently from one another, focusing, and thus basing their pricing decisions, on different types of object features, which, in turn, may lead to systematic differences in selling and buying prices. A series of studies by Irmak et al. (2009) provides initial support for this analysis. First, they showed that sellers indeed tended to think about an object in terms of its value for the buyer (indicating a distal perspective), whereas buyers tended to think about the object in terms of its value for themselves (indicating a proximal perspective). Second, sellers, compared with buyers, were more likely to construe actions related to those objects in high-level, endsrelated terms rather than low-level, means-related terms. Third, selling prices were higher than buying prices for objects whose attractiveness derived from high-level construal features, but this was not the case for objects whose attractiveness derived from low-level construal features. For example, selling prices were higher than buying prices when the object was superior on the desirability dimension and inferior in the feasibility dimension, but not vice versa, and when the primary (goal-relevant) aspects of the object were superior and the secondary (goal-irrelevant) aspects of the object were inferior, but not vice versa. These findings demonstrate that sellers and buyers derive value from different aspects of the object. Sellers take a psychologically distal perspective and thus extract value from the highlevel, primary aspects of the object, whereas buyers assume a psychologically proximal perspective and their perception of value is influenced less by primary aspects of the object and more by its secondary aspects.",
          "Proximal versus distal senses: We view the four psychological distance dimensions as being anchored on a single starting point (zero-distance point): me, here, and now. This view may be oversimplified, however, as some direct experiences may be more proximal than others. More specifically, the five senses-sight, hearing, smell, touch, and taste-may be mapped along spatial distance according to the maximum physical distance of the sensed object. An object has to be in one's mouth to be tasted, it has to be within one's reach to be touched, it may be farther away to be smelled, and it can be still farther away to be heard or seen. The distant senses, sight and hearing, enable people to extend the scope of perception far beyond the range of the near senses, which are limited to the person's immediate environment (Boring, Langfeld, & Weld, 1939; see also Rodaway, 1994). Do proximal senses produce a psychologically closer experience than distal senses? If they do, then the antecedents and consequences of psychological distance should apply to stimuli that are perceived with proximal versus distal senses, such that proximal senses would be associated with low-level construal and psychological proximity, whereas distal senses would be associated with high-level construal and psychological distance. For example, sharing an experience of tasting something with another person might produce more social closeness to that person than sharing an experience of looking at something. Similarly, touching a product might make a consumer give more weight to its feasibility properties and less weight to its desirability properties than only looking at that product. Novelty: Novel events are unfamiliar and often (but not always) subjectively improbable. Novel objects may therefore be perceived as more psychologically distant. The question, then, is whether novelty affects construal and construal-mediated outcomes in the same way as psychological distance does. Addressing this question, Förster, Liberman, and Shapira (2009) found that when stimuli were presented as novel they were construed at a higher level than when they were presented as old. Novelty and familiarity were manipulated by either framing a task as new versus familiar or by asking participants to reflect upon novel versus familiar events prior to the task (i.e., procedural priming). The studies showed that relative to the control, no priming or no framing group, novelty priming or framing enhanced global perception, broad categorization, and high-level action identifications. Familiarity priming or framing had the opposite effect. Another study showed that presenting a product as novel (e.g., \"a novel model of a cellular phone\") versus old (e.g., \"Version 3 of an existing model of a cellular phone\") made participants assign greater weight to the products' high-level, desirability features and less weight to their low-level, feasibility features. It would be interesting to examine in future studies the effect of novelty on other construalmediated outcomes, such as creativity and self-control. Because creativity has been shown to be related to high-level construal (Förster et al., 2004), we might predict that presenting a task as novel (vs. old) would enhance creative performance. It is also possible that because high-level construal is conducive to self-control (see Fujita, Trope, et al., 2006), presenting self-control programs (e.g., dieting or smoking cessation programs) as novel (vs. old) would promote success in those programs. Generalizing from experience: Our responses to a new object are often based on generalizations from a familiar, previously experienced object. The generalization may result from the co-occurrence or some resemblance between the familiar object and the new object. Of special interest are generalizations, often called projections, from ourselves to new people who become associated with us. By definition, the self is more proximal than another person. Therefore, we are likely to form low-level construals of ourselves, while generalizing our high-level construals of ourselves to others. High-level construals of the self are more likely to drive responses to others, whereas low-level construals of the self are more likely to drive responses to self. As a result, the more people value their high-level self characteristics relative to their low-level self characteristics, the more likely are they to form favorable impressions of unfamiliar others. Again, the bias toward projecting one's highlevel characteristics rather than low-level characteristics onto another person is likely to be augmented with greater psychological distance from that person.",
          "Do psychological distances and their links to construal level correspond to the structure and functions of the brain? A growing body of research suggests that they do. For example, recent research suggests that a common brain network involving the prefrontal cortex and the medial temporal lobe is implicated in prospection, retrospection, and taking another person's perspective (e.g., Buckner & Carroll, 2007;J. P. Mitchell, Macrae, & Banaji, 2006;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). In particular, different subregions of the medial prefrontal cortex (mPFC) have been associated with psychological processes that vary in their levels of abstraction. In their review on this subject, Amodio and Frith (2006) noted that representations of low-level goals and actions tend to be associated with activity in more posterior and ventral regions of the mPFC, whereas more complex and hierarchically organized representations are typically associated with more anterior and dorsal regions. On the basis of anatomical studies of connectivity in this region, Amodio and Frith (2006) proposed that activity in the mPFC corresponds to two different axes associated with the degree of abstraction (running posterior to anterior) and to action (running ventral to dorsal). In a related vein, J. P. Mitchell et al. (2006) found that processing information about a similar other engages the ventral region of the mPFC, whereas processing information about a dissimilar other engages more dorsal subregions of the mPFC. Moreover, recent research by Mitchel, Ames, and Gilbert (2008) observed an association in the brain between social distance and temporal distance. Specifically, the ventral mPFC was implicated in processing information about similar others and the present self, whereas the dorsal mPFC was implicated in processing information about dissimilar others and the future self. More generally, there is research showing that the brain is hierarchically organized, with higher points in the cortical hierarchy representing increasingly more abstract aspects of stimuli (Grill-Spector & Malach, 2004;Lieberman, Gaunt, Gilbert, & Trope, 2002). For example, progressively anterior and dorsal subregions of the prefrontal cortex have been found to be associated with more abstract representations (Badre, 2008;Koechlin & Summerfield, 2007;Ramnani, & Owen, 2004). This organization of information in the brain might be related to distance from stimuli, such that activation systematically progresses to higher points in the hierarchy as psychological distance from the stimuli increases. In sum, there may be a \"construal axis\" in brain systems that maps onto a \"distance axis.\" Brain systems that process high-construal information may be biased toward distal information, and brain systems that process low construal information may be biased toward proximal information.",
          "In reality, the different distance dimensions are separable. The fact that something happened long ago does not necessarily mean that it took place far away, that it occurred to a stranger, or that it is improbable. Nevertheless, as the research reviewed here demonstrates, there is marked commonality in the way people respond to the different distance dimensions. CLT proposes that the commonality stems from the fact that responding to an event that is increasingly distant on any of those dimensions requires relying more on mental construal and less on direct experience of the event. The findings we reviewed in the present article substantiate this claim in showing that (a) the various distances are cognitively related to each other, such that thinking of an event as distant on one dimension leads one to thinking about it as distant on other dimensions, (b) the various distances influence and are influenced by level of mental construal, and (c) the various distances are, to some extent, interchangeable in their effects on prediction, preference, and self-control. Past psychological research has typically investigated the different distance dimensions within disparate theoretical frameworks and methodologies. The research reviewed here suggests that psychological distance, as conceptualized here, captures a fundamental aspect of meaning common to all distances and may provide a unifying framework for understanding a wide range of seemingly unrelated psychological phenomena. Finally, taking an even broader perspective on psychological distance, it is worth noting that both collective and personal human development are associated with traversing increasingly greater distances. The turning points of human evolution include developing tools, which required planning for the future; making function-specific tools, which required considering hypothetical alternatives; developing consciousness, which enabled the recognition of distance and perspective taking; developing language, which enabled forming larger and more complex social groups and relations; and domestication of animals and plants, which required an extended temporal perspective (Flinn, Geary, & Ward, 2005). Human history is associated with expanding horizons: traversing greater spatial distances (e.g., discovering new continents, space travel), forming larger social groups (families vs. cities vs. states vs. global institutions), planning and investing in the more distant future, and reaching farther back into the past. Human development in the first years of life involves acquiring the ability to plan for the more distant future, consider possibilities that are nonpresent, relate to and take the perspective of more distant people (from self-centeredness to acknowledging others, from immediate social environment to larger social groups; Suddendorf & Corballis, 2007). Although the areas of evolution, history, and child development have different time scales, research in these domains seems to converge on the notion that transcending the present requires and is enabled by the human capacity for abstract mental representation. We hope that the present research on psychological distance, its relationship to level of construal, and its consequences for thought, feeling, and action advances our understanding of how individuals and groups transcend the here and now. Example of the four Spatial Location × Word Meaning combinations in Bar-Anan, Liberman, Trope, and Algom (2007). The illustration demonstrates words related to hypotheticality. Sample items from the Street Gestalt Completion Test (Street, 1931), provided courtesy of Teachers College, Columbia University. Identifying the pictures (from top right to bottom left: a boat, a rider on a horse, a rabbit, a baby) requires visual abstraction. Example of four Social Distance × Medium (picture vs. word) combinations in Amit, Algom, and Trope (2009). For the Israeli participants in this study, a shekel was socially proximal, and a dollar was distal."
        ],
        "ground_truth_definitions": {
          "psychological distance": {
            "definition": "a subjective experience that something is close or far away from the self, here, and now.",
            "context": "They serve to transcend the immediate situation and represent psychologically distant objects. Psychological distance is a subjective experience that something is close or far away from the self, here, and now. Psychological distance is thus egocentric: Its reference point is the self, here and now, and the different ways in which an object might be removed from that point-in time, space, social distance, and hypotheticality-constitute different distance dimensions.",
            "type": "explicit"
          },
          "high-level construal": {
            "definition": "relatively abstract, coherent, and superordinate mental representations, compared with low-level construals.",
            "context": "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental.",
            "type": "explicit"
          },
          "correspondence bias": {
            "definition": "a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions",
            "context": "A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995; E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995; Semin & Fiedler, 1988; Semin & Smith, 1999). Distance should therefore enhance the correspondence bias.",
            "type": "explicit"
          },
          "centrality": {
            "definition": "Changing a high-level feature has a greaterimpact on the meaning of an object than does changing a low-level feature",
            "type": "explicit",
            "context": "The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is."
          },
          "subordination": {
            "definition": "The meaning of low-level features depends on highlevel features more than vice versa",
            "type": "explicit",
            "context": ". The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. T"
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Peña, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Peña's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fernández, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fernández, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fernández, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "https://aclanthology.org/2024.lrec-main.952",
        "sections": [
          "Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets (Krieger et al., 2022), which exhibit limited generalizability across diverse domains (Wessel et al., 2023). This paper builds upon the work of Wessel et al. (2023), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) (Aribandi et al., 2021a;Chen et al., 2021;Kirstein et al., 2022) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method (Spinde et al., 2022) underperforms due to its narrow task focus and does not surpass baseline outcomes. In this study, we make five main contributions: ). This enables it to outperform models based on single-task learning (STL) paradigms. 1. We present MAGPIE -the first large-scale multi-task pre-training approach for media bias detection. By pre-training on diverse bias types such as persuasive and subjective, a classifier based on MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1). 2. We introduce LBM (Large Bias Mixture), a pretraining composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias. 3. We provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks. 4. We demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset (Spinde et al., 2021c) and achieves competetive results on the Media Bias Identification Benchmark (MBIB) collection (Wessel et al., 2023). 5. We make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub: github.com/magpie-multi-task These contributions highlight the potential of MTL in improving media bias detection. Our findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results. Another key insight of our research is the value of MTL in contexts where the primary dataset is smallfoot_0 . By learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data. Overall, our research offers a multitask learning approach to media bias detection with first large-scale resources in the domain.",
          "",
          "Media bias is a complex issue (Lee et al., 2021a;Recasens et al., 2013;Raza et al., 2022) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias (Wessel et al., 2023). In their literature review, Spinde et al. (2023) provide an extensive overview of research on media bias and related subtypes of bias. Media bias detection approaches have evolved from hand-crafted features (Recasens et al., 2013;Hube and Fetahu, 2018;Spinde et al., 2021d) to neural models (Spinde et al., 2022;Chen et al., 2021;Spinde et al., 2021c;Huguet Cabot et al., 2021;Sinha and Dasgupta, 2021;Raza et al., 2022). However, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets (Wessel et al., 2023). As most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019)). We will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain. Media bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications (Recasens et al., 2013;Spinde et al., 2021b), which, so far, are not explored in relation to each other (Spinde et al., 2023). Wessel et al. (2023) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias. Their study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities. MAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks to mitigate a variety of limitations in MBIB (see Section 3.1).",
          "MTL shows significant improvements in various NLP tasks, including sentiment analysis (He et al., 2019), text summarization (Kirstein et al., 2022), and natural language understanding (Raffel et al., 2020). In MTL, a model leverages knowledge gained from one task to improve the performance of others. Aribandi et al. (2021b) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021) show that prefinetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold. As described above, media bias can be seen as a composite problem composed of various interrelated bias types (Wessel et al., 2023). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b). For instance, benchmarks such as GLUE and Super-GLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection (Wahle et al., 2023(Wahle et al., , 2022)), and semantic evaluation (Ruas et al., 2019(Ruas et al., , 2020)), thereby substantially improving performance across a range of NLU tasks (Wang et al., 2018(Wang et al., , 2019)). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues. The selection of tasks is pivotal to the efficacy of MTL. There have been several attempts to automate task selection, including learning the data selection with Bayesian optimization (Bingel and Søgaard, 2017) or estimating task relations (Ruder and Plank, 2017). The most model-agnostic approach is GradTS (Ma et al., 2021), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE. GradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task's attention. The selected tasks are trained jointly and share representations across tasks.",
          "We implement MAGPIE using pre-finetuning as introduced in (Aghajanyan et al., 2021) (See also Section 2). As such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), a large-scale task collection of bias-related datasets. We incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1). As outlined in Figure 2, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset 2 (Spinde et al., 2021c), and 2 BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model's generalizability within a single dataset the Media Bias Identification Benchmark (MBIB) collection.",
          "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias more indirectly, such as sentiment analysis or emotion classification. MAG-PIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3. First, we manually assess a list of 115 media bias-related datasets in English language, categorized into task families by Wessel et al. (2023). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks in Section 4.3, such as Aribandi et al. (2021b) proposed for general NLP tasks. We filter the collection of the 115 datasets based on the following criteriafoot_2 : • Accessibility: Datasets have to be publicly accessible. • Text granularity: We only use datasets labeled on a sentence level or its fragments (tokens)(not on, for instance, article level) • Quality of annotations: We exclude datasets with no documentation, low annotation agreement or employment of machine annotation. • Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levelsfoot_3 . We remove 5 datasets due to or unreliable source of annotations and discard 4 duplicates. Applying these criteria leaves context. originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. In order to standardize examples from various domains, we apply a unified text-cleaning procedure to each dataset. The process involves: (1) discarding sentences with fewer than 20 characters, (2) removing URL links, special characters, and whitespace, and (3) eliminating duplicates potentially created with steps (1) and (2). The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks and task families can be found in Table 4.",
          "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa (Liu et al., 2019) encoder due to its proven state-of-the-art performances across various media bias applications (Spinde et al., 2021c;Krieger et al., 2022).",
          "Pre-finetuning. To effectively harness the generalization abilities of MTL for media bias detec-tion, we adopt a pre-finetuning procedure (Aghajanyan et al., 2021). Pre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training. While incorporating MTL directly into the pre-training stage has demonstrated performance gains (Aribandi et al., 2021b), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL (Aghajanyan et al., 2021). Sharing representations. We use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads. For each task, we attach dense layers, or \"heads\", to the shared encoder. These heads are optimized individually per task while the shared encoder learns general bias representations. However, multi-task optimization presents challenges due to differing gradient directions and magnitudes (Yu et al., 2020). For instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum. On the other hand, if Task A's gradient greatly surpasses that of Task B, gradient A becomes dominant. We counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling (Yu et al., 2020). Conflicting gradients and loss scaling. In multi-task training involving n tasks, encoder parameters receive n potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad (Yu et al., 2020), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks. Therefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of n sets per update. Adopting Muppet's method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps (Aghajanyan et al., 2021).",
          "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL (Aribandi et al., 2021b;Spinde et al., 2022). We also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster. However, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM. When task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B. We aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches: 1. Head-Specific-Early-Stopping (HSES) 2. Resurrection HSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers' continuous learning, especially after they have reached an optimal state. Resurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again. This enables the task to adapt its head parameters to the latest latent representation. HSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4. However, we stress the need for further extensive analysis in Section 5.",
          "Multi-Task Learning often involves selecting wellused datasets, leading to potential selection biases. Furthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes. To automate the process of task selection, we utilize the GradTS algorithm (Ma et al., 2021). We choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability. Gradient-based Task selection. In line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrixfoot_4 for each task. Tasks are sorted by correlation between each task's matrix and BABE task's matrix. We pre-finetune m -1 models on the first k tasks from the sorted list, where k varies from 1 to m -1 and m is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal k determined by evaluation loss. For further details on the GradTS algorithm, please see Ma et al. (2021).",
          "We split the BABE dataset into train, dev and test split with 70%,15%,15% portions respectively. For both hyperparameter tuning and auxiliary task selection, we evaluate on dev split and only use test split for the final evaluation (Section 4.2). As fine-tuning transformers on small datasets often leads to inconsistent results, such as high performance variance (Dodge et al., 2020), we use a fixed random seed 321 for all runs in auxilliary task selection. For the final evaluation on the test set (Table 2) we evaluate the models using 30 random seeds and report the average performance, to minimize the influence of random weight initializations. We use values 0..29. For optimizing the models, we use a per-task batch size of 32, an AdamW optimizer, and a polynomial learning scheduler. We run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory.",
          "In this section, we present the results of our MTL approach. First, we report the set of auxiliary tasks selected by GradTS for pre-finetuning. Next, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks. We also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark. Then, we analyze the LBM taxonomy through a study on knowledge transfer between families. Lastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study.",
          "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices. We use Kendall's correlation coefficient, as suggested by Puth et al. (2015). We find a local minimum for the BABE evaluation loss when pre-finetuning on the first k = 10 most correlated tasks. The final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1. The tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection. Task Type τ (correlation) Persuasive techniques 0.73 Lexical/Informational bias 0.72 Rumour detection 0.69 Sentiment analysis 0.68 Global warming stance detection 0.68 Subjective bias 0.67 Veracity classification 0.64 Gender bias 0.64 Fake news detection 0.63 Table 1: Attention importance correlation w.r.t. the media bias task.",
          "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline. The multi-task sets are the following: • MTL:Random -Random Subset of 10 tasks • MTL:GradTS -Subset of 10 tasks selected by GradTS algorithm • MTL:All -Set of all tasks We also evaluate the model pre-finetuned on the set of all tasks on MBIB. We follow the guidelines set by Wessel et al. (2023) for the evaluation. Given that MAGPIE's pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases. Multi-Task Learning Performance. Table 2 summarizes our performance results on the BABE dataset. We observe that all of our MTL prefinetuning schemas lead to performance improvements. In particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c). While both MTL baselines -Muppet (Aghajanyan et al., 2021) and UnifiedM2 (Lee et al., 2021b) outperform singletask baseline, they underperform all of our MTL models. On MBIB benchmark, MAGPIE ranks first on 5 out of 8 tasks. However, the improvements are only marginal. The results can be found in Appendix in Table 5.  Step efficiency. In addition to the performance improvements achieved, we also assess our model training efficiency. In Figure 5, we show the F1 score on the development set for the BABE task, averaged over all 30 runs. Our findings show that Multi-Task Learning only requires 15% of the training steps used in single-task finetuning on BABE.",
          "F1 Acc loss Baseline (RoBERTa base) 80.83 (±0.69) 81.19 (±0.69) 43.6 (±3.54) DA-RoBERTa 77.83 (±1.4) 78.56 (±1.3) 47.84 (±2.97) MUPPET 80.56 (±1.3) 81.18 (±1.16) 44.19 (±4.65) UnifiedM2 81.91 (±0.91) 82.41 (±0.88) 44.86 (±3.99) MTL:Random 81.88 (±1.02) 82.28 (±0.97) 40.35 (±1.73) MTL:GradTS 82.32 (±0.79) 82.64 (±0.8) 40.96 (±2.36) MTL:All 84.1 (±1.33) 84.44 (±1.25) 39.46 (±2.41) Table 2: Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds. This result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future.",
          "In Section 3.1, following Aribandi et al. (2021b), we introduce data task families. Aribandi et al. (2021b) uses task families for selection and knowledge transfer. To assess task families' significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer. To account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families. We train tasks from the same family together and report the average change in task performance, as depicted in Figure 6. Negative knowledge transfer is prevalent across most of our task families. However, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance. Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Section 4.3. Our results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families. Conversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%. On the other hand, the Emotionality family is significantly impaired by negative transfer from other task families. The full table of transfers can be found in Figure 7. Considering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5. Task Family Transfer from Transfer to media bias -2.07% -0.94% subjective bias -1.26% 0.89% hate speech -0.87% 0.17% gender bias -1.01% -1.07% sentiment analysis 0.11% 0.72% fake news -0.13% 1.79% group bias -1.04% 0.09% emotionality 0.34% -6.56% stance detection -0.79% -1.83% Table 3: Evaluation of averaged transfer between task families.",
          "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation. We calculate the average evalua- tion loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection. The boxplot in Figure 8 shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%. However, we hypothesize that a random constellation of tasks 6 can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.",
          "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multitask learning (MTL) approach that enhances traditional models with a multi-task structure. Additionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pretraining of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias. Our study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative trans- 6 Particularly variance in task sizes and quality. fer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far. Furthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches. These findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias. While results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum. Understanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies. As media bias is emerging globally, incorporating multilingual models is a natural extension.",
          "We acknowledge the necessity for a more comprehensive analysis of the performance of the HSES and Resurrection methods to ensure robust evaluation. Given the significant computational resources required for a single multi-task training of all tasks, we allocated these resources towards robustly evaluating the model performance rather than conducting an in-depth analysis of the optimization techniques. Consequently, the presented methods may be less reliable and have limited applicability. While there are various models available, previous research suggests that RoBERTa exhibits strong performance on media bias tasks (Wessel et al., 2023;Spinde et al., 2021c). However, due to resource constraints, we were unable to explore models with different architectures and further refine our selection. As the landscape of publicly available highquality datasets for media bias is not as extensive as desired, we acknowledge our inability to capture all manifestations of media bias comprehensively. As mentioned in Section 5, a systematic and comprehensive analysis of the dataset landscape will be part of our future work. Furthermore, although analyzing media bias on a sentence level enables a detailed examination of occurring biases, certain forms of bias, such as bias resulting from the omission of information, extend beyond the linguistic aspects of individual statements. Detecting such biases may require considering an external context. We leave it to future work to investigate other levels of granularity, such as paragraph or article levels. In addition to these technical limitations, conducting a detailed analysis of agreement and label distribution in all utilized datasets will be necessary to make stronger claims about which datasets provide more reliable coverage of the overall concept of media bias. This is particularly important since media bias is a complex phenomenon that is not always easily identified during the annotationgathering process.",
          "Detecting (and as a result also highlighting) media bias instances can mitigate the negative effects of media bias, e.g., on collective decisionmaking (Baumer et al., 2015). However, media bias strongly depends on the context and individuals involved, making it a sensitive issue. Some bias forms depend on factors other than the content, e.g., a different text perception due to a reader's background. Therefore, a public classification of possible polarization and one-sidedness in the news must be performed transparently and reliably. The datasets used for training the classifier must be transparent and, ideally, constantly monitored by independent experts. To address possible implications of MAGPIE or related media bias applications cautiously, it is crucial to constantly control the classifications and datasets with recent findings on the perception of media bias, which is a main part of our ongoing and future work. To do so, we use resources such as, e.g., standardized questionnaires on media bias perception (Spinde et al., 2021a). When bias detection is balanced and transparent, it can positively affect collective opinion formation and decision-making. We see no immediate negative ethical or societal impacts of our work. However, in addition to system transparency, we want to highlight that we believe it to be required for future datasets to report in greater detail about a manipulation protection strategy when developing, training, and presenting any media bias classifier. To ensure the validity of media bias detection systems, it is essential to prevent participants, especially in public studies, from manipulating algorithms by, for example, flagging neutral content as biased. Therefore, annotations should be compared among multiple users to ensure a higher level of trustworthiness. Most of the datasets available report only limitedly about such strategies. In open or crowdsourcing scenarios, it is important to collect user characteristics and deliberately include specific content that aims to provide obvious answers but may be answered differently when users follow certain patterns. This approach helps in detecting and mitigating potential biases introduced by participants, thereby maintaining the integrity and reliability of the media bias detection process. To ensure and propose stronger standards in the future, we aim to analyze all LBM datasets with regard to potential inherent bias in future work."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "a skewed portrayal of information favoring certain group interests, which manifests in multiple facets, including political, gender, racial, and linguistic biases.",
            "context": "1. Introduction Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "dx.doi.org/https://doi.org/10.1016/j.ipm.2021.102505",
        "sections": [
          "Media has a substantial impact on public perception of events, and, accordingly, the way media presents events can potentially alter the beliefs and views of the public. One of the ways in which bias in news articles can be introduced is by altering word choice. Such a form of bias is very challenging to identify automatically due to the high context-dependence and the lack of a large-scale gold-standard data set. In this paper, we present a prototypical yet robust and diverse data set for media bias research. It consists of 1,700 statements representing various media bias instances and contains labels for media bias identification on the word and sentence level. In contrast to existing research, our data incorporate background information on the participants' demographics, political ideology, and their opinion about media in general. Based on our data, we also present a way to detect bias-inducing words in news articles automatically. Our approach is feature-oriented, which provides a strong descriptive and explanatory power compared to deep learning techniques. We identify and engineer various linguistic, lexical, and syntactic features that can potentially be media bias indicators. Our resource collection is the most complete within the media bias research area to the best of our knowledge. We evaluate all of our features in various combinations and retrieve their possible importance both for future research and for the task in general. We also evaluate various possible Machine Learning approaches with all of our features. XGBoost, a decision tree implementation, yields the best results. Our approach achieves an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79, which outperforms current media bias detection methods based on features. We propose future improvements, discuss the perspectives of the feature-based approach and a combination of neural networks and deep learning with our current system.",
          "News articles in online newspapers are considered a crucial information source that replace traditional media such as television or radio broadcasts and print media, and new sources of information such as social media (Dallmann, Lemmerich, Zoller, & Hotho, 2015). Many people consider such articles a reliable source of information about current events, even though it is also broadly believed and academically confirmed that news outlets are biased (Wolton, 2017). Given the trust readers put into news articles and the significant influence of media outlets on society and public opinion, media bias may potentially lead to the adoption of biased views by readers (Spinde, Hamborg, Donnay, Becerra, & Gipp, 2020a). However, ''unrestricted access to unbiased information is crucial for forming a well-balanced understanding of current events'' (Hamborg, Donnay, & Gipp, 2018). Highlighting media bias instances may have many positive implications and can mitigate the effects of such biases (Baumer, Elovic, Qin, Polletta, & Gay, 2015). While complete elimination of bias might be an unrealistic goal, drawing attention to its existence can not only warn readers that content is biased but also allow journalists and publishers to assess their work objectively (Dallmann et al., 2015). Furthermore, such insights could be very interesting for research projects, e.g., in social science. We want to point out that it is uncertain if and how actual news consumers would like to obtain such information. Only a few systems are already employed to help readers mitigate the consequences of media bias impact. Most of them focus on the aggregation of articles about the same event from various news sources to provide different perspectives (Lim, Jatowt, & Yoshikawa, 2018a). For example, the news aggregator Allsides 1 allows readers to compare articles on the same topic from media outlets known to have different political views. Various media bias charts, such as the Allsides media bias chart, 2 or the Ad Fontes media bias chart 3 provide up-to-date information on media outlets' political slants. The main objective of this paper is to present a prototypical system for the automated identification of bias-inducing words in news articles. In the following chapter, we will give an overview of research on the topic and show major currently existing drawbacks on the issue. They lead us to more fine-grained research contributions. Mainly, we will: 1. create a labeled data set for media bias analysis on different levels; 2. analyze and engineer features potentially indicating biased language; 3. train a classifier to detect bias-inducing words and 4. evaluate the performance. This study holds both theoretical and practical significance. We summarize all existing research to give a full overview of possible classification features for media bias. We also show the relevance of all these features. We provide a data set of media bias annotations. It is the first such data set in the field, reporting word and sentence level annotations and detailed information on annotator characteristics and background. Our current data set already significantly extends available data in this domain, providing a unique and more reliable insight into bias perception. It also offers grounds for future extension. Lastly, we train and present a classifier for biased words that outperforms existing feature-based classifiers for bias. The rest of the paper is organized as follows. Section 2 presents the literature review on media bias and its automated detection. Section 3 details the methodology, and Section 4 presents the results of this study. Finally, in Section 5 we present a discussion of the project and an outlook on future work.",
          "",
          "Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975). On the other hand, bias can also be unintentional and even unconscious (Baumer et al., 2015;Williams, 1975). Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently. In turn, an author can manipulate a reader's perception by implying a particular opinion or perspective or inducing positive or negative emotions. The following two examples present instances of media bias by word choice, respectively: 1. Practicing pro-life litigators know that Trump judges are saving lives by permitting restrictions on abortion to go into effect. 4 2. Tens of millions of children under 12 months are potentially at risk for diseases such as diphtheria and polio as the Chinese coronavirus pandemic interrupts routine vaccinations, according to data published by global public health experts on Friday. 5 In the first example, the author chooses the vaguer word ''pro-life'' to describe the very concrete ''anti-abortion'' position as highly positive. In the second example, labeling the coronavirus pandemic with the word ''Chinese'' implies China's fault in the pandemic.",
          "To the best of our knowledge, there are no tools or systems for automatic identification of media bias based on word choice. The task is a challenging one for several reasons. Firstly, while a vast amount of text data from the news is available, articles naturally are created without such sophisticated labels that could allow us to detect bias inducing words; therefore, existing data are unlabeled. Existing annotated data sets are very small. That means that currently, there is no large-scale gold standard data set for the identification of media bias by word choice (Hamborg et al., 2018). Furthermore, it has been proven that bias identification is a non-trivial task for non-experts (Lim et al., 2018a;Recasens, Danescu-Niculescu-Mizil, & Jurafsky, 2013), which can cause problems while creating such a data set. Secondly, bias words are highly dependent on the context. Therefore, simple presence of specific words is not a reliable indicator of bias (Hube & Fetahu, 2018). 1. An abortion is the murder of a human baby embryo or fetus from the uterus, resulting in or caused by its death (Hube & Fetahu, 2018). 2. In 2008, he was convicted of murder (Hube & Fetahu, 2018). In the first example, the word ''murder'' describes abortion as something highly negative. In the second example, ''murder'' is used to describe a pure fact of murder. In the first case, the word induces bias, whereas, in the second, it does not. Another good example of context dependence of bias is the bidirectionality of the epistemological bias, i.e., this bias can occur in two cases: when a truthful proposition is questioned or when a false or controversial proposition is presupposed or implicated. The word that will not cause bias in the first case, e.g., factive verb, will cause it in the second, and vice versa (Recasens et al., 2013). Finally, media bias and bias in reference works are subtle, implicit, and intricate since, in general, the news is expected to be factual and impartial (Baumer et al., 2015;Hube & Fetahu, 2018;Lim et al., 2018a;Recasens et al., 2013). As fairly noticed by Williams (1975), remarkable bias is not extreme but rather reasonable and plausible: extreme bias is obvious and does not threaten values or institutions. Despite the challenging nature of the task, several researchers attempted to annotate media bias by word choice automatically. We also derive valuable insights for this project from the research attempting to identify the biased language in the related fieldreference sources such as encyclopedias, where neutral language is also desired and required. Lim et al. (2018a) use crowdsourcing to construct a data set consisting of 1235 sentences from various news articles reporting on the same event, namely, the arrest of a black man. The data set provides labels on the articles and word level. The authors then train a Support Vector Machine on the Part of Speech (POS) tags and various handcrafted linguistic features to classify bias on the sentence level, achieving the accuracy of 70%. In related work, Lim, Jatowt, Färber, and Yoshikawa (2020) propose another media bias data set consisting of 966 sentences and containing labels on the sentence level. The data set covers various news about four different events: Donald Trump's statement about protesting athletes, Facebook data misuse, negotiations with North Korea, and a lawmaker's suicide. Baumer et al. (2015) focus on the automated identification of framing in political news and construct a data set of 74 news articles from various US news outlets covering diverse political issues and events. They then train Naïve Bayes on handcrafted features to identify whether a word is related to framing, and achieve 61% accuracy, 34% precision, 70% recall, 0.45-0.46 𝐹 1 -score. Hamborg, Zhukova and Gipp (2019) constructed a data set using content analysis. They created a codebook describing frame properties, coding rules, and examples. The data set consists of 50 news articles from various US news outlets and covers ten political events. The authors distinguish the target concept and phrases framing this concept, and define a number of framing properties, e.g., ''affection'', ''aggression'', ''other bias'', etc. The authors then automatically extract the candidates for target concepts and identify frames by looking for words semantically similar to the previously defined framing properties via exploiting word embeddings properties. Then, identified framing properties are assigned to the candidates via dependency parsing. The authors achieve an F1-score of 45.7%. Fan et al. (2019) create the data set BASIL, annotated by two experts, covering diverse events and containing lexical and informational bias. The data set allows analysis at the token level and relatively to the target, but only 448 sentences are available for lexical bias. Then, they employ BERT lexical sequence tagger to identify lexical and informational bias at the token level and achieve an 𝐹 1 -score of 25.98%. Chen, Al-Khatib, Wachsmuth, and Stein (2020) create a data set of 6964 articles covering various topics and news outlets containing political bias, unfairness, and non-objectivity labels at the article level. They then train the recurrent neural network to classify articles according to these labels. Finally, the authors conduct a reverse feature analysis and find that, at the word level, political bias correlates with such LIWC categories (Pennebaker, Boyd, Jordan, & Blackburn, 2015) as negative emotion, anger, and affect. Recasens et al. (2013) create static bias lexica based on Wikipedia bias-driven edits due to NPOV (Neutral Point of View) violations. 6 The bias lexicon and a set of various linguistic features are then fed into the logistic regression classifier to predict which words in the sentences are bias-inducing. The authors reached 34.35% to 58.70% accuracy for predicting 1 to 3 potential bias-inducing words in a sentence, respectively. Hube and Fetahu (2018) propose the semi-automated approach to extract domain-related bias words lexicon, based on the word embeddings properties. The authors then feed obtained bias words and other linguistic features into a random forest classifier to detect language bias in Wikipedia at the sentence level. The authors achieve 73% accuracy, 74% precision, 66% recall, and an 𝐹 1score 0.69 on the newly created ground truth based on Conservapedia,foot_1 and state that the approach is generalizable for Wikipedia with a precision of 66%. In their later work, Hube and Fetahu (2019) train a recurrent neural network on a combination of word embeddings and a few handcrafted features to classify bias in Wikipedia at the sentence level and achieve 81.9% accuracy, 91.7% precision, 66.8% recall, and 0.773 𝐹 1 -score. Spinde, Hamborg, andGipp (2020b, 2020c) analyze media bias in German news articles covering the refugee crisis. The three components: an IDF component, a combined dictionary-based component, and a component based on a semantically created bias dictionary, are analyzed to identify bias on the word level. The combination of the dictionary component and the topic-dependent bias word dictionary achieves an 𝐹 1 -score of 0.31, precision of 0.43, and recall of 0.26. The authors point out that considering adjectives separately increased the performance.",
          "",
          "The general workflow of a prototypical system for automated identification of bias-inducing words in news articles is presented in Fig. 1. In our work, we start from collecting the sentences and gathering annotations via a crowdsourcing process (1). We then obtain various features (3) described in more detail in Section 3.5. One of the features is a bias lexicon built semi-automatically by computing words similar to potential bias words using outlet-specific word embeddings (2). We then train a supervised classifier on our engineered features and annotated labels (4). After the best model is selected and optimized, we evaluate the performance of the feature-based approach for detection of media bias. Furthermore, we evaluate all features individually (5).",
          "One of the challenges in the automated detection of media bias is the lack of a gold standard large-scale data set with labeled media bias instances. The existing data sets described in Section 2.2 either do not allow for the analysis of media bias on the word level or can induce drawbacks due to the following limitations: (1) they only include a few topics (Lim et al., 2020(Lim et al., , 2018a)), (2) they mostly focus exclusively on framing (Baumer et al., 2015;Hamborg, Zhukova et al., 2019), (3) annotations are target-oriented (Fan et al., 2019;Hamborg, Zhukova et al., 2019), (4) annotations are not on the word level (Lim et al., 2018a), or (5) training data are too small (Fan et al., 2019). Therefore, we decided to create a diverse, robust, and extendable data set to identify media bias. We hand-picked 1.700 sentences from around 1.000 articles. According to our collection strategy, most of the sentences should contain media bias instances, while the smaller number of sentences should be neutral. However, the final annotations are made by the crowd-source annotators. The sentences equally represent the full political spectrum since we used the articles from the major left and right-wing outlets, classified by their political ideology within the media bias ratings of www.allsides.com. We covered 14 different topics (selected randomly out of a variety of possible topics), from very contentious (e.g., abortion, elections) to less contentious topics (e.g., student debt, sport). To gather annotations of the sentences and the words, we developed our own survey platform, combining classical survey functionality with text annotations, and hired participants via Amazon Mechanical Turk to complete microtasks. Annotation quality ensured by experts is often preferable, but we expressively wanted to collect a large number of annotations from non-experts. It has been shown that many complex problems can be resolved successfully through crowdsourcing if the existing crowdsourcing platforms are used in combination with appropriate management techniques and quality control (Mitrović, 2013;Mladenović, Mitrović, & Krstev, 2016)  8 . Seven hundred eighty-four annotators participated in the survey, all located in the United States. The vast majority (97.1%) of the annotators were native English speakers, 2.8% were near-native speakers. The annotators from diverse age groups participated in the survey; people from 20 to 40 years old (67.4%) prevail over other age groups. The annotators' gender is balanced between females (42.5%) and males (56.5%). The annotators have a diverse educational background; more than half have higher education. The political orientation is not well balanced: liberal annotators prevail (44.3%) over conservative annotators (26.7%) and annotators from the center (29.1%). The vast majority of the annotators read the news sometimes, more than half -one (46.4%) or more (23.1%) times per day. Each annotator received 20 randomly reshuffled sentences. We showed each sentence to ten annotators. Within our platform,foot_3 we first instruct participants about the general goals of the study. We explain the tasks in detail and ask them to leave aside their personal views. We also give them a few examples of bias and ask a control question to check whether participants understood media bias's general concept. If the control question was not answered correctly, participants had to reread the instructions. Within the annotation task itself, we provide detailed instructions on the workflow. We then ask each annotator to highlight words or phrases that induce bias according to the provided instructions. After that, we ask them to annotate the whole sentence as biased or impartial, and whether they would describe it as opinionated, factual, or mixed. To the best of our knowledge, our data set is the first in the research area to collect detailed background demographic information about the annotators, such as gender, age, education, English proficiency, but also information on political affiliation and news consumption. Overall, our data set allows for performing three different tasks: bias identification on the word level, sentence level, and a classification of the sentence as being opinionated, factual, or a mixture of both. We discuss the results of the annotation in Section 4.1.",
          "As one of our features, we present a lexicon of biased words, built explicitly for the news domain (Hube & Fetahu, 2018). Interestingly, although such a lexicon cannot serve as an exhaustive indicator of media bias due to high context-dependence (Hube & Fetahu, 2019), it can potentially serve as a useful feature of a more complex media bias detection system. To extract a biased word lexicon of high quality, we replicate the method proposed by Hube and Fetahu (2018). The authors proposed a semi-automated way to automatically extract biased words from corpora of interest using word embeddings. We present the whole pipeline of the approach in Fig. 2. We first manually create a list of words that describe contentious issues and concepts. Then, we use this list to manually select ''seed'' biased words in the two separate word embedding spaces trained on news articles potentially containing a high number of biased words. We select seed biased words among the words that have high cosine similarity to the words describing contentious issues. We publish our list of seed biased words at https://bit.ly/36guHdu. 10We assumed that news outlets with presumably stronger political ideology would use bias words when describing contentious issues with a higher likelihood than neutral mediums. To capture both liberal and conservative biases, we train word embeddings separately on the corpora of news articles from HuffPost and Breitbart, respectively. In the choice of the outlets, we relied on the information provided by Allsides: both outlets are presented at the media bias chart,foot_5 and for both outlets, the confidence level of the assigned ratings is high.foot_6 Noteworthy, these two sources are also ones of the most popular media sources that left-and right-leaning communities share respectively in Soliman, Hafer, and Lemmerich (2019). The articles from both sources, published from 2010 to 2020, are scraped from Common Crawlfoot_7 using NewsPlease (Hamborg, Meuschke, Breitinger and Gipp, 2017). We split the initial text into lower-cased tokens, remove punctuation marks and numbers, and train Word2Vec word embeddings (Mikolov, Chen, Corrado, & Dean, 2013). The chosen hyper-parameters are summarized in Table 1. Since evaluation of such an unsupervised task as word embeddings creation is quite challenging (Bakarov, 2018), we choose the hyper-parameters based on the existing research (Spinde, Rudnitckaia, Hamborg, & Gipp, 2021a). The number of dimensions is set to 300 and is not increased further due to the scarcity of the training data (Mikolov et al., 2013). The window size is set to 8 since  larger window size can capture broad topical content (Levy, Goldberg, & Dagan, 2015). We increase the number of iterations to 10 since the size of training data is small and cannot be increased. In the scope of this project, it is important to avoid unstable low-qualitative vectors, therefore words appearing less than 25 times are excluded. Finally, treating n-grams as single units may lead to better training of a given model (Camacho-Collados & Pilehvar, 2018). We use the default scoring for n-grams generation and run two passes over training data. The thresholds for n-grams inclusion are based on manual analysis of the generated n-grams. The rest of hyper-parameters are set to the default values. As a next step, we divide the set of seed biased words into random batches consisting of ten words and repeat this process ten times to create batches with various combinations of words. Then, for each batch, the average vector in the word embedding space trained on a 100 billion Google news data setfoot_8 is calculated. For each average vector, we extract the top 100 words close to this average vector. Hube and Fetahu (2018) do not reshuffle words in batches and extract the top 1000 words. However, the average cosine similarity of farthest words among the top 1000 is 0.47, whereas the average cosine similarity of farthest words among the top 100 is 0.52. Besides, extracting the top 1000 words introduces noise. Finally, we add extracted words to the resulting bias word lexicon, and remove duplicates.",
          "We define bias-inducing words detection as a binary classification problem where we have only two mutually exclusive classes: whether a word is biased (class 1) or not (class 0). With our binary classifier, and in the context of media bias by word choice, no exhaustive set of precise media bias characteristics exist. Therefore, we combine different linguistic features of biased language proposed by Recasens et al. (2013) and a variety of other syntactic and lexical features (Lim et al., 2020). As the context is highly important when distinguishing between unbiased and biased words, we attempt to capture useful information from context by including collective features adding two previous and two following words into a word's feature vector. We admit that such a way T. Spinde et al.",
          "The complete set of features used in our approach for detecting biased words.",
          "POS tags POS tag indicating the syntactic role of each word, e.g., noun, adverb, etc. Honnibal and Montani (2017).",
          "Dependencies revealing how words in the text relate to each other, e.g., whether a word is a root, object, or subject (Bird, Loper, & Klein, 2009;Honnibal & Montani, 2017).",
          "Named entities, e.g., persons, organizations, locations, etc. Bird et al. (2009) and Honnibal and Montani (2017).",
          "Norms of GloVe word embedding vectors pre-trained on the Common Crawl a Honnibal and Montani (2017). TF-IDF Frequency of the term in a document and in the whole article collection (Lim, Jatowt, & Yoshikawa, 2018b;Pedregosa et al., 2011).",
          "Word is a report/implicative/assertive/factive/positive/negative word, is strongly or weakly subjective, or a hedge (Recasens et al., 2013).",
          "Classifications as kill verb (Greene & Resnik, 2009), hyperbolic term (Chakraborty, Paranjape, Kakar, & Ganguly, 2016), boosters, and attitude markers (Hyland, 2019). LIWC features LIWC features based on psychological and psychometric analysis (Pennebaker et al., 2015). Semi-automated bias lexicon Previously described semi-automatically created bias word lexicon (Hube & Fetahu, 2018). a https://commoncrawl.org, accessed on 2020-10-31. to account for context is not optimal and requires elaboration in future. We compare different combinations of the features, and also train different machine learning classification algorithms, such as logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), complement Naïve bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), decision tree (DT), random forest (RF), XGBoost and a simple neural network -multilayer perceptron (MLP). To the best of our knowledge, we present the first detailed comparison of classifiers for word-level bias detection.",
          "We present our entire feature list in Table 2. In continuation, we describe the individual features and the intuition behind using them for our task. For POS tags, syntactic dependencies, named entity types, word vector norms, and linguistic features, we refer to the previous work on these topics as described by Hube and Fetahu (2018) and Recasens et al. (2013). TF-IDF. Lim et al. (2018b) propose detection of bias using inverse document frequency (IDF) as one of the features to detect media bias, under the assumption that more rarely occurring terms are more likely to be extreme in any direction and are hence more likely to induce bias. As our data set consists of sentences where the terms are probably rarely repeated within one sentence, we adjusted Lim et al.'s assumption (Lim et al., 2018b) slightly by calculating the TF-IDF statistic basing on the whole text of articles the sentences were collected from. LIWC Features. Linguistic Inquiry and Word Count (Pennebaker et al., 2015) is a common approach to analyzing various emotional, cognitive, and structural components in language. It identifies linguistic cues related to psychological processes such as anger, sadness, or social wording. We consider all feature categories from LIWC, as this has shown to be the most effective usage of the resource to identify bias. Additional lexical features. It is not well known which features are the most efficient indicators of media bias (Baumer et al., 2015). Therefore, we test additional features that have been used by researchers to study similar constructs but have not been applied for the detection of media bias yet. According to Greene and Resnik (2009), the so-called ''kill verbs'' together with the relevant grammatical relation (governor or dependent term) cause different sentiment perception. In the following example, the second one is perceived as more negative since it implies an intention (Yano, Resnik, & Smith, 2010): 1. Millions of people starved under Stalin. 2016) study click baits in online news and find that click bait headlines usually include hyperbolic wordswords with highly positive sentiment. We assume that hyperbolic words used in click bait titles to attract readers' attention can be used to emphasize some concepts and induce bias in news articles. Hyperbolic words are, for example, ''absolutely'', ''brilliant'', or ''impossibly''. Hyland (2019) introduces linguistic features that help authors to express their views on the discussed proposition. One such subcategory are boosters -words that express certainty about a particular position, e.g., ''believed'', ''always'', ''no doubt''. In some regard, boosters are opposite to hedges, which, on the contrary, reduce the confidence of a statement. Another subcategory are attitude markers -indicators of the author's expression of affective attitude to statements, e.g., ''fortunately'', ''shockingly'', ''disappointed'', etc. ",
          "",
          "",
          "In the final data set, classes are highly imbalanced (Section 4.1). Since accuracy does not capture the capability of a model to predict rare class correctly (Branco, Torgo, & Ribeiro, 2015), we focus on such evaluation metrics as confusion matrix, precision, recall, F1-score, and receiver operating characteristic area under curve (ROC AUC). We compare the performance of our system to several possible baselines: • Baseline 1 (B1) -a purely random classifier; • Baseline 2 (B2) -occurrence of a word in the negative sentiment lexicon; • Baseline 3 (B3) -occurrence of a word in the negative or positive lexicon; • Baseline 4 (B4) -occurrence of a word in the semi-automated bias lexicon. In our final data set, each observation corresponds to one word, its feature vector (including the collective context features), and the label. We perform 10-fold cross-validation when compare performance of different classifiers, 5-fold cross-validation when optimize hyper-parameters of the selected model, and finally, estimate the final performance on a test set of words that did not participate in training and manually investigate correctly and wrongly classified instances.",
          "",
          "To gain insights into the characteristics of our data set, we analyze it quantitatively and qualitatively. The results of sentences classification are presented in Figs. 3 and 4. The annotation results confirm our data sampling strategy: biased and non-biased statements are not balanced in the data set: biased statements prevail over non-biased statements. Besides, most media bias instances are taken from liberal and conservative news sources, whereas sources from the center were used mainly to retrieve non-biased statements. Note that this does not imply that liberal and conservative news outlets generally experience media bias by word choice and provide opinionated news more often than news outlets from the center. It is valid only due to our data collection scheme. We assigned a biased or impartial label to a sentence if more than half of respondents annotated a sentence as biased or impartial, respectively. 149 sentences could not be labeled due to a lack of agreement between annotators. Many measures for assessing interannotator agreement have been used in similar computational linguistics projects. Based on the way in which we have organized our crowdsourcing workflow, i.e. having 10 annotators per task, we have decided to use Fleiss (1971) to assess the inter-annotator agreement. It represents the task's general difficulty: for example, Hube and Fetahu (2018) reported 𝛼 = 0.124 on word-level bias, and Recasens et al. (2013) reported a 40.73% agreement when looking at only the most biased word in Wikipedia statements. The value of 0.21 that we achieved can be considered as a fair agreement. We assigned an opinionated, factual, or mixed label to a sentence if most respondents annotated a sentence as opinionated, factual, or mixed, respectively. We could not label 174 sentences due to the lack of agreement between annotators. According to our crowdsourced annotations, the data set contains an almost equal number of factual, opinionated, and mixed statements. The annotation scheme for biased words allowed respondents to highlight not only the words but also short phrases. A word was considered biased if at least four respondents highlighted it as biased. On average, a sentence that contains biased words contains two biased words. Out of 31,794 words for training, only 3018 are biased, which constitutes only 9.5% of our current data. The types of words annotated as biased are presented in Table 3.  We observe that annotators select not only extreme and emotional words that can be considered biased even without context, but also context-dependent bias words. For instance, while the word ''Chinese'' is generally not biased it can be in specific contexts, such as ''House Democrats' Chinese coronavirus relief package bails out coastal millionaires and billionaires while ensuring big businesses are able to freely hire illegal aliens and visa overstayers over unemployed Americans''. 15Albeit emphasizing in the instructions that words that are connected to very controversial topics or have very negative sentiment are not necessarily biased, some of such words were still annotated as biased. For example, the term ''neo-Nazis'' in the sentence ''For years now, Fox News has been mainstreaming arguments that used to be the province of fringe websites run by neo-Nazis and other groups who believe the U.S. is meant to be a country of white people and for white people''. 16Furthermore, we find that annotators sometimes fail to mark words as biased if a sentence contains clearly extreme and emotional words. For example, a majority of annotators marked ''cray-cray'' as biased but did not notice ''totally'' in the sentence ''Over the past few decades, RFK Jr.'s famous name has helped him get in the door to talk to important people, and it probably is not long before the person who is all jacked up to meet a Kennedy realizes the guy is totally cray-cray''. 17  As expected, we find a positive correlation between marking sentences as biased and opinionated, and factual and non-biased. Furthermore, more controversial topics are annotated as non-biased, on average, 7.4 p.p. less than less controversial topics. Interestingly, in 49.3% of the sentences labeled as non-biased, annotators still labeled some words as biased. Annotators who estimate themselves as conservative mark 3.76 p.p. more sentences as biased than others who describe themselves as being liberal -except if the sentence stems from a conservative news outlet (Yano et al., 2010). Furthermore, annotators who report that they check news at least sometimes, label sentences as biased 6.85 p.p. more than those who report to check news very rarely, and 19.95 p.p. more than those who report that they never check news.",
          "In this section, we first present the characteristics of the articles we used to train our word embedding models and the performance of the trained word embeddings. We also provide the characteristics of the pre-trained Google News embeddings. We measure semantic word similarity and word analogy (Bruni, Tran, & Baroni, 2014;Finkelstein et al., 2001;Mikolov et al., 2013). Table 4 depicts the results of our measures. Two data sets -WordSim-353 (Finkelstein et al., 2001) and MEN (Bruni et al., 2014) -allow to estimate the Pearson correlation for the semantic similarity between pairs of words in respective word embeddings and as estimated by human assessors. The Google analogy test set (Mikolov et al., 2013) allows to evaluate accuracy. Even though those evaluation data sets are not perfectly suited for our task, the comparison shows that our data sets are large enough to give comparable results to the full Google News data set. We also manually inspected the embeddings' results and confirmed that they capture bias to a reasonable extent. Second, we qualitatively investigate the lexicon of biased words resulting from the semi-automated expansion (Section 3.3). We manually inspected a random sample of 100 words and find that the vast majority (Around 69% in a random sample of 100 words) are negatively connotated, are emotional, and convey strong opinion (Fig. 5). Furthermore, the dictionary consists of disproportionally many rather uncommon words (e.g., ''teapartiers'', ''obamian'', ''eggheaded'', ''mobocracy''), that are only interpretable when knowing about the circumstances they developed in. We find only one word (''similarly'') that cannot directly be related to bias, while we ourselves would classify all 99 other words as being very likely to induce bias. Among 96 words for which POS tag can be identified unambiguously, 41.7% are nouns, 24.0% are verbs, 21.9% are adjectives, and 11.5% are adverbs. Finally, we compare the method of batches developed by Hube and Fetahu (2018) to the naive approach where close words are retrieved for a single seed bias word instead of an average of a batch. We find the employed batched extraction to be superior. Specifically, while both approaches yield a high proportion of biased words, naive approach also yields many words that are not biased but co-occur with biased words, such as ''abortion'', ''personhood'', ''immigrants'', ''mexicans'', etc. Table 5 contrasts extraction for two methods. So far, the lexicon seems to be valuable especially in finding negatively connotated neologisms and words that convey strong opinion even by themselves. Despite being more efficient than the naive approach, the method of batches, nevertheless, still cannot avoid some degree of noisy words and words falsely included as biased. Among such words are misspellings, abbreviations, and words that describe a contentious or a negative issue or concept, e.g., ''xenophobia'', ''criticize'', ''anti-Semite'', ''solipsist''. We will further evaluate the resource in future work.",
          "We first train ''quick and dirty'' models (Gron, 2017) on all available features with default parameters (as implemented in Scikit-Learn (Pedregosa et al., 2011) and XGBoost (Chen & Guestrin, 2016) libraries) and compare the performance based on scores T. Spinde et al. averaged from ten-fold cross-validation (Pedregosa et al., 2011). We compare 𝐹 1 -score, precision, recall, and ROC AUC. Since data are imbalanced (only ≈ 10% are biased), weighting of classes is employed for all methods (where possible). Table 6 shows that no model yields a high 𝐹 1 -score. Instead, best performing models yield either high precision or high recall. Since results of our method are, for now, intended to be verified by a user, we prefer recall over precision while still aiming for moderately high 𝐹 1 -score. We choose XGBoost for further optimization since it achieved both the highest ROC AUC score and the highest 𝐹 1 -score. It also has a relatively high recall: the model predicts more True Positives (biased words as biased) than False Negatives (biased words as unbiased). The model suffers from predicting many False Positives (unbiased words as biased) but to the smaller extent than other models with higher recall (Logistic regression, QDA, NB, SVM). XGBoost is ''a scalable end-to-end tree boosting system'' (Chen & Guestrin, 2016). The algorithm is based on gradient boosting -an ensemble method that adds predictors sequentially to an ensemble, each new one is fit to the residual errors made by the previous one (Gron, 2017). Thus, the final model -a combination of many weak learners -is itself a strong learner. In addition to the fact that XGBoost already achieved best results on our data set, it provides several advantages: it accounts for sparsity caused by one-hot encoding (Chen & Guestrin, 2016), allows for a fine parameter tuning using a computationally efficient algorithm (Bentéjac, Csörgo, & Martínez-Muñoz, 2019), and allows to estimate feature importance since we do not have reliable prior information about the importance of the features (Baumer et al., 2015). We fine-tune five hyper-parameters that help to control for overfitting. 18  For the fine-tuned model, we quantitatively evaluate the performance and feature importance. Table 7 shows that fine-tuning yields an insignificant performance improvement of 𝐹 1 = 1p.p. We find that the model suffers from underfitting since performance 18 We find the following values to be optimal. max-depth = 6, min-child-weight = 18, subsample = 1, colsample-bytree = 1, and eta = .2. Other hyper-parameters are set to the default values. The maximum number of boosting rounds is set to 999, and early stopping is applied if the 𝐹 1 -score for validation set does not increase within ten rounds. The model is weighting the imbalanced classes. The evaluation metric is the 𝐹 1 -score averaged on five-fold cross-validation. is also low on training (𝐹 1 = 0.51) and validation sets (𝐹 1 = 0.50). Comparing XGBoost performance to the defined baselines, we see that XGBoost significantly outperforms the random baseline (B1) but fails to significantly outperform the naive usage of the negative sentiment lexicon (B2). However, when analyzing results in a confusion matrix, we see that using just the negative dictionary, in fact, predicts 53% of biased words incorrectly as non-biased words whereas XGBoost predicts only 23% incorrectly. High 𝐹 1 -score and ROC AUC of baseline B2 is mostly due to the low number of False Positives, but this is a behavior close to simply predicting all words as non-biased. Since we do not have the prior information on which features are the most contributing into media bias detection, we first trained the classifier on the all the available features. When analyzing feature importance, we find that the most important features are the occurrence of a word in a negative lexicon (gain = 1195) and being a proper noun (470). The bias lexicon that we created semi-automatically is among the top 10 important features (gain = 106). Among linguistic features proposed by Recasens et al. (2013) as indicators of bias, only sentiment lexica, subjectivity lexica, and assertive verbs are among the top 30 important features. While report verbs and hedges still have minor importance, factive and implicative verbs have zero importance. We train several models feeding them with features that have different importance. Excluding features with low feature importance does not improve the performance (Table 7). Besides, we test how the model performs when different feature groups are not included. Thus, we train a model with all features except one particular feature or group of features. We notice that the performance drops significantly only when linguistic features are not used, most likely because of the negative sentiment lexicon's high importance. Lastly, we qualitatively investigate automatically detected bias candidates. Examples of correctly classified biased words (TPs) include mostly emotional words that can be considered biased even without context. Words that can be described as causing negative emotions occur more often than those causing positive emotions. 12.5% of TPs correctly indicate less obvious bias, which is most likely generally more rare. The following examples illustrate (1) obvious negative bias, (2) obvious positive bias, and (3) slightly more subtle bias among correctly classified words. 1. Large majorities of both parties seem to like the Green New Deal, despite efforts by Fox News to paint it as disastrous. 19  2. Right-wing media sprung into action to try to discredit her, of course, by implying that a woman who graduated summa cum laude with an economics degree is a bimbo and with Twitchy using a screenshot to make the usually genial Ocasio-Cortez somehow look like a ballbuster. 20 3. As leading 2020 Dems advocate spending big on the Green New Deal, it turns out most Americans are worried about other issues. 21   Examples of biased words incorrectly classified as non-biased (FNs) include words that are (1) parts of phrases, (2) ambivalent as to whether they are biased, (3) not generally biased but only in a particular context, (4) mistakes in the annotation, and random misclassifications: 1. By threatening the kids and their families with deportation, the administration's U.S. Citizenship and Immigration Services was effectively delivering death sentences. 22 19 https://www.alternet.org/2019/04/just-a-cover-for-sexism-and-white-nationalism-paul-krugman-explains-why-the-rights-attacks-on-new-democratic-  lawmakers-are-bogus/, accessed on 2020-10-31. 20 https://www.alternet.org/2019/01/alexandria-ocaio-cortez-is-absolutely-right-there-shouldnt-be-any-billionaires/, accessed on 2020-10-31. 21 https://fxn.ws/370GuwZ, accessed on 2020-10-31. 22 https://www.msnbc.com/rachel-maddow-show/trump-admin-backs-plan-deport-critically-ill-children-msna1280326, accessed on 2020-10-31. T. Spinde et al. 2. When the Muslim ban was first enacted, it triggered chaos at airports and prompted widespread protest and legal challenges, and it continues to impose devastating costs on families and people who wish to come to the U.S. 23 3. The specter of ''abortion regret'' has been used by lawmakers and judges alike to impose or uphold rules making it harder for people to get abortions. 24 4. Gun enthusiasts cannot admit that they like firearms because they fear black people. 25   Examples of non-biased words misclassified by the model as biased (FPs) include words that (1) are ambivalent as to whether they are biased, (2) describe negative or contentious issues, (3) are due to erroneous annotation, and (4) random misclassifications: 1. Justice Sonia Sotomayor, in her dissent, accused the majority of weaponizing the First Amendment -an unconscionable position for a person tasked with ''faithfully and impartially'' discharging the duty to protect the inherent rights of all Americans. 26 2. He also denounced the policy of Chancellor Angela Merkel and the attitude of the German media, which ''are constantly pushing'' for Europe to welcome more and more migrants, in opposition to the will of the Hungarian people. 27 3. Michelle Williams won a Golden Globe for her role in ''Fosse/Verdon'' on Sunday night, but perhaps her biggest moment came during her acceptance speech when she defended abortion rights and encouraged women to vote ''in your own self-interest''. 28 4. The case was sent back to lower courts to determine whether the gun owners may seek damages or press claims that the amended law still infringes their rights. 29",
          "One of the main contribution of our work is the creation and annotation of a robust and diverse media bias data set. Other than already existing studies on the topic, our data contain background information about the annotators, increasing our results' transparency and reliability. We perform visual analysis and observe the following findings. Topics that are less controversial are annotated as non-biased slightly more often than very controversial topics. Conservative annotators perceive statements as biased slightly more often than liberal annotators, but for both, it is only true unless the statement is from a conservative media outlet. We also find that annotators who read news never or very rarely are less likely to annotate statements as biased. Besides, our annotation results show the connection of bias and opinion. Even though our data set was developed on a small scale and cannot serve as a gold standard for the field of media bias research, it offers a complete framework for further data extension, especially in combination with our specifically developed survey platform. We plan to extend the data set in the future. While using crowdsourcing gave us an insight into the perception of bias by a broad audience, some related issues could not be resolved, e.g., the submission of random words. Furthermore, even honest workers made mistakes because the identification of media bias is, in general, not a trivial task, especially for non-experts (Lim et al., 2018a;Recasens et al., 2013). We will follow a dual strategy in future work: While extending the existing data set, we will develop an expert annotation guideline and evaluate the same sentences in cooperation with experts in the field. We will also test the difference between the annotation of single, isolated sentences, and sentences within an article's scope. Lastly, we will evaluate from a psychological perspective how different types of questioning affect the perception of bias, and have already collected around 500 varying questions on the issue. Regarding the quality of the annotations, our main strategy is to exclude noise from the final labels by setting up a threshold of the number of annotators required to label a word as biased or not. However, after manual analysis of final annotations, we find that setting up any strict threshold will introduce some noise and result in some words being omitted. The threshold of four is the most reasonable, but we admit that some words are omitted, and some non-biased words are included. We will experiment with more annotators per sentence to see whether we can reduce the percentage of errors. Our semi-automatically created bias lexicon is indeed able to find emotional words and words that convey a strong opinion. However, we conclude that while capturing emotional and negative opinionated words, the lexicon is unlikely to be exhaustive. So far, the approach lacks an additional method on how to expand the lexicon without adding non-biased words. Overall, our prototypical system achieves an 𝐹 1 -score of 0.43, precision of 0.29, recall of 0.77, and ROC AUC of 0.79. Our data set is the largest and most transparent in the area to date and our classifier is the first built on these data, making a direct comparison to other methods unfeasible. On their respective data sets, researchers who detected media bias on the word level achieved an T. Spinde et al. F1-score of 0.26 (Fan et al., 2019) and 0.31 (Spinde et al., 2020b); researchers, who detected framing on the word level, achieved an F1-score of 0.45-0.46 (Baumer et al., 2015;Hamborg, Zhukova et al., 2019). We present the most complete collection of features for classification to date, extending the work of Hube and Fetahu (2018) and Recasens et al. (2013). Especially Boosters were not used in previous research, but are among the most important features. We will continue our detailed analysis of feature importance for the overall task with our larger crowdsourced data set and the expert data. We will also improve the quality of our features. For example, for implicative verbs, Pavlick and Callison-Burch (2016) introduced a method to automatically predict implicativeness of a verb based on the known constraints on the tense of implicative verbs. We could also expand our sentiment and subjectivity lexicons by using WordNet, a de facto lexico-semantic network (Fellbaum, 1998;Miller, 1995), or SentiWordNet 3.0, a lexical resource that assigns sentiment scores to each synset of WordNet (Baccianella, Esuli, & Sebastiani, 2010). While recognizing around 77% of biased words correctly, our approach misclassifies around 20% of non-biased words. Due to the classes' imbalance, 20% of the misclassified majority class significantly decreases overall performance. In this section, we discuss the drawbacks of the implementation that lead to low performance. Especially words that are biased only in a particular context are rarely classified correctly, highlighting how media bias is usually very subtle and context-dependent. However, so far, we only accounted for context by using one collective feature for the window of four words surrounding the word. Overall, we believe that the feature-based approach is especially valuable because of its explanatory character, relating bias to specific features, which is impossible with automated feature extraction. It is also not as dependent on the amount of data as a neural network. However, we will integrate deep learning into our approach, as such an architecture especially helps to account for inter-dependencies between words. Both methods can also be combined, with our specific features giving meaning to the identification of words with automatically identified features. We will also investigate whether the classifier works better on the political left, right, or center sources. We will also determine whether we can distinguish bias for any particular ideology in our overall vocabulary of biased words. In this paper, we propose an approach to identifying media bias using an automatic feature-based classification. To evaluate our method, we also present a 1700-sentence data set of crowdsourced biased word annotations, where we show each sentence to ten survey participants. For the first time in the research area, we report each person's background and make our data more transparent and robust. We extend existing feature sets for the task and especially evaluate each feature in detail. We also experiment with different classifiers, with our final choice returning an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79. Our results slightly outperform existing methods in the area. To increase our results further, we show how we can improve our data and features in future work and how neural networks could be a suitable option to combine with our method, even though some drawbacks have to be overcome. We publish our code and current system at https://github.com/Media-Bias-Group/Automa-CRediT authorship contribution statement Timo Spinde: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Lada Rudnitckaia: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Jelena Mitrović: Helped to implement and completely financed the successful survey execution. Felix Hamborg: Wrote the discussion chapter. Michael Granitzer: Gave permanent expert feedback on our work. Bela Gipp: Gave permanent expert feedback on our work. Karsten Donnay: Helped to implement and completely financed the successful survey execution."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "slanted news coverage or internal bias, reflected in news articles.",
            "context": "2.1. Media bias Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975).",
            "type": "explicit"
          },
          "bias by word choice": {
            "definition": "when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept.",
            "context": "Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "4d65bab6f9b1fd61885fe187e6014a89592df187",
        "sections": [
          "Openness and transparency have long been considered key pillars of the scientific ethos (Merton 1973). Yet there is growing awareness that current research practices often deviate from this ideal, and can sometimes produce misleading bodies of evidence (Miguel et al. 2014). As we survey in this article, there is growing evidence documenting the prevalence of publication bias in economics and other scientific fields, as well as specification searching, and widespread inability to replicate empirical findings. Though peer review and robustness checks aim to reduce these problems, they appear unable to solve the problem entirely. While some of these issues have been widely discussed within economics for some time (for instance, see Leamer 1983;Dewald, Thursby, and Anderson 1986;DeLong and Lang 1992), there has been a notable recent flurry of activity documenting these problems, and also generating new ideas for how to address them. The goal of this piece is to survey this emerging literature on research transparency and reproducibility, and synthesize the insights emerging in economics as well as from other fieldsawareness of these issues has also recently come to the fore in political science (Gerber, Green, and Nickerson 2001;Franco, Malhotra, and Simonovits 2014), psychology (Simmons, Nelson, and Simonsohn 2011;Open Science Collaboration 2015), sociology (Gerber and Malhotra 2008a), finance (Harvey, Liu, and Zhu 2015), and other research disciplines as well, including medicine (Ioannidis 2005). We also discuss productive avenues for future work. With the vastly greater computing power of recent decades and the ability to run a nearly infinite number of regressions (Sala-i-Martin 1997), there is renewed concern that nullhypothesis statistical testing is subject to both conscious and unconscious manipulation. At the same time, technological progress has also facilitated various new tools and potential solutions, including by streamlining the online sharing of data, statistical code, and other research materials, as well as the creation of easily accessible online study registries, data repositories, and tools for synthesizing research results across studies. Data-sharing and replication activities are certainly becoming more common within economics research. Yet, as we discuss below, the progress to date is partial, with some journals and fields within economics adopting new practices to promote transparency and reproducibility and many others not (yet) doing so. The rest of the paper is organized as follows: Section 2 focuses on documenting the problems, first framing them with a simple model of the research and publication process (subsection 2.1), then discussing publication bias (subsection 2.2), specification searching (subsection 2.3), and the inability to replicate results (subsection 2.4). Section 3 focuses on possible solutions to these issues: improved analytical methods (subsection 3.1), study registration (subsection 3.2) and pre-analysis plans (subsection 3.3), disclosure and reporting standards (subsection 3.4), and open data and materials (subsection 3.5). Section 4 discusses future directions for research as well as possible approaches to change norms and practices, and concludes.",
          "Multiple problems have been identified within the body of published research results in economics. We focus on three that have come under greater focus in the recent push for transparency: publication bias, specification searching, and an inability to replicate results. Before describing them, it is useful to frame some key issues with a simple model.",
          "A helpful model to frame some of the issues discussed below was developed in the provocatively titled \"Why Most Published Research Findings Are False\" by Ioannidis (2005), which is among the most highly cited medical research articles from recent years. Ioannidis develops a simple model that demonstrates how greater flexibility in data analysis may lead to an increased rate of false positives and thus incorrect inference. Specifically, the model estimates the positive predictive value (PPV) of research, or the likelihood that a claimed empirical relationship is actually true, under various assumptions. A high PPV means that most claimed findings in a literature are reliable; a low PPV means the body of evidence is riddled with false positives. The model is similar to that of Wacholder et al. (2004), which estimates the closely related false positive report probability (FPRP). 2For simplicity, consider the case in which a relationship or hypothesis can be classified in a binary fashion as either a \"true relationship\" or \"no relationship\". Define Ri as the ratio of true relationships to no relationships commonly tested in a research field i (e.g., development economics). Prior to a study being undertaken, the probability that a true relationship exists is thus Ri/(Ri+1). Using the usual notation for statistical power of the test (1-β) and statistical significance level (α), the PPV in research field i is given by: (eqn. 1) PPVi = (1-β)Ri⁄((1-β)Ri + α) Clearly, the better powered the study, and the stricter the statistical significance level, the closer the PPV is to one, in which case false positives are largely eliminated. At the usual significance level of α = 0.05 and in the case of a well-powered study (1 -β=0.80) in a literature in which half of all hypotheses are thought to be true ex ante (Ri = 0.5), the PPV is relatively high at 89%, a level that would not seem likely to threaten the validity of research in a particular economics subfield. However, reality is considerably messier than this best case scenario and, as Ioannidis describes, this could lead much high rates of false positives in practice due to the presence of underpowered studies, specification searching and researcher bias, and the possibility that only a subset of the analysis in a research literature is published. We discuss these extensions in turn. We start with the issue of statistical power. Doucouliagos and Stanley (2013), Doucouliagos, Ioannidis, and Stanley (2016), and others have documented that many empirical economics studies are actually quite underpowered. With a more realistic level of statistical power for many studies, say at 0.50, but maintaining the other assumptions above, the PPV falls to 83%, which is beginning to potentially look like more of a concern. For power = 0.20, fully 33% of statistically significant findings are false positives. This concern, and those discussed next, are all exacerbated by bias in the publication process. If all estimates in a literature were available to the scientific community, researchers could begin to undo the concerns over a low PPV by combining data across studies, effectively achieving greater statistical power and more reliable inference, for instance, using meta-analysis methods. However, as we discuss below, there is growing evidence of a pervasive bias in favor of significant results, in both economics and other fields. If only significant findings are ever seen by the researcher community, then the PPV is the relevant quantity for assessing how credible an individual result is likely to be. Ioannidis extends the basic model to account for the possibility of what he calls researcher bias. Denoted by u, researcher bias is defined as the probability that a researcher presents a non-finding as a true finding, for reasons other than chance variation in the data. This researcher bias could take many forms, including any combination of specification searching, data manipulation, selective reporting, and even outright fraud; below we attempt to quantify the prevalence of these behaviors among researchers. There are many checks in place that attempt to limit this bias, and through the lens of empirical economics research, we might hope that the robustness checks typically demanded of scholars in seminar presentations and during journal peer review manage to keep the most extreme forms of bias in check. Yet we believe most economists would agree that there remains considerable wiggle room in the presentation of results in practice, in most cases due to behaviors that fall far short of outright fraud. Extending the above framework to incorporate the researcher bias term (ui) in field i leads to the following expression: (eqn. 2) PPVi = ((1 -β)Ri + uiβRi)/( (1 -β)Ri + α + uiβRi + ui(1-α)) Here the actual number of true relationships (the numerator) is almost unchanged, though there is an additional term that captures the true effects that are correctly reported as significant only due to author bias. The total number of reported significant effects could be much larger due to both sampling variation and author bias. If we go back to the case of 50% power, Ri = 0.5, and the usual 5% significance level, but now assume that author bias is low at 10%, the PPV falls from 83% to 65%. If 30% of authors are biased in their presentation of results, the PPV drops dramatically to 49%, meaning that nearly half of reported significant effects are actually false positives. In a further extension, Ioannidis examines the case where there are ni different research teams in a field i generating estimates to test a research hypothesis. Once again, if only the statistically significant findings are published, so there is no ability to pool all estimates, then the likelihood that any published estimate is truly statistically significant can again fall dramatically. In Table 1 (a reproduction of Table 4 from Ioannidis (2005), we present a range of parameter values and the resulting PPV. Different research fields may have inherently different levels of the Ri term, where presumably literatures that are in an earlier stage and thus more exploratory presumably have lower likelihoods of true relationships. This simple framework brings a number of the issues we deal with in this article into sharper relief, and contains a number of lessons. Ioannidis (2005) himself concludes that the majority of published findings in medicine are likely to be false, and while we are not prepared to make a similar claim for empirical economics research -in part because it is difficult to quantify some of the key parameters in the model -we do feel that this exercise does raise important concerns about the reliability of findings in many literatures. First off, literatures characterized by statistically under-powered (i.e., small 1-β) studies are likely to have many false positives. A study may be under-powered both because of small sample sizes, and if the underlying effect sizes are relatively small. A possible approach to address this concern is to employ larger datasets or estimators that are more powerful. Second, the hotter a research field, with more teams (ni) actively running tests and higher stakes around the findings, the more likely it is that findings are false positives. This is both due to the fact that multiple testing generates more false positives (in absolute numbers) and also because author bias (ui) may be greater when the stakes are higher. Author bias is also a concern when there are widespread prejudices in a research field, for instance, against publishing findings that contradict core theoretical concepts or assumptions. Third, the greater the flexibility in research design, definitions, outcome measures, and analytical approaches in a field, the less likely the research findings are to be true, again due to a combination of multiple testing concerns and author bias. One possible approach to address this concern is to mandate greater data sharing so that other scholars can assess the robustness of results to alternative models. Another is through approaches such as pre-analysis plans that effectively force scholars to present a certain core set of analytical specifications, regardless of the results. With this framework in mind, we next present empirical evidence from economics and other social science fields regarding the extent of some of the problems and biases we have been discussing, and then in section 3 turn to potential ways to address them.",
          "Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors. If the research community is unable to track the complete body of statistical tests that have been run, including those that fail to reject the null (and thus are less likely to be published), then we cannot determine the true proportion of tests in a literature that reject the null. Thus it is critically important to understand how many tests have been run. The term \"file drawer problem\" was coined decades ago (Rosenthal 1979) to describe this problem of results that are missing from a body of research evidence. The issue was a concern even earlier, see, for example, Sterling (1959) which warned of \"embarrassing and unanticipated results\" from type I errors if not significant results went unpublished. Important recent research by Franco, Malhotra, and Simonovits (2014) affirms the importance of this issue in practice in contemporary social science research. They document that a large share of empirical analyses in the social sciences are never published or even written up, and the likelihood that a finding is shared with the broader research community falls sharply for \"null\" findings, i.e., that are not statistically significant (Franco, Malhotra, and Simonovits 2014). Cleverly, the authors are able to look inside the file drawer through their access to the universe of studies that passed peer review and were included in a nationally representative social science survey, namely, the NSF-funded Time-sharing Experiments in the Social Sciences, or TESS3 . TESS funded studies across research fields, including in economics, e.g., Walsh, Dolfin, and DiNardo (2009) and Allcott and Taubinsky (2015), as well as political science, sociology and other fields. Franco, Malhotra, and Simonovits successfully tracked nearly all of the original studies over time, keeping track the nature of the empirical results as well as the ultimate publication of the study, across the dozens of studies that participated in the original project. They find a striking empirical pattern: studies where the main hypothesis test yielded null results are 40 percentage points less likely to be published in a journal than a strongly statistically significant result, and a full 60 percentage points less likely to be written up in any form. This finding has potentially severe implications for our understanding of findings in whole bodies of social science research, if \"zeros\" are never seen by other scholars, even in working paper form. It implies that the positive predictive value (PPV) of research is likely to be lower than it would be otherwise, and also has negative implications for the validity of meta-analyses, if null results are not known to the scholars attempting to draw broader conclusions about a body of evidence. Figure 1 reproduces some of the main patterns from Franco, Malhotra, and Simonovits (2014), as described in (Mervis 2014b). Consistent with these findings, other recent analyses have documented how widespread publication bias appears to be in economics research. Brodeur et al. (2016)  (i.e., above 1.96 for t-statistics). This is a strong indication that some combination of author bias and publication bias is fairly common. Using a variety of possible underlying distributions of test statistics, and estimating how selection would affect these distributions, they estimate the residual (\"the valley and the echoing bump\") and conclude that between 10 to 20% of marginally significant empirical results in these journals are likely to be unreliable. They also document that the proportion of misreporting appears to be lower in articles without \"eye-catchers\" (such as asterisks in tables that denote statistical significance), as well as in papers written by more senior authors, including those with tenured authors. A similar pattern strongly suggestive of publication bias also appears in other social science fields including political science, sociology, psychology, as well as in clinical medical research. Gerber and Malhotra (2008a) have used the caliper test, which compares the frequency of test statistics just above and below the key statistical significance cutoff, which is similar in spirit to a regression discontinuity design. Specifically, they compare the number of z-scores lying in the interval [1.96 -𝑋𝑋%, 1.96] to the number in (1.96, 1.96 + 𝑋𝑋%], where X is the size of the caliper, and they examine these differences at 5%, 10%, 15%, and 20% critical values.4  These caliper tests are used to examine reported empirical results in leading sociology journals (the American Sociological Review, American Journal of Sociology, and The Sociological Quarterly) and reject the hypothesis of no publication bias at the 1 in 10 million level (Gerber and Malhotra 2008a). Data from two leading political science journals (the American Political Science Review and American Journal of Political Science) reject the hypothesis of no publication bias at the 1 in 32 billion level (Gerber and Malhotra 2008b). Psychologists have recently developed a related tool called the \"p-curve,\" describing the density of reported p-values in a literature, that again takes advantage of the fact that if the null hypothesis were true (i.e., no effect), p-values should be uniformly distributed between 0 and 1 (Simonsohn, Nelson, and Simmons 2014a). Intuitively, under the null of no effect, a p-value <0.08 should occur 8% of the time, a p-value <0.07 occurs 7% of the time, etc., meaning a pvalue between 0.07 and 0.08, or between any other 0.01-wide interval, should occur 1% of the time. In the case of true non-zero effects, the distribution of p-values should be right-skewed (with a decreasing density), with more low values (0.01) than higher values (0.04) (Hung et al. 1997). 5 In contrast, in bodies of empirical literature suffering from publication bias, or \"phacking\" in their terminology, in which researchers evaluate significance as they collect data and only report results with statistically significant effects, the distribution of p-values would be leftskewed (assuming that researchers stop searching across specifications or collecting data once the desired level of significance is achieved). To test whether a p-curve is right or left skewed, one can construct what the authors call a \"pp-value,\" or p-value of the p-value-the probability of observing a significant p-value at least as extreme if the null were true-and then aggregate the pp-values in a literature with Fisher's method and test for skew with a χ 2 test. The authors also suggest a test of comparing whether a p-curve is flatter than the curve that would result if studies were (somewhat arbitrarily) powered at 33%, and interpret a p-curve that is significantly flatter or left skewed than this as lacking in evidentiary value. The p-curve can also potentially be used to correct effect size estimates in literatures suffering from publication bias; corrected estimates of the \"choice overload\" literature exhibit a change in direction from standard published estimates (Simonsohn,   Nelson, and Simmons 2014b). 6   Thanks to the existence of study registries and ethical review boards in clinical medical research, it is increasingly possible to survey nearly the universe of studies that have been undertaken, along the lines of Franco, Malhotra, and Simonovits (2014). Easterbrook et al. (1991) reviewed the universe of protocols submitted to the Central Oxford Research Ethics Committee, and both Turner et al. (2008) and Kirsch et al. (2008) employ the universe of tests of certain anti-depressant drugs submitted to the FDA, and all found significantly higher publication rates when tests yield statistically significant results. Turner et al. found that 37 of 38 (97%) of trials with positive, i.e., statistically significant, results were published, while only 8 of 24 (33%) with null (or negative) results were published; for a meta-meta-analysis of the latter two studies, see Ioannidis (2008). A simple model of publication bias described in McCrary, Christensen, and Fanelli (2015) suggests that, under some relatively strong assumptions regarding the rate of nonpublication of statistically non-significant results, readers of research studies could potentially adjust their significance threshold to \"undo\" the distortion by using a more stringent t-test statistic of 3.02 (rather than 1.96) to infer statistical significance at 95% confidence. They note that approximately 30% of published test statistics in the social sciences fall between these two cutoffs. It is also possible that this method would break down and result in a \"t-ratio arms race\" if all researchers were to use it, so it is mostly intended for illustrative purposes. As an aside, it is also possible that publication bias could work against rejection of the null hypothesis in some cases. For instance, within economics in cases where there is a strong theoretical presumption among some scholars that the null hypothesis of no effect is likely to hold (e.g., in certain tests of market efficiency) the publication process could be biased by a preference among editors and referees for non-rejection of the null hypothesis of no effect. This complicates efforts to neatly characterize the nature of publication bias, and may limit the application of the method in McCrary, Christensen, and Fanelli (2015). Taken together, a growing body of evidence indicates that publication bias is widespread in economics and many other scientific fields. Stepping back, these patterns do not appear to occur by chance, but are likely to indicate some combination of selective editor (and referee) decision-making, the file-drawer problem alluded to above, and/or widespread specification searching (the focus of the next subsection), which is closely related to what the Ioannidis (2005) model calls author bias.",
          "Scholars working in several specific literatures within economics have argued for the presence of considerable publication bias, including labor economics literatures on minimum wage impacts and on the value of a statistical life, and we discuss both briefly here, as well as several other bodies of evidence in economics. Card and Krueger (1995) conducted a meta-analysis of the minimum wage and unemployment literature, and test for the \"inverse-square-root\" relationship between sample size and t-ratio that one would expect if there was a true effect and no publication bias, since larger samples should generally produce more precise estimates (for a given research design). 7 They find that t-statistics from the 15 studies using quarterly data available at the time of writing are actually negatively correlated with sample sizes. A possible explanation is that a structural change in the effect of the minimum wage (a decline over time) has taken place, but the authors consider publication bias and specification searching a more likely explanation. Neumark and Wascher (1998) construct an alternative test for publication bias, which produces an attenuation of the effect size with larger sample sizes (as sample sizes increased over time) that is qualitatively similar to that in Card and Krueger (1995), but Neumark and Wascher thus place more emphasis on the structural change explanation (i.e., actual effects declined over time) and Several studies have also documented the presence of publication bias in the literature estimating the value of a statistical life (VSL). As government regulations in health, environment and transportation are frequently based on this value, accurate estimation is of great public importance, but there is growing consensus that there is substantial publication bias in this literature, leading to a strong upward bias in reported estimates (Ashenfelter and Greenstone 2004). Using the collection of 37 studies in Bellavance, Dionne, and Lebeau (2009), Doucouliagos, Stanley, and Giles (2012) find that correcting for publication bias reduces the estimates of VSL by 70-80% from that produced by a standard meta-analysis regression. Similar analysis shows that, correcting for publication bias, the VSL also appears largely inelastic to individual income (Doucouliagos, Stanley, and Viscusi 2014). An updated analysis of publication bias in the VSL literature (Viscusi 2015) shows that although publication bias is large and leads to meaningfully inflated estimates, he argues much of it may stem from early studies in the literature that used voluntary reporting of occupational fatalities, while more recent studies estimates employing the Census of Fatal Occupational Injuries (CFOI) suffer from less measurement error and tend to produce larger estimates. Evidence for publication bias has been documented in many other economics research literatures, although not in all. See Longhi, Nijkamp, and Poot (2005) and Knell and Stix (2005) for notable examples. Table 2 describes a number of related publication bias studies that might be of interest to readers, but for reasons of space they are not discussed in detail here. In the most systematic approach to date (to our knowledge), (Doucouliagos and Stanley 2013) carry out a meta-meta-analysis of 87 meta-analysis papers (many of which are reported in Table 2), and find that over half of the literatures suffer from \"substantial\" or \"severe\" publication bias, with particularly large degrees of bias in empirical macroeconomics and in empirical research based on demand theory, and somewhat less publication bias in subfields with multiple contested economic theories. The Journal of Economic Surveys has published many meta-regression papers, including a special issue devoted to meta-regression and publication bias (Roberts 2005). The statistical techniques for assessing publication bias are summarized in Stanley (2005), and many of these are applied in the articles listed in Table 2. One common data visualization approach is the use of funnel graphs, see Stanley and Doucouliagos (2010), Light and Pillemer (1984), and our discussion in section 3 below.",
          "Another important issue related to publication bias and null hypothesis testing is the reporting of the magnitude of effect sizes. Although it appears that economics may fare somewhat better than other social science disciplines in this regard, since economics studies typically report regression coefficients and standard errors while articles in some other disciplines (e.g., psychology) have historically only reported p-values, there is some evidence that under-reporting of effect magnitudes is still a concern. In a review in this Journal, McCloskey and Ziliak (1996) find that 70% of full-length American Economic Review articles did not distinguish between statistical and practical significance. Follow-up reviews in 2004 and 2008 conclude that the situation had not meaningfully improved (Ziliak and McCloskey 2004;Ziliak and McCloskey 2008). DeLong and Lang (1992) is an early contribution that addresses the issue of publication of null findings and effect sizes. They show that only 78 of 276 null hypotheses tested in empirical papers published in leading economics journals at the time were not rejected. However, using the uniform distribution of p-values under a true null hypothesis, and the startling lack of published p-values close to 1, they conclude it is likely that practically all economic hypotheses are indeed false. They also conclude that the null results that actually do get published in journals may also result from publication bias: a null result is arguably more interesting if it contradicts previous statistically significant results. DeLong and Lang go on to suggest that since almost all economic hypotheses are false, empirical evidence should pay more attention to practical significance and effect size rather than statistical significance alone, as is too often the case.",
          "While publication bias implies a distortion of a body of multiple research studies, bias is also possible within any given study (for instance, as captured in the author bias term u in Ioannidis (2005)). In the 1980's and 1990's, expanded access to computing power led to rising concerns that some researchers were carrying out growing numbers of analyses and selectively reporting econometric analysis that supported pre-conceived notions -or were seen as particularly interesting within the research community -and ignoring, whether consciously or not, other specifications that did not. One the most widely cited articles from this period is Leamer's (1983), \"Let's Take the Depending on which set of control variables, or covariates, were included (among state median income, unemployment, percent population nonwhite, percent population 15-24 years old, percent male, percent urban, percent of two-parent households, and several others), the main coefficient of interest-the number of murders estimated to be prevented by each executionranges widely on both sides of zero, from 29 lives saved to 12 lives lost. Of the five ways of classifying variables as important or doubtful that Leamer evaluated, three produced a range of estimates that included zero, suggesting that inference was quite fragile in this case. Leamer's recommendation that observational studies employ greater sensitivity checks, or extreme bounds analysis (EBA), was not limited to testing the effect of including different combinations of covariates, as in Leamer (1983). More detailed descriptions of EBA in Leamer (1978) and Leamer and Leonard (1983)  This flexibility was highlighted in Lovell (1983), who shows that with a few assumptions regarding the variance of the error terms, searching for the best k of c explanatory variables means that a coefficient that appears to be significant at the level 𝛼𝛼 � is actually only significant at the level 1 -(1 -𝛼𝛼 �) 𝑐𝑐/𝑘𝑘 . In the case of 𝑘𝑘 = 2 and 5 candidate variables, this risks greatly overstating significance levels, and the risk is massive if there are, say, 100 candidate variables. Lovell (1983) goes on to argue for the same sort of transparency in analysis as Leamer (1983). Denton (1985) expands on Lovell's work and shows that data mining can occur as a collective phenomenon even if each individual researcher tests only one pre-stated hypothesis, if there is selective reporting of statistically significant results, an argument closely related to the file drawer publication bias discussion above (Rosenthal 1979). Related points have been made in other social science fields in recent years. In psychology, Simmons, Nelson, and Simonsohn \"prove\" that listening to the Beatles' song \"When I'm Sixty-Four\" made listeners a year and a half younger (Simmons, Nelson, and Simonsohn 2011). The extent and ease of this \"fishing\" in analysis is also described in political science by Humphreys, Sierra, and Windt (2013)  The greater use of extra robustness checks in applied economics is designed to limit the extent of specification search, and is a shift in the direction proposed by Leamer (1983), but it is unclear how effective these changes are in reducing bias in practice. As noted above, the analysis of 641 articles from three top economics journals in recent years presented in Brodeur et al. (2016) still shows a disturbing two-humped distribution of p-values, with relatively few p-values between 0.10 and 0.25 and far more just below 0.05. Their analysis also explores the correlates behind this pattern, and finds that this apparent misallocation of p-values just below the accepted statistical significant level was less pronounced for articles written by tenured authors, and tentatively find it less pronounced among studies based on randomized controlled trials (suggesting that improved research design itself may partially constrain data mining), but they did not detect any discernible differences in the pattern based on whether the authors had publicly posted the study's replication data in the journal's public archive.",
          "One area of analytical flexibility that appears particularly important in practice is sub-group analysis. In many cases, there are multiple distinct interaction effects that could plausibly be justified by economic theory, and current datasets have a growing richness of potential covariates. Yet it is rare for applied economics studies to mention how many different interaction effects were tested, increasing the risk that only statistically significant false positives are reported. While there are few systematic treatments of this issue in economics, there has been extensive discussion of this issue within medical research, where the use of non-prespecified subgroup analysis is strongly frowned upon. The FDA does not use subgroup analysis in its drug approval decisions (Maggioni et al. 2007). An oft repeated, and humorous, case comes from a trial of aspirin and streptokinase use after heart attacks conducted in a large number of patients (N=17,187). Aspirin and streptokinase were found to be beneficial, except for patients born under Libra and Gemini, for whom there was a harmful (but not statistically significant) effect (ISIS-2 COLLABORATIVE GROUP 1988). The authors included the zodiac subgroup analysis because journal editors had suggested that 40 subgroups be analyzed, and the authors relented under the condition that they could include a few subgroups of their own choosing to demonstrate the unreliability of such analysis (Schulz and Grimes 2005).",
          "",
          "There have been longstanding concerns within economics over the inability to replicate the results of specific published papers. The pioneering example is a project undertaken by the Journal of Money, Credit, and Banking (JMCB) (Dewald, Thursby, and Anderson 1986). The journal launched the JMCB Data Storage and Evaluation Project with NSF funding in 1982, which requested data and code from authors who published papers in the journal. 9 Despite the adoption of an explicit policy of data sharing by the JMCB during the project, only 78% of authors provided data within six months after multiple requests, although this was certainly an improvement over the 34% data sharing rate in the control group, namely, those who published before the new journal policy went into effect. Of the papers that were still under review by the JMCB at the time of the requests for data, one quarter did not even respond to the request, despite the request coming from the same journal considering their paper. The data that was submitted was often an unlabeled and undocumented mess, a problem that has persisted with recent data sharing policies, as discussed below. Dewald, Thursby, and Anderson (1986) attempted to replicate nine empirical papers, and despite extensive assistance from the original authors, they were often unable to reproduce the papers' published results. Little changed for a long time after the publication of this landmark article. A decade later, in a follow-up piece to the JMCB Project published in the Federal Reserve Bank of St. Louis Review, Anderson and Dewald (1994)  failed to comply with the policy of providing data and code, editor Ben Bernanke made the data and code sharing policy mandatory in 2004 (Bernanke 2004;McCullough 2007). The current AER data policy states: \"It is the policy of the American Economic Review to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide to the Review, prior to publication, the data, programs, and other details of the computations sufficient to permit replication. These will be posted on the AER Web site. The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\"10  In addition to all the journals published by the American Economic Association in April 201611 . Table 3 summarizes journal policies regarding data sharing, publication of replications or comments, and funding or conflict of interest disclosures at twelve of the top economics and finance journals (according to Scientific Journal Rankings). There has clearly been considerable progress along all of these dimensions over the past decade, but journal policies remain a mixed bag. Among these leading journals, most but not all now have some data sharing requirements, and are officially open to publishing papers that could be considered \"replications.\"12 There is also greater use of disclosure statements. The AER conducted a self-review and found relatively good, though still incomplete, compliance with its data sharing policy (Glandon 2010). Despite this positive self-assessment, others observers believe that much work remains to ensure greater access to replication data in economics. Recent studies document that fewer than 15 of over 150 articles in the JMCB archive could be replicated; there is typically little to no verification that the data and code submitted to journals actually generate the published results; and the majority of economics journals still have no explicit data sharing requirements ( McCullough, McGeary, and Harrison 2006;Anderson et al. 2008;McCullough 2009). The uneven nature of progress along these dimensions across economics journals is ",
          "The American Economic Association's journal data sharing policy -which has been adopted by several other journals and organizations nearly verbatim, as shown in Table 3 -allows for some exceptions, importantly, for proprietary data. In particular, the policy reads: \"The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\" In practice, this exemption is requested fairly often by empirical researchers, and the rate is increasing over time. During the past decade, the May American Economic Review Papers & Proceedings issue has featured a \"Report of the Editor\" which details the number of submissions to the journal, as well the number of papers published, those with data, and those that were granted exemptions. Figure 2 presents the percentage of papers in each issue of the AER since 2005 (when information becomes available) through 2015. A few patterns are noteworthy. First, proportion of papers that include data has risen over time, starting at roughly 60% and since increasing into the 70-80% range, capturing the shift towards empirical research in the discipline as a whole. During this period, the proportion of papers using data that received exemptions from 22 the data-sharing policy has risen rapidly, from roughly 10% to 40% over time. Thus replication data is in practice not available in practice for nearly half of all empirical papers published in the AER in recent years. There are many common sources of proprietary or otherwise non-sharable data driving this trend. One of the most common is U.S. government data. There are currently 23 Federal Statistical Research Data Centers (RDC), which provide researchers access to sensitive federal government data that cannot simply be shared publicly on a journal website, typically due to individual or corporate privacy concerns (e.g., IRS tax records). 16 We do not believe that research conducted with this data should be penalized in any way, and in fact, studies employing administrative data may be particularly valuable both intellectually and in terms of public policy decisions. However, despite the exemption from data sharing, it would still be useful for researchers (and journals) to make their work as reproducible as possible given the circumstances, for instance, by at least posting the associated statistical code and providing details about how other scholars could gain similar access to the data. Beyond government data, there are, of course, also an increasing number of proprietary datasets created by corporations or other entities that are willing to share sensitive commercial data with researchers, but not with the public at large where similar issues arise. Beyond commercially proprietary or legally restricted government data, there is also the important issue of norms regarding the sharing of original data collected by scholars themselves. Given the years of effort and funding that goes into creating an original dataset, what special intellectual property rights (if any) do scholars involved in generating data have? Economists should be aware of the incentives created by temporary monopoly rights to intellectual property, and in many ways the issues regarding original data collection are closely linked to traditional arguments around granting private patents. Such monopoly rights, even if temporary, could be socially beneficial if they help to drive the creation of innovative new data sources, such as the explosion of original new survey datasets in development economics over the past two decades. Yet we know of no empirical research that discusses the optimal length of such \"research dataset\" patents; this is an area that demands further attention, especially around the optimal length of exclusive access afforded to originators of new data.17  The increasingly common requirement to share data at the time of journal publication is a cause for concern in some fields. For example, in response to a proposal from the International Committee of Medical Journal Editors to require data sharing within six months after the publication of an article (Taichman et al. 2016) an editorial in the leading New England Journal of Medicine caused an outcry when they responded by describing those who do secondary analysis without the co-authorship and cooperation of the original data collecting author as \"research parasites\" (Longo and Drazen 2016). The journal re-affirmed their commitment to data-sharing (Drazen 2016) and published a supporting piece by Senator Elizabeth Warren (Warren 2016), but also a separate piece calling for a longer embargo period after publication: \"2 years after publication of the primary trial results and an additional 6 months for every year it took to complete the trial, with a maximum of 5 years before trial data are made available to those who were not involved in the trial\" (The International Consortium of Investigators for Fairness in Trial Data Sharing 2016). Presumably the increasing \"patent length\" here for each additional year it took to complete data collection is an attempt to reward research effort in collecting unusually rich longitudinal data. Yet these sorts of rules regarding timeframes seem quite ad hoc (to us, at least), further highlighting the need for a more serious examination of how best to balance the research community's right to replicate and extend existing research with scholars' incentives to invest in valuable original data. In political science, many journals have recently adopted policies similar to the AEA policy described above. For example, the current policy of the American Journal of Political Science states: \"In some limited circumstances, an author may request an exemption from the replication and verification policy. This exemption would allow the author to withhold or limit public access to some or all of the data used in an analysis. All other replication materials (e.g., software commands, etc.) still must be provided. The primary reasons for such exemptions are restricted access datasets and human subjects protection.\" 18 We lack data on how often this exemption is granted, however. Additionally, this journal goes much further than economics journals in one important way: instead of simply collecting and publishing data and code from authors, the editors use a third party research center (namely, the Odum Institute for Research in Social Science, at the University of North Carolina, Chapel Hill for quantitative analysis and the Qualitative Data Repository (QDR), at Syracuse University for qualitative analyses) to verify that the data and statistical code produce the published results.",
          "There given examples such as those discussed below in which there are discrepancies between the original published findings and later replication results, it remains possible that the high-profile cases of failed replication may simply be the tip of the iceberg. Thankfully, a few recent papers have begun to provide some evidence on this question, which we highlight below. We ourselves are no strangers to replication and re-analysis debates: papers by one of the authors of this article, described below, have been part of lively debates on replication and reanalysis using data that we shared publicly. These debates have led us to appreciate the great promise of replication research, as well as its potential pitfalls: exactly like original research studies, replication studies have their own particular strengths and weaknesses, and may serve to either advance the intellectual debate or could obscure particular issues. Yet there is no doubt in our minds that an overall increase in replication research will serve a critical role in establishing the credibility of empirical findings in economics, and in equilibrium, will create stronger incentives for scholars to generate more reliable results. Further complicating matters, an imprecise definition of the term \"replication\" itself often leads to confusion. A taxonomic proposal in Hamermesh (2007) distinguished between \"pure\", \"statistical\", and \"scientific\" replications, while a more recent effort (Clemens 2015) uses the terms \"verification,\" \"reproduction,\" \"reanalysis,\" and \"extension\" to distinguish between replications (the first two) and robustness exercises (the latter two). We first present some existing evidence on the replicability of economics and social science research in the next subsection, and then provide examples of each of Clemens' categories.",
          "The articles in the 1986 Journal of Money Credit and Banking project and the 1994 St. Louis Federal Reserve follow-up mentioned above provided some of the first attempts at systematic replication in economics, with fairly discouraging results. Have things improved in the last few decades? New evidence is emerging about the reliability of empirical economics research. One of the most important recent studies is Camerer et al. (2016), which repeated 18 behavioral economics lab experiments originally published between 2011 and 2014 in the American Economic Review and the Quarterly Journal of Economics to assess their replicability. Figure 3 below reproduces a summary of their findings. Their approach is similar in design to a large scale replication of 100 studies in psychology known as the \"Replication Project: Psychology\", which we discuss in detail below. The replication studies were designed with sample sizes that aimed to have 90% power to detect the original effect size at the 5% significance level. In all, the estimated effects were statistically significant with the same sign in 11 of the 18 replication studies (61.1% Other examples of replication failures abound. Clemens (2015) provides a useful taxonomy, and we provide an example of from each of the categories there to help distinguish between them, namely the two types of replication he discusses (verification and reproduction), and the two types of robustness exercises (reanalysis and extension). Of course not all papers fit easily into one of these categories as most tend to include elements from multiple categories.",
          "Perhaps the most straightforward type of replication in economics involves using the same specification, the same sample, and the same population. Essentially, this is running the same code on the same data and testing if you get the same results. Hamermesh (2007) referred to this as a \"pure replication.\" We believe this basic standard should be expected of all published economics research, and hope this expectation is universal among researchers. One tiny tweak to the definition of verification is that it also includes errors in coding. If an author describes a statistical test in the paper, but the code indisputably does not correctly carry out the test as described, this is also considered a verification failure. One of the earliest cases of quantitative economics research failing a verification test comes from an investigation of the effect of social security on private savings. Feldstein (1974) estimates a life cycle model showing that social security reduces private savings by as much as 50%. There were significant theoretical challenges to carrying out this exercise related to assumptions about the intergenerational transfer of wealth, but Leimer and Lesnoy (1982) discovered that a flaw in Feldstein's computer program that overestimated the growth rate of Social Security wealth for widows led to larger effects of Social Security wealth than when the mistake was corrected. Feldstein replied to the critique saying he was grateful for having the error corrected, but that the central conclusion of the study remains largely unchanged (namely, that Social Security decreased private savings by 44%) (Feldstein 1982). Much of the change in coefficients in the replication exercise resulted from Leimer and Lesnoy including an expanded time series of data-this is not a failure of verification, but rather an extension, which we discuss below.  3). 20 In many (but not all) cases discussed in Clemens, the original authors clearly admit to the failure of verification, but there is vigorous, and we think, healthy scholarly debate about how important those mistakes are and whether the results are still significant -statistically and/or practicallywhen the code or data are corrected. Of course, authors whose papers are subject to replication debates should be commended for providing other scholars with access to their data and code public in the first place, especially for these earlier articles published before journal data-sharing requirements were established.",
          "The other type of replication in Clemens' taxonomy is a reproduction. This approach uses the same analytical specification and the same population, but a different sample. Hamermesh (2007) refers to this as a statistical replication. In economics, this approach would be exhibited in a study that generated a certain set of results using a 5% sample of the census, while a different 5% census sample produced different results, or an experimental economics lab study that produced one set of results with a certain sample while the reproduction study analyzed a different sample from broadly the same population (e.g., U.S. university students). There is, of course, some  2007), the possibility is discussed that one factor contributing to lack of verification is that intermediary datasets constructed from raw data were over-written when the raw data was updated, as sometimes happens with U.S. government data. The work of one of the authors of this paper could be included on this list; see Miguel and Kremer (2004), Aiken et al. (2015) and Hicks, Kremer and Miguel (2015) on the impact of school-based deworming in Kenya. Camerer et al. (2016) could be better classified as failures of reproduction, as long as the samples were in fact collected in broadly the same manner (i.e., in person vs. online). Reproduction failures are perhaps more precisely defined in the hard sciences where experimenters routinely attempt to do the exact same physical process as another lab, albeit with a different sample of molecules, or in the biological sciences where experiments may employ a different sample of animal subjects. For instance, in defining reproduction, Clemens mentions the infamous case of the \"discovery\" of cold fusion by Fleischmann and Pons (1989), which failed to reproduce in Lewis et al. (1989).",
          "Robustness exercises come in two varieties, reanalysis and extensions. Reanalysis uses a different analytical specification on the same population (with either the same or a different sample). Many economics replication studies include both a verification aspect as well as some re-analysis. For instance, Davis (2013) conducts a successful verification of Sachs and Warner (1997), but concludes that reanalysis shows the estimates are somewhat sensitive to different statistical estimation techniques. Other well-known recent reanalysis debates in empirical economics include Miguel, Satyanath, and Sergenti (2004), Ciccone (2011) and Miguel and Satyanath (2011) on civil conflict and GDP growth using rainfall as an instrumental variable; and Acemoglu, Johnson, and Robinson (2001), Albouy (2012), and Acemoglu, Johnson, and Robinson (2012) on institutions and GDP growth with settler mortality as an instrumental variable. The debates over these and other studies makes it clear that reanalysis does not typically settle all key research questions, and the exercise often reveals that empirical economists have considerable flexibility in their analytical choices. This insight makes the development of methods to account for -and possibly constrain -this flexibility, which we discuss below in section 3, all the more important.",
          "Under Clemens' classification system, an extension uses the same analytical specification as an original study but a different population and a different sample. Most often this would be conducting the same analysis carried out in a different time or place. A well-known example of an extension involves Burnside and Dollar (2000), which showed that foreign aid seemed to be effective in increasing GDP if the recipient country was well-governed. However, using the exact same regression specification but including additional countries and years to the dataset, Easterly, Levine, and Roodman (2004) do not obtain the same result. Burnside and Dollar (2004) discuss the differences between the findings and conclude that they occur largely because of the additional countries, rather than lengthening the time series. One widely debated topic in economics that has features of both replication and robustness exercises is the topic of minimum wage impacts on unemployment. In early work, Welch (1974) concluded that early minimum wage legislation decreased teenage employment, increased the cyclicality of teenage employment with respect to the business cycle, and shifted teenage employment towards sectors not covered by the law. However, in the course of using Welch's data, Siskind (1977) discovered that Welch had used data for teenagers 16-19 years old instead of 14-19 years old for certain years, and once this was corrected, the minimum wage did not appear to reduce teenage employment. This was a fairly easy mistake to understand since the Current Population Survey was undergoing changes at the time, and table headings for unpublished data had not even been updated. Welch graciously acknowledged the error, and used the corrected data to extend the analysis to probe impacts by industry sector (Welch 1977). Scholars working on this important topic have, for several decades now, continued to find significant room for disagreement on key issues of sampling, data sources, and statistical analysis methods 21 , matters on which well-intended researchers may well disagree. In this and other similarly contentious debates, we believe that the use of pre-specified research designs and analysis plans could be useful for advancing scientific progress, a point we return to below.",
          "Though we believe (or at least, would prefer to believe) that most instances in which economics studies cannot be replicated are due to inadvertent human error or analytical judgment calls, fraud cannot be completely discounted in all cases. Popular books such as Broad and Wade's Betrayers of the Truth (1983) make it clear that scientists are not always saints. A survey of 234 economists at the 1998 ASSA/AEA meeting investigated falsification of research, inappropriate inclusion or omission of co-authors, and exchange of grades for gifts, money, or sexual favors (List et al. 2001). Both a randomization coin-toss technique to elicit true responses to sensitive questions, as well as a more standard question design, indicate that 4% of respondents admit to having at some time falsified research data, 7-10% of respondents admit to having committed one of four relatively minor research infractions, while up to 0.4% admitted to exchange of grades for gifts, money, or sexual favors. Given the seriousness of some of these offenses, an obvious concern is that these figures understate the actual incidence of fraudulent research practices. A more recent survey of members of the European Economics Association described in Necker ( 2014) asks individuals about the justifiability of certain practices as well as their behavior regarding those practices. Necker shows that 2.6% of researchers admit to having falsified data, while 94% admit to at least one instance of a practice considered inappropriate by the majority of the survey, and there is a clear positive correlation between justifiability and behavior, as well as between perceived professional publication pressures and questionable research practices. Similar surveys in other fields such as Anderson, Martinson, and Vries (2007), which surveyed researchers across disciplines funded by the U.S. National Institutes of Health, and John, Loewenstein, and Prelec (2012) in psychology, as well as a meta-analysis of 18 surveys of academic misbehavior, do not paint a very rosy picture, with 2% of respondents admitting to data fabrication, and 34% admitting to lesser forms of academic misconduct (Fanelli 2009). We are not aware of a recent case in economics that received media attention similar to the Michael Lacour fraud scandal uncovered by Broockman, Kalla, and Aranow (2015) in political science, or the case of Diedrick Stapel (see Carey 2011; Bhattacharjee 2013) in psychology. However, there is considerable evidence of plagiarism and other forms of research malpractice in economics. This Journal itself published the results of a survey sent to 470 economics journal editors, which revealed significant problems (Enders and Hoover 2004). Among the 127 editors who responded, only 19% claimed that their journal had a formal policy on plagiarism, and 42 cases of plagiarism were discovered in an average year, with nearly 24% of editors encountering at least one case. A follow-up survey of rank-and-file economists revealed a general lack of consensus on how to respond to cases of alleged plagiarism (Enders and Hoover 2006). 22Article retraction is another useful indicator of research misconduct. A search of four popular article databases for terms related to article retractions identified by Karabag and Berggren (2012)   and Stapel (2009), which was retracted in 2015 27 , clearly reads \"THIS PAPER HAS BEEN RETRACTED,\" the title has been altered to begin with \"Retracted: ,\" and the pdf features an obvious RETRACTED watermark on every page. This is also the case with all six of the retractions in Karabag and Berggren (2012), as well as other notable recent retractions such as LaCour and Green (2014), which was retracted by Marcia McNutt (2015). Innovation and New Technology for re-publication in Kyklos (Frey, Frey, and Eichenberger 1999). The most recent incident that seemed to attract significant attention was the submission of a substantively identical article to multiple journals within economics, which is also a serious lapse (\"Correspondence: David H Autor and Bruno S. Frey\" 2011). Even if plagiarism of this manner would seem significantly easier to catch in the Internet age, the proliferation of journals partially counteracts this ease. 23 https://plagiarism.repec.org/index.html 24 Although note that journals may present these policies online as opposed to formally publishing them in the journal; for instance, see the The bottom line is that there is little reason to believe that economists are inherently more ethical than other social scientists or researchers in other disciplines, so policies regarding fraud and retraction from other disciplines might potentially be beneficially applied to economics.",
          "This section discusses several new methods and tools that have emerged in economics research over the past two decades -and more forcefully over the past ten years -to address the concerns discussed in Section 2. These approaches have in common a focus on greater transparency and openness in the research process. They include improved research design (including experimental designs and meta-analysis approaches), study registration and pre-analysis plans, strengthened disclosure and reporting practices, and new norms regarding open data and materials. We discuss each in turn.",
          "There have been a number of different responses within economics to the view that pervasive specification searching and publication bias was affecting the credibility of empirical literatures. As mentioned above, there has been a shift towards a greater focus on prospective research design in several fields of applied economics work. Experimental (Duflo, Glennerster, and Kremer 2007) and quasi-experimental (Angrist and Pischke 2010) research designs arguably place more constraints on researchers relative to earlier empirical approaches, since there are natural ways to present data using these designs that researchers are typically compelled to present by colleagues in seminars and by journal referees and editors. Prospective experimental studies also tend to place greater emphasis on adequately powering an analysis statistically, which may help to reduce the likelihood of publishing only false positives (Duflo, Glennerster, and Kremer 2007). There is also suggestive evidence that the adoption of experimental and quasiexperimental empirical approaches is beginning to address some concerns about specification search and publication bias: Brodeur et al. (2016) present tentative evidence that the familiar spike in p-values just below the 0.05 level is less pronounced in randomized control trial studies than in studies utilizing non-experimental methods. Yet improved research design alone may not solve several other key threats to the credibility of empirical economics research, including the possibility that null or \"uninteresting\" findings never become known within the research community.",
          "In addition to improvements in research design, Leamer (1983) argued for greater disclosure of the decisions made in analysis, in what became known as \"extreme bounds analysis\" (described in Section 2). Research along these lines has dealt with model uncertainty by employing combinations of multiple models and specifications, as well as comparisons between them. Leamer himself has continued to advance this agenda (see Leamer 2016). We describe several related approaches here.",
          "A natural way to deal with statistical model uncertainty is through Bayesian model averaging. In this approach, each model in the space of plausible models is assigned a probability of being true based on researcher priors and goodness of fit criteria. Averaging the resulting estimates generates a statistic incorporating model uncertainty: The weighted average they generate implies a large but imprecisely estimated deterrent effect of executions on homicides in the U.S. Of course, even without employing explicit probability weights, simply visualizing the distribution of estimates across the entire space of statistical models can also be quite informative on its own. Two well-cited examples of model averaging engage in a thorough investigation of the determinants of cross-country economic growth. Sala-i-Martin's (1997) famous \"I Just Ran Two Million Regressions\" article uses model weights proportional to the integrated likelihoods of each model, picks all possible three-variable combinations out of 60 covariates that have been reported as being significantly related to economic growth, and finds that only about one-third of the 60 variables can be considered robustly positively correlated with economic growth across models. Sala-i-Martin, Doppelhofer, and Miller ( 2004) conduct what they call Bayesian Averaging of Classical Estimates (BACE), weighting estimates using an approach analogous to Schwarz's BIC, and find that just 18 of 67 variables are significantly and robustly partially correlated with economic growth, once suggesting that many findings reported in the existing empirical literature may be spuriously generated by specification searching and selective reporting.",
          "While specification searching or data mining often has a negative connotation in applied economic research, some scholars have taken a more favorable view of it, as long as the data mining is carried out appropriately (Pagan 1987;Phillips 1988). Advocates of this approach have sometimes been known as the \"LSE school\" of econometrics, or the general-to-specific modelling approach (Gilbert 1989;Hendry 1987;Hendry 1995). This approach is related in spirit to the idea of \"encompassing,\" which is the principle that one statistical model can account for, or explain, another (Mizon and Richard 1986;Bontemps and Mizon 2008). To our knowledge, this approach is more often applied in time series econometrics than in applied microeconomics, but perhaps a closer consideration is warranted. There have also been recent calls within applied economics for greater application of machine learning methods and other data science techniques that share some features with these approaches (Kleinberg, Ludwig, et al. 2015;Kleinberg, Lakkaraju, et al. 2015). For a wide set of views on data mining more broadly and the LSE approach specifically, see the Journal of Economic Methodology, which devoted a special issue to the topic (Backhouse and Morgan 2000).",
          "Simonsohn, Simmons, and Nelson (2015b) propose a method, which they call the \"specification curve,\" that is similar in spirit to Leamer's extreme-bounds analysis, but recommends researchers test the exhaustive combination of analytical decisions, not just decisions about which covariates to include in the model. If the full exhaustive set is too large to be practical, a random subset can be used. After plotting the effect size from each of the specifications, researchers can assess how much the estimated effect size varies, and which combinations of decisions lead to which outcomes. Using permutation tests (for treatment with random assignment) or bootstrapping (for treatment without random assignment), researchers can generate shuffled samples with no true effect by construction, and compare the specification curves from these placebo samples to the specification curve from the actual data. Many comparisons are possible, but the authors suggest comparing the median effect size, the share of results with predicted sign, and share of statistically significant results with predicted sign. A key comparison, which is analogous to the traditional p-value, is the percent of the shuffled samples with as many or more extreme results. Bakkensen and Larson 2014). The specification curve shows that 46% of curves from permuted data show at least as large a median effect size as the original, 16% show at least as many results with the predicted sign, and 85% show at least as many significant results with the predicted sign. This indicates that the results are likely to have been generated by chance. The Bertrand and Mullainathan (2004) specification curve, on the other hand, shows that fewer than 0.2% of the permuted curves generate as large a median effect, 12.5% of permuted curves show at least as many results with the predicted sign, and less than 0.2% of permuted curves show at least as many significant results with the predicted sign, providing evidence that the results are very unlikely to have been generated by chance.",
          "There have been significant advances in the methodological literature on quantifying the extent of publication bias in a given body of literature. Early methods mentioned above include Rosenthal's (1979) method (the \"fail-safe N\"), while Galbraith (1988) advocated for radial plots of log odds ratios, and Card and Krueger (1995) tested for relationships between study sample sizes and t-statistics. Statisticians have developed methods to estimate effect sizes in meta-analyses that control for publication bias (Hedges 1992;Hedges and Vevea 1996). The tools most widely used by economists tend to be simpler, including the widely used funnel plot, which is a scatter plot of some measure of statistical precision (typically the inverse of the standard error), versus the estimated effect size. Estimates generated from smaller samples should usually form the wider base of an inverted funnel, which should be symmetric around more precise estimates in the absence of publication bias. The method is illustrated with several economics examples in Stanley and Doucouliagos (2010), and two of these are reproduced in Figure 4. In addition to scrutinizing the visual plot, a formal test of the symmetry of this plot can be conducted using data from multiple studies and regressing the relevant t-statistics on inverse standard errors: (eqn. 4) The resulting t-test on 𝛽𝛽 0 , referred to as the Funnel Asymmetry Test (FAT) (Stanley 2008), captures the correlation between estimated effect size and precision, and thus tests for publication bias. Using the FAT, Doucouliagos and Stanley (2009) find evidence of publication bias in Card and Krueger's (1995) sample of minimum wage studies (𝛽𝛽 0 ≠ 0), consistent with their own interpretation of the published literature at that time. 𝛽𝛽 1 here can also be interpreted as the true effect (called the precision effect test, PET) free of publication bias, and Doucouliagos and Stanley (2009) find no evidence of a true effect of the minimum wage on unemployment. The authors also conduct the FAT-PET tests with 49 additional more recent studies in this literature and find the same results: evidence of significant publication bias and no evidence of an effect of the minimum wage on unemployment. Additional meta-analysis methods, including this \"FAT-PET\" approach, are summarized in Stanley and Doucouliagos (2012).",
          "Other applied econometricians have recently called for increasing the use of multiple testing corrections in order to generate more meaningful inference in study settings with many research hypotheses (Anderson 2008;Fink, McConnell, and Vollmer 2014). The practice of correcting for multiple tests is already widespread in certain scientific fields (e.g., genetics) but has yet to become the norm in economics and other social sciences. Simply put, since we know that pvalues fall below traditional significance thresholds (e.g., 0.05) purely by chance a certain proportion of the time, it makes sense to report adjusted p-values that account for the fact that we are running multiple tests, since this makes it more likely that at least one of our test statistics has a significant p-value simply by chance. There are several multiple testing approaches, some of which are used and explained by Anderson (2008), namely, reporting index tests, controlling the family-wise error rate (FWER), and controlling the false discovery rate (FDR). These are each discussed in turn below.",
          "One option for scholars in cases where there are multiple related outcome measures is to forego reporting the outcomes of numerous tests, and instead standardize the related outcomes and combine them into a smaller number of indices, sometimes referred to as a mean effect. This can be implemented for a family of related outcomes by making all signs agree (i.e., allowing positive values to denote beneficial outcomes), demeaning and dividing by the control group standard deviation, and constructing a weighted average (possibly using the inverse of the covariance matrix to weight each standardized outcome). This new index can be used as a single outcome in a regression model and evaluated with a standard t test. Kling, Liebman, and Katz (2007) implement an early index test in the Moving to Opportunity field experiment using methods developed in biomedicine by O'Brien (1984). This method addresses some concerns regarding the multiplicity of statistical tests by simply reducing the number of tests. A potential drawback is that the index may combine outcomes that are only weakly related, and may obscure impacts on specific outcomes that are of interest to particular scholars, although note that these specific outcomes could also be separately reported for completeness.",
          "The family-wise error rate (FWER) is the probability that at least one true hypothesis in a group is rejected (a type I error, or false positive). This approach is considered most useful when the \"damage\" from incorrectly claiming any hypothesis is false is high. There are several ways to implement this approach, with the simplest method being the Bonferroni correction of simply multiplying every original p-value by the number of tests carried out (Bland and Altman 1995), although this is extremely conservative, and improved methods have also been developed. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher discount factors (Holm 1979). A related and more efficient recent method is the free step-down resampling method, developed by Westfall and Young (1993), which when implemented by Anderson (2008) implies that several highly cited experimental pre-school interventions (namely, the Abecedarian, Perry, and Early Training Project studies) exhibit few positive long-run impacts for males. Another recent method improves on Holm by incorporating the dependent structure of multiple tests. Lee and Shaikh (2014) apply it to reevaluate the Mexican PROGRESA conditional cash transfer program and find that overall program impacts remain positive and significant, but are statistically significant for fewer subgroups (e.g., by gender, education) when controlling for multiple testing. List, Shaikh, and Xu (2016) propose a method of controlling the FWER for three common situations in experimental economics, namely, testing multiple outcomes, testing for heterogeneous treatment effects in multiple subgroups, and testing with multiple treatment conditions.28 ",
          "In situations where a single type I error is not considered very costly, researchers may be willing to use a somewhat less conservative method than the FWER approached discussed above, and trade off some incorrect hypothesis rejections in exchange for greater statistical power. This is made possible by controlling the false discovery rate (FDR), or the percentage of rejections that are type I errors. Benjamini and Hochberg (1995) details a simple algorithm to control this rate at a chosen level under the assumption that the p-values from the multiple tests are independent, though the same method was later shown to also be valid under weaker assumptions (Benjamini and Yekutieli 2001). Benjamini, Krieger, and Yekutieli (2006) describes a two-step procedure with greater statistical power, while Romano, Shaikh, and Wolf (2008) propose the first methods to incorporate information about the dependence structure of the test statistics. Multiple hypothesis testing adjustments have recently been used in finance (Harvey, Liu, and Zhu 2015) to re-evaluate 316 factors from 313 different papers that explain the cross-section of expected stock returns. The authors employ the Bonferroni; Holm (1979);and Benjamini, Hochberg, and Yekutieli (2006) methods to account for multiple testing, and conclude that tstatistics greater than 3.0, and possibly as high as 3.9, should be used instead of the standard 1.96, to actually conclude that a factor explains stock returns with 95% confidence. Index tests and both the FWER and FDR multiple testing corrections are also employed in Casey, Glennerster, and Miguel (2012) to estimate the impacts of a community driven development program in Sierra Leone using a dataset with hundreds of potentially relevant outcome variables.",
          "Another method that controls for, or acts as a reality check, for data snooping or data mining was developed in White (2000). The testing of multiple hypotheses, or repeated use of the same data, is a particularly central problem with time series data used over and over again by multiple scholars, such as data on stock returns, which makes this research quite important in empirical finance. Like the model averaging approach described above, the reality check requires a researcher to estimate the entire space of plausible models, but now compares the performance of the preferred model to a benchmark model (e.g., a model for stock market predictions based on the efficient market hypothesis), and does so repeatedly with bootstrapped samples. To assess whether a certain preferred model actually outperforms the benchmark after accounting for snooping with multiple models, the econometrician first calculates the performance of the preferred model (using mean squared error improvement over the benchmark, or relative profit of the strategy). She then selects a bootstrap sample (with replacement), and calculates the mean squared error improvement (or profit) with the new sample for all of the different plausible statistical models, recording the best mean squared error improvement (or profit) across all the models. This approach can then be repeated 1,000 (or more) times, gathering the 1,000 best mean squared errors (or profits). In the final step, one must compare the original preferred model's mean squared error to the best performance from each of the 1,000 bootstraps. The p-value is the fraction of bootstrapped best fits that outperform the preferred model. (A truly predictive model would have returns higher than 95% of the best performing models from each of the bootstrapped samples.) This method was implemented on a large number of trading rules in Sullivan, Timmermann, and White (1999) and a similar method that addresses the presence of poorly performing or irrelevant alternatives was developed in Hansen (2005).",
          "A leading proposed solution to the problem of publication bias is the registration of empirical studies in a public registry. This would ideally be a centralized database of all attempts to conduct research on a certain question, irrespective of the nature of the results, and such that even null (not statistically significant) findings are not lost to the research community. Top medical journals have adopted a clear standard of publishing only medical trials that are registered (De Angelis et al. 2004). The largest clinical trial registry is clinicaltrials.gov, which helped to inspire the most high-profile study registry within economics, the AEA Randomized Controlled Trial Registry (Katz et al. 2013), which was launched in May 2013. 29   While recent research in medicine finds that the clinical trial registry has not eliminated all under-reporting of null results or other forms of publication bias and specification searching (Laine et al. 2007;Mathieu et al. 2009), they do allow the research community to quantify the extent of these problems and over time may help to constrain inappropriate practices. It also helps scholars locate studies that are delayed in publication, or are never published, helping to fill in gaps in the literature and thus resolving some of the problems identified in Franco, Malhotra, and Simonovits (2014). Though it is too soon after the adoption of the AEA's trial registry to measure its impact on research practices and the robustness of empirical results, it is worth noting that the registry is already being used by many empirical researchers: since inception in 2013, over 850 studies conducted in over 100 countries have been registered, and the pace of registrations continues to rise rapidly. allowing users to create a frozen time-stamped web URL with associated digital object identifier (DOI) for any materials uploaded to OSF. Several popular data storage options (including Dropbox, Dataverse, and GitHub) can also be synced with the OSF and its storage, creating a flexible way for researchers to register their research and materials. As of October 2016, over 7,300 public registrations have been created on OSF since the service launched in 2013.",
          "In addition to serving as a useful way to search for research findings on a particular topic, most supporters of study registration also promote the pre-registration of studies, including preanalysis plans (PAPs) that can be posted and time stamped even before analysis data are collected or otherwise available (Miguel et al. 2014). Registration is now the norm in medical research for randomized trials, and registrations often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the U.S. template may be particularly useful to researchers themselves when developing their own preanalysis plans, and instructors may find it useful in their courses. Building on, and modifying, the FDA's 1998 checklist with insights from these other recent treatments of pre-analysis plans, there appears to be a growing consensus that pre-analysis plans in economics should consider discussing at least the following list of ten issues: Pre-analysis plans are relatively new to economics, and this list is likely to evolve in the coming years as researchers explore the potential, and possible limitations, of this new tool. For those concerned about the possibility of \"scooping\" of new research designs and questions based upon a publicly posted pre-analysis plan or project description, several of the social science registries allow temporary embargoing of project details. For instance, the AEA registry allows an embargo until a specific date or project completion. At the time of writing, the OSF allows a four-year embargo until the information is made public. 31",
          "Recent examples of economics papers based on experiments with PAPs include Casey, Glennerster, and Miguel (2012) and Finkelstein et al. (2012), among others. Casey, Glennerster, and Miguel (2012) discuss evidence from a large-scale field experiment on community driven development (CDD) projects in Sierra Leone. The project, called GoBifo, was intended to make local institutions in post-war Sierra Leone more democratic and egalitarian. GoBifo funds were spent on a variety of local public goods infrastructure (e.g., community centers, schools, latrines, roads), agriculture, and business training projects, and were closely monitored to limit leakage. The analysis finds significant short-run benefits in terms of the \"hardware\" aspects of infrastructure and economic wellbeing: the latrines were indeed built. However, a larger goal of the project, reshaping local institutions, making them more egalitarian, increasing trust, improving local collective action, and strengthening community groups, which the researchers call the \"software effects,\" largely failed. There are a large number of plausible outcome measures along these dimensions, hundreds in total, which the authors analyze using a mean effects index approach for nine different families of outcomes (with multiple testing 31 See http://help.osf.io/m/registrations/l/524207-embargoes. Accessed October 10, 2016. adjustments). The null hypothesis of no impact cannot be rejected at 95% confidence for any of the nine families of outcomes. Yet Casey et al (2012) go on to show that, given the large numbers of outcomes in their dataset, and the multiplicity of ways to define outcome measures, finding some statistically significant results would have been relatively easy\". In fact, the paper includes an example of how, if they had had the latitude to define outcomes without a pre-analysis plan, as has been standard practice in most empirical economics studies (and in other social science fields), the authors could have reported either statistically significant and positive effects, or significantly negative effects, depending on the nature of the \"cherry-picking\" of results. We reproduce their results here as Table 4, where Panel A presents the statistically significant positive impacts identified in the GoBifo data and Panel B highlights negative effects. This finding begs the question: how many empirical economics papers with statistically significant results are, unbeknownst to us, really just some version of either Panel A or Panel B? This is important because, as in Casey et al (2012), the researchers tested a large number of outcomes: hospital admissions through the emergency room (ER) and not through the ER; hospital days; procedures; financial strain (bankruptcy, judgments, liens, delinquency, medical debt, and non-medical debt, measured by credit report data); self-reported health from survey data, and so on. When running such a large number of tests, the researchers again could have discovered some \"significant\" effects simply by chance. The pre-analysis plan, in conjunction with multiple hypothesis testing adjustments, give us more confidence in the main results of the study: that recipients did not improve significantly in terms of physical health measurements, but they were more likely to have health insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Additional studies that have resulted from the experiment have also employed preanalysis plans, and they show that health insurance increased emergency department use (Taubman et al. 2014), had no effect on measured physical health outcomes after two years, but did increase health care use and diabetes management, as well as leading to lower rates of depression and financial strain (Baicker et al. 2013). The health care expansion had no significant effect on employment or earnings (Baicker et al. 2014). Other prominent early examples of economics studies that have employed pre-analysis plans include poverty targeting programs in Indonesia, an evaluation of the Toms shoe company donation program, and a job training program in Turkey, among many others (Olken, Onishi, and Wong 2012;Alatas et al. 2012;Wydick, Katz, and Janet 2014;Hirshleifer et al. 2015). The PAP tool is also spreading to other social sciences beyond economics. For instance, in psychology, a pre-specified replication of an earlier paper that had found a link between female conception risk and racial prejudice failed to find a similar effect (Hawkins, Fitzgerald, and Nosek 2015). One issue that arises for studies that did register a pre-analysis plan is the question of characterizing the extent to which the analysis conforms to the original plan, or if it deviates in important ways from the plan. To appreciate these differences, scholars will need to compare the analysis to the plan, a step that could be seen as adding to the burden of journal editors and referees. Even if the analysis does conform exactly to the PAP, there is still the possibility that authors are consciously or unconsciously emphasizing a subset of the pre-specified analyses in the final study. Berge et al. (2015) develop an approach to comparing the distribution of p-values in the paper's main tables versus those in the PAP in order to quantify the extent of possibly selective reporting between the plan and the paper. The Finkelstein et al (2012) study is a model of transparency regarding the presentation of results. To the authors' credit, all analyses presented in the published paper that were not prespecified are clearly labeled as such; in fact, the exact phrase \"This analysis was not prespecified.\" appears in the paper six times. Tables in the main text and appendix that report analyses that were not pre-specified are labelled with a \"^\" character to set them apart, and are clearly labeled as such.",
          "There remain many open questions about whether, when, and how pre-analysis plans could and should be used in economics research, with open debates about how useful they are in different subfields of the discipline. Olken (2015), for example, highlights both their \"promises and perils.\" On the positive side, pre-analysis plans bind the hands of researchers and greatly limit specification searching, allowing them to take full advantage of the power of their statistical tests (even making one-sided tests reasonable). A further advantage of the use of pre-analysis plans is that they are likely to help shield researchers from pressures to affirm the policy agenda of donors and policymakers, in cases where they have a vested interest in the outcome, or when research focuses on politically controversial topics (such as health care reform). This is especially the case if researchers and their institutional partners can agree on the pre-analysis plan, as a sort of evaluation contract. On the negative side, PAPs are often complex and take valuable time to write. Scientific breakthroughs often come at unexpected times and places, often as a result of exploratory analysis, and the time spent writing PAPs may thus lead less time to spend on less structured data exploration. Coffman and Niederle (2015) argue that there is limited upside from PAPs when replication (in conjunction with hypothesis registries) is possible. In experimental and behavioral economics, where lab experiments utilize samples of locally recruited students and the costs of replicating an experiment are relatively low, they argue that replication could be a viable substitute for pre-analysis plans. Yet there does appear to be a growing consensus, endorsed by Coffman and Niederle, that pre-analysis plans can significantly increase the credibility of reporting and analysis in large-scale randomized trials that are expensive or difficult to repeat, or when a study that relies on a particular contextual factor makes it impossible to replicate. For instance, Berge et al. (2015) carry out a series of lab experiments timed to take place just before the 2013 Kenya elections. Replication of this lab research is clearly impossible due to the unique context, and thus use of a pre-analysis plan is valuable. Olken (2015) as well as Coffman and Niederle (2015) discuss another potential way to address publication bias and specification search: results-blind review. Scholars in psychology have championed this method; studies that are submitted to such review are often referred to as \"registered reports\" in that discipline. Authors write a detailed study protocol and pre-analysis plan, and before the experiment is actually run and data is collected, submit the plan to a journal. Journals review the plan for the quality of the design and the scientific value of the research question, and may choose to give \"in-principle acceptance\". This can be thought of as a kind of revise and resubmit that is contingent on the data being collected and analyzed as planned. If the author follows through on the proposed design, and the data are of sufficiently high quality (e.g., with sufficiently low sample attrition rates in a longitudinal study, etc.), the results are to be published regardless of whether or not they are statistically significant, and whether they conform to the expectations of the editor or referees, or to the conventional wisdom in the discipline. Several psychology journals currently have begun using results-blind review, either regularly or in special issues (Chambers 2013;Chambers et al. 2014;Nosek and Lakens 2014). 32A forthcoming issue of Comparative Political Studies will be the first to feature results-blind review in political science (Findley et al. 2016), and it will include both experimental and observational research studies. In our view, it would also be useful to experiment with results-blind review and registered reports in economics journals. The rise in experimental studies and pre-analysis plans in economics, as evidenced by the rapid growth of the AEA registry, is likely to facilitate the eventual acceptance of this approach.",
          "An important open question is how widely the approach of study registration and hypothesis prespecification could be usefully applied in non-prospective and non-experimental studies. This issue has been extensively discussed in recent years within medical research but consensus has not yet been reached in that community. It actually appears that some of the most prestigious medical research journals, which typically publish randomized trials, are even more in favor of the registration of observational studies than the editors of journals that publish primarily in non-experimental research (see the dueling editorial statements in Epidemiology 2010; The Lancet 2010;Loder, Groves, and MacAuley 2010;Dal-Ré et al. 2014). A major logical concern with the pre-registration of non-prospective observational studies using pre-existing data is that there is often no credible way to verify that pre-registration took place before analysis was completed, which is different than the case of prospective studies in which the data has not yet been collected or accessed. In our view, proponents of the preregistration of observational work have not formulated a convincing response to this obvious concern. A more recent study bases its analysis on Neumark's exact pre-specified tests estimate the effect of minimum wages in Canada and found larger unemployment effects, but they had access to the data before estimating their models and did not have an agreement with the journal, so the value of this \"pre-specification\" is perhaps less clear (Campolieti, Gunderson, and Riddell 2006). In political science, a pre-specified observational analysis measured the effect of the immigration stances of Republican representatives on their 2010 election outcomes (Monogan 2013). It is difficult to see how a researcher could reach Neumark's level of pre-specified detail with a research question with which they were not already intimately familiar. It seems more likely that in a case where the researcher was less knowledgeable they might either pre-specify with an inadequate level of detail, or choose an inappropriate specification; this risk makes it important that researchers should not be punished for deviating from their pre-analysis plan in cases where the plan omits important details or contains errors, as argued in Casey et al (2012). It seems likely to us that the majority of observational empirical work in economics will continue largely as is for the foreseeable future. However, for important, intensely debated, and well-defined questions, it would be desirable in our view for more prospective observational research to be conducted in a pre-specified fashion, following the example in Neumark (2001). Although pre-specification will not always be possible, the fact that large amounts of government data are released to the public on regular schedules, and that many policy changes are known to occur well in advance (such as in the case of the anticipated federal minimum wage changes discussed above, with similar arguments for future elections), will make it possible for the verifiable pre-specification of research analysis to be carried out in many settings.",
          "Another frontier topic in this realm is the use of pre-specified algorithms, including machine learning approaches, rather than exact pre-analysis plans for prospective studies. For instance, the exact procedure to be used to determine which covariates should be included in order to generate the most statistically precise estimates can be laid out in advance, even if those covariates are unknown (and unknowable) before the data has been collected. This approach has not yet been widely adopted in economics (to our knowledge), but has begun to be used in medical trials and biostatistics (van der Laan et al. 2007;Sinisi et al. 2007). A proposal related to, but slightly different than, pre-analysis plans is Nobel Prize winning physicist Saul Perlmutter's suggestion for the social sciences to use \"blind analysis\" (MacCoun and Perlmutter 2015). In blind analysis, researchers add noise to the data while working with it and running the analysis, thus preventing them from knowing which way the results are turning out, and thus either consciously or unconsciously biasing their analysis, until the very end, when the noise is removed and the final results are produced. This technique is apparently quite common in experimental physics (Klein and Roodman 2005), but we are not aware of its use in economics or other social sciences. Major differences are also beginning to emerge in the use of pre-analysis plans, and in the design and interpretation of experimental evidence more broadly, among economists versus scholars in other fields, especially health researchers, with a much greater role of theory in the design of economics experiments. Economists often design experiments to shed light on underlying theoretical mechanisms, to inform ongoing theoretical debates, and measure and estimate endogeneous behavioral responses. These behavioral responses may shed light on broader issues beyond the experimental intervention at hand, and thus could contribute to greater external validity of the results. As a result, pre-analysis plans in economics are often very detailed, and make explicit reference to theoretical models. For example, Bai et al. (2015) preregistered the theoretical microeconomic model and detailed structural econometric approach that they planned to apply to a study of commitment contracts in the Indian health sector. This distinction between the types of studies carried out by medical researchers versus economists (including those working on health topics) has a number of important implications for assessing the reliability of evidence. One has to do with the quality standards and perceptions of the risk of bias in a particular design. For medical trialists accustomed to the CONSORT standards or other medical efficacy trial reporting guidelines (described below), studies that do not feature double-blinding, and thus run the risk of endogeneous behavioral responses to the medical intervention, are considered less reliable than those studies that employ double-blinding (for a detailed discussion, see Eble, Boone, and Elbourne 2014). While a few studies conducted by economists do feature double-blinding (e.g., Thomas et al. 2003;Thomas et al. 2006), in nearly all settings blinding participants to their status is either logistically difficult (for instance, if government partners are unwilling to distribute placebo treatments to some of their population) or even impossible. To illustrate, how would you provide a placebo treatment in a study investigating the impact of the distribution of cash transfers on household consumption patterns? Even in settings that might seem promising for placebo treatments, such as the community-level deworming treatments discussed in Miguel and Kremer (2004), blinding participants to their status is basically impossible: deworming generates side effects (mainly gastrointestinal distress) in roughly 10% of those who take the pills, so community members in a placebo community would quickly deduce that they were in fact not receiving real deworming drugs if there are few or no local cases of side effects. As noted above, endogeneous behavioral responses are often exactly what we economists (and other social scientists) set out to measure and estimate in our field experiments, as described in our pre-analysis plans, and thus are to be embraced rather than rejected as symptomatic of a \"low quality\" research design that is at \"high risk of bias\". Taken together, it is clear to us that the experimental literature in economics (and increasingly in other social sciences such as political science) often has very different objectives than medical, public health and epidemiological research, and thus different research methodologies are often called for. Despite the value of learning from recent experience in biomedical research, and the inspiration that the experience of medical research has played to the rise of new experimental research methods in the social sciences, economists have not simply been able to import existing medical trial methods wholesale, but are developing new and tailored approaches to pre-registration, preanalysis plans, reporting standards, and transparency more broadly.",
          "Another approach to promoting transparency is to establish detailed standards for the disclosure of information regarding study design, data, and analysis. These could serve to limit at least some forms of data mining and specification searching, or at least might make them more apparent to the reader. Detailed reporting standards have become widespread in medical research for both experimental and observational research. Most notably for clinical trials, the Consolidated Standards of Reporting Trials (CONSORT) was developed (Begg C et al. 1996). A before-andafter comparison showed improvement in some measures of study reliability (Moher D et al. 2001), and the standards have been twice revised (Moher, Schulz, and Altman 2001;Schulz et al. 2010) and since extended to at least ten specific types of research designs, interventions, or data. Among others, and possibly particularly relevant for some types of economics research, these include cluster randomized trials (Campbell, Elbourne, and Altman 2004;Campbell et al. 2012), non-pharmacological treatment interventions (Boutron et al. 2008), and patient-reported outcomes (Calvert et al. 2013) In psychology, researchers have created an extension of CONSORT for social and psychological interventions (CONSORT-SPI) (Montgomery et al. 2013;Grant et al. 2013). Others psychologists have proposed that an effective way to reform reporting and disclosure norms within their discipline is for referees to enforce desirable practices when reviewing articles (Simmons, Nelson, and Simonsohn 2011). These authors recommended six conditions for referees to consider include the following: 1. Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. 2. Authors must collect at least 20 observations per cell or else provide a compelling costof-data-collection justification. Recently, we, the authors of this article, were part of an interdisciplinary group of researchers that developed a detailed set of journal guidelines called the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015). This modular set of guidelines for journals features eight categories, namely: citation standards, data transparency, analytic methods (code) transparency, research materials transparency, design and analysis transparency, preregistration of studies, preregistration of analysis plans, and replication -with four levels (0-3) of transparency that journals could choose to endorse or require. For example, with regards to data transparency, the level zero standard is that the journal either encourages data sharing or says nothing, while the level three standard is that \"data must be posted to a trusted repository, and reported analyses will be reproduced independently prior to publication\"; levels 1 and 2 fall somewhere in between. Journals could choose to adopt higher standards in some categories than others, as they feel most appropriate for their research community. In the six months after the guidelines were published in Science, 538 journals and 57 organizations across a wide variety of scientific disciplines, including many in the social sciences, expressed their support for the standards and agreed to evaluate them for potential adoption. Science has now announced that it will be implementing the standards, effective  2016), which argues that differences in implementation between original and replication experiments were inappropriate and introduces noise in addition to the expected sampling error. When taking this into account, one should actually expect the relatively low reported replication rate, and they thus argue there is no replication crisis. Some of the original RPP authors respond that differences between original and replication studies were in fact often endorsed by original study authors and take issue with the statistical analysis in Gilbert et al. (Anderson et al. 2016). Simonsohn (2015) engages in further discussion of how one should evaluate replication results, suggesting that powering a replication based on the effect size of the original study is problematic, and to distinguish the effect size from zero, replications (at least in psychology, with their typically small sample and effect sizes) should have a sample at least 2.5 times as large as the original. An optimistic take by Patil, Peng, and Leek (2016) suggests that researchers should compare the effect in the replication study to a \"prediction interval\" defined as 𝑟𝑟ô𝑜 𝑜𝑜𝑖𝑖𝑜𝑜 ± 𝑧𝑧 0.975� 1 where 𝑟𝑟̂𝑜𝑜 𝑜𝑜𝑖𝑖𝑜𝑜 is the correlation estimate in the original study, 𝑛𝑛 𝑜𝑜𝑜𝑜𝑖𝑖𝑜𝑜 and 𝑛𝑛 𝑜𝑜𝐸𝐸𝑟𝑟 are the sample sizes in the original and replication studies, respectively; and 𝑧𝑧 0.975 is the 97.5% quantile of the normal distribution, which incorporates uncertainty in the estimates from both the original and replication study. Applying this approach leads to much higher estimates of study replication (75%) for the RPP. Economists may be interested to know that the researchers behind the RPP also included a prediction market in their project, and the market did a fairly good job of predicting which of the effects studies would ultimately be reproduced (Dreber et al. 2015). Unlike the prediction market in Camerer et al. (2016), the RPP prediction market outperformed a survey of researcher beliefs. 43 43 For related research on expert predictions, see DellaVigna and Pope (2016) . Other psychology researchers have tried another way to crowdsource replication: instead of bringing different research groups together to all independently run the same classic experiment, other researchers have independently analyzed the same observational dataset and attempted to answer the same question, in this case, the question of whether darker skintoned soccer players receive more red cards as a result of their race, conditional on other factors (Silberzahn and Uhlmann 2015). For instance, it has been shown in multiple instances that it is often trivially easy to identify individuals in purportedly \"de-identified\" and anonymous datasets using publicly available information. In one dramatic illustration, MIT computer science Ph.D. student Latanya Sweeney sent then-Massachusetts Governor William Weld his own complete personal health records only days after anonymized state health records were released to researchers (Sweeney 2002). A new focus of computer science theorists has been developing algorithms for \"differential privacy\" that simultaneously protect individual privacy while allowing for robust analysis of datasets. They have established that there is inherently a trade-off between these two objectives (Dwork and Smith 2010;Heffetz and Ligett 2014), though few actionable approaches to squaring this circle are currently available to applied researchers, to our knowledge.",
          "The rising interest in transparency and reproducibility in economics reflects broader global trends regarding these issues, both among academics and beyond. As such, we argue that \"this statements, and open data and materials lead to improved research quality in a way that can be credibly measured and assessed? To this point, the presumption among advocates (including ourselves, admittedly) is that these changes will indeed lead to improvements, but rigorous evidence on these effects, using meta-analytic approaches or other methods, will be important in determining which practices are in fact most effective, and possibly in building further support for their adoption in the profession. There are many potential avenues for promoting the adoption of new and arguably preferable practices, such as the data sharing, disclosure and pre-registration approaches described at length in this article. One issue that this article does not directly address is how to  Notes: Reproduced from Table 4 of Ioannidis (2005) 65 Notes: Table shows a sample of recent papers conducting meta-analyses and testing for publication bias in certain literatures in economics. Positive evidence for publication bias indicated by '+', evidence for no publication bias with '-', and mixed evidence with '~'. The number of papers and total estimates used in the meta-analysis are also shown.  Longhi, Nijkamp, and Poot (2005) Immigration and wages -18 (348) Publication bias is not found to be a major factor. The negative effect of immigration is quite small (0.1%) and varies by country. Knell and Stix (2005) Income elasticity of money demand -50 (381) Publication bias does not significantly affect the literature. Income elasticities for narrow money range from 0.4 to 0.5 for the US and 1.0 to 1.3 for other countries. Doucouliagos and Laroche (2003) Union productivity effects + 73 (73) Publication bias is not considered a major issue. Negative productivity associations are found in the UK, with positive associations in the US. Gorg and Strobl (2001) Multi-national corporations and productivity spillovers + 21 (25) Study design affects results, with cross-sectional studies reporting higher coefficients than panel data studies. There is also some evidence of publication bias. Ashenfelter, Harmon, and Oosterbeek (1999) Returns to Education + 27 (96) Publication bias is found, and controlling for it significantly reduces the differences between types of estimates of returns to education.  Duvendack, Palmer-Jones, and Reed (2015) or The Replication Network list (http://replicationnetwork.com/replication-studies/) as well as journal websites. Since \"replication\" is an imprecise term, this categorization is perhaps subject to some debate.      S1 shows a non-normalized version). Fourteen studies have a significant effect in the same direction as the original study in the meta-analysis [77.8%;95% CI = (56.5%,99.1%)]."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "certain types of statistical results are more likely to be published than other results, conditional on the research design and data used.",
            "context": "2.2 Publication Bias Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "Rosen_gjcpi_2018_3",
        "sections": [
          "Humanity and societies today face challenges to sustainability, which fundamentally relates to the ability to sustain humanity, civilizations and ecosystems on Earth. These challenges are expected to become more significant in the future. Achieving sustainability is therefore one of the most important objectives of a society and its people. The issues and concerns embodied by sustainability are broad and numerous. They cover such diverse issues as resource supply (water, energy, mineral, food), climate change and pollution, management of wastes (toxic, hazardous, radioactive, convention- ISSN 2283-7949 GLOCALISM: JOURNAL OF CULTURE, POLITICS AND INNOVATION 2018, 3, DOI: 10.12893/gjcpi.2018.3.4 Published online by \"Globus et Locus\" at www.glocalismjournal.net  al), sanitation, land use and desertification, species extinction and loss of biodiversity, ecosystem degradation, water quality and drought, industrial development, production and consumption patterns, population growth, urbanization, globalization, cultural and social sustainability, natural and anthropogenic disasters, peace and stability and government policies. The breadth of topics related to sustainability suggests that a holistic and comprehensive approach to sustainability is necessary. Sustainability activities are increasingly becoming part of the agendas and operating plans of governments and businesses. The elements of sustainability need to be connected if activities are to attain sustainability goals. In the remainder of this chapter, the historical context for sustainability is briefly described, sustainability concepts and definitions are reviewed, and sustainability is discussed, focusing on its economic, environmental and social dimensions as well as issues and concerns. The related concept of sustainable development is examined. Then, assessment measures for sustainability are considered, and some sustainability applications are presented.",
          "The challenge of achieving sustainability for societies is not limited to the present day, even though the global nature of the problem has come to the fore in recent decades. Sustainability was an objective, at least implicitly, for early civilizations. Since humans transitioned from mobile hunter-gatherers to agriculture and settlements approximately 12,000 years ago during the Neolithic age, in what has come to be known as the agricultural revolution, the sustainability of the lifestyle was crucial in determining if society would thrive or collapse. The decline of the Western Roman Empire constitutes another noteworthy historical illustration of a society that failed to sustain itself and ultimately collapsed. Waning returns from natural resource production as well as social factors contributed to the decline (Tainter 1998). The multidimensional nature of sustainability became more evident in more recent history, especially with concerns over environmental degradation. ISSN 2283-7949 GLOCALISM: JOURNAL OF CULTURE, POLITICS AND INNOVATION 2018, 3, DOI: 10.12893/gjcpi.2018.3.4 Published online by \"Globus et Locus\" at www.glocalismjournal.net  The case of the Polynesians on Easter Island, located in the Pacific Ocean, provides an important historical illustration of how unsustainable practices and lifestyles can contribute or lead to societal collapse. The inhabitants of Easter Island exhausted the resources of their remote habitat to such an extent that they could no longer feed themselves or even build canoes to escape. The ecological damage to Easter Island led to a total societal collapse that decimated its population compared to its peak in the 1600s (Graedel, Allenby 2010).",
          "Numerous definitions of sustainability exist, but none apply for all circumstances. Making sustainability operational rather than vague and theoretical is a challenge, which is made all the more daunting due to the numerous definitions of sustainability. In theory, sustainability can be defined as \"enduring in perpetuity\". However, such a definition is neither practical nor useful. Little if anything can be sustained forever, while much can be sustained over the very short term. A timescale of 50 to 100 years, representing two to four generations, is often viewed as more practical. A simple definition sustainability was stated by Ehrenfeld, who defined sustainability as \"the possibility that human and other forms of life will flourish on the planet forever\". Although this definition incorporates environmental and societal aspects, the timescale is impractical. Taking a more technical perspective, sustainability can be defined in terms of carrying capacity, which indicates the maximum number of people that can be supported in a given area, accounting for availability of resources and the ability of the environment to accept waste emissions. The carrying capacity for a region is in particular dependent on the demand and supply of natural resources. But sustainability encompasses more than technical factors and cannot be defined from an environmental perspective exclusively. Rather, sustainability is often conceptually defined as having three dimensions: environmental, economic and social. An important feature of this approach is the extension of sustainability beyond carrying capacity to include economic and social fac- tors. A multidimensional view of sustainability is consistent with the understanding of how several factors affect whether societies thrive or decline. But these three dimensions are often in tension (e.g., environmental and social sustainability may be achieved to the detriment of economic sustainability). Achieving a sustainable balance is challenging. Sustainability, and its three main dimensions, can be conceptually illustrated in various ways: a) Sustainability can be viewed as founded or supported by three legs or pillars, representing environmental, economic and social sustainability (fig. 1). This is akin to a three-leg table that can only remain standing if environmental, economic and social sustainability dimensions are satisfied. This balance can prove challenging, since the sustainability dimensions are often in conflict; b) Sustainability can also can be thought of as the common point of overlap of three intersecting circles, where the three circles represent the environmental, economic and social dimensions of sustainability (fig. 2); c) A variation on this representation displays sustainability using concentric circles (fig. 3). Economic sustainability, being an activity of people and occurring within communities, is seen to be a subset of social sustainability, which involves many social and cultural dimensions as well as economic factors. Then social sustainability is seen to be a subset of environmental sustainability, since activities of people and societies are carried out within the environment and thus threaten environmental sustainability; d) The three dimensions of sustainability can be also shown in a hierarchal structure, where the economy is a subsystem of society, and society is embedded in the environment (fig. 4). These illustrations are similar, but have subtle differences. All of the illustrations demonstrate the interconnected nature of the environment, the economy and society, and their relations to and effects on sustainability for any a system or process. The above points also demonstrate the truly multidisciplinary nature of sustainability, and its linkages to diverse fields such as science, engineering, environment, ecology, economics, business, sociology and philosophy. Addressing sustainability requires consideration of topics such as resource use (energy and material), economic development, social development, health, environmental stewardship, engineering, design and architecture. It also requires an understanding of how people in the different disciplines interact and relate in addressing these factors.     Sustainability concepts can be focused around a discipline when needed or useful. For example, consider sustainability in engineering, which constitutes the application of science and mathematics principles for practical purposes such as the design, manufacturing and operation of products and processes, while accounting for economic, environmental, sociological and other constraints. Engineering significantly affects economic development, standards of living, societal well-being, the environment and cultural development. Numerous factors need to be addressed in moving towards engineering sustainability (fig. 5), including appropriate selection of resources bearing in mind sustainability criteria, use of sustainable engineering processes, enhancement of efficiency of engineering processes and resource use, and holistic adoption of environmental stewardship in engineering activities. In moving towards engineering sustainability, other key sustainability measures also need to be addressed, such as economics, equity, land use, lifestyle, sociopolitical factors and population. The multidimensional nature of sustainability suggests that achieving it is a systems challenge. That is because the overall in- teraction between subsystems needs to be integrated to achieve a sustainable system or process. For clarity, the three main dimensions of sustainability are each examined below.",
          "The economy and society are subsystems of the environment, which is the source and sink of all material and energy interactions on Earth. The sustainability of humanity implies ensuring an ability of the Earth to support human and activities related to it. Human economies and populations have grown such that anthropogenic activities now have global and long-term impacts, with various consequences. These can degrade the ability of the planet to support life. Many environmental issues affect sustainability. The loss of biodiversity throughout the world, due to economic development and other factors, and threatens the sustainability. Deforestation and development often lead to the destruction of natural habitats and cause the migration of animals to suitable environs. The extraction or use of limited resources and the release into the environment of emissions and wastes to the air, water and land also put sustainability at risk (Aghbashlo, Rosen 2018). Air pollutants released by internal combustion engines in vehicles, fossil fuel-fired electrical power plants and various industrial activities can adversely affect air quality and the health of humans and other living species. Industrial liquid emissions, including wastewater, and runoff from agricultural activity can lead to environmental problems, e.g., bioaccumulation of toxic compounds in the cells of aquatic animals, and eutrophication of water bodies. Two of the most significant challenges regarding environmental sustainability follow: Climate change. Stabilizing the concentrations of greenhouse gases (GHGs) in the atmosphere, in order to prevent the harmful effects of global warming and climate change, constitutes according to most studies one of today's most significant challenges. In the atmosphere, GHGs absorb infrared radiation emitted at the surface of the Earth. This leads to a greenhouse effect and the associated planetary warming. The primary GHG is carbon dioxide (CO 2 ), but there are others such as methane (CH 4 ) and nitrous oxide (N 2 O). The main anthropogenic sources of GHG emissions include fossil fuel combustion, agricultural nitrogen utilization, and enteric fermentation in ruminant animals. Global warming and the risks of climate destabilization are exacerbated by positive feedback effects, such as increased solar radiation absorption from the loss of reflecting surfaces like ice. The Intergovernmental Panel on Climate Change (IPCC) has since 1990 published comprehensive assessment reports reviewing the latest climate science and predictions about future trends. The fifth assessment report, finalized in 2014, reports warming trends due to anthropogenic activities as \"very likely\". As climate models become more sophisticated, the predicted amount of warming has increased, as have the related climate and other impacts. Much effort globally and at the United Nations is focused on achieving international agreements or treaties to stabilize greenhouse gas concentrations in the atmosphere at a level that avoids dangerous anthropogenic climate change (e.g. Berthiaume, Rosen 2017). Stratospheric ozone depletion. The layer of ozone (O 3 ) in the stratosphere reduces the transmission of solar radiation to the surface of the Earth. This stratospheric ozone absorbs ultraviolet radiation, particularly UV-B, which can harm animals and plants. Declines in ozone levels in the stratosphere, especially over polar regions, were observed since the late 1970s. The main ozone depleting substances are chlorofluorocarbons (CFCs), used for decades as refrigerants, which participate in ozone-destroying chemical reactions in the stratosphere. A phase out of CFCs was agreed to in 1989 via the Montreal Protocol, but they are still used in low-income countries. The long residence times of CFCs in the atmosphere implies that the reduced ozone layer concentrations will persist for many decades.",
          "An economy that provides good living standards, the services that people require and jobs is necessary for sustainability in society. A sustainable society requires long-term economic development rather than just economic growth. This latter, often meas- ured as growth in gross domestic product, is what occurs today, where capitalist economies depend on economic growth to generate wealth and jobs. Since the economy operates within a planet having finite resources and capacities (Aghbashlo, Rosen 2018), a continually growing economy is not necessarily sustainable over the long term. Eventually, therefore, the global economy needs to operate more in a steady-state manner, with little or zero growth. The best options for economic sustainability may vary from country to country. For instance, Daly (1990) suggests that wealthy countries need to develop instead of grow their economies in order to conserve resources and waste-assimilation capacities, and that poor countries benefit more from economic growth. Sometimes, economic sustainability arguments are divided into strong and weak sustainability categories: Strong sustainability. This view of sustainability is based on natural capital (the planet's stock of natural resources, including air, water, geology, soils and all living organisms) providing ecosystem services that are not substitutable with human capital (the stock of knowledge, habits, social and personality attributes, creativity, and labor). Environmentalists and natural scientists often prefer this viewpoint of sustainability, seeing natural and human capital as complementary, but not interchangeable. Weak sustainability. This view of sustainability is based on the concept of a constant total capital stock, which is the sum of human and natural capital, and allows for human capital to substitute for natural capital. Economists often favor this view of sustainability, which permits natural resources to decline provided there is an increase in human capital, including know-how and knowledge. The difference between these two views of economic sustainability mainly focuses on the substitutability of human for natural capital. What constitutes societal sustainability may change temporally. For instance, people in the future may inhabit smaller houses, live in higher density neighborhoods, possess less material goods and travel less. Even if this leads to a lower gross domestic product per capita in the future, it may lead to a higher quality of life. Two major factors in societal sustainability follow: Equity. The concept of societal sustainability includes equity within and between generations. Intragenerational equity speaks to equity between people of the same generation, and requires more balanced distribution of wealth. This may necessitate shifts in resources between rich and poor nations and within them. Intergenerational equity focuses on equity between present and future generations, so as to ensure that future generations have are able to a attain a reasonably good quality of life compared to preceding generations. Intergenerational equity spanning two to four generations into the future, roughly equivalent to 50 to 100 years, is often envisioned as a necessary factor for societal sustainability. Health. Human health and well-being is an important factor social sustainability. Important measures of human health include life expectancy and infant mortality (Smil 2007). Many factors contribute to human health, including access to healthy and clean food and drinking water, safe waste disposal, and an environment without harmful substances that can lead to chronic or acute diseases (e.g. toxins and carcinogens). SUSTAINABLE DEVELOPMENT To be developed: people (e.g., life expectancy, quality of life, education, equity), society (e.g., security, institutions) and the economy (e.g. wealth creation, jobs). Social and economic systems of people can be enhanced through development. For instance, the quality of life of people in developing countries can be improved through more equitable distribution of wealth, better education, and less reliance on non-renewable resources, leading to a more equitable society; b) To be developed: people (e.g., life expectancy, quality of life, education, equity), society (e.g., security, institutions) and the economy (e.g. wealth creation, jobs). Social and economic systems of people can be enhanced through development. For instance, the quality of life of people in developing countries can be improved through more equitable distribution of wealth, better education, and less reliance on non-renewable resources, leading to a more equitable society; c) To be sustained: life support systems (e.g. resources, land, soil, water, air), nature (e.g. ecosystems, habitats, biodiversity) and communities (e.g. cultures). Earth and its ecosystems provide ecosystem services (e.g. climate stability, water and air purification) and natural resources. These permit human communities to endure into the future and thrive. Thus, to sustain the services provided by nature that people have adapted to and depend upon, the Earth and its life-support systems need to be sustained. Ensuring that sustainable development is incorporated pervasively and comprehensively into industry, government, planners and people's lives is of great importance. Policies and strategies for sustainable development are needed that are sound, coordinated, meaningful realistic and achievable.  Tab 1. Descriptive Outline of Sustainable Development Goals adopted in 2015 by the United Nations Goal number Description 1 End poverty everywhere, in all its forms 2 End hunger, achieve food security and improved nutrition, promote sustainable agriculture 3 Ensure healthy lives, promote well-being for all at all ages 4 Ensure inclusive and equitable quality education, promote lifelong learning for all 5 Achieve gender equality, empower all women and girls 6 Ensure availability and sustainable management of water and sanitation for all 7 Ensure access to affordable, reliable, sustainable and modern energy for all 8 Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all 9 Build resilient infrastructure, promote inclusive and sustainable industrialization, foster innovation 10 Reduce inequality within and among countries 11 Make cities and human settlements inclusive, safe, resilient and sustainable 12 Ensure sustainable consumption and production patterns 13 14 Take urgent action to combat climate change and its impacts Conserve and sustainably use the oceans, seas and marine resources for sustainable development 15 Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, halt and reverse land degradation, halt biodiversity loss 16 Promote peaceful and inclusive societies for sustainable development, provide access to justice for all, build effective, accountable and inclusive institutions at all levels 17 Strengthen the means of implementation and revitalize the global partnership for sustainable development Note that the term sustainable development is similar to but different from sustainability, even though they are often used interchangeably. Sustainability is a state that can be maintained into the future. But, since development means \"bringing gradually to an improved state\", sustainable development implies a course of action that improves the quality of life of people and societies, and that can endure into the future.",
          "The assessment of sustainability is important for making sustainability operational and measuring and monitoring progress towards sustainability. But assessing sustainability is challenging, since no universally accepted method exists for the task. There are many reasons for this, including the difficulty in measuring the main dimensions of sustainability (Hacatoglu et al. 2016). For example, although emissions of GHGs and ozone depleting substances are measurable, quantifying their economic and social impacts is challenging. Also, despite standard of living often being measured as gross domestic product per capita, quality of life can be a more significant measure of human well-being and satisfaction. Sustainability assessment can also be contentious (Morse, Fraser 2005). Nonetheless, various methods for assessing or measuring sustainability have been developed. Some examples: a) Some assessment methods utilize sustainability indicators, which are typically simple quantitative proxies that measure economic, social, and environmental factors. Some indicators are integrated, combining various dimensions of the environment, the economy and society, while others are not integrated, measuring only a single aspect of sustainability; b) Some sustainability indexes have been developed based on an aggregate or composite of selected sustainability indicators. A single-value measure of sustainability based on an aggregate index is beneficial for understanding and communication, due to its simplicity. But the determination of such indicators necessitates normalization, weighting and aggregation of data. These steps usually lead to a loss of useful information and can be challenging in their own right. A single-value sustainability measure can mask details associated with the multidimensional nature of sustainability and thus be misleading. Nonetheless, aggregate sustainability indexes are often preferred by policy makers in industry and government since they can be more easily interpreted and communicated to stakeholders or the public than multiple sustainability values; c) Daly (1990) developed operational principles for sustainable development. Although useful, these are limited to quasi-sustainable use of non-renewable resources and the use of renewable resources. Few if any sustainability assessment methods consider the environment, the economy and society together. Rather, many assessment methodologies focus on only one dimension of sustainability (e.g. economic development or environmental sustainability). Several examples are considered to illustrate this. First, biophysical assessment approaches are suited to assessing environmental sustainability, by quantifying resource use and environmental impact. But they are normally inadequate for addressing societal and economic facets of sustainability. Second, monetary values can be applied to social and environmental capital to assess sustainability. However, financial valuations for non-market goods and services are not well developed, improper valuations, and difficult due to our limited understanding of ecosystems and the services they provide. Third, the Environmental Sustainability Index ranks countries based on an aggregate of several environmental indicators. But these are usually unable to correlate environmental sustainability and economic growth. A weakness of many existing methods of assessing sustainability is their lack of a systems approach, which regards the system being considered as a whole and accounts for the interactions among its subsystems. This is important because achieving a more sustainable society is a systems problem, where the environment, the economy and society are all interdependent. Coupled humanenvironmental systems have interactions among different systems that lead to trade-offs, e.g. reducing costs may cause a process to have higher emissions or lower efficiency. A non-systems approach focusing on single factors can usually be seen to be inadequate for assessing sustainability holistically. As noted earlier, for example, biophysical approaches focus mainly on environmental sustainability and neglect economic and social dimensions, while methods based on weak sustainability focus on economic factors and neglect the biophysical facets of sustainability. Clearly, the sustainability of a system needs to be assessed with a systems approach. Life cycle analysis is usually part of such an approach, as it identifies material and energy inputs and outputs of a system or process and uses this information to assess environmental, economic and social effects. Although there are environmental concerns regarding the project, it nonetheless is viewed as providing significant environmental benefits. The Red-Mediterranean-Dead Seas Canal project has been shown to be the type of large project that may contribute significantly to sustainability. The benefits, which in many ways drive its contribution to numerous facets of sustainability, are shown in fig. 8. Surveys also indicate examples of common applications of sustainability, and these typically show that there is a strong focus on the implementation of sustainability concepts, actions, by both individuals and corporations. For example, the present author recently examined the actions of engineering practitioners regarding sustainability (Rosen 2013). Some specific findings were that the main sustainable technology priorities are using less energy and natural resources, reducing emissions and material wastes, and utilizing renewable, recyclable and recycled materials, and that challenges to sustainability exist, like economics viability. A breakdown of the degree of involvement of engineers with   sustainability or sustainable technologies showed that more than two-thirds of engineers are involved to some degree in sustainability, usually by working on sustainable products and processes. Also, the factors most likely to influence an organization's use of green design practices and procedures are regulatory requirements, client demand and rising energy costs (fig. 9).",
          "Although sometimes vague and often complex, sustainability is an important aim for societies and humanity. Sustainability is a multidimensional concept, encompassing economic, social, environmental and other factors. Sustainability assessment tools are needed to evaluate processes and systems, and how changes affect their sustainability. This usually necessitates a comprehensive set of indicators of the sustainability of the environment, the economy, technology, society, and institutional systems. Many human civilizations in the past proved to be unsustainable to some degree. It is hoped that the coverage provided in this chapter, including sustainability definitions and historical contexts, its economic, environmental and social dimensions, sustainable development, sustainability assessment measures and relevant applications, will assist efforts to address sustainability challenges today and, as an even more important priority, in the future. "
        ],
        "ground_truth_definitions": {
          "Sustainable development": {
            "definition": "development that has the ability to be sustained into the future for some lengthy period of time, and can be viewed as the sustained and continuous improvement of a system or process.",
            "context": "Sustainable development can be thought of as development that has the ability to be sustained into the future for some lengthy period of time, and can be viewed as the sustained and continuous improvement of a system or process (Kates et al. 2005; Rosen 2017a). Development implies a qualitative improvement and dif- fers from simple growth, which is a quantitative increase scale.",
            "type": "explicit"
          },
          "Sustainability": {
            "definition": "“the possibility that human and other forms of life will flourish on the planet forever”.",
            "context": "A simple definition sustainability was stated by Ehrenfeld, who defined sustainability as “the possibility that human and oth- er forms of life will flourish on the planet forever”. Although this definition incorporates environmental and societal aspects, the timescale is impractical.",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "22cb049f711570de72ea60a5f6b49fdf4459d589",
        "sections": [
          "Social movement actors seeking to influence public and political agendas are highly dependent upon the mass media to help them communicate their political messages to a wider audience. In his classic work, \"Protest as a Political Resource,\" Lipsky argues that leaders of groups lacking routine access to the policy process can appeal to their ultimate (legislative or governmental) targets only indirectly by attracting influential third parties to their causes. Because they lack direct contact to these influential publics, most relatively powerless groups can only communicate with these audiences through the mass media {Lipsky, 1969}. The message that is conveyed, and the government response it helps generate, are conditioned largely by the influence of the bystander publics that movements seek to reach. Since the time of Lipsky's seminal article, there has been a tremendous growth in the use of public protests as a form of political action, particularly in Washington D.C. [INSERT NUMBERS??] This growth in the frequency of public protest contributed to the development of policing institutions designed to manage public protests {McCarthy, 1998 #xxx}. As a consequence, some have argued that protest activity has become normalized, and is therefore less effective as a political resource for powerless groups {??Everett, 1991; OTHERS --CITE???. Whatever these claims, social movement actors continue to advocate change through the use of public demonstrations. Protests are an almost daily fixture in the United States's capital city, and their numbers show little sign of yielding, despite more effective policing and the fact that the social movement sector has developed other methods of communicating with a wider audience -such as door-to-door canvassing and direct mail --that do not depend on the sympathies of the mass media. Indeed, the importance of the mass media for social movement actors in the United States may be further enhanced by the greater reliance of citizens on mass media news sources --especially televised ones --and the declining influence of locally published newspapers {Iyengar, 1985}. Our research has attempted to illuminate how the mass media packages the images of protest they project to the general public, including those influential third parties Lipsky views as vital to protest success. Lacking direct contact with influential groups, social movements must rely on the mass media as an indirect means of communicating with potential sympathizers. But movements' heavy reliance on the mass media complicate their efforts to convey political messages, as media institutions operate under logics that are independent of --and often contradictory to --movement agendas. This can undermine social movements' impact on public policy. Understanding relationships between media routines and social movements is therefore crucial to illuminating processes of social change. Social movements try to transform the general notions in society about what problems deserve attention and about how these problems should be addressed. This means that they are often engaged in framing contests with other actors in society as they seek to raise issues to the social agenda and to suggest ways of thinking about these issues. Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem. But political actors, including social movement actors, are not simply concerned with influencing what people think about, but they also hope to shape how people think about social problems and their solutions. Thus, protesters are ultimately interested in shaping the agenda-building process by helping define how issues are framed in the mass media. In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story. This paper seeks to uncover the factors that shape the portrayal of public protests in the mass media by examining media reports on protest events that took place in Washington, D.C. during 1982 and1991. By doing so, we hope to illuminate the political dynamics of agenda-building processes as well as to provide an interpretive baseline for researchers using media records as evidence of protest activity.",
          "Our findings regarding media selection bias suggested that media agendas can drive the selection of protest events that are reported, independently of the numbers or activities of protesters themselves. In other words, \"objective\" matters such as the demonstration's size and characteristics have little bearing on whether or not the event is reported in the mass media. This appears increasingly true in more recent years, and is especially the case in television news reporting. Thus, social movement agents seeking to influence media agenda-setting cannot simply rely on generating large numbers of protesters or on engaging in unusual forms of collective action. Rather, they must also consider how their issue might be tied to media issue cycles. In other words, rather than simply reflecting \"objective\" occurrences, the operations and procedural logics of mass media institutions are important predictors of what events get reported. The practical need to select a limited number of issues on which to report leads to claims of selection bias in how editors and reporters go about the business of determining the news that is \"fit to print.\" Since they are, by definition, \"outsiders\" to political institutions and processes, social movements are typically marginalized by mainstream media routines. Once they achieve media attention to their protest event, demonstrators often find that the coverage of their protest fails to portray their cause as they would have liked. Often the more unruly activists are highlighted or the most vocal leaders advanced by reporters seeking images that they hope will appeal to their mass readership {Gitlin, 1980}. The specifics of protesters' policy concerns or the organizational details involved are largely irrelevant to those reporting on public protests. This may be a function of media gatekeepers, whose motivations, routines, and professional interests support the status quo. It may also result from the failure of many social movement challengers to engage in conscious efforts to wage an \"insider media game\" {Ryan, 1990 #xxx :5}. Plagued with limited staff, time, and money and with the inherently contradictory necessity of simultaneously appealing to constituents, publics, and targets, social movement actors often lack media strategies. Ryan (1990) summarizes the main theoretical models of news production routines according to: gatekeeper models which focus on daily operations and routines of media institutions; political economy or propaganda models, which explain media routines according to broad socioeconomic forces including media ownership patterns; and cultural studies, which emphasizes the role of mass media in maintaining the political and social hegemony of elites {Ryan 1990 #xxx : 22-25}. Clearly the operations of and influences on the mass media as well as the consequences of how the news is received are of interest to social movement researchers and activists. And recent media research suggests that a dynamic, process-oriented approach will best explain how the news is produced and to what consequence {#xxx see, e.g. Clayman, 1998; OTHER CITATIONS HERE?}. Gatekeeper Model. Gatekeeper models presume that in order to understand how social movements interact with the mass media, we must account for the institutional logics that shape how mass media operate. This approach might also be called the \"newsmaking approach,\" as it sees the mass media as not simple mirrors that reflect what goes on in society, but rather as actors in and of themselves that construct images and compile messages {Kielbowitz, 1986 #xxx :75}. Viewing media institutions as news makers leads one to account for how they determine what constitutes a newsworthy event. Social movements are faced with the dilemma of justifying why some chronic problem such as poverty or racial injustice is more worthy of attention today than it was yesterday. In other words, those seeking to raise routine issues onto media agendas must portray these issues as changing or somehow exceptionally interesting in order to mesh with media newsgathering routines. The identification and use of sources is another area where gatekeeper models help us understand media routines. Gans {#xxx 1979} and others have pointed out the institutional logic behind the decisions about deploying reporters and selecting sources --all of which work against the access of social movements to the mass media. Editors favor reporters who are generalists rather than specialists in the area on which they write, because generalists are cheaper, easier to control, and they are more reliant on their sources for information (thus removing responsibility from the media organization). A preference for government and official experts as sources derives from the ease of reaching their centralized press offices and the norm that has evolved of viewing government sources as credible and unbiased, requiring no routine fact-checking or corroboration. The reliance on government sources thus both reduces the costs of newsgathering and limits the risks {Herman, 1995}. It also tends to reproduce the agendas of these sources {Gans, 1979; Kielbowitz, 1986}. The need to understand media logics and to find ways to mesh with media news gathering routines means that movements whose leaders provide the most appealing media images and rhetoric will be the most likely to gain media coverage {Gitlin, 1980}. Movements, moreover, need to provide consistent streams of images that appeal to the mass media in order to sustain media focus on the issues. This leads activists to escalate their tactics in an ongoing effort to create \"new\" hooks for media attention {Kielbowitz, 1986}. Social movement actors have long recognized that generating media attention requires the creation of an event that is somehow \"newsworthy.\" Social movements have traditionally relied on public demonstrations --among other activities --to attract media attention that might further their aims and amplify their issues on social agendas. However, the tactics that are most likely to attract media attention are not often those that win respectability among the legislative and governmental targets that movement actors ultimately hope to influence {Gamson, 19xx}. Propaganda Model. Propaganda models generally assume that the structure of media ownership leads to an inherent bias in the mass media in favor of capitalist interests. Scholars adopting this perspective such as Edward Herman have argued that the privatization and commercialization of broadcasting during the 1970s and 1980s has severely constrained access to the mass media and limits the range of ideas conveyed through them {Herman, 1995 #xxx: 169}. The \"free market model\" outlined by Herman sees market forces \"naturally\" marginalizing dissent by crowding out any ideas for which there is no substantial economic backing, or \"effective demand.\" The commercialization of the airwaves restricts democratic access to the mass media by: 1) limiting media control to those with the large amounts of capital required to purchase and run large corporate structures that control media networks; 2) creating a market where reliance on advertising is essential to media operations; 3) driving the search for cost-effective news production, where the crucial need for cheap, \"credible\" sources leads editors to favor centralized government sources over hard to gather dissident sources that (unlike government sources) require fact-checking; 4) creating a fear of \"flak\" or \"negative feedback\" that imposes costs on or threatens the media, often leading to self-censorship i ; and 5) reinforcing ideological premises that favor capitalist institutions such as private property, the merits of free enterprise, the threat of state ownership and intervention, the benevolent role of the government in international affairs, and anti-communism. \"One of the great merits of this system of control is that it operates so naturally, without collusion or explicit censorship, merely by the decentralized pursuit of a set of micro-interests. It 'just works that way'\" {Hermann, 1995 #xxx :170}. One effect of the corporate control of media institutions is that news production becomes dependent upon what advertisers will support. When they buy air time, sponsors align their products with certain images. As consumers demanding services from news production agencies, advertisers can withold support of certain kinds of news programming. For instance, the portrayal of news about the Persian Gulf war was shaped to make it more appealing to advertisers: Pressure from advertisers led to the proliferation of upbeat \"home front\" stories of the war as network executives sought to assuage the worries of their sponsors. For instance, CBS executives \"offered advertisers assurances that the war specials could be tailored to provide better lead-ins to commercials' with 'patriotic views from the home front '\"{Andersen, 1995 #xxx:211, Quoted from Carter 1991}. While this kind of influence over programming may be good for business, it does little to advance peoples' understandings of complex social and political issues. And it divides the agendas of social change agents even further from those of the news media, which --in theory --serves as the public's \"watchdog\" against abuses by government, business and other elites. The propaganda model would lead one to expect little if any media coverage of social movements that threaten the capitalist order. And whatever coverage such movements might attain would be expected to marginalize or otherwise de-politicize their messages. One additional theme in the propaganda perspective is the focus on the increasing centralization of mass media institutions into corporate structures that control other aspects of social and economic life. The emergence of media monopolies has serious implications for the free exchange of ideas. With a media environment that is relatively free of competition, there are smaller news holes and fewer incentives to seek out innovative stories. This further threatens social movement access to the mass media {Kielbowitz, 1986}. Cultural Studies Model. Like propaganda models, cultural studies models of media operations explain the choices made in the selection and interpretation of the news in terms of broader power relations. The focus of these models, however, is more upon the subtle ways in which economic and political elites control images and meaning. According to these perspectives, the conveyance of information is used to reinforce ideas and interpretations of events that support existing power structures. Such models would lead us to expect that mass media coverage of social movements, particularly those that promote ideas that differ most strongly from dominant ideologies, will tend to marginalize social movement agendas. One way that this done is by framing stories in a way that suggests individualized or non-systemic causes and solutions to problems on which social movement activists work. Iyengar's {1991} field experiments led him to support the ideas in the cultural studies model; he concluded that how the media portray a problem affects public attributions of blame and their presumptions about appropriate solutions. Iyengar's study begins with the assumption that news stories can be categorized as either \"episodic\" or \"thematic\" in nature. Episodic news frames are event-oriented, and they focus on concrete issues such as an individual welfare mother or a terrorist bombing. In contrast, thematic news frames provide information that helps contextualize social problems, focusing more on general trends and conditions that set the stage for more dramatic events such as terrorist attacks. \"The essential difference between episodic and thematic framing is that episodic framing depicts concrete events that illustrate issues, while thematic framing presents collective or general evidence\" (Iyenger 1991:14). Thematic coverage in Iyengar's study was associated with systemic attributions of blame, while episodic coverage typically led subjects to see individuals as responsible for the problem at hand. Relating this idea to the relationship of social movements to the mass media, we note that social movements often seek to attract media attention to some broad social concern by generating an event that may be newsworthy in itself. But this demands that the coverage of the protest event focuses on the issues the movement seeks to address, rather than on the event used to attract media coverage. By creating a newsworthy event or episode, movement leaders hope to draw attention to underlying themes of injustice, inequality, or oppression. Social movements address chronic social problems that they attribute to some abstract, systemic cause. In order to appeal to the mass media and the general public, they must find ways to link these chronic problems with some more concrete, tangible event or focus. In other words, they must try to repackage what they see as an ongoing, systemic problem (e.g., poverty, racism) into an \"acute\" issue that demands urgent attention on a crowded social agenda {see Rochon, 1997 #xxx :178-9}. Movements generally seek support for their claims that the political and/or economic system is the source of some problem and that without systematic reforms the problem will persist. ii Thus, we would expect that more thematic --rather than episodic --coverage of their issues best advances the aims of social movement actors. Moreover, because they use protest events as \"pegs\" on which a news story might be based, we would expect to see a tendency towards more episodic reporting of social movement protests.",
          "Our understandings of media routines and institutions led us to expect that the print media would provide more thematic coverage of political issues related to protest events, given that its audience tends to be among the more politically active and attentive segments of the general public {Iyengar, 1985; OTHER CITES on participation & media attentiveness}. However, our earlier study of selection bias suggest that the relatively fewer stories on protest that are selected by the electronic media are chosen because they correspond to ongoing media issue agendas {McCarthy, 1996}. This may mean that television coverage tends to be more thematic than print coverage, which was less sensitive to issue agendas in its selection of protest events on which to report. Our first hypothesis accounts for these contradicting tendencies. H1: Television news is more selective in the range of social movement events it reports, but the coverage of these events is more thematic than print coverage. Media routines and gatekeeper motivations would also lead us to expect that extraordinary activities such as arrests and violence will attract more episodic coverage. This is partly due to the fact that these events themselves are considered newsworthy, especially in cases such as the demonstrations by and against the Ku Klux Klan, which triggered large-scale riots in downtown Washington D.C. Arrests and violence also signal some social controversy, which reporters and editors often seek out in their daily reporting routines. Certainly the presence of dramatic incidents and controversy on which to report does not preclude a reporter from incorporating some discussion of the underlying social tensions that motivate public demonstrations. Indeed, the main strategy behind public demonstrations is to trigger attention to some broad social problem through a dramatic event, and the character of the drama (e.g., size of demonstration or personal sacrifices made by protesters, etc.) should highlight the relative importance or scale of injustice the issue involves and thereby generate more in-depth consideration of the issue. But small newsholes, reporters' limited knowledge, and financial concerns of media gatekeepers constrain the extent to which the underlying concerns of protesters become the primary focus of stories on highly controversial protest events. In short, while demonstrators may seek to use extraordinary activities such as violence or civil disobedience to gain media attention, the attention they get in response to these activities may contribute less to their overall political objectives than was initially hoped. H2: Controversy such as the presence of counter-demonstrations, arrests and/or violence generate more episodic than thematic coverage of a protest event. Issue attention cycles are also likely to be important forces shaping media gatekeepers' reporting on public protests. As we noted earlier, the issues motivating protests were important factors in explaining which demonstrations received media attention. In some cases, protests provide a segue into reports on ongoing social problems just as reports on airline crashes often turn into discussions of the overall safety of air transportation infrastructures. So the daily updates on the progress of the war in the Persian Gulf were often accompanied by highlights from the ongoing demonstrations against and for the war. Or a protest by homeless advocates may trigger a more in-depth story on the causes of homelessness or national policies to mitigate its effects. This leads us to our third hypothesis: H3: Issues that are related to ongoing media issue agendas will attract more thematic coverage than episodic coverage. Propaganda and cultural models of media reporting on social movement activity would lead to expectations that most coverage of protest events would tend to favor authorities in their \"spin,\" and that they would rely on sources outside the movements for information about the protest and/or the issues motivating it. These models would also predict that coverage of protest demonstrations --particularly those promoting goals that contradict major economic interests --would tend to be episodic. Less controversial or threatening issues may attract more thematic coverage, and in these cases news reporting may even appear to favor demonstrators and even to rely on movement leaders for information. iii H4: Social movements that most threaten the existing socioeconomic order will attract more episodic than thematic coverage. H5: Media coverage of social movements will tend to favor the status quo; the \"spin\" of news stories will in general favor government and other authorities. Our account here of the relationships between episodic and thematic coverage and social movement protests builds upon the belief that social movement aims are best served by coverage that is more thematic, or that delves into the underlying social problems to which movements attribute some injustice. We would hold, then, that stories taking a thematic rather than an episodic frame would be most supportive of movement actors. This leads us to our sixth hypothesis about the relationship between the \"news spin\" and the framing of a story: H6: The \"spin\" of thematic stories is more likely to favor social movement actors than is that of episodic stories.",
          "Media stories on public protests in Washington, D.C. were collected for the entire years of 1982 and 1991 from the Washington Post and The New York Times, and from the Vanderbilt Network News Index and Abstracts summaries of television network news broadcasts. Media records were matched with evidence on the protest event drawn from police records. iv A detailed codebook was developed to assess various features of the coverage, focusing particularly on the extent to which the coverage reflects a thematic or episodic frame. v Coders read each media story and coded the reporting on each demonstration mentioned in the story. Thus, a single article may have been coded three separate times, each reflecting the details of a distinct protest event. Roughly two-thirds of all articles or news abstracts reported on more than one protest event. Often, news stories reported on protest campaigns or series of linked protests that are carried out over a number of days by the same groups and for the same issues. Our work to develop and test a codebook pointed to the difficulty of systematically assessing the extent of episodic or thematic coverage in each protest event report. We therefore developed multiple different measures of each concept, asking coders to record their level of confidence for judgements related to episodic or thematic coverage. [INTER-CODER RELIABILITY; CONFIDENCE RATINGS] We included specific questions asking for separate evaluations of how much (on a 7 point scale) of the text of an article was devoted to describing the goals of protesters, and to the details of the protest event. Following these more concrete questions, coders were asked to make judgements (again, using a 7 point scale) about how episodic an article appeared (not at all or \"highly episodic\") and how thematic the article appeared. We did not presume that these concepts were mutually exclusive, and we left it to the coder to judge each concept independently. Finally, the coder was asked to place each protest event report on a 3 point scale as to whether it was \"primarily episodic,\" \"primarily thematic,\" or a \"mixture of episodic and thematic.\" In addition to coding evidence about the framing of a particular event report, coders recorded information on the location of the story in the news lineup, the amount of time or space devoted to an event report, the \"spin\" of the coverage (whether it favored protesters or authorities), and whether there were reports of counter demonstrators, violence, or arrests. Using this trichotomous scale we find that, of 635 protest event reports for which relevant data were present, 34% were considered \"primarily episodic,\" 24% were \"primarily thematic,\" and 41% were a mixture of both thematic and episodic coverage. Table 1 presents correlations between this summary measure and other indicators we collected on the episodic and thematic nature of each event report. vi Table 1 About Here On the specific questions where coders were asked to determine how much of the article was devoted to the details of the protest or to the issues underlying protesters' activities, we found strong correlations which reflected the expected relationships. Reports with primarily episodic coverage (coded 1) tended to have greater emphasis on the details of the event, while those that were primarily episodic (coded 3) devoted more attention to the issues and specific goals addressed by protesters. Also, coder evaluations of the extent of episodic or thematic coverage correspond highly with their ranking of a story as primarily episodic or primarily thematic. The one measure that does not appear to adequately capture the concept we hoped it would is the question of how much the report emphasized incidental details of the event such as the weather or protesters' clothing, which we had presumed would be related to more episodic framing. Given the correspondence of our other measures, however, we believe that we have constructed a reliable and valid measure of news framing. Having determined that our measure of news framing is valid, we examined whether the framing of news coverage is related to variables associated with the source of the news report, the context of the demonstration, or the issues the protesters seek to address. In addition to the episodic/thematic variable, we look at the relationship of our predictor variables to two other measures of relationships between social movements and media coverage. These include news spin, or the extent to which the story favors demonstrators (coded 1) or authorities (coded -1) and a 7-point scale of the extent to which the story discusses the issues protesters are addressing (ranging from 0-\"none\" to 6-\"all\"). vii",
          "Table 2 shows positive relationships among the dependent variables we examine in this study. Stories whose spin favors demonstrators tend to devote more attention to discussion of the underlying issues (as opposed to the details of the protest event) and to encompass a more thematic frame. As expected, there is a strong tie between the amount of the story devoted to the issue and thematic framing. Source variables were related to the extent to which a report was thematic or episodic. Television news tended to be more thematic than print news coverage. The reliance on any demonstrator source (e.g., organizer, literature, posters, etc.) as opposed to a government or \"expert\" source for information on the story was related to more thematic coverage. Over time, the coverage of the issues was slightly more thematic, and more attention was devoted to the issue itself rather than the demonstration, but there was less tendency for stories to \"spin\" towards demonstrators in the later time period. viii Context measures focused on whether there was some kind of controversy involved in the protest event. The presence of a counter-demonstration tended to produce more episodic framing of the report. Also, the presence of arrests and/or violence was associated with more episodic frames. This finding is reinforced by the negative relationships between the amount of the story devoted to discussion of the issue and conflict measures. Bivariate correlations also show that where controversy or counter protests are present, the spin of the story tends to favor authorities rather than protesters. The propaganda and gatekeeper models of news production would lead us to expect the issues at stake in a public demonstration to be important predictors of the news frame. Indeed, we found that issues were important in determining whether or not a protest was reported in the media, so it would not be surprising to find that issue cycles also drive the nature of media coverage. We took the top issues around which demonstrations were held in 1982 and 1991 for which there were significant numbers of media reports. ix Many issues that attracted the largest numbers of protests did not receive enough media coverage to assess description bias in the reporting on those protests. x The bivariate correlations between particular issues and news frames reveal none or very weak relationships. The strongest pattern appears in the coverage of protests involving the Ku Klux Klan, which tended to favor authorities in their spin, to devote less space to discussion of the issues, and to be more episodic. These patterns are probably due to the frequent presence of violence and arrests at these events, which tends to focus coverage more on the protest event itself rather than on the motivations of demonstrators. The disruption and violence that helped attract media attention to the story also led to stories favoring authorities rather than movement actors. Other patterns of note are that the news spin in these instances reveal a tendency of the media to support nationalistic frames. The coverage of the protests against the Gulf war tended to support government authorities, while that of demonstrations marking the opening of the Vietnam Veterans memorial in Washington, D.C. supported demonstrators. The coverage of the Gulf war, moreover, tended to be more thematic than coverage of other issues. This ongoing, major media story attracted massive amounts of news coverage, and demonstrations provided a regular source of \"hooks\" for these reports which quickly became routine in the daily newspapers and nightly news reports. Table three presents regression estimates for models predicting the amount of a story devoted to the issue, and news frame (episodic/thematic scale) from variables related to the media source, the context of the protest, and issue. Table four presents the analysis of our news spin variable, examining what factors appear to be related to whether or not the stories' subtext supports demonstrators or authorities.",
          "The analyses in table three show consistent patterns of greater thematic emphases in television as opposed to print media reports. xi The models also reveal that stories relying on government or expert sources had characteristics that worked against social movement framing efforts: They were more episodic, and they devoted less space to a discussion of the issues at stake. Looking to table four, we can see that stories depending on government or expert sources also tended to favor the interests of authorities rather than demonstrators. Another pattern that was fairly strong and consistent across the models was the role of controversy in shaping coverage of protests. The presence of a counter-demonstration tended to detract media attention away from the underlying issues relevant to a demonstration and towards more episodic coverage. It also fostered coverage that tended to favor authorities. Similar patterns are seen when arrests and/ or violence was present as part of a demonstration event. Issues had less consistent and generally weaker relationships with the dependent variables we examine, and they contributed a relatively small amount to explaining variation in the dependent variables. One unexpected finding is that coverage of the Gulf war tended to be more thematic, but the negative coefficient for the model predicting amount of the story devoted to discussing the issue suggests that the coverage remained superficial. xii Coverage of demonstrations around the establishment of a Vietnam Veterans Memorial tended to be thematic and their spin favored demonstrators. In contrast, coverage of anti-Ku Klux Klan demonstrations (which degenerated into violent confrontation) tended to be episodic, to spin towards authorities, and to devote relatively less attention to the issues underlying protests. In table four we also examine how the news frame is related to the spin of the story. We hypothesized earlier that thematic coverage would generate news spin that favored demonstrators. The bivariate correlation between our thematic/ episodic scale and news spin suggest some support for this argument (r=.289). The last column of table four provides further support for this argument while controlling for other variables. Adding the episodic/thematic scale to the model explains an additional 6% to of the variance in news spin. Thus, our hypothesis that thematic coverage reinforces the goals of social movements is supported. The substantive importance of thematic coverage for social movement framing contests lead us to ask for more detail about the relationships between news framing across print and electronic sources. Iyengar's research led him to expect greater thematic emphases in the print media, given that this source generally has a larger potential newshole than national network news programs {Iyengar, 1991 #xxx :129}. Table 5 contrasts the coverage in the print media sources with that in the electronic media, attempting to uncover any systematic differences in episodic versus thematic framing. The first two columns display the analysis of all cases, and they reveal a strong difference in news frames between print and electronic media. Indeed, the strongest effect is found here, and electronic sources tended to provide more thematic coverage. The next strong effects, which hold throughout most of the other models, are those measuring whether protesters were relied upon as information sources for the report and whether or the \"spin\" of the report favors protesters or authorities. As the propaganda and cultural models would predict, when protesters and their banners or flyers constituted the main source of information for a report, news frames tended to be more episodic. Experts and government sources were more likely to be consulted in stories with a thematic emphasis. Also in cases where the \"spin\" of the story appeared to favor demonstrators, the coverage tended to be thematic, and where the \"spin\" favored authorities, the coverage of the protest was more episodic. xiii   ",
          "The presence of controversy around a demonstration --such as a counter-protest or the presence of violence or arrests --had a smaller impact on the nature of coverage, though it led consistently to more episodic reporting, as we had expected. Finally, in the model including all the cases, we found that issue agendas contributed relatively little to explaining variation in news frames. The Gulf war coverage tended to be thematic, but other issues had little overall effect. In contrast, the model including only the electronic media reports shows incredibly strong effects of issue agendas. Measures of the model's explanatory power increased dramatically with the addition of issue variables. In television reports, coverage of veterans' issues and of protests surrounding the Ku Klux Klan tended to be episodic, while Gulf war coverage, which made up a large proportion of the electronic reports, were typically thematic. In the print media reports, we found much smaller effects of issue agendas on the tendencies of news frames.",
          "We set out in this study to examine how social movement attempts to influence media coverage impact their efforts to affect public debates about social and political change. Based on the work of other social movement researchers, we know quite well that public demonstrations have uncertain and often contradictory effects on social movement campaigns. And yet they remain one of the main tactics available to social movement actors in their efforts to shape broader public agendas. The hypotheses stated above set out expectations about the relationships we should expect to see given what we know about social movements' relationships with the mass media. In short, we expected that media reports overall would not favor demonstrators' interests, particularly on issues that most threaten existing economic and political elites. Supporting our earlier findings about coverage of protests by television news media, we found strong tendencies in both 1982 and 1991 for reports on public demonstrations in the television media to be more thematic than print media reports. This finding reinforces the notion that television news is more driven by issue agendas than by some independent selection of events deemed newsworthy. Demonstrations selected for television news coverage tended to fit within ongoing issue agendas, and the content of the stories tended to focus on the issues themselves more than on the details of the protest event. xiv   In keeping with the expectations of the gatekeeper model, we found that controversy generated media coverage that was more episodic. The presence of counter-demonstrators, arrests, and/or violence produced more reporting on the demonstration event itself and less attention to the issues at stake. These conditions also tended to produce news spin that favored authorities rather than demonstrators. Also, we have limited evidence on which to base our claim that issues related to ongoing media agendas may receive more thematic coverage. The evidence that suggests support for this claim is the reporting on the anti-Gulf war demonstrations, which provides only one case that is suitable for testing this hypothesis. Additional research based on a larger range of issues should test this hypothesis further. We were not able to adequately test our hypothesis derived from propaganda models of media reporting, largely because our dataset did not include enough stories on demonstrations that could be considered to be about issues potentially threatening the socioeconomic order. xv One issue that Herman and Chomsky have directly linked with their claims about the pro-capitalist bias of the news media is the coverage of protests against U.S. intervention in Latin America. Although a large proportion of all Washington D.C. demonstrations were focused on this issue [INSERT % of ALL 1982 demos on LATIN], only 8 of these were covered. xvi This suggests that a pro-capitalist bias may be more strongly operating in the agenda-setting process than in the priming or agenda-building that the media does. The reports we examined on protests related to socioeconomic issues did not provide strong evidence one way or another here. Although the sign of the regression coefficients for our episodic/thematic scale and for the amount of the story describing the issue run counter to our expectations, the coefficients are weak and non-significant. Further examination is warranted to test the hypothesis that movements posing a threat to the capitalist order receive media attention --to the extent that they receive it at all --that marginalizes their messages. Our last two hypotheses relate to our expectations about the news spin of the reports on protest demonstrations. Our fifth hypothesis, that the \"spin\" of protest event reports will in general favor government and other authorities was not supported by the evidence we have assembled here. Only 27% of all the stories we coded revealed any discernible bias, and the bulk of those stories favored demonstrators rather than government authorities. Nevertheless, the news spin variable still showed patterns that are consistent with what we expected. The \"spin\" of thematic stories was more likely to favor social movement actors than was that of episodic stories. And stories that relied on government and expert sources tended to reinforce the agendas of those sources by displaying news spin that favored authorities. In conclusion, social movements seek to both to draw attention to issues they deem important. They also seek to influence how these issues are portrayed to the general public. A common strategy for influencing public and policy agendas is to stage mass demonstrations as a way to dramatize a problem. But this common social movement strategy is not an effective tool for either influencing media agendas (roughly 2% of all public demonstrations in Washington D.C. receive any media attention) or for influencing the media frames used to portray issues. Of all news stories on demonstrations in 1982 and 1991, only 24% were reported in a manner that emphasized primarily the social issues manifest in protesters' demands. An additional 41% of the stories included some discussion of the underlying issues along with reports on the protest event itself. Thirty-four percent of all reported demonstrations were treated primarily as events in their own right: the underlying demands were left unexplored by the media reports as the activities of protesters and police, incidental factors such as the weather, and other details of the protest event constituted the focus of the story. According to research on the impact of news frames on peoples' interpretation of social problems, the episodic frame through which many public protests are portrayed leads consumers of mass media away from systemic explanations for the problems protesters are seeking to address: [E]xposure to episodic framing tends to elicit individualistic (or nonsocietal) attributions of responsibility for most of the issues studied. . . . Because television news generally fails to activate (and may indeed depress) societal attributions of responsibility, however, it tends to obscure the connections between social problems and the actions or inactions of political leaders. By attenuating these connections, episodic framing impedes electoral leadership. . . . In the long run, episodic framing contributes to the trivialization of public discourse and the erosion of electoral accountability. {Iyengar, 1991 :141, 143} News frames that are thematic tend to portray social movements in a positive light. However, public demonstrations do not appear to be an effective means of communication for challengers. Changes in the mass media industry also threaten opportunities for wider public debate, including the consideration of social movements' agendas. Corporate ownership and centralization, combined with management strategies that have downsized the news staffs of major networks {#xxx see Kimball, 1994} generate news gathering routines that are increasingly less likely to include social movement sources or activities. -Implications for social movements--media strategy -Implications for research ii. This may be called system attribution, and it is seen as a key to the emergence of social movements. McAdam {#115 1982} refers to this as \"cognitive liberation.\" iii. Recall Iyengar's finding that thematic coverage leads viewers/ readers towards more systemic explanations for social problems. Thus, thematic coverage of issues would tend to support many social movements' objectives, since movements generally seek the transformation of broad social and political institutions and require a system critique in order to mobilize and motivate adherents. iv. These data are described in McCarthy, McPhail, and Smith (1996). v. The codebook is available from Smith or McCarthy. vi. Developing questions that would generate reliable and valid measures of thematic and episodic coverage proved more difficult than we had anticipated. Our codebook basically leads the coder to ask whether the coverage tends to focus on the details of the demonstration itself or on the issue more generally. Of course, we would have liked to ask whether the coverage of the issue tended to be episodic --e.g., focused on individualistic or localized interpretations rather than systemic ones --but this proved impossible given the range of issues covered in this study. The latter kind of question is most appropriate in a study that focuses on a smaller range of issues in greater detail. vii. Just over six percent of all cases were coded as having a spin that favored authorities, compared to 21% favoring demonstrators. The remaining cases (73%) were considered neutral. The mean score for the amount of the story devoted to discussing the relevant issues was 2.47 (st.dev.=1.42). viii.Further research is needed to determine whether these are trends or idiosyncratic. ix. In both years, this includes the top protest issue, which was the Israeli-Lebanon war in 1982 and the Gulf war in 1991. x. These issues included, for instance, demonstrations against U.S. Central America policy in 1982, demonstrations against the policies of foreign governments in both years, and pro-environmental demonstrations in 1991. xi. The large number of television reports devoted to coverage of the Gulf war in 1991 meant that there may have been an unusually high emphasis on thematic coverage of antiwar demonstrations. But the patterns of greater thematic emphases in the reporting on demonstrations in the television news was found when we repeated the analysis for only the 1982 cases. xii. This may be a function of the nature of this study, where we had to ask coders to base decisions about thematic vs. episodic coverage on the story's emphasis of the issue at stake versus the details of the protest itself rather than on some decision about the framing of the issue the demonstrators sought to advance . For instance, in the Gulf war case, there were likely to be news stories of the war regardless of whether or not protests occurred, and the stories that appeared seemed to use the demonstrations as a \"hook\" for routine coverage of the war. These stories involved much discussion of the war effort and administration policy, but were not generally concerned with detailing the critical views of antiwar protesters (see also note ??6?? above). xiii. Most reports (73%) were considered to have no readily observed \"spin\" that favored authorities or demonstrators. Roughly 6% of the reports were coded as favoring authorities and 21% favoring demonstrators. xiv. This points to a limitation of Iyengar's claim that television news coverage is less thematic than print news, although it is relevant for only a small portion of news stories devoted to covering protest events. Indeed, had we --as Iyengar did--reviewed all the coverage of the issues about which we were concerned, we might also find that print media were more thematic in their reporting. Instead, we look only at stories on public demonstrations in the nation's capital. xv. Our attempts to characterize demonstrations according to the extent to which each posed a threat to the socio-economic order were highly problematic, in part because few demonstrations posed any serious threat. This suggests that cross-national or historic research may be necessary to systematically assess this hypothesis. xvi. Of the eight stories on anti-interventionist protests regarding Central America, three were considered \"primarily episodic,\" one was \"primarily thematic,\" and four were \"a mixture of episodic and thematic coverage.\""
        ],
        "ground_truth_definitions": {
          "agenda-setting": {
            "definition": "the process of identifying and advocating issues for inclusion on public and governmental agendas.",
            "context": "This means that they are often engaged in framing contests with other actors in society as they seek to raise issues to the social agenda and to suggest ways of thinking about these issues. Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem.",
            "type": "explicit"
          },
          "agenda-building": {
            "type": "explicit",
            "definition": "efforts to influence the context in which social problems are considered",
            "context": "Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem"
          },
          "Selection Bias": {
            "type": "explicit",
            "definition": "agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report.",
            "context": "In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story."
          },
          "Description Bias": {
            "type": "explicit",
            "definition": "agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest eventis portrayed in a media story",
            "context": "In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story."
          }
        }
      },
      {
        "paper_id": "2106.16117v1",
        "sections": [
          "The concept of the 'energy security' was first introduced by industrialized market economies in 1947, when the United States adopted a document regulating the actions of the state in the field of national security. Although it has been investigated and developed by a wide range of scholars and specialists, still there is no universal understanding of what is meant by this notion.",
          "In international relations, the term 'energy security' acquired significance and became widely used after the global oil crisis of 1973-1974, when the world faced a sudden and noticeable increase in prices for fuel and energy resources caused by the following political events [1]: • collective embargo imposed by the Arab OPEC countries on the export of oil to the USA; • support of Israel in the October war by a number of European countries. Being faced with the first largest energy crisis, industrial countries acutely felt their vulnerability and consequently energy security became one of the top priorities, considered a part of the country's national security. As mentioned above, currently, despite the abundance of modern research and publications on energy topics, a single generally accepted definition is still not formed. In point of fact, the meaning of energy security differentiates from country's dependence to their energy imports. Accordingly, countries which are highly dependent on imported oil and gas adheres energy security to supply whereas, countries which export oil and gas adheres energy security to demand. This is due to the fact that different groups of countries, namely exporting countries, importing countries and transit countries interpret this concept in their own way, based on their national and economic interests and priorities. Therefore, in order to give a clear definition of the concept of \"energy security\", it is imperative to consider it from the perspective of all three parties. But, nevertheless, in modern scientific literature there are many concepts and interpretations of international energy security, and representatives of different schools or countries offer their own definitions of this notion. The International Energy Agency (IEA), which is an international body that unites 29 industrialized countries under the Organization for Economic Development and Cooperation, defines energy security as 'the uninterrupted availability of energy sources at an affordable price' [2]. The limitations of this approach are directly implied by the fact that in this context, it considers energy security from the point of view of importing countries that reflects their national priorities in the face of growing dependence on imports of oil and natural gas. Nevertheless, the IEA admits that the concept of 'energy security' can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces 'timely investments to supply energy in line with economic developments and environmental needs', while the latter deals with 'the ability of the energy system to react promptly to sudden changes in the supplydemand balance' [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply. Beijing defines 'energy security' as 'reliable uninterrupted supply of energy, which is necessary for economic development of the country' [1]. In other words, energy security is seen as a need for uninterrupted supply of energy resources in order to maintain stable economic growth. Meanwhile, the exporting countries, including Russia, do not share this position. They are primarily interested in diversifying the markets of energy exports, preserving their national sovereignty and of course guaranteeing sustainable demand for the energy they export. Because of diverging interests in the energy market, exporting countries interpret 'energy security' in another way. The OPEC group (a permanent intergovernmental organization of 15 oilexporting developing nations that coordinates and unifies the petroleum policies of its Member Countries [4]) defines energy security as 'reliable and environmentally friendly energy supplies at prices reflecting fundamental principles of market economy'. This approach is also limited, as the IEA one, because it reflects only the interests of exporting countries that are focused on determining a decent price for energy resources. The interests of transit countries consist mainly in maximizing transit rent of energy resources. This is the approach Turkey follows, -the country is a natural energy corridor between the Middle East, the Caspian basin and Europe and contributes significantly to the attempts of Europe to diversify its energy suppliers for natural gas. In the meantime, the concept of energy security framed as an access to fossil fuels created the so-called 'energy nationalism' [3] that created a reality in which the behavior and decisions made on energy markets and the delivery of resources ultimately depended not on the economic market factors, but on producers, which resulted in the transformation of the energy market into the subject of inter-state relations. Oil and natural gas were used as geopolitical weapons, and geopolitics and geo-economics became an important part of world politics and foreign policy of the main players on the energy market. Energy security undoubtedly depends on the national and international background. In the context of state-controlled markets, the main guardians of energy security are the governments. On the contrary, given the energy markets are liberalized and the main actors are private companies, the security of supply consists of an efficient risk management strategy by governments, companies and consumers. Therefore, it is important not to fall into purely ideological approaches. Since Adam Smith, the state is authorized to intervene in the economy in order to prove security, including its energy component, to its citizens. Baring that in mind, it should be noted that such intervention should be done as far as possible in a concerted manner along with the companies and consumers, following the principle of subsidiarity. At the same time, energy security is associated with the evolution of the energy markets, the geopolitical situation and long-term international scenarios. There are several scenarios drawn for the long term that extrapolate to some extent conceptual differences, for example, the 'Markets and Institutions' scenario and its alternative called 'Empires and Regions' [3], and the three scenarios drawn by Shell Global Scenarios [5]: 'Low Trust Globalization', 'Open Doors' and 'Flags'. The 'Low Trust Globalization' is based on the trade-off between efficiency and security, and is characterized by limited international integration, intrusive state intervention and institutional discontinuities. The 'Open doors' arises from the dilemma between efficiency and environmental sustainability, premium market incentives and the participation of the civil society. It highlights the urgent necessity of transnational integration and harmonization, and mutual recognition of standards. The 'Flags' responds to the mercantilist logic and involves regulatory fragmentation, nationalism and the conflict between the values of the different regions. To some extent, the differentiation of scenarios roots in the neoliberal and neorealist paradigms to international energy issues. It is an old debate between two alternative visions of the world: a world in which the market disturbances are resolved by cooperation, or a fragmented world conceived as a billiard table where conflicts are resolved through the exercise of the political, economic and military power. Under the neoliberal paradigm, that is so praised by the EU, energy security is achieved through the development of markets and the management of conflicts at the multilateral level through supranational institutions. The neorealist paradigm of energy security, instead, implies the development of bilateral relations and the subordination of markets to foreign policy. The potential risks and threats related to energy security arise mainly from two circumstances: • the projected upcoming peak in the production of hydrocarbon resources, which is vital for the modern economy, and • the security of their supplies. However, recently the energy sector has started to develop a few key trends that were caused by new, very strong factors: the global financial and economic crisis and the shale revolution in the production of oil and gas. Today, energy security policies require a paradigm shift and a new model of factors and conditions for its implementation. The first factor that radically changed the context of energy policy was the global economic crisis. Since 2008, experts have determined it as the financial crisis, the economic crisis, the crisis of democracy and governance, the crisis of the culture of public consumption and material culture in general, and the environmental crisis that will eventually lead to global natural disasters. It would be reasonable to say that the world experienced a multidimensional global crisis, or the first systemic crisis of the global era. In the energy sector, this crisis coincided with the start of the gradual transition from 'industrial' and 'hydrocarbon' to 'neoindustrial' and 'smart' energy, which includes the following aspects: smart grids, energy efficiency (in a broad sense), renewable energy, new principles of energy systems and focus shift from producers to consumers [3]. The second factor that dramatically changed the energy markets was the quiet shale revolution in oil and gas production. The shale revolution, which has become a reality in the United States and Canada, as estimated by the experts, will have serious consequences for the global energy market. The unconventionally produced natural gas fundamentally changed the world market. The most serious consequence of the shale gas revolution is a shift in the focus from producers to consumers. In the context of the old paradigm, energy security is directly related to energy independence. The idea was that if a country is self-sufficient with respect to energy resources, and has an efficient (energy-saving) economy, then this will be accompanied with lower prices for energy carriers. The reality of oil prices in the United States after the shale boom has shown that this is utopia.",
          "Meanwhile, achieving self-sufficiency in the energy sector is almost impossible. Even such countries as Russia, Saudi Arabia, Venezuela, Brazil and Canada, which are rich in hydrocarbons, import a part of the energy in the form of refined petroleum products due to insufficient refining opportunities. This dependence could theoretically be eliminate with a little effort and investment in the construction of new refineries, but in practice it is not happening. Revolutionary changes of the energy security require a paradigm shift, which should be reflected in the energy security policies. Currently we are on the verge of transition to a post-industrial, 'smart' energy system, which means 'smart' networks, alternative energy sources for transport, decentralization of energy, integration of energy into the technical sphere, accompanied by an increase in energy efficiency. To conclude, it can be said that the definition of 'energy security' can be complete only if the interests of all the energy market participants are taken into account, and namely importing countries, exporting countries and transit countries. Interests and priorities of all three parties are different, which presents a difficulty in agreeing on a common concept. At the same time, with the paradigm shift all of the actors of the energy market are interested in ensuring the reduction of geopolitical and environmental risks and thus creating new opportunities."
        ],
        "ground_truth_definitions": {
          "short-term energy security": {
            "definition": "the ability of the energy system to react promptly to sudden changes in the supplydemand balance",
            "context": "Nevertheless, the IEA admits that the concept of ‘energy security’ can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‘timely investments to supply energy in line with economic developments and environmental needs’, while the latter deals with ‘the ability of the energy system to react promptly to sudden changes in the supply- demand balance’ [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply.",
            "type": "explicit"
          },
          "long-term energy security": {
            "definition": "timely investments to supply energy in line with economic developments and environmental needs",
            "context": "Nevertheless, the IEA admits that the concept of ‘energy security’ can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‘timely investments to supply energy in line with economic developments and environmental needs’, while the latter deals with ‘the ability of the energy system to react promptly to sudden changes in the supply- demand balance’ [2].",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "885c3154386e3dd1ff6a12e566152846186f9569",
        "sections": [
          "hypotheses based on a careful mapping of psychological profiles of over 2,300 American Twitter users linked to behavioral sharing data and sentiment analyses of more than 500,000 news story headlines. The findings contradict the ignorance perspective but provide some support for the disruption perspective and strong support for the partisan polarization perspective. Thus, individuals who report hating their political opponents are the most likely to share political fake news and selectively share content that is useful for derogating these opponents. Overall, our findings show that fake news sharing is fueled by the same psychological motivations that drive other forms of partisan behavior, including sharing partisan news from traditional and credible news sources. With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018). Political observers have linked the spread of fake news to recent key political events, like the \"Brexit\" vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016). Accordingly, media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017). In light of these sizable investments, it is surprising how little we know about why people share fake news on social media. Studies suggest the spread of false information cannot be attributed only to bots or \"sock puppet\" accounts (Guess, Nagler andTucker 2019, Vosoughi, Roy andAral 2018). But researchers have largely failed to conduct adequate tests of why ordinary citizens take part in the distribution of fake news (Wagner and Boczkowski 2019). This is unfortunate because potential solutions to the spread of fake news rest on psychological assumptions about the root causes of the problem. For example, fact-checking should reliably work only if citizens actually want to share true information but need help to weed out falsehoods. Presently, we do not know whether this portrayal of citizens' motivations is correct. As summarized in a recent study, \"most existing studies rely solely on publicly available social media data and there is little research that seeks to understand people's motivations for (...) sharing news\" (Chadwick, Vaccari and O'Loughlin 2018, 4258). In this study, we ask: What are the individual-level psychological motivations behind fake news sharing? The first major challenge in addressing this research question is data unavailability (King andPersily 2019, Stier et al. 2019). Data on citizens' personality and political profiles are rarely available in combination with behavioral data on their sharing activities on social media (for notable exceptions, see Guess, Nagler and Tucker 2019, Guess, Nyhan and Reifler 2020, Grinberg et al. 2019). Here, we circumvent these data limitations by linking unique behavioral data on news sharing activity and sentiment analyses of more than 500,000 news story headlines to individual-level survey data with detailed psychological measures for a large sample of American Twitter users. This approach allows us to conduct a comprehensive study of competing psychological theories of fake news sharing. The second challenge is that multiple psychological motivations could potentially influence the sharing of false and factually correct news; these motivations may differ across individuals and across news content. Building on theories about information sharing in general, we first outline these potential psychological motivations before applying them to our research topic: the sharing of political fake news on social media, a domain characterized by intense conflict. 1Then, we deduce testable implications about relevant individual differences and differences in political news content.",
          "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience. Whereas once audiences were more or less passive consumers, they are now actively participating in the distribution, and sometimes even the production of news. People today can start their own blogs, live-stream from political events they participate in (e.g. protests), and may even contribute to small media organizations often catering to niche market segments. To be sure, just as with legacy media, online audiences still face a choice of exposure: which news do they choose to consume. But on top of that, they also face a new choice of which news to share with their friends and followers. These trends, we contend, have implications for the psychological dynamics underlying the way audiences engage with this media, the novel step of news sharing potentially recruiting new psychological mechanisms. These trends have pushed to the forefront questions about information veracity. In advanced democracies, strong editorial norms and standards for good reporting meant that citizens could -at least most of the time -trust legacy media outlets to publish stories that accurately reflected the world. For most citizens and for most legacy media, then, it made sense to pay more attention to the political usefulness of news stories and, thus, it is not surprising that this news dimension has been critical in driving selective exposure. Yet, on the internet, editorial norms have lost power, and information circulating on social media will to a much larger extent vary on both dimensions of usefulness and veracity. Truth cannot be taken for granted and veracity judgments are thus, potentially, more important than ever. Yet, these two information dimensions -veracity and usefulness -are not unique for social media users. In everyday conversations when, for example, discussing or relaying a rumor, people must consider both whether they believe it and how useful they think it is. Psychologists have studied the psychology of rumor transmission for decades, highlighting the importance of both accuracy-oriented and goal-oriented motivations (for an overview, see DiFonzo and Bordia 2007). For example, people may share rumors they believe are true to be cooperative, to signal competence, or to get others to affirm the accuracy of the rumors. Sharing a rumor that turns out to be wrong, in contrast, may damage people's reputations (Altay, Hacquin and Mercier 2019). Nonetheless, goal-oriented processes may still take precedent, notably under intense social conflict. Some of the most well-studied, and most extreme, examples come from ethnic conflicts, where rumors are heavily used to mobilize the in-group against the out-group. In his comprehensive review of this role of rumors, Horowitz (2000, p. 74) concludes: \"Rumors are (. . . ) embedded in the riot situation, because they are satisfying and useful to rioters and their leaders. (. . . ) Rumor is likely to prevail over accurate information. (. . . ) Rumor prevails because it orders and organizes action-in-process.\" Below we apply these general considerations about the psychology of information sharing to the object of our empirical inquiry: The sharing of news -fake or real -about politics on social media.",
          "The prevalence of false information on social media may strengthen the role of accuracyoriented motivations and the dimension of veracity for understanding news sharing behavior. People may worry about damaging their reputation by accidentally sharing fake news (Altay, Hacquin and Mercier 2019). To the extent such reputational worries induce accuracy-oriented social media behavior, we should expect that sharing fake news stories is a rare phenomenon relative to sharing real news, but also that the predictors of sharing fake versus real news will differ. Successfully discerning between fake and real news is difficult, first, because fake news is specifically designed to appear real and, second, because the fast-paced social media platforms deplete attention and cognitive resources (Pennycook et al. 2020). Accordingly, individual differences in the ability to detect cues about information veracity should largely determine whether people share fake news (Pennycook and Rand 2019b). In essence, from this perspective, fake news sharing happens because people are ignorant that the stories they share are false and, accordingly, below we refer to this argument as the ignorance theory. Goal-oriented motivations, however, may also critically shape sharing of political news on social media. As noted above, goal-oriented motivations affect information sharing during periods of social conflict. Politics is conflictual, but political conflict in the United States in recent years has been deepened by an increasing political polarization (Mason 2018). Goaloriented motivations put a premium on the usefulness of information. Sharing a political news story (fake or real) can be used to signal one's political affiliation, by denigrating political opponents or by mobilizing politically like-minded peers. In this regard, fake news stories can be particularly useful: as fake news is not constrained by reality, they can be excessive in their negative portrayal of political opponents (Acerbi 2019). Accordingly, to the extent goal-oriented motivations dominate social media users' interactions with news stories, the same psychological motivations should drive the sharing of real and fake news. People will share useful news that fits their political goals, and, in the extreme case, they will not pay attention to the veracity dimension of the information at all. In current American politics, two types of social or political goals could be served by sharing extreme information such as fake news. First, sharing news stories could reflect motivations to help one's preferred party against opposing parties in the increasingly polarized electoral competition (Iyengar, Sood andLelkes 2012, Lelkes andWestwood 2017). We will refer to this as the polarization theory as it focuses on partisan feelings rather than ideological disagreements between partisans of different stripes. The theory is prominent in current discussions about political events, like elections and referenda, and it has a long research history in political science (Mason 2018, Taber andLodge 2006). The theory has also informed studies on partisans' beliefs in political conspiracies (Miller, Saunders and Farhart 2015). Second, sharing news stories could be fueled by deep-seated discontent with the status quo and serve a desire to disrupt the existing social and political order. For example, Tucker et al. (2017, 53) convincingly argue that \"social media can lend a voice to ... anti system forces that actively seek to undermine liberal democracy.\" Below, we will refer to this as the disruption theory. Finally, there are also multiple scenarios for the intermediate (and likely) case where both veracity and usefulness considerations are relevant for social media users' sharing behavior. People may first evaluate the veracity of news and then, conditional on it being accurate, consider its usefulness. Or people may prioritize usefulness but still pay some attention to the veracity. Hence, the sharing of blatantly false information may, for example, complicate mobilization processes by derailing discussions about whether events really happened (Petersen, Osmundsen and Tooby 2020). Thus, when assessing usefulness, people may factor in veracity and selectively share information that is as extreme as they believe they can get away with. To elucidate whether and how the ignorance, polarization, and disruption theories account for the sharing of fake news, we use two approaches: First, we examine the role of individual differences, deducing for each theory who should be most likely to share fake news and, second, we examine the role of news story content, deducing what is most likely to be shared.",
          "The accuracy-oriented ignorance theory assumes people want to share accurate information, but end up sharing falsehoods because they lack the cognitive reflection or motivation to discern between true and false information. One line of evidence for the ignorance theory comes from Pennycook and Rand (2019b), who show that people who perform better on a cognitive reflection test can better tell if news story headlines are true or false. Another line of evidence comes from Guess, Nagler and Tucker (2019), who demonstrate that older people are more likely to share fake news on Facebook, presumably because they are less digitally literate than their younger peers. Building on this, we test the ignorance theory by focusing on four individual-level predictors of fake news sharing on Twitter: age, cognitive reflection, factual political knowledge, and digital literacy. The goal-oriented polarization theory focuses on partisanship as a key driver of fake news sharing. Currently, there is mixed evidence on how partisan motivations influence fake news sharing. In general, partisans seem to share information from ideologically similar sources (Barberá et al. 2015) and to treat articles shared by political opponents with greater suspicion (Lee, Kim and Coe 2018), suggesting that considerations about the political usefulness of information matters. Guess, Nagler and Tucker (2019, 3) find that partisanship also affects fake news sharing in that conservatives and Republicans were more likely to share fake news on Facebook in 2016 than liberals and Democrats, a partisan asymmetry attributed to \"the overwhelming pro-Trump orientation in both the supply and consumption of fake news during that period.\" Pennycook and Rand (2019b), however, tone down the effect of partisanship. They conclude that \"people fall for fake news because they fail to think; not because they think in a motivated or identity-protective way\" (ibid., 48). To test the polarization theory, we examine whether Democratic partisans are willing to share politically congruent fake news from *pro*-Democratic sources or if the partisanship asymmetry identified by Guess, Nagler and Tucker (2019) and others prevails. Second, an important question in the literature on affective polarization is whether in-party love (Lelkes and Westwood 2017) or out-party hatred (Abramowitz and Webster 2018) sparks polarization. We provide the first test of which of these dynamics explain fake news sharing by relying on two types of individual differences as predictors of news sharing: 1) standard party identification measures and 2) batteries of positive versus negative feelings towards Democrats and Republicans. Finally, we turn to the goal-oriented disruption theory. A central focus in media discussions and psychology has been on so-called \"online trolls\" who take pleasure in misleading and harassing others online (Buckels, Trapnell and Paulhus 2014). Trolling experts speculated that sharing sensational and often implausible fake news reflects a specific form of apolitical online trolling. However, political cynicism may also drive fake news sharing. For example, a recent study showed that Americans and Danes who are discontent with political elites and the political system report substantially greater willingness to circulate conspiracy theories online (Petersen, Osmundsen and Arceneaux 2018). Here, we provide the first behavioral test of this theory and zoom in on two individual differences as predictors of fake news sharing: a psychological measure of trolling behavior and a measure of political cynicism.",
          "To elucidate the psychological motivations underlying fake news sharing, we also need to understand what news content people share. Specifically, to adjudicate between the three theories, the analyses differentiate news stories along two dimensions: (1) Whether a news story originates from a fake news source (i.e., a source containing registered debunked stories) or a real news source (i.e., a generally credible source), and (2) whether a news story comes from a *pro*-Republican or a *pro*-Democratic source. The three theories make very different predictions about which dimension matters for people's sharing preferences. The ignorance theory predicts that the veracity dimension matters most and that people with high cognitive abilities will be less willing to share fake news stories and more likely to share real news stories, irrespective of the political slant of the stories. The latter two theories make the opposite prediction: they posit that people place less value on whether stories are true or false as long as the stories are politically useful. Specifically, the disruption theory implies that disruptive individuals should selectively share as many fake news as possible to sow confusion and uncertainty. For the polarization theory, however, the content-related predictions are more complex, and so it is these predictions we unfold now. The polarization theory holds that fake news sharing is not an accident caused by ignorance, it is partisan business as usual. The media landscape in the US is politically fragmented, with some news sources catering to Republicans while others offer content that fit Democrats' taste (Groeling 2013). From the perspective of the polarization theory, fake news is an extreme version of traditional partisan news that polarized partisans may find attractive due to its rarity and extremity. Accordingly, partisans motivated to selectively share hyper-partisan content will tend to view the online media landscape of news sources as a one-dimensional continuum reflecting the political usefulness of the stories they provide. For a polarized Republican social media user, the order of the usefulness of news sources goes from *pro*-Republican fake news sources, to *pro*-Republican real news sources over centrist sources to *pro*-Democratic real news sources, and finally, to *pro*-Democratic fake news. For Democrats, the order reverses. This insight, we contend, leads to three observable implications about the content of shared news. First, partisan social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. Accordingly, we expect that sharing fake news favoring one party over the other should be best predicted by sharing real news strongly favoring the same party. Conversely, sharing fake news favoring one party should correlate negatively with sharing fake news favoring the other party. Second, if news sources differ systematically in how strongly they cater to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Specifically, fake news should show the strongest partisan sorting in terms of who wants to share from these sources. Third, and finally, the negativity of news source content should change as we move across the partisan news continuum. If partisans selectively share stories with content that fits their political tastes, the observable implication is that stories shared from news sources located towards the Democratic extreme on the partisan news continuum should portray Republican elites negatively while stories from the Republican extreme should cast Democratic elites in a negative light.",
          "",
          "We commissioned the YouGov survey agency to recruit U.S. Twitter users to complete a 20-minute online survey fielded from mid-December 2018 to mid-January 2019. The survey contained questions measuring our set of potential predictors of fake news sharing (see below) and a range of demographic variables. The survey also asked participants to provide their Twitter ID and asked for permission to scrape their publicly available Twitter data and link this data to their confidential survey responses. See SI Section 1 for the invitation letter and link to the consent form. We scraped respondents' Twitter accounts by early March, 2019. Our data includes tweets and retweets posted by the panelists before that period. YouGov invited over 27,000 U.S. Twitter users from their standing web panel to participate in our study. 8,741 participants accepted the invitation and answered all questions before the final prompt to give consent to link their public Twitter data to the survey. A majority of these respondents (5,725 people) did not share their Twitter IDs at the end of the survey. Of the 2,976 respondents who did attempt to share their Twitter handles, another 639 participants presented IDs that we could not access via Twitter's Application Programming Interface. Consequently, our final sample size is N = 2,337. Because of high and non-random attrition, our sample is unlikely to perfectly reflect any welldefined population. Still, our study participants' demographic characteristics resemble the characteristics obtained from participants in other high-quality Twitter studies. Relying on a recent Pew study (Wojcik and Hughes 2019), SM Section 2a demonstrates that our sample is comparable to the Twitter-population on race, gender, income, education, and age. It also shows that participants in our final sample were demographically very similar to the 8,741 participants who accepted to participate in the survey but failed to provide their Twitter handle. In our final sample, 50% of participants are female, 70% are white/Caucasian, the median age is 40 years old (SD = 14), the median highest level of completed education is \"4-year college degree,\" and the median family income level is \"$60,000-$69,999.\" We scraped Twitter data from all our 2,337 participants. In total, we extracted 2,709,052 tweets and retweets posted by our panelists until mid-January 2019. The median number of (re)tweets for our participants was 488 (Min #of tweets = 1, Max #of tweets = 6433). Importantly, a large majority of the tweets -around 70% -were posted after the 2016 US presidential election; the period that has been the focus of previous work. This allows us to extend earlier findings and test how they hold up in periods characterized by less intense electoral competition.",
          "We follow best practices (Guess, Nyhan and Reifler 2020, Grinberg et al. 2019, Lazer et al. 2018) and operationalize \"sharing of fake news sources\" as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news. To this end, we cross-referenced all tweeted URLs against a list of 608 fake news sources, constructed by journalists and scholars (for details, see SM Section 10 and Guess et al. 2019, Allcott, Gentzkow andYu 2019). The list was compiled to document the prevalence of fake news during the 2016 U.S. presidential election but has been updated to account for fake news publishers that have emerged since. To the extent possible, the list creators have coded the most popular fake news sources for partisan slant: 12 of the most popular were *pro*-Democratic fake news sources while 30 were *pro*-Republican sources. While this number may seem low, note that almost 86% of all shared fake news sources originated from one of these 42 domains for which we have partisan ratings. (See SM Section 6 for the complete list of fake news domains and Figure SM 2b for the 20 fake news domains that were shared the most.) As a robustness test, supplementary analyses discussed below group the fake news sources according to a more fine-grained coding scheme proposed by Grinberg and colleagues (2019) that distinguishes among three classes of fake news sources differing in their likelihood of publishing misinformation. In SM Section 8, we also replicate the main results using a different, but partially overlapping list of fake news sources. Our measure of fake news sharing records actual online behavior and thus does not suffer from biases that can plague self-reported measures of online activity. Further, while the present study is certainly not the first to couple survey and social media data (e.g., Guess, Nagler and Tucker 2019), it has the most detailed set of psychological measures; see below. Still, it is important to keep in mind when interpreting our results that we cannot tell if the specific stories people share are true or false: Our fake news measure is at the publisher level, not the story level. To measure the \"sharing of real news sources,\" we cross-referenced participants' tweets against a list of real news publishers obtained from the AllSides organization. AllSides seeks to help citizens navigate the online media landscape by providing \"balanced news and civil discourse.\" Their web page (www.allsides.com/media-bias/) contains a list of 260 real news publishers with ideological ratings: \"Left\" (labeled \"Strong Democratic\" below, n = 45), \"Lean Left\" (\"Leaning Democratic\", n = 63), \"Center/Mixed\" (n = 88), \"Lean Right\" (\"Leaning Republican\", n = 29) or \"Right\" (\"Strong Republican\", n = 39). While AllSides combines different methods for estimating ideological bias (e.g., editorial reviews, third-party judgments), SM Section 7 shows that their ratings correlate highly -Pearson's r = .90with network-based measures of ideological alignment of media sites (Bakshy, Messing and Adamic 2015). See SM Section 7 and Figure SM 2b for the 20 most shared real news domains. Our measures of real and fake news sharing account for ideological and partisan slant at the source level and are thus useful for understanding the content people care about when sharing news. To gain further leverage on testing the relevance of content for sharing behavior, we also built two unique data sets of news headlines (see SM Section 11 and 12 for details). We focus on headlines because these are more accessible than full texts and because prior research demonstrates that simply seeing a headline can exert large effects on decisions to share news on social media (Effron and Raj 2019). The first data set was based on the headlines of stories shared by our participants, which gives us a clear representation of the content they deemed sufficiently important to share. To construct the data set, we first retrieved the full URLs from all 85,084 tweeted news links shared by our participants. Based on these URLs, we parsed the news domains' HTML code and successfully extracted the headlines of 75,560 news articles (89% of all article headlines). To construct our second data set, we used the Archive.org web page -an indexing system for the Internet's web pages for historical archiving -to scrape headlines that had appeared on the daily front pages of the most popular (among our participants; see Figure 5 below) news sites in the period 2016-2019 (i.e., covering roughly the same period for which we have tweets from our participants). Compared to our Twitter headlines, this data set contains a much larger number of news story headlines (we successfully scraped a little over 500,000 headlines). More importantly, the front page headlines offer a glimpse of the universe of news stories our participants could potentially see and read before deciding which stories to share. In this way, it sidesteps issues of \"selection\" biases in sharing decisions due to psychologically motivated reasoning; a point we return to during the analyses. We subjected these headlines to two types of automated analyses. To measure the sentiment of the headlines, we relied on the sentimentR R package, which includes a sophisticated algorithm for calculating text polarity sentiment. This algorithm improves the performance of more traditional dictionary-based methods by calculating sentiment on the sentence level and considering valence shifters (e.g. not, very, hardly). To identify the political affiliations of those mentioned in the headlines, we created two custom dictionaries of prominent Republican and Democratic elites. Our primary dictionary is based on YouGov's list of the 100 most famous Democrats and Republicans. 2 We replicate our results with an alternative dictionary listing all members of the US Congress, Senate and the Trump Administration in the supplemental materials. Finally, we matched these dictionaries to the headline data sets and recorded whether a given headline mentions anyone from the Republican or Democratic party. We use these more detailed content measures in the final part of the analysis, whereas the initial parts focus on the crude distinction between fake and real news sources.",
          "All participants responded to the same survey questions, designed to measure the potential predictors of fake news sharing derived from the three theories discussed earlier: the ignorance theory, the disruption theory, and the polarization theory. SM Section 1c lists the questions asked and specific wordings.  (Pennycook and Rand 2019b). Second, we included participant age to test if we could replicate earlier findings that older social media users share more fake news stories than younger users. Finally, we included measures of factual political knowledge and digital media literacy to test the claims about why older social media users share more \"fake news.\" We included a 5-item battery from the American National Election Studies (ANES) to measure factual political knowledge (e.g., \"How many members of the U.S. Supreme Court are there?\"). To measure digital media literacy in the domain of politics, we created a 10-item battery based on previous research (Feezell 2016, Gil   2 Accessible at https://today.yougov.com/ratings/politics/fame/Democrats/all and https:// today.yougov.com/ratings/politics/fame/Republicans/all de Zúñiga, Molyneux and Zheng 2014) asking participants how often they engage in specific online political behaviors (\"Use the Internet to participate in volunteer activities related to a political or social campaign\") (α = .93). As this measure also taps political engagement and online civic skills, we adjust for political interest in the analyses. In addition, SM Section 4 replicates the main analysis using a more narrow, validated measure of digital literacy from Hargittai and Hsieh (2011). The disruption theory. We included two measures of disruptive motivations. To measure apolitical trolling behavior, we solicited the \"cyber-bullying\" literature and implemented a 4-item trolling scale (Buckels, Trapnell and Paulhus 2014), asking participants whether they agreed or disagreed with statements like \"I like to troll people in forums or the comments section of websites\" (α = .86). We measured political cynicism via an 8-item survey instrument (Dekker and Meijerink 2012), asking participants whether they agreed or disagreed with statements like \"Politics in the United States is sick\" (α = .74). The polarization theory. We included two measures of political polarization: Partisanship and feelings towards Republicans and Democrats. We used a standard ANES question to measure partisanship on a seven-point scale, ranging from \"Strong Democrat\" to \"Strong Republican\" identifier. To measure partisan feelings, we asked participants how strongly they felt various positive (hope, enthusiasm, pride) and negative (anger, frustration, fear) emotions when thinking about Republicans and Democrats. We created two feelings scales to test how in-party love and out-group hate related to news sharing by aggregating responses and taking the averages for each participant (the negative emotion items were flipped). The key models presented below analyze motivations to share news from *pro*-Republican and *pro*-Democratic news sources separately. Accordingly, we coded the \"in-party\" emotion scale such that higher values always correspond to more positive emotions towards the political side supported by the news source (i.e., higher values indicate the participant has more positive feelings towards Republicans (Democrats) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). Analogously, we constructed the \"out-party\" emotion scale such that higher values indicated more negative feelings towards the political side opposed by the news source (i.e., higher values indicate the participant has more nega-tive feelings towards Democrats (Republicans) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). SM Section 2b presents summary statistics for all main variables.",
          "",
          "Before testing the key hypotheses about the psychological motivations for fake news sharing, we provide descriptive analyses of the data. Overall, we find that news sharing is a small part of our sample's activity on Twitter. (See Figure SM2c for an overview.) Of the 2.7 million tweets in our data, only 85,344 (≈ 3%) contain links to national news websites, fake or real. Of the tweeted news links, 3,269 (≈ 4%) come from websites with a history of publishing fake news (see Grinberg et al. 2019, for slightly larger estimates during the 2016 US presidential election). Of these stories, a large majority -2,563 stories -originates from *pro*-Republican fake news publishers. In contrast, over twice as many real news stories come from *pro*-Democratic sources (60%) than from *pro*-Republican sources (23%). Together, these two observations imply that exposure to fake news depends heavily on the ideological leanings of one's Twitter network. In *pro*-Democratic networks, only a small fraction of the news diet is likely to come from fake news sources, whereas *pro*-Republican networks are likely to display a substantially higher fake-to-real news ratio. While these aggregate numbers suggest fake news sharing could be widespread within certain social networks, they also mask large heterogeneity in sharing behavior. The left-hand panel in Figure 1 below makes clear that sharing content from fake news sources flourishes mostly among small segments of the population: 1% of the panelists are responsible for sharing about 75% of all links to fake news publishers and 11% of them are responsible for sharing all fake news sources. Meanwhile, the top 1% of real news \"supersharers\" are only responsible for sharing about 30% of all real news sources. Figure 1's two other panels show similar results when splitting the data based on the partisan slant of the news sources. Importantly, not all fake news is equally troublesome. SM Section 8 uses Grinberg and colleagues' (2019) fine-grained categorization of fake news sources and find that about 30% of the fake news links come from sources \"publishing almost exclusively fabricated stories\" (i.e. \"black\" sites). Another 5% come from sites that \"spread falsehoods that clearly reflected a flawed editorial process\" (i.e. \"red\" sites). Finally, two-thirds of the links referred to fake news sources \"where annotators were less certain that the falsehoods stemmed from a systematically flawed process\" (i.e. \"orange\" sites). While this means that the number of individual stories containing blatant falsehoods is likely smaller than 3,325 in our data, we note that sharing news stories from websites with a record of disseminating falsehoods can still be problematic. Given the still limited penetration of fact-checking, sharers most likely do not know whether a certain news story is true or false when deciding to share it. Instead, they must make informed guesses based on the reputation of the news source. Thus, decisions to share news from these sources still reflect either ignorance or lack of concern about such matters.  Note. The x-axes show percentage of participants responsible for sharing a given percentage of all news links (y-axes). The left-hand panel gives results for sharing links to fake (black line) versus real (grey line) news publishers (N Fake News = 3,269; N Real News = 78,400). The middle and right-hand panels display results for sharing links to *pro*-Democratic (blue line) versus *pro*-Republican (red line) fake and real news sources, respectively. For real news sources, we collapse the \"Leaning\" and \"Strong\" categories. The dashed black lines denote a hypothetical uniform distribution of news sharing. (N Fake News, pro-Rep = 2,563; N Fake News, pro-Dem = 263; N Real News, pro-Rep = 16,081; N Real News, pro-Dem = 46,385.)",
          "We now turn to the analysis of the psychological motivations for sharing fake news sources. Figure 2 gives average marginal effects (AME) 3 from logistic regression models estimating the association between our main predictors and the probability of sharing at least one news story from a fake news (left panel) or a real news source (right panel). To increase power and simplify the analyses, we here combine the real news sources that Allsides rate as strongly partisan and leaning partisan (e.g., we combine news stories from \"Strong Democratic\" and \"Leaning Democratic\" real news sources). As our main interest is in whether people share fake news or not, we dichotomize the dependent variables, with 1 indicating that a participant As shown in the lower right-hand panel, the partisan factors predict sharing from real news sources in an almost identical fashion. The motivations underlying fake news and real news sharing are, psychologically speaking, highly overlapping. One difference compared to sharing of fake news sources is that our estimates are obtained with greater precision due to the larger number of participants sharing real news sources. Another difference relates to the partisan asymmetry: Concerning real news sharing, we find no asymmetry between Democrats and Republicans; both are equally partisan and strongly motivated by negative feelings towards political opponents. Altogether, these results fit well with predictions by theories on partisan motivations: Partisans share politically congenial news, primarily because of hostile feelings towards the out-party. Further, the lack of evidence for the ignorance theory suggests that these sharers pay more attention to the political usefulness of news rather the information quality. Below, we put the polarization theory to an even stronger test by considering the role of news content in sharing decisions. ",
          "For the sake of clarity, our main analysis employs crude binary distinctions between fake and real news as well as between those who share any news and those who do not share news at all. This section tests the robustness of our results with respect to these distinctions. Most importantly, we operationalize the sharing of fake news as the sharing of all links to news sources with a history of publishing falsehoods. However, some fake news sources produce more misinformation than others. It is possible that when it comes to blatant falsehoods, ignorant rather than polarized people are the main culprits. To test this possibility, we rely on the fake news classification of Grinberg et al. ( 2019) described above and re-run our analysis separately for each of three fake news categories. Our results reported in SM Section 8 show high consistency across the three categories. People who share links from sources that publish \"almost exclusively fabricated stories\" are also more polarized, but not more ignorant, and only slightly more disruptive than those who do not share any such links. We also replicate our results relying on expert ratings of the trustworthiness of 60 news sources (20 mainstream news sources, 20 hyperpartisan websites, and 20 fake news sources) based on Pennycook and Rand (2019a). Whereas this conceptualization offers an even more fine-grained look into the quality of news sources, it comes at a cost of dropping more than 90% of all news sources from our analysis. Following Pennycook and Rand (2019a) we calculate the average trustworthiness of news shared for each individual in our sample and use it as our dependent variable. SM Section 9 demonstrates that the findings reflect those presented in the main text: Political variables -partisanship and feelings towards Republicans and Democrats -correlate strongly with the trustworthiness of shared news sources whereas \"ignorance variables\" -CRT and political knowledge -do not. Finally, we replicate our analysis by modeling the actual count of fake and real news shared instead of binary variables of sharing versus not sharing. This involves a change from logistic regression to Quasi-Poisson models. The two models have important substantive differences. After all, sharing a single fake news link amidst dozens of real news links could be an honest mistake. Relying on the count variables, in contrast, put more weight on respondents with higher numbers of shares within a category. SM section 5 indicates this alternative modelling procedure does not affect our main conclusions. While the effect of political cynicism is significantly reduced, partisan motivation -especially negative affect towards the out-party -remains the primary predictor of fake news sharing. 4 The Role of News Content: Fake news as the Extreme Ends of a",
          "News source sharing patterns across the partisan news continuum. If partisan motives dictate both fake and real news sharing, social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. To test this, Figure 3 plots a heatmap of correlations based on news sharing across the seven news source types. The emerging patterns firmly favor a partisan story: People who share the most articles from ideologically biased real news sources are also most likely to share fake news sources with a similar political bias. In contrast, the off-diagonal correlations show that people shy away from sharing politically opposed sources. Importantly, the plot also reveals a partisan asymmetry. Whereas sharing *pro*-Republican real news correlates highly with sharing *pro*-Republican fake news, the correlation is substantially weaker between sharing *pro*-Democratic fake and real news. Instead, participants sharing *pro*-Democratic real news often share centrist news as well. Thus, while social media users generally favor partisan news sources, the effect is most pronounced in the Republican end of the continuum. 4 In SM Section 4, we present a series of additional robustness tests. First, we show that our results hold after removing 61 panelists who followed -or were being followed by -at least one other panelist. Second, we demonstrate that applying YouGov sample-matching weights do not alter our results. Third, we re-run models that control for age and partisanship; this does not change our results. Fourth, we leverage a potentially better measure of digital literacy (Hargittai and Hsieh 2011); using this measure does not change our substantive conclusions. Fifth, we show that our results are robust to an alternative coding scheme that treats InfoWars as a *pro*-Republican fake news source rather than a real news source. Further, SM Section 8 shows that our conclusions remain intact when using an alternative list of fake news sources compiled by Grinberg et al. (2019). The predictive power of partisanship across the partisan news continuum. If news sources differ systematically in how strongly their content caters to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Figure 4 tests this assertion. The left-hand panel plots estimated logistic regression coefficients from models that examine the association between a seven-point partisanship scale (re-scaled to range from 0 = Democratic Identifier to 1 = Republican Identifier) and sharing content from the seven news source types, while the right-hand panel gives raw counts of shared links stacked by participants' partisanship (where, to ease interpretation, we trichotomize the partisanship scale). Overall, Figure 4 provides more evidence in favor of the partisan logic of news sharing: Both panels show that Republicans are more likely than Democrats to share extreme *pro*-Republican news    Left-hand panels: Headlines shared on Twitter by panelists (upper panel) and headlines scraped from front pages of news sources (lower panel). Red dots: Headlines that mention Republican politicians. Blue dots: Headlines that mention Democratic politicians. In the lower panel, we scraped headlines from the following news sources: Fake Republican: babylonbee.com, dailywire.com, ilovemyfreedom.org, theconservativetreehouse.com, iotwreport.com; Strong Republican: breitbart.com, dailycaller.com, freebeacon.com, nypost.com, townhall.com; Lean Republican: foxnews.com, hotair.com, telegraph.co.uk, washingtonexaminer.com, washingtontimes.com; Center: thehill.com, usatoday.com, npr.com, bbc.com, wsj.com; Lean Democratic: politico.com, nbcnews.com, nytimes.com, theguardian.com, washingtonpost.com; Strong Democratic: alternet.org, dailykos.com, huffingtonpost.com, rawstory.com, vox.com; Fake Democratic: bipartisanreport.com, indiatimes.com, newspunch.com, palmerreport.com, themindunleashed.com. Righthand panels: Headline counts from news sources, conditional on whether headline mentions Republican elites, Democratic elites, or both. Note that headlines mentioning both Republicans and Democrats counted twice in the sentiment analysis. See SM Section 11 and 12 for details on the analysis. are negative towards Democrats. We can go further. For partisans, a critical consideration when promoting news sources on social media is not just the level of negativity expressed towards the out-party. Instead, the most politically useful news source is one that is negative towards the out-party and positive towards the in-party. This implies that we should also examine, for each type of news source, the difference in negativity between mentions of Republican and Democratic elites in Figure 5. Recall that Democrats are more likely to share centrist news sources while Republican partisans prefer fake news sources. The net differences in headline sentiment when mention- We suspect this explains why Republican partisans find fake news more appealing than do Democratic partisans. To get a steady supply of news that caters to their political tastes, Republicans must turn to more extreme news sources, including those known to publish falsehoods. Democrats, in contrast, will have to look no further than to Centrist and *pro*-Democratic real news sources to meet their partisan goals. Importantly, this explanation cannot easily be dismissed by a \"self-selection\" story in which Republican panelists simply decide to share more extreme stories. Focusing on the front page headlines of stories journalists and editors have deemed sufficiently newsworthy to write and publish leads to the same conclusion. Still, whether this speculation is warranted, the combined observations point to an astonishing overlap in the asymmetries in (1) the sources shared by Democrat and Republican partisans, respectively, and (2) the tone of the stories shared from these sources when covering Democratic and Republican elites, respectively. Together, these three analyses corroborate that Republicans approach news source types as if they are placed on a one-dimensional partisan continuum. Democratic partisans, to be sure, do too, but with the very important exception that *pro*-Democratic fake news sources are less attractive to highly polarized Democrats when we compare (a) Democrats to Republicans and (b) *pro*-Democratic fake news to traditional *pro*-Democratic news sources. As we turn to below, this overall pattern suggests that the sharing of fake news is This suggests that fake news sharing is \"business as usual,\" dictated by the same logic partisans use to process other types of information, including real news. From a partisan motivated perspective, fake news is not categorically different from other sources of political information. As the analyses of the news content showed, partisans' decisions to share both fake and real news sources depend on how politically useful they are in derogating the out-party. Under some circumstances, this goal is best achieved with stories from fake news websites. Under other circumstances, this can best be achieved by stories from more credible websites. To the partisan mind, fake news is the extreme end of a news source continuum where the news sources differ in how well they cater to partisan goals. Accordingly, when deciding to share a story from a fake news website, most people do not prioritize whether the story is true or not. This helps explain why cognitive reflection is not related to fake news sharing even though it correlates with the ability to discern fake news from real news. These abilities are not employed in the context of real-life sharing decisions. At the same time, the sum of findings does suggest a dual involvement of accuracy-oriented and goal-oriented motivations. First, fake news sharing is a relatively rare phenomenon that flourishes only among small segments of the population. Like others, and in contrast to what some commentators have feared, we find that only 11% of our participants shared stories from fake news sources during our period of study while half of the participants shared stories from real news web domains. Second, as discussed below, Republicans were more likely than Democrats to share fake news (e.g., Guess, Nagler and Tucker 2019), even though fake news could help Democrats achieve their partisan goals. This suggests that social media users prioritize the usefulness of information when making sharing decisions but prefer useful information that is true rather than false; they care about the veracity of news as long as it does not compromise its usefulness. Two observations merit further discussion. First, it is relevant to discuss the finding that negative feelings towards political opponents rather than positive feelings towards the inparty are the strongest predictor of fake news sharing. There is strong evidence that partisans generally prefer helping the in-party rather than hurting the out-party (Lelkes and Westwood 2017). A hostile political climate, however, may shift the balance. Research suggests that the danger of \"symbolic\" threats against the moral values of one's group can make it \"necessary to strike back, to undermine the credibility of the opposition, and to 'hit them where it hurts\"' (Amira, Wright and Goya-Tocchetto 2019). Twitter -with its fast diffusion of moralizing and emotion-laden political content (Brady et al. 2017) -may have fostered such a hostile environment, making considerations of hurting the out-party loom larger when deciding which news stories to share. Second, we found a significant partisan asymmetry: Republicans were more likely than Democrats to share fake news sources. This could reflect that Republicans are less motivated by accuracy-motivations than Democrats. Yet, accuracy motivations have low explanatory power overall and additional analyses presented in the replication code show that that the associations between ignorance-related variables and fake news sharing do not consistently differ between Democrats and Republicans. Accordingly, other explanations may be at play. One possibility highlights differences in the supply of news (Grinberg et al. 2019): Democrats and Republicans may be equally motivated by accuracy goals but Republicans end up sharing more falsehoods because their Twitter networks offer a greater supply of fake news sources to share from. Another -complementary -supply-side explanation shifts the focus from accuracy-to goal-oriented motivations: Democrats and Republicans are equally motivated to share information that is useful for furthering their political agenda but the supply of useful real news stories is lower for Republicans, propelling them to turn to fake news sites for material. The sentiment analyses of news headlines are consistent with this explanation. Only *pro*-Republican fake news sources were more negative towards Democrats than towards Republicans. This pattern was consistent across news stories shared by the panelists, both partisans and Independents (see SM section 11 for analyses of Independents), and the news stories from the news site front pages. This at least suggests that this asymmetry reflects a difference in the supply of content. A Republican scanning the Internet for materials to share must turn to fake news sources to find news that, on average, satisfy their partisan need to derogate Democrats. Democrats operating in the same manner, in contrast, can stop their search at posts linking to *pro*-Democratic real news websites or even centrist websites. Without individual-level data on news exposure, it is very difficult to tease apart these explanations (see Eady et al. (2019)) and, most likely, all are at play. Still, if the latter explanations are at least partly correct, the partisan asymmetry in fake news sharing may have less to do with psychological differences between Democrats and Republicans and more to do with differences in the supply of useful information (in a partisan sense). This would dovetail with studies demonstrating that Democrats and Republicans are equally motivated by partisan goals (Brandt et al. 2014). Also, we want to note that it is unclear whether a supply-oriented explanation is evidence of actual media bias. Bias is notoriously difficult to establish. Some prior studies have found similar evidence of a liberal bias in legacy media (Groseclose and Milyo 2005), but the general negative coverage of leading Republicans could also reflect a \"bias in reality\" (Groeling 2013) during an unusual period in US politics. Overall, these conclusions have important implications for how to intervene successfully against the circulation of \"fake news.\" If people care primarily about a story's ability to hurt political enemies, we should not be surprised if fact-checking fails to reduce sharing. In fact, our findings suggest that offline realities shape online sharing of \"fake news.\" Fake news sharing relates to the increasingly polarized political debates in the United States, and it may be difficult to fix the problem of fake news without fixing the larger problem of political polarization. Unfortunately, this is much harder to accomplish than adding automatic factchecks on social media platforms. This does not imply that it is impossible to demotivate sharing of \"fake news.\" Evidence is emerging that fake news sharing decreases other people's trust in the sharer (Altay, Hacquin and Mercier 2019). Reputational concerns may explain the low prevalence of fake news. This implies that successful interventions could work by reminding people to \"not make a fool of themselves\" when they are about to share information from non-credible sources. Yet, even if such interventions can drive down sharing of genuine fake news (Pennycook et al. 2019), the underlying problem remains. Polarized partisans can often find plenty of negative content on ordinary news sites. While each individual news story may be true, the overall picture painted by exclusively sharing derogatory news stories may not be. In this perspective, the real problem is the tendency to selectively engage with derogatory news"
        ],
        "ground_truth_definitions": {
          "fake news": {
            "definition": "fabricated information that mimics news media content in form but not in organizational process or intent",
            "context": "With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10816-016-9274-2",
        "sections": [
          "When archaeologists present the results of an archaeological survey, whether to an academic audience or to cultural resource managers, the consumers of these reports product is not like television monitors or automobiles. Consequently, some aspects of QA require modification to fit our unique situation, although we do share with other research endeavors some aspects of quality research, and we also share with manufacturing businesses (especially in the case of CRM-related archaeology) the needs to monitor costs and maintain or improve the satisfaction of clients. The purpose of QA is not to create unwieldy bureaucracy or still more unwanted paperwork; it is to put mechanisms in place so that we may consistently maintain and improve the quality of work. Undoubtedly, this creates real costs, in time, money, and training, but the failure to ensure quality also has costs, such as the need to resurvey territory that was not adequately surveyed the first time or, worse yet, the need to mitigate a site that construction crews unexpectedly uncover in an area supposedly clear of significant archaeological resources. For example, Schiffer (1987: 353) mentions a survey that failed to record a large Hohokam site in Arizona. This led to enormous and unexpected mitigation costs ahead of construction of an aqueduct that could easily have been re-routed had the survey reported the site. Yet, another cost of such failure is the psychological and social impact to descendants and the community in general when a development project inadvertently exposes an unexpected cemetery site. In what follows, we will not attempt to grapple with the entire gamut of QA as it applies to archaeological surveys, which would have to include such topics as creating manuals and checklists, managing the quality of publications and reports, monitoring the condition of artifact collections, evaluating the effectiveness of sampling designs, and even improving the marketing of results for archaeological assessment firms. Instead, we focus on ways that the QA paradigm might apply to the design of archaeological surveys, especially by fieldwalking, the evaluation of completed surveys and the performance of field personnel, improvements to survey quality, and the mitigation of nonconformance to standards in survey. For the most part, this concerns those parts of the Quality System that deal with inspection and testing to verify that the product-in this case archaeological data-meets or exceeds expectations (ISO 9000, section 4.10-12, Paradis and Small 1996:41-49). As even this is a very broad topic, we will further focus on the important problem of survey coverage or, to put it another way, the probability that we have successfully detected the sites and artifacts that are potentially available to be found.",
          "In archaeological surveys, some very important, although by no means only, benchmarks of quality are a survey's ability to detect archaeological materials, classify them correctly, and adequately represent their character, density, and distribution. This ability in turn depends on the skill and attentiveness of field personnel, the quality of the survey design, the character of the archaeological materials, and characteristics of various Benvironmental^factors that affect surveyors' attention and ability to see artifacts (Banning et al. 2006(Banning et al. , 2011;;Hawkins et al. 2003). A number of archaeologists have called attention to the importance of ensuring survey effectiveness (e.g., Hirth 1978;Ives 1982;Miller 1989;Shott 1992;Sundstrom 1993;Zubrow 1984), yet this call has had relatively little impact on practice.",
          "Many jurisdictions have standards to which consulting archaeologists must adhere when undertaking archaeological survey of lands subject to impacts by development. In the USA, the Secretary of the Interior's Standards and Guidelines (SISG) serves as a basis that is modified or added to by individual states depending on local conditions (National Parks Service 1983). In Canada, standards are determined individually by province (e.g., Ontario Ministry of Tourism and Culture 2011). Both SISG and the Canadian systems define minimum qualifications of lead archaeologists (licensed archaeologists, permit holders, or principal investigators) and the minimum fieldwork standards to be met during survey. The SIGS qualifications for a professional archaeologist are to ensure that individuals leading projects are able to undertake complicated projects and carry them to fruition. These and other similar standards typically require license holders to have credentials, such as a graduate degree in archaeology, and minimum field experience and sometimes expect them to belong to an archaeological organization with a code of ethics. A number of jurisdictions have minimum requirements for field directors, but few have specific standards for the qualifications of field workers (Ontario Ministry of Tourism and Culture 2011; Georgia Council of Professional Archaeologists 2014; Tennessee Division of Archaeology 2009). Where it is mentioned, it appears that QA is the responsibility of the lead archaeologist. Minnesota, for example, requires the principal investigator to ensure that standards are met and notes that Bqualified personnel^are one of the elements that contribute to quality (Anfinson 2005). In all cases, these standards are based on experience and education; none of them pertain to demonstrated skill of an individual to carry out survey effectively. Many jurisdictions set standards for survey in terms of transect interval for pedestrian survey, acceptable amount of cover (by snow or vegetation) of the surface to be examined, test-pit interval, depth of test pits for shovel testing, and screen mesh size. Table 1 shows that these vary widely. In some cases, states prefer that archaeologists determine methods appropriate to the specific project (e.g., Tennessee Division of Archaeology 2009), but the norm is for state guidelines to specify minimum standards. While there is some uniformity in the size of shovel tests, the maximum interval for these ranges from 5 to 30 m, and not all jurisdictions state that screening is necessary. While some state guidelines indicate that at least 75 % of a surface should be visible for pedestrian survey, others indicate that a switch should be made to test pitting when only 10 % of the surface can be seen. Transect intervals for pedestrian survey range from 5 to 30 m. A number of states indicate that survey methods should be adapted to the specific conditions. In some cases, for example, they indicate that transect intervals should be tightened in high-probability areas (Anfinson 2005;Council of South Carolina Professional Archaeologists 2009). Standards in some jurisdictions, most notably in Europe, show recognition that geomorphological and other factors can make pedestrian survey, or even shovel testing, inadequate for site discovery and require a range of other techniques, such as coring or augering (e.g., Crombé and Verhegge 2015;Willems and Brandt 2004: 45). ",
          "A large number of factors can affect the effectiveness of any survey.",
          "The personnel are only one element of the survey design. Other aspects include the allocation of search effort to a given area of space (usually expressed in the spacing of transects in fieldwalking or of test pits or auger locations in subsurface survey) and the selection of spaces to search, whether by formal sampling, predictive modeling, or more judgmental designs. In current archaeological survey, the selection of transect intervals is largely arbitrary. From the QA point of view, the best that can be said is that surveys meet or somewhat exceed the requirements of state or provincial laws and guidelines for the spacing of fieldworkers. Yet, as noted above, these guidelines vary substantially, often allow intervals as high as 30 m, and, most importantly, are never based on empirical data on the relationship of transect interval to discovery probabilities of artifacts or intersection probabilities for sites. There is no reason why this should be so. From the QA perspective, surveyors should be able to assure their clients or academic audiences that they have selected transect intervals that not only meet regulatory requirements, but will have resulted in some minimum (and reasonably high) probability of detecting the kinds of archaeological resources purportedly of interest. At the very least, survey reports should provide realistic estimates of survey coverage that take into account both imperfect detection by surveyors and the spacing of transects, augers, or test pits. For test pitting, a number of archaeologists have emphasized that the probability that test pits will both intersect a site and detect its presence (on the basis of detected artifacts) depends on the area and spacing of test pits, the area, shape, and orientation of the site, the density of artifacts within the site, and whether the distribution of these artifacts is random or clustered (e.g., Banning 2002: 88-105;Nance 1979;1983;Shott 1985;1989). However, much the same issues are involved in fieldwalking as in subsurface testing. The spacing of transects or test pits is related to the density of search effort, which some archaeologists describe as survey intensity (Plog et al. 1978: 389). More generally, we can summarize search density for fieldwalking as the amount of time or total length of transects devoted to searching each unit of space (Koopman 1980: 74;Stone 1975: 25). The relationship between search time and discovery probability is not linear, and increasing search effort exhibits diminishing returns (Banning et al. 2006). From the quality perspective, we should allocate enough effort to searching each space to meet some standard for the probability of detecting targets of interest, yet we should also not squander search effort on one space that could be more economically used in another one. That is, additional search effort in the first space might only increase discovery probabilities by 5 or 6 %, because the search has reached a plateau in the detection function, while the same amount of search effort in a new space might yield a discovery probability of 50 or 60 %. Since the total time available for survey is often limited (a Bfixed-budget survey^), the allocation of search time has important implications and is arguably even more important for relatively costly test pitting than for fieldwalking. Somewhat analogous to transect or test-pit spacing, the mesh size of screens and amount of time devoted to screening sediments from subsurface testing have a nonlinear relationship to the probability of detecting artifacts in the sediment (Crombé and Verhegge 2015). This is an issue that deserves attention in a separate paper, but at the very least QA requires us to report on the quality characteristics of screening in some detail. A survey design's plan for selecting areas to be searched, including, where relevant, the rationale for sampling, is a critical aspect of survey quality. For sample surveys, it is necessary to provide justification for the sample size and assurance that the correct formulae for statistics have been used (especially for cluster and stratified sampling, Orton 2000: 99, 211-213) and, for stratified samples, the rationale for stratification and a post-survey evaluation of the stratification's effectiveness (i.e., are there statistically significant differences between the strata?). Personnel There are good reasons to expect that crew members' training, experience, motivation, health, attentiveness, and ability to see and recognize small objects on a varying background of soil surfaces and vegetation will vary over time and among individuals (Plog et al. 1978: 413-15). However, implicit in the reports of typical archaeological surveys is the assumption that all field crews had closely similar or identical detection abilities, some even suggesting, implicitly or explicitly, that within some specified range they detected 100 % of the artifacts exposed on the surface (this is the definite detection model; Koopman 1980:59;Banning 2002:57-59). We suggest that this assumption is, to say the least, over-optimistic, and our field experiments have shown that surveyors' detection of artifacts, even under rather ideal conditions, is both variable and far from perfect (Banning et al. 2006(Banning et al. , 2011;;Hawkins et al. 2003). If we are to take the results of surveys seriously, especially in their claims for the absence of archaeological remains, we should have some assurance that they have taken surveyors' detection abilities into account. Such assurance requires both training of crew members and periodic assessment of their detection abilities under a variety of conditions. Artifact Obtrusiveness The characteristics of the artifacts we are trying to detect also affect our ability to detect them. For several decades, archaeologists have summarized this effect as obtrusiveness (Schiffer et al. 1978: 6). Research by us (Banning et al. 2006(Banning et al. , 2011) ) and others (e.g., Schon 2002;Wandsnider and Camilli 1992) has confirmed the intuitive expectation that, holding other conditions constant, the size, shape, color, and reflectivity of artifacts, especially as these contrast with their environment, create substantial variation in their detectability. For example, fairly bright and glossy sherds of glazed porcelain or glass are quite easy to detect on a variety of backgrounds, while mottled grey flakes of chert are quite difficult to see, especially on stony backgrounds. Since different kinds of archaeological resources vary in the kinds and diversity of remains that signal their presence, and these remains themselves vary in detectability, we should expect claims about the density and distribution of different kinds of archaeological sites to take artifact obtrusiveness into account. For example, we should be wary of claims for a particular ratio of Early Woodland sites to historic ones in the American Northeast if these claims did not take into account that the kinds of artifacts expected on an Early Woodland site are quite unobtrusive, while those on historic sites can include bright porcelain, reflective glass fragments, and even chunks of brick. In addition, aside from the obtrusiveness of individual artifacts, their distribution can also have an impact on their probability of detection (Krakker et al. 1983). In some cases, higher degrees of artifact clustering can result in greater artifact detection than in cases where the same kinds of artifacts are distributed randomly or relatively evenly. Since archaeologists are often interested in discovering precisely these highly clustered manifestations of artifacts, this can be advantageous. However, we also need to be aware of the potential bias that can result when archaeological resources that are more likely to exhibit high degrees of clustering are over-represented relative to less clustered remains. The tendency for surveyors to pay closer attention once they find even one artifact has the potential to exacerbate this bias. Visibility In addition, variations in surface texture and color, vegetation cover, lighting, rainfall, plowing, and other environmental conditions can have profound impacts on the detection even of artifacts that are exposed at the surface, not to mention those that may be buried to various depths by a number of geological and cultural processes (Schiffer et al. 1978: 6;Banning 2002: 46-48;Fanning and Holdaway 2004;Stark and Geraty 2008). Assurance of the quality of an archaeological survey and its reports requires analysis of how these factors may have impeded artifact detection and their potential impacts on the apparent distributions of archaeological materials. A number of archaeological studies have emphasized the effects of rainfall (Shott 1995;Shott et al. 2002) and plowing (e.g., Ammerman 1985;Clark and Schofield 1997). As noted above, some jurisdictions' standards and guidelines for archaeological survey do require reporting of fieldwork conditions, rainfall, vegetation cover, and other factors that affect visibility, yet subsequent research and predictive models that employ the data from surveys do not always take these factors into account.",
          "An extremely important aspect of archaeological survey, too often overlooked, is the evaluation of its effectiveness. That evaluation is a critical aspect of QA and should apply to all of the aspects discussed in the last section. Since space is limited, here we focus on sweep width and coverage as the best way to summarize the interacting factors of surveyor abilities, visibility, and artifact obtrusiveness.",
          "Some survey practictioners have attempted to evaluate shortcomings in surveys by reference to one or two of the factors that can affect detection, most usually focusing on visibility in pedestrian survey (e.g., Stark and Geraty 2008;Terrenato 2000), while others focus on inter-surveyor differences, especially for neighboring surveyors in parallel transects (e.g., Hawkins et al. 2003;cf. Gnaden and Holdaway 2000). But the quality of a survey depends on all these characteristics-survey personnel, artifacts, and environment-and they interact with one another. Consequently, it is most practical to summarize these effects simultaneously. Two basic ways to accomplish this are resurvey and a calibration approach. These are applicable to any survey method, but here we focus particularly on pedestrian survey or Bfieldwalking.T",
          "Any QA program requires periodic tests and audits to ensure compliance with standards, assess the effectiveness of designs and reliability of results, and provide data with which to improve those results. Typically, such tests are carried out by specially trained inspectors, who may observe people at work, make measurements on and statistical analyses of samples of their output, or both. One of the most obvious ways to test the effectiveness of archaeological surveys is an example of this approach and involves resurveying a sample of the spaces that have already been subject to survey. A number of survey projects have employed this strategy. An early example of this was the 1977-78 East Hampshire Survey, which detected marked differences in results when resurveying a sample of four fields (Shennan 1985: 44). Hirth (1978) used survey of the same spaces in three different seasons to evaluate the effects of rainfall and agricultural practices on the detection of artifacts in the Basin of Mexico. Wandsnider and colleagues used resurvey as one component in their evaluation of surveys in the Green River Basin, Wyoming, the Mesilla Bolson, and the Sandilla Mountains, both in New Mexico (Wandsnider and Camilli 1992;Wandsnider and Ebert 1986). In an example that one of us undertook in Cyprus in the 1980s and 1990s, there were also selective examples of resurvey to test survey effectiveness (Stewart 2006). To check on the accuracy of site and find-spot locations, and to assess changing site and artifact visibility in the field, the team resurveyed a number of units from two survey projects (Stewart 2006:85-86, 103-104, 106, 111, 146-147;Stewart and Morden 2016). Initial analysis of the overall numbers of artifacts within a survey unit or across the study area suggests no differences between the original and second surveys. However, when we consider the location of finds within the area, it turns out that findspots from the resurvey were never in the same location as those from the initial survey. Although the crews collected some artifacts on the first survey, they did not attempt to collect all artifacts so, theoretically, the findspots located on the first survey should still have been detectable on the resurvey. The mostly likely explanation for this marked difference is the survey's use of sinusoidal, rather than straight, transects, which made it impossible for the resurvey to duplicate exactly the pathways of the initial survey. Broadly speaking, Burger et al.'s (2004;2006) Bnested-intensity sampling^is also an example of the resurvey approach. Using a variation of the Whittaker multi-scale sampling plot, they resurvey a sample of spaces at increasingly smaller resolutions and intensities, including even crawling on hands and knees, and excavation and screening of the top 10 cm of small spaces. Comparison of the results at these different levels of intensity allows estimates of how much the less intensive survey has missed. In a particularly interesting case, Almagro-Gorbea et al. (1996;2002) attempted to ensure reliability of survey in the Comunidad de Madrid by using an independent Bcontrol survey^of a sample of the areas that had already been surveyed. Originally, this was a Bblind but directed^sample (Almagro et al. 1996:252), but the authors later realized that, for the quality control itself to be valid, it needed to be a random sample (Almagro et al. 2002: 47). In its final form, their method involved comparing the initial and Bcontrol^survey statistically, in terms of their classification of survey spaces as Bsites^and Bnonsites,^using an estimate of the actual number of sites and nonsites, and thus really evaluates the degree of agreement between the two surveys. This project comes closest among those we have seen to the formal principles of QA and involved use of a military standard for inspection sampling to decide how many units required resurvey. From the quality perspective, the survey-resurvey approach works best when the resurvey is always by the same, highly experienced, and highly skilled team, whose members were not involved in the original survey. In an attempt to check on the reliability of surface collections from sites, Shott et al. (2002) re-collected an Oneota site in Iowa that had already been surveyed the previous year and compared the results. They concluded that the materials collected during resurvey differed significantly from those from the original survey and attributed a good deal of the difference to rainfall. As they note in the paper, Barchaeologists rarely allow for the effects of rainfall amount when interpreting survey results^ (Shott et al. 2002: 180). While the survey-resurvey strategy is clearly much better than no testing at all, it does have several disadvantages. First, the resurvey is not always by a team of experts, and not even always by the same team. Consequently, we cannot be certain that the second survey is any more reliable or accurate than the first, and it may not be justified to take comfort even in a high degree of agreement between the two surveys. Second, even if the resurvey is by our best, most reliable surveyors, it is not plausible to assume that their results will be perfect. Indeed, our experiments have shown that even highly skilled and experienced surveyors only detect a fraction of the artifacts exposed on the surface under typical conditions. Third, since the resurvey strategy usually operates in the absence of knowledge of the actual distribution of artifacts, we cannot assess the real detection rates of any of the surveyors, including the most skilled ones, but can only assess their relative abilities. Fourth, the resurvey strategy is highly vulnerable to other threats to validity. It is an example of the test-retest research design and thus can suffer from the effects of history and maturation (Campbell and Stanley 1963: 5). That is, ground conditions, weather conditions, or some other relevant factor could have changed during the interval between the first and second surveys, making them not strictly comparable. This is especially likely in the event that the first survey removed significant numbers of artifacts from the surface, when surveys were in different seasons (Hirth 1978) or, as Shott et al. (2002) point out, when there were significant differences in rainfall. Only a few examples of archaeological re-survey have employed controls that help mitigate some of these differences. One method for control is for the initial surveyors to seed survey areas with a known number of artifacts that are similar in some respects to the target artifacts, yet clearly identifiable as nonarchaeological (Wandsnider and Camilli 1992: 173-76). The proportion of these that the resurvey team discovers serves as a means to estimate their survey effectiveness, rather than just assuming that the resurvey team is perfect.",
          "Our preferred methods for assessing and improving the quality of archaeological surveys depend on tests of surveyors' abilities to detect a variety of artifact types under a variety of controlled but realistic field conditions. To ensure that we know in advance what the population of artifacts is like, we carry out this assessment on a series of test fields, which we have seeded with a variety of artifacts in known locations (cf. Schon 2002;Wandsnider and Camilli 1992). To control for maturation and history, we assess a group of surveyors repeatedly over the course of a survey project to produce Baverage^results. To control for the effects of range, we measure the variation in artifact recovery at different distances from the transect lines. To control for search speed, we ask surveyors to walk at a speed that mirrors their typical speed on Breal^survey. Effective sweep width (W) is the single most useful measure of survey effectiveness because it summarizes all of the variables that affect survey detection in a single number, with the proviso that we either need to measure it separately for each artifact type or make some assumptions about the likely mixture of artifact types. Where possible, it is better to take the former approach, since we rarely have much a priori information about the likely proportions of artifacts to expect. In the example below, however, we compromise by grouping artifacts into two broad categories, lithics and pottery, and omit very small artifacts, such as microliths, that are very difficult to detect. One reason for this is that it improves our sample size, as we have found that our data sets for individual artifact classes are sometimes too sparse for really good estimates of sweep width. On the basis of data from controlled calibration transects, described below, we can calculate sweep widths from the exponential detection function where p(r) is the probability of detection at range r in meters, b is the y-intercept (expected detection probability right on the transect, where r = 0), e is the exponential constant (approximately 2.718), k is a constant that summarizes the effects of the various contributors to detectability per square meter, r is the range or perpendicular distance away from the transect line in meters, and kr 2 describes the steepness of falloff in detectability away from that line (Koopman 1980: 64;Banning et al. 2011: 3448). This function typically describes an S-shaped curve of declining probability away from the transect line (Fig. 1). Sweep width in meters is equal to the area under this curve (or double the area if we only consider positive values of range). The reason that this area is in meters (not square meters) is that the y-value is a probability, with no units at all. This is clear from the fact that the sweep width corresponds with a box (Fig. 1) whose height is 1.0 (the probability), whose width is W (in meters), and whose area, in the figure, is 2(a + b).",
          "), whereby we expect to find all the artifacts within W/2 of the transect line, but none at all beyond that range. In the more realistic exponential model described by the curve, the area under the curve, likewise, has an area of 2(a + b). Consequently, the box describes how many artifacts we can expect to detect as though we found all of them within W/2 of the transect line, when in reality we found 2b artifacts within W/2 and 2a artifacts outside W/2. What this means, and makes sweep width useful and intuitively attractive, is that we expect to find the same number of artifacts as we would if we had perfect detection within the sweep width (Fig. 2). Keep in mind that our sweep width and the exponential detection function on which it is based only apply to the artifacts that are potentially visible on the surface, when we are using fieldwalking and visual inspection as our search method. We would have a very different sweep width if we were employing some other method, such as magnetic survey, augering, or test pitting, and wanted to consider buried artifacts, but in principal the sweep width measure is applicable to all these methods. 0 0 . 5 -1.0 1.0 -0.5 1.5 Range (W) 1.0 -1.5 a a a a b b Fig. 1 Detection functions and the definition of sweep width (W). The rectangle defines the definite detection model, with perfect detection within W/2 of the transect line. With a width of W m and a height of 1.0 (with no units), the area of this box is W m = 2(a + b). The curve is an example of a more realistic exponential model, with 2b detections within W/2 and 2a detections outside that range, in the tails. Consequently, the area under the curve is also 2(a + b) = W m",
          "",
          "To demonstrate the operation and results of such calibrations, and their use in the planning and evaluation of actual surveys, here we use data from the Wadi Quseiba Survey of 2012-13 in northern Jordan and the 2014 Tremithos Valley Survey in southwestern Cyprus. The main objective of the Wadi Quseiba Survey was to discover late prehistoric (Epipalaeolithic, and Chalcolithic) sites, which are commonly underrepresented in Jordanian surveys because of erosion, colluviation, the rarity and poor preservation of Neolithic pottery (cf. Bintliff et al. 1999), and sometimes the shortage of lithics expertise on survey teams. In an attempt to maximize the recovery of such sites, we employed a Bayesian algorithm for allocating search effort. Shortage of space precludes detailed discussion of this approach here, but suffice to say that it involves allocating new search effort on the basis of a predictive model that is continually updated in light of information on results, including coverage, as survey progresses (Hitchings et al. 2013). The Tremithos Neolithic Survey Project is a pedestrian survey of sections of the Tremithos River Valley, running southeast from the Troodos foothills to the sea, in south-central Cyprus (Stewart n.d.). The goal of the project is to identify early Neolithic use of the valley to access the resource-rich areas in the Troodos foothills, particularly their abundant and high-quality chert sources, and whether this river system provided a transportation route from the sea to this central area. As in the Wadi Quseiba Survey, the Tremithos Neolithic Survey Project has employed Bayesian optimal allocation of survey effort, making it critical to estimate coverage and to update these estimates daily.",
          "In contrast to our previous simulations that employed a grid of strings as the test fields for Bseeded^artifacts (Banning et al. 2006(Banning et al. , 2011)), to calibrate these surveys we experimented with test fields without a formal grid so as to be more similar to actual survey conditions, but that appeared to be devoid of ancient artifacts. In Jordan, test fields were set up in pasture that had a mixture of bare rock and patchy vegetation, a plowed guava field, a plowed olive grove, and a Bmixed^field that had nearly equal portions of plowed field with some stubble and sparse trees, pasture with denser trees, and pasture with bare rock and sparse shrubs (Fig. 3a-d). In Cyprus, the test fields included a bare, plowed field and a field with wheat stubble (Fig. 3e-f). In each test field, we laid out 50-m tapes to mark the path of a transect 120 or 150 m long, in some cases needing to bend this transect once or twice to avoid obstacles or allow for the shape of the field, and planted artifacts at locations randomly assigned to various locations left and right of the tape at distances up to 10 m (in the case of the Bpasture^) or 20 m. Artifacts included the categories of large and small lithic flakes and large and small ceramic sherds in several colors, mostly Bred^and Byellow^and somewhat similar to those anticipated in survey. However, for the purposes of this paper, we group the artifacts into all lithics and all pottery, whose sizes range from about 4 to 12 cm in greatest length. Initially, the calibrations also included very small lithics (microliths), but surveyors never detected them and we exclude them from the examples here. Each member of the survey team walked along the tape multiple times over the course of several hours on each occasion that we conducted a calibration. Using a form on an iPad, on each attempt the surveyor marked his or her start and finish times and listed (but did not collect) each artifact seen along with its artifact category, distance along the 50 m tape, and, because there was no grid, estimated distance to left or right. We kept the density of seeded artifacts low enough for us to be reasonably certain of our identification of Bsuccessful^detections (as opposed to Bfalse targets^), even given the imprecision with which surveyors sometimes estimated the distances to artifacts they saw. Generally, these estimates were accurate and precise at close range, but we allowed for errors of up to 2 m in any direction at ranges of 15 to 20 m, and 1 m in any direction between 5 and 15 m. By checking the data for all the transects and all crew members against the known locations of seeded artifacts, we are able to tabulate the number of successful detections by range and use the data to fit to a curve for the detection function (1) using nonlinear regression in either the SPSS ™ or R platform to provide robust estimates of b and k using guesses of their values at the beginning of a constrained (0 ≤ b ≤ 1.0) model with estimation by sequential quadratic programming. We then use a numerical integration function in R to plot curves with these values and estimate the areas under the curves between ranges of -100 and 100 m to obtain sweep widths (Figs. 4 and 2). As the curves and Table 2 make clear, there is considerable variation with type of field. Although small sample size may be a factor, the Bbest^sweep widths are in the pasture and the plowed olive grove in Jordan, where the sweep widths for lithics are on the order of 6-7 m and those for pottery are more than 4 m. The guava orchard, despite also being plowed, has noticeably poorer visibility. This probably has less to do with the plowing (although plowing was coarser, with deeper furrows than in the olive grove) than with the spacing and leafiness of the trees, which were young and had low branches. The very low value of b for lithics on the stubble field in Cyprus and the depressed shape of the curve close to the y-axis compared with that for lithics in other locations can best be explained by our calibration methods. We chose to randomize locations of seeded artifacts, and, in this case, no lithics were seeded within 2 m of the transect and the resulting curve was based on artifacts observed at distances of 3 m and more. Consequently, we recommend, rather than using a completely random distribution, to use a uniform distribution with range and only randomize with distance along the transect. We should not generalize too much from these results, since of course they depend in part on the abilities of the particular crew members involved, and not just on visibility or artifact obtrusiveness. We should also consider these somewhat optimistic estimates, since the artifacts were fresh and clean, rather than half-buried or dirty, and a few crew members appear to have walked a bit more slowly than they would normally 0 2 4 6 8 1 0 r 1.0 0.8 0.6 0.4 0.2 p(r) a b 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) c d e f 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r 0 2 4 6 8 1 0 r p(r) 1.0 0.8 0.6 0.4 0.2 p(r) Fig. 4 Detection functions for all lithics (solid curve) and all pottery (dashed curve) along with half the corresponding sweep widths (vertical lines) at the calibration sites a pasture, b olive grove, c guava orchard, d mixed field, all in Jordan, e stubble field, and f plowed field in Cyprus. p(r) is Bprobability at range r^and r is range in meters do. However, they did provide us with some empirical data with which to assess our survey effectiveness, beginning with very preliminary estimates of sweep width (after only day of calibration) and leading up to the estimates provided here. Since the sweep widths for different artifact types differ, we were Bconservative^by using the lesser of the lithic or pottery one. In addition, because visibility conditions in our actual surveys could only be approximately matched with the test fields, we adjusted our estimates slightly upward or downward from the estimates derived from the most similar test field, again attempting to be conservative by risking under-estimate of sweep width rather than over-estimate. However, because we have photographs of every transect segment in our database, it remains possible to gauge these estimates with image analysis at some later date, a method we will leave to another paper.",
          "During the surveys in both Jordan and Cyprus, we used the sweep widths in conjunction with other information to help us allocate our survey effort each day through an iteratively updated predictive model. GPS coordinates for the beginnings and ends of transects walked allowed us to estimate the lengths of transects. Multiplying these lengths by our estimated sweep widths yields the total area Bswept^by our transects in each survey unit. Dividing area swept by the total area of the survey unit tells us the coverage of that unit. This estimate of coverage is the key for us to reassess the probability that any survey unit (or Bpolygon^in our usage) might still contain undetected resources of interest. Because the kinds of sites that interest us tend to have very low artifact densities that make clustering not very evident, we did not explicitly take clustering into account but at each iteration of the predictive model recalculated the probability that each polygon might contain an undetected site of interest given the total amount of coverage to date. Thus, when we surveyed a polygon without finding identifiable late prehistoric materials, the polygon's probability went down, but not necessarily by very much. Applying our allocation algorithm on the next iteration of the predictive model, it might easily lead us to survey that polygon again, and, indeed, we surveyed some polygons several times before finding anything of interest (Fig. 5; Hitchings et al. 2013). Note that in these instances we varied the orientation of transects to allow viewing of the ground from different angles and reduce redundancy in coverage (Banning 2002: 90-91;Koopman 1980: 218).",
          "In addition, the final coverage estimates provide our final assessment of how thorough our surveys were in both Wadi Quseiba and the Tremithos Valley, allowing other researchers to assess which parts of these survey regions, if any, might still repay further fieldwork. One way to express the overall quality of the survey's coverage is to plot the coverage values on an Bexhaustion map,^which displays our final estimate of how thoroughly each space in the survey region has been searched (Fig. 6). Exhaustion, for the purposes of this map, expresses the probability that there could still be undetected archaeological resources in a particular space, given the total amount of coverage that the survey has completed. Exhaustion maps are particularly useful when a survey has only covered a portion of the survey region. When, for example, the survey has only intensively examined a sample of the region's spaces, it is important to show exactly where survey took place and where it did not, so as not to give the misleading impression that unsurveyed spaces are devoid of archaeological materials. Reports should also provide justification for differential allocation of search effort, and sometimes tests to ensure that these differences did not bias the results in unintended ways. For particular spaces in the survey region, another approach for assessing survey thoroughness is to determine whether the detection function has begun to level off with increases in search effort. Using the examples of polygons 229 and 240 in the Tremithos Valley survey, we can see how the cumulative numbers of lithic finds increase over three increments of survey effort (Fig. 7). This graph would likely level off sooner had we been collecting or flagging all artifacts as they were found, but we only collected a sample, so some of the artifacts found in the second and third increments of survey could be ones that the first transects already identified. Even so, once this curve begins to level off, it is usually more effective to move onto a different space that has not received as much survey effort (Banning et al. 2006).",
          "Not only do the tests described in the previous section allow us to assess the quality of present and past surveys, they also provide information with which we can better design future ones.  ",
          "An excellent example of this is our ability to use the range functions to help us select appropriate transect intervals. The range functions allow us to estimate, within error margins, what transect interval is necessary to result in recovery of some proportion of the artifacts exposed in each transect. Effective visibility (E), for example, is the conventional term for half the interval that we would expect to result in detection of 50 % of the targets by parallel transects (Koopman 1980: 76). In some circumstances, finding half the artifacts might be adequate for achieving the goals of a research project or heritage assessment. For example, blue transfer ware might be found in high enough densities at typical nineteenth-century farmsteads in southern Ontario that only finding half of them is still adequate for the identification of these farms. In others, however, we might require better recovery than this and could calculate the appropriate interval for discovery of perhaps 80 % of the artifacts exposed on the surface or even use a transect spacing of W, which has an expected yield of 100 % of the artifacts. In still others, even 50 % might be a hopelessly unrealistic or costly goal, and we might have to settle for, say 20 %. Furthermore, the recovery of different artifact types varies, requiring us to prioritize their use in deciding transect intervals. In any of these cases, we can assure clients or academic audiences that we have met some predetermined level of artifact recovery, which constitutes the standard for the project or assessment firm.",
          "Members of field crews can vary substantially in their overall detection abilities as well as in which artifact types they are most effective at detecting (Bintliff et al. 1999: 153;Stewart 2006: 140-144). This has a number of implications for our use of these personnel in future surveys, and, although here we have only considered the combined or average abilities of crews, it is possible to use our methods to assess individual performance as well. In some cases, repeated testing might show that some crew members, despite further training, simply do not have the knack for artifact detection during fieldwalking. It might therefore make sense to reassign them to other tasks, such as test pitting, excavation, or lab work, where their talents are more suited. In most cases, however, the tests will identify weaknesses in surveyors' detection of certain artifact types that training or relatively simple changes in behavior can mitigate. Increasing their familiarity with artifact types that they tend to overlook, perhaps by having them spend some time in processing or helping to analyse them in the lab, is one way to improve their recognition of these types. Repeated practice survey on a test field seeded with this artifact type, or on an archaeological scatter that includes a good deal of this type of material, will also likely improve their detection of more difficult artifact types. In a few cases, something as simple as updating a prescription for corrective lenses or removing sunglasses may make the necessary difference. Even after training and repeated testing, it is still likely that some surveyors will be better at detecting, say, lithics, while others are better at detecting nails or pottery. This has important implications for the mix of surveyors on field crews. Doubtless, a crew whose members complement one another's strengths and weaknesses will have greater overall success at detecting a range of materials and site types than one that simply reinforces one kind of strength at the expense of others (Hawkins et al. 2003;Stewart 2006: 144-147). Consequently, survey managers should make crew assignments that tend to maximize the overall probability of detecting a range of artifact types that are likely to be important to the survey's goals. In the Wadi Quseiba and Tremithos Valley surveys, we only used sweep widths for the aggregate data of entire field crews, providing an average assessment of survey quality that is adequate for estimating overall survey coverage by those crews. This allowed us to have a reasonable sample size with which to fit the detection functions, although the raw data do indeed show considerable variation among surveyors. Where the composition of field crews varies substantially over the course of a survey, or there are other reasons to measure individuals' sweep widths, it is necessary to ensure an adequate sample size of calibration transects for each surveyor.",
          "The reputability and professionalism of archaeological surveys depend, in part, on their ability to convince us that spaces in which they report a lack of archaeological materials are actually devoid of such materials. They also depend on the ability to characterize the existing materials correctly in terms of their type, density, and distribution. Consequently, ensuring the quality of archaeological surveys requires both careful attention to design and periodic testing of surveyors and survey procedures to determine their effectiveness and to strive for improvement. Periodic assessments of crews' effectiveness at artifact detection, using real artifacts under fairly realistic conditions, provide the data with which to accomplish these goals. In some regions, these assessments would need to be repeated in different seasons or be calibrated to recent rainfall amounts, but the key is to conduct them under conditions that are as close as possible to those of the actual survey. In surveys in northern Jordan and central Cyprus, where it virtually never rains during seasons when surveys typically occur, our calibration transects were across spaces with realistic field conditions. We had seeded these with modern artifacts similar to the kinds of artifacts we expected to find, which allowed us to estimate average sweep widths for the field crews. Multiplying the transect lengths by these sweep widths allows us to calculate total area swept in each survey space or Bpolygon,^and dividing this by the polygon's area provides a reliable measure of coverage. These coverage estimates themselves provide a measure of survey thoroughness and thus one of the most important aspects of quality in survey. Using these coverage estimates in a Bayesian modeling environment further allows us to update the probabilities that each space in our survey frame might contain undiscovered archaeological materials, which we use both for allocating further survey effort and as another measure of the thoroughness of survey. The result, we argue, provides a much stronger empirical basis for inferences about the distributions and character of regional archaeological resources."
        ],
        "ground_truth_definitions": {
          "Quality Assurance": {
            "definition": "policies, procedures, manuals, standards, and systems, arranged with the goal of ensuring and continually improving the quality of products or services and consumers’ satisfaction with them",
            "context": "In the USA, standards for quality are found in ISO9000 and ISO9001, and ANSI/ASQC Q91-87 through Q94-87, while the Canadian equivalents are CSA Z299.0 and CSA Z299.1/.4, and those in the United Kingdom are BS 4891 and BS 5750. Briefly, QA involves policies, procedures, manuals, standards, and systems, arranged with the goal of ensuring and continually improving the quality of products or services and consumers’ satisfaction with them. Unfortunately for archaeologists, most of the literature on Quality Assurance and Management concerns manufacturing and service industries.",
            "type": "implicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Extract (term, definition) pairs present in this section.\nReturn a dictionary of definitions, e.g. {\"hate speech\": [\"language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\", \"context sentences...\"], ...}.\nIf none found, return {}.\nRules:\n- Prefer definitions stated or strongly implied in this section.\n- Do not over-generate: only extract what is clearly defined in the section and what is a clearly a definition, not description, explanation, effect, or other information.\n- Each term must be explicitly defined in the section.\n- Do not hallucinate outside this section.\n- Each definition must be ideally 1 sentence long.\n- Remove the prefixes like \"<term> is defined as\" from the definition text and keep lowercase.\n- If multiple definitions are present, extract each one separately.\n- Unless absolutely certain, prefer returning no definitions to false positives.\n- Unless strongly required, copy the definition word by word from the source text!\n- If term has synonyms defined (not abbreviations!), divide them with '/' in the 'term' field.\n- For context, include 1 sentence before and 1 sentence after the definition sentence, if possible and don't change any words or formatting.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Extracted Terms:",
          "description": "[Term, Definition, Context, Type] tuples extracted from the section. Context sentences where the definition was found (i.e., 1 previous sentence, the definition sentence, and 1 next sentence; if possible)."
        }
      ]
    },
    "lm": null
  },
  "determine.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "section": "Given that racial/ethnic disparities in food insecurity have continued for more than a decade, the need to move beyond just adjusting for race/ethnicity in research to examining exposures that may impact people of color differently from whites is warranted. There is growing recognition in the health sciences, particularly public health, that discrimination and structural racism are key contributors to inequity in health behaviors and outcomes. [19][20][21][22][23] The basic definition of racial discrimination is the unequal treatment of persons or groups on the basis of their race or ethnicity. 24 As highlighted in the National Academies Press Consensus report on Measuring Racial Discrimination, racial discrimination includes two components: (1) \"differential treatment on the basis of race that disadvantages a racial group\" (disparate treatment) and ( 2) \"treatment on the basis of inadequately justified factors other than race that disadvantages a racial group\" (disparate impact). 25 Related to the first component, \"disparate treatment\", studies have shown that racial discrimination limits people of color's access to educational and employment opportunities resulting in social and economic consequences that could lead to food insecurity. 26,27 The later component, \"disparate impact\", also has implications for creating racial/ethnic disparities in food insecurity. For example, African Americans are incarcerated in state prisons at a rate that is five times that of whites. 28 Consequently, policies that restrict employment for individuals who were previously incarcerated could disadvantage people of color, contributing to food insecurity. Burke et al., 2016 examined the association between severity of food insecurity and lifetime racial discrimination in a sample of African-American food-insecure households with children in South Carolina. 29 A one-unit increase in the frequency of lifetime racial discrimination was associated with a 5% increase in the odds of being very low food secure, even after adjusting for socioeconomic and demographic confounders. Structural racism refers to \"the totality of ways, in which societies foster racial discrimination, via mutually reinforcing inequitable systems (e.g. housing, employment, earnings, benefits, credit, media, health care, criminal justice, etc.)\" 30 Beyond interpersonal discrimination, structural racism acknowledges the influence of historic and contemporary policies and practices that are deeply rooted within systems. 21,22,30 While it is important that solutions to address disparities in food insecurity focus on combating interpersonal discrimination, the need to target structural racism is critical in the fight for achieving equity in food security and improve related outcomes in people of color.",
        "reasoning": "The section provides explicit definitions for \"racial discrimination\" and \"structural racism\". It uses phrases like \"is the unequal treatment of persons or groups on the basis of their race or ethnicity\" and \"refers to 'the totality of ways, in which societies foster racial discrimination...'\" to introduce these definitions.",
        "is_definition": true
      },
      {
        "augmented": true,
        "section": "",
        "reasoning": "The section is empty, so it cannot contain a definition.",
        "is_definition": false
      },
      {
        "paper_id": "daw084",
        "sections": [
          "Given the central role of nutrition in health and chronic disease prevention, shaping dietary patterns is of particular importance for public health (Nishida et al., 2004). Improving dietary habits of the population is a societal and multifaceted task, which demands an understanding of the social context, but also food related skills and abilities of individuals. In this regard, nutritional science and education researchers are currently discussing the concepts of nutrition literacy and food literacy. Today, researchers use one or the other term to describe the areas of competence upon which healthy dietary behaviour depends; i.e., 'nutrition literacy' (Spronk et al., 2014), or 'food literacy' (Brooks and Begley, 2014;Vaitkeviciute et al., 2015). So far, the terms are indistinct and each is defined variously and sometimes inconsistently (Vaitkeviciute et al., 2015;Vidgen and Gallegos, 2014).Thus, it is hard to extract specific outcomes of health-promoting activities or interventions from the literature on either nutrition literacy or food literacy, or to choose appropriate and scientifically sounds tools for measuring those outcomes. In order to achieve conceptual clarity, this paper aims to identify the constituent elements of nutrition literacy and food literacy. This work will also help us identify important competencies covered by neither concept. To create a structured overview of the definitions and competencies that nutrition and food literacy entail, Velardo (2015) recommends using the already established, and closely related, the concept of health literacy by Nutbeam. Nutbeam's multicomponent concept of health literacy has gained increasing interest in health promotion. Health literacy encompasses several skills and competencies needed to make good decisions about health. The Nutbeam's concept has been applied in different settings (Nutbeam, 2000(Nutbeam, , 2008)), including the realms of diet, health, and nutrition education (St Leger, 2001;Thomson and Hoffman-Goetz, 2012). The concept describes three forms of health literacy: functional, interactive and critical. We base our work on the description of these forms by Smith et al. (2013): Functional health literacy includes the ability to obtain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information. People with better interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critically assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health. Improved critical health literacy increases the likelihood that a person will interpret and relate health information in their social context. Each form represents competencies that increase the awareness, motivation, and ability of individuals as they engage with individual, family, community, and society health issues (Nutbeam, 2000(Nutbeam, , 2008)). We created an analytical grid based on this model of functional, interactive, and critical health literacy to systematically review definitions of nutrition literacy and food literacy.",
          "",
          "A systematic search of the literature was performed by one researcher (CK) using the terms 'food literacy' and 'nutrition literacy'. Databases were searched from the earliest data of coverage (1974) to 31 December 2014. (Figure 1 illustrates the literature search and review process). We searched the following databases: Web of Science, PubMed, ScienceDirect, CINAHL (Ebsco), SocIndex (Ebsco) and ERIC (Ebsco). We identified additional publications (scientific reports, dissertations) by conducting a hand search of references in included publications. All references were saved in EndNote version X6. Duplicates, indices, tables of contents, and publications not written in English, French, or German (formal inclusion criteria) were removed. We used poster abstracts and conference proceedings published in peer-reviewed journals for forward search by author name, but they were not considered as full text publications. Backward search was undertaken on the reference lists of retrieved articles and books by screening for the terms nutrition or food literacy in titles. The full text of the resulting 173 publications was screened for the terms nutrition literacy and food literacy. Once those terms were identified in the text, we included only publications that explained or defined nutrition literacy or food literacy. The publications we finally included in the review provided original definitions of nutrition or food literacy.",
          "One researcher (CK) extracted, summarized, and tabulated the following key information from each publication that provided an explanation of nutrition or food literacy: author; publication year; explanation of the term nutrition or food literacy; and, cited references. Based on the summary table, two reviewers (KS, SB) independently reviewed each explanation the first author had identified and determined if they provided a concise definition, or a more comprehensive conceptual PubMed N= 25 (Nutriton/Food Literacy) (17/ 8) Web of Science N= 47 (Nutriton/Food Literacy) (29/18) Science Direct N= 120 (Nutriton/Food Literacy) (78/42) EBSCO (CINAHL, ERIC, SocIndex) N= 26 (Nutriton/Food Literacy) ( 19/7) N=218 Duplicates removed N= 55 Excluded due to formal criteria N=5 N= 146 Forward searching N=6 Screened by full text N=173 PublicaƟons providing no explanaƟon N=137 ExplanaƟon of nutriton literacy N=11 Abstracts, Conference Proceedings N=12 ExplanaƟon of food literacy N=25",
          "",
          "No original definƟon of nutriƟon literacy N=5",
          "",
          "No original definiƟon of food literacy N=12 framework. An exact statement or description of the nature, scope, or meaning of nutrition literacy or food literacy qualified as a definition. If a publication referred to an existing definition of nutrition literacy or food literacy, we included only the definition from the original source. We defined a conceptual framework as a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a definition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition literacy or food literacy, or described the consequences of acquiring these competencies, we considered the publication to have a conceptual framework. For our detailed analysis, we developed a matrix based on Nutbeam's forms of functional, interactive, and critical health literacy that included the skills and abilities named in Nutbeam's concept (see Introduction). Three authors (CK, KS, SB) independently assigned competencies specified in definitions and conceptual frameworks of nutrition literacy and food literacy to our analytical grid (see Appendix, Table A1). If definitions or conceptual frameworks referred directly to Nutbeam's forms of health literacy, we used the same assignment of competencies as the authors.",
          "We identified 19 original definitions of nutrition literacy or food literacy (see Figure 1). For a detailed overview on definitions and conceptual frameworks of nutrition literacy and food literacy see Appendix, Tables A2-A4.",
          "Six publications presented an original definition (see Appendix, Table A2), but none provided a conceptual framework for nutrition literacy. All definitions of nutrition literacy centered on an individual's cognitive capacities and strongly emphasized basic literacy and numeracy skills needed to understand and use information about nutrition. They argue that without these skills people cannot access and understand nutrition information and thus cannot build on nutritional knowledge, which is one of the keys to healthier eating practices. Only one definition (Guttersrud et al., 2014) introduced more skills, namely, the ability to search and apply nutrition information and the ability to communicate and act upon this information in the broader social environment to address nutritional barriers in personal, social, and global perspectives. Nutrition literacy was defined in the context of literacy surveys or studies (Blitstein and Evans, 2006;Watson et al., 2013;Zoellner et al., 2009) and research in nutrition education (Guttersrud et al., 2014;Neuhauser et al., 2007;Silk et al., 2008). Definitions of nutrition literacy were linked directly to existing definitions or concepts of health literacy. Nutrition literacy was understood as a 'specific form of health literacy' (Blitstein and Evans, 2006), 'similar to health literacy' (Silk et al., 2008), or 'health literacy applied to the field of nutrition' (Watson et al., 2013). Four of the six definitions of nutrition literacy (Blitstein and Evans, 2006;Neuhauser et al., 2007;Silk et al., 2008;Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term 'health' with 'nutrition'. They defined nutrition literacy as an individual's capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions. The remaining two publications (Guttersrud et al., 2014;Watson et al., 2013) referred to either Nutbeam's (2000) or Peerson and Saunders (2009) definition of health literacy.",
          "Using the analytical grid, we found all definitions of nutrition literacy contained elements of functional health literacy. However, only one definition (Guttersrud et al., 2014) described skills that could be assigned to interactive and critical literacy since this definition was based on Nutbeam's model of health literacy. Guttersrud et al. (2014) used the terms 'interactive' and 'critical nutrition literacy'. For a general overview, see Table 1.",
          "Definitions emphasized basic literacy and numeracy skills, including the ability to get and process nutrition information to improve decisions about nutrition. Only two definitions offered concrete examples of these skills; the ability to interpret front label packaging or menu labeling and the ability to understand basic nutrition concepts (Neuhauser et al., 2007;Watson et al., 2013) .",
          "'Interactive nutrition literacy' was described as 'cognitive and interpersonal communication skills' which are, for example, needed to interact with nutrition counsellors. Moreover, interactive nutrition literacy was described as an interest in searching for and applying nutrition information to improve personal nutritional status. We identified two main aspects of critical nutrition literacy in Guttersrud et al's. (2014) definition: the ability to evaluate the quality of nutrition information; and the willingness to take action to improve nutritional health in families, communities, or broader social and global movements.",
          "Thirteen publications introduced original definitions of food literacy. For a detailed overview, see Tables A3 and A4 in the Appendix. Six of these were conventional, but seven were integrated into a more comprehensive conceptual framework (Figure 1). In contrast to definitions of nutrition literacy, definitions of food literacy focused not only on the ability to obtain, process, and understand basic information on food and nutrition, but named also the competence to apply this information. They highlighted skills in preparing food, emphasized the abilities and skills people need to make healthy food choices (Fordyce-Voorham, 2011) and to understand the effects of food choices on health, environment, and economy (Sustain, 2013;Thomas and Irwin, 2011). Definitions of food literacy were provided by publications on nutrition education projects or interventions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001;Sustain, 2013;Thomas and Irwin, 2011) and studies that explored the need for more nutrition education in schools (Fordyce-Voorham, 2011;Slater, 2013). In contrast to definitions of nutrition literacy, which all referred to health literacy, only three out of the six definitions of food literacy referred to health literacy. Two definitions (Government of South Australia, 2010 cited by Pendergast et al., 2011;Kolasa et al., 2001) were adapted from the U.S. Department of Health and Human Services definition of health literacy, by replacing 'health information' with 'food and nutrition information' and adding 'the competence to use this information'. Slater (2013) used Nutbeam's concept of health literacy, and described food literacy as a framework for a school food and nutrition curriculum. The remaining three definitions were not directly linked to health literacy by the authors.",
          "We identified seven conceptual frameworks of food literacy. For a detailed overview, see Table A4 in the Appendix. Core elements of all conceptual frameworks included practical knowledge and skills to regulate food intake, including skills for planning meals, selecting, and preparing food. Most authors also emphasized some knowledge about nutrition (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013), and the ability to understand and judge the impact of food and nutrition on personal and public health (Howard and Brichta, 2013;Schnoegl et al., 2006;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Most conceptual frameworks also highlighted the importance of attitudes, awareness, motivation, or concrete behaviour to act on knowledge and skills. Volitional and behavioural factors were either directly mentioned in the definitions (Block et al., 2011;Howard and Brichta, 2013;Topley, 2013;Vidgen and Gallegos, 2014), or were described as important components or educational goals (Desjardins and Azevedo, 2013;Schnoegl et al., 2006). The emphasis on food appreciation, and on feeling motivated to prepare healthy food (Desjardins and Azevedo, 2013;Schnoegl et al., 2006), showed that cooking and eating were seen as enriching daily life (Schnoegl et al., 2006;Topley, 2013) as well as increasing satisfaction, confidence, or resilience (Desjardins and Azevedo, 2013;Topley, 2013). Only Smith (2009a) focused mainly on improving students' abilities and did not explicitly mentioned concrete behaviour. All of the conceptual frameworks presented food literacy as an important factor in making healthy food choices, and a powerful resource for improving individual and public health. Food literacy could create a pleasant and positive relationship with food (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014). Food literacy may also encourage more self-determination, strengthen personal and public health and well-being, and reduce health costs (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). However, Vidgen and Gallegos (2014) noted that the link between food literacy and healthy nutrition is indirect. For them, food security and the ability to prepare food enhance choice and pleasure, which, in turn, can stimulate healthy eating behaviour. Several authors saw food literacy as an important factor in a more equal (Schnoegl et al., 2006;Smith, 2009a) and sustainable society (Smith, 2009a;Topley, 2013). Food literacy was described as a dynamic process (Vidgen and Gallegos, 2014), developed over a life course (Block et al., 2011;Howard and Brichta, 2013;Schnoegl et al., 2006). All but one conceptual framework (Smith, 2009a) highlighted contextual factors that influence the development or application of food literacy skills. The authors focused especially on social and cultural context, environmental, and legal factors (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Specific population groups, such as those with low numeracy skills, children, seniors, indigenous peoples, immigrants, and those of lower socioeconomic status, might have fewer food literacy skills (Howard and Brichta, 2013). Vidgen and Gallegos (2014) pointed out that food literacy skills are developed in context, and the constitution and meaning of these abilities may vary across individuals and cultures. Homeless or socioeconomically deprived people must plan and managing their food intake differently than financially secure people. The authors pointed out that food literacy is only one factor in household decision making, and should be seen in the broader context of food availability, policy, socialization, and marketing strategies (Block et al., 2011;Schnoegl et al., 2006). Conceptual frameworks we identified were developed in the context of discussions or exploratory studies that focused on practical aspects of food literacy (Block et al., 2011;Desjardins and Azevedo, 2013;Vidgen and Gallegos, 2014), projects that reviewed current food programs and food literacy status (Howard and Brichta, 2013;Topley, 2013), and efforts to promote or implement food literacy in populations (Schnoegl et al., 2006;Smith, 2009a). The only group who did not link its conceptual framework of food literacy to health literacy was Schnoegl et al. (2006). Block et al. (2011) and Smith (2009a) directly built their conceptual frameworks on existing frameworks for health literacy. Others understood food literacy as a subset of health literacy (Howard and Brichta, 2013), or as a concept that emerged from it (Desjardins and Azevedo, 2013;Topley, 2013), or recognized that food literacy was consistent with health literacy (Vidgen and Gallegos, 2014). Assigning skills and abilities of food literacy to functional, interactive, and critical health literacy All definitions of food literacy, and every conceptual framework we identified described skills and abilities of functional health literacy. One definition (Slater, 2013) and four conceptual frameworks (Desjardins and Azevedo, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014) considered competencies that are related to the skills covered by interactive health literacy. Abilities that demand critical evaluation and understanding were mentioned in all conceptual frameworks but one definition (Slater, 2013) of food literacy. For a general overview, see Table 2.",
          "Like definitions of nutrition literacy, definitions of food literacy highlighted skills needed to obtain and understand information about food and nutrition. However, general numeracy and literacy skills were only mentioned once. Only Desjardins and Azevedo (2013) mentioned the ability to access information. All conceptual frameworks, and two definitions of food literacy (Sustain, 2013;Thomas and Irwin, 2011), put emphasis on increasing knowledge about nutrition and food. Food literacy frameworks gave a detailed description of these areas of knowledge. In total, we identified five major topics. First, all conceptual frameworks emphasized procedural or practical knowledge necessary to making informed decisions and preparing food as a key element of food literacy. All frameworks and two definitions (Sustain, 2013;Thomas and Irwin, 2011) named the basic cooking skills required to prepare a fresh meal. Among other skills they named planning and budgeting for food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Schnoegl et al., 2006;Vidgen and Gallegos, 2014), and general shopping skills (Block et al., 2011;Howard and Brichta, 2013;Thomas and Irwin, 2011), including the ability to choose highquality food (Schnoegl et al., 2006). They also listed respect for basic hygiene rules when storing and preparing food (Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Sustain, 2013;Vidgen and Gallegos, 2014). Second, all conceptual frameworks and one definition included (Sustain, 2013) knowledge about the origin of food, because the food system is increasingly complex. Knowing and understanding the steps along the food chain (production, processing, transport, purchase, and disposal) was understood to be important. Third, all conceptual frameworks included as components of food literacy the ability to interpret nutritional facts, read food labels, judge the size of plates (Block et al., 2011;Desjardins and Azevedo, 2013;Howard and Brichta, 2013;Smith, 2009a), as well as having a general understanding of food composition (Block et al., 2011;Schnoegl et al., 2006;Vidgen and Gallegos, 2014). Fourth, five conceptual frameworks and one definition of food literacy included an understanding of the effect of food choice on health and well-being. Food literacy includes knowing which foods should be included in the daily diet for good health (EU 2006, Vidgen and Gallegos 2014, Smith 2009), and a general understanding of the effect of nutrition on one's personal health (Howard and Brichta, 2013;Sustain, 2013;Topley, 2013;Vidgen and Gallegos, 2014). Fifth, three conceptual frameworks included culinary history and an understanding of the influence of social, cultural, historic, and religious factors on food choice and eating habits (Schnoegl et al., 2006;Smith, 2009a;Topley, 2013).",
          "Five publications of food literacy included skills and abilities assigned to interactive health literacy. Two of them used the term 'interactive food literacy', and directly referred to Nutbeam's concept of health literacy. Slater's definition of 'interactive food literacy' is based on the presumption that knowledge about food and nutrition builds personal skills like decision-making and goal-setting, which then improve nutritional health and well-being (Slater, 2013). Smith (2009a) conceptual framework differentiates between several types of food literacy that have interactive elements, highlighting the following competencies: sharing life experience; empathizing with others ('lifeworld food literacy'); cooperative learning ('interactive/interpretive food literacy'); and, using storytelling and narratives to explore the meanings of food ('narrative food literacy'). We assigned three more aspects of food literacy ['join in and eat in a social way', (Vidgen and Gallegos, 2014), the ability 'to share information and transfer skills' (Desjardins and Azevedo, 2013) and 'creating community' (Topley, 2013)], to interactive health literacy.",
          "Two definitions and seven conceptual frameworks of food literacy described elements of the dimension of critical health literacy. We identified the following three areas: (i) The ability to judge the quality of nutrition information; (ii) the ability to critically reflect on factors that influence dietary behaviour; and, (iii) the ability to recognize the effect of food and nutrition decisions on society. First, people need sufficient knowledge and skills to judge or evaluate information about nutrition and food (Guttersrud et al., 2014;Slater, 2013;Smith, 2009a;Topley, 2013;Vidgen and Gallegos, 2014). Specifically, they need the ability to interpret claims made in food marketing, advertising and in the media (Howard and Brichta, 2013;Schnoegl et al., 2006), and to critically question advice especially the ability to judge the statements made by nutrition experts (Schnoegl et al., 2006). Second, food literacy frameworks mentioned critical reflection on factors that influence dietary behaviour. The authors described food choices and dietary behaviour as situational and influenced by various factors, so a food literate person must be able to understand and reflect on the effect of social, cultural, historic and religious factors on eating habits (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Topley, 2013). The authors also mentioned the need to recognize that situational factors, like the smell of food or the company of others, influence food choice (Desjardins and Azevedo, 2013;Smith, 2009a). Third, food literacy demands that people recognize the effect of their personal food and nutrition decisions on society. Publications that address these competencies described the complex economic and social effects of individual food choice. Food literacy was seen as 'contributing toward the sustainable, democratic development of citizenship' (Schnoegl et al., 2006). Food literacy enables an in-depth understanding of the effect of an individual's food choice on the environment and local communities, and helps people understand the ways their decisions about food affect social development (Schnoegl et al., 2006;Slater, 2013;Smith, 2009a;Sustain, 2013;Topley, 2013). Smith (2009a) named 'examining the macro-food environment' as an important topic that should be taught in home economics classes, since it develops critical thinking skills and abilities that enable people to select food that supports the welfare and fair treatment of others, and that are sustainable. Slater (2013) also mentioned the will to advocate to improve nutritional health in families, communities, and broader social and global movements as part of the food literacy definition.",
          "This review paper is to our knowledge the first to examine systematically the differences and constituents of nutrition literacy and food literacy. Nutrition literacy and food literacy have coexisted in the literature while the borders between them were unclear. As a result, it has been difficult to measure the effects and comparing the efficacy of interventions focusing on nutrition literacy or food literacy.We thus tried to clarify the current uncertainties in the distinction between these terms and to examine the relationship between nutrition, food and health literacy. Based on the results, we suggest to conceptualize nutrition literacy as a subset of food literacy and that both (nutrition literacy and food literacy) can be fruitfully framed as specific forms of the broader concept of health literacy. Our analysis showed that nutrition literacy and food literacy are distinct but complementary concepts. The most obvious difference between nutrition literacy and food literacy is in the scope of skills and abilities they include. All but one definition of nutrition literacy (Guttersrud et al., 2014) exclusively described basic literacy skills necessary to understanding and obtaining information about nutrition. We could not describe in detail nutrition literacy skills or the factors that influence their development because we could not identify a conceptual framework for nutrition literacy. Food literacy, however, described a wide range of skills and was elaborated in more detail. It was the more commonly used term for discussing concrete applications, and better describes the range of different skills it encompasses. Research in the field of food literacy is ongoing and continues to add to the understanding of the concept (Cullen et al., 2015;Palumbo, 2016). Cullen et al. (2015) presented an integrated definition (see Appendix, Table A5) and framework for food literacy based on a review of food literacy definitions in grey and scientific literature. We and Cullen et al. (2015) identified a similar set of elements of food literacy. Our intent, however, was not to present another new framework. Instead, we offer a more detailed overview of the single skills and abilities that comprise nutrition literacy and food literacy in order to support health promotion researchers and practitioners in the design of study instruments and education programs. Our analytical grid enabled us, for example, to show that only four conceptual frameworks of food literacy included skills such as sharing information and interacting with others (Desjardins and Azevedo, 2013;Topley, 2013;Vidgen and Gallegos, 2014). The ability to exchange information on food and nutrition with family, peers, and experts or to extract information from different sources of communication grows in importance along with the amount of nutrition-related information from different sources. We recommend that future definitions and conceptual frameworks include more communicative or interactive skills. In summary, skills described in nutrition literacy might represent a prerequisite for competencies described in food literacy, but they do not cover the whole range of skills and competencies people need if they are to make healthy and responsible nutrition and food decisions. This interpretation is supported by Smith (2009b), who argued that food literacy is a more powerful concept than nutrition literacy for guiding nutrition education, since food literacy addresses 'skills that people really need' (Smith, 2009b). A further strength of food literacy is that it integrates volitional and behavioural factors, namely awareness, attitudes, and motivation. These are crucial factors in implementing knowledge and practical skills in everyday life and are thus particularly important for health promotion practice (Contento, 2008). Given the similarities between nutrition food literacy, health literacy, we observed that nutrition literacy and food literacy are forms of health literacy, rather than freestanding concepts. Most authors linked their definitions of nutrition literacy and food literacy, and their conceptual frameworks to health literacy. Every definition of nutrition literacy and half of the food literacy definitions were based on an existing definition of health literacy. In their conceptual frameworks, the authors described food literacy as either a subset of (Howard and Brichta, 2013), based on (Block et al., 2011), or having emerged from (Desjardins and Azevedo, 2013) or as linked to health literacy (Smith, 2009a;Topley, 2013). We also found that components of functional, interactive and critical health literacy are reflected in nutrition literacy and food literacy definitions. All publications listed skills that we identified as elements of functional health literacy. Either basic skills people need to get and understand nutrition information (nutrition literacy) or the importance of knowledge about different food and nutrition topics (food literacy) were named. Nutbeam considered knowledge as a secondary outcome, rather than a fixed component in functional health literacy (Nutbeam, 2000). However, Nutbeam's model was adapted in newer models of health literacy that integrate knowledge about health into health literacy (Paakkari and Paakkari, 2012;Schulz and Nakamoto, 2005). These newer models also distinguish between theoretical and practical knowledge as do conceptual frameworks of food literacy. Interactive skills were described less often than functional skills. Only six of 19 publications mentioned interactive skills. We recognized that authors mentioned different aspects of interactive literacy even when directly referring to Nutbeam's concept. Interactive nutrition literacy highlights communication and information-seeking skills (Guttersrud et al., 2014) while interactive food literacy highlights decisionmaking and goal-setting (Slater, 2013;Smith, 2009a). Finally, all conceptual frameworks showed elements of critical health literacy and highlighted the links between socially responsible eating and decisions about nutrition, and the need to understand the wider context of food production, and its impact on the environment and the economy. These authors reprise the debate over the meaning of health literacy, where social determinants of health and questions of empowerment are hotly debated. (Freedman et al., 2009;Nutbeam, 2000). Others have recently begun differentiate the forms of health literacy by discussing applications and contents in specific contexts, such as mental health literacy, cancer literacy, and e-health literacy (Diviani and Schulz, 2012;Massey et al., 2012;Velardo, 2015). Indeed, health literacy is a very broad concept, which must be concretely applied (operationalized) to promote health (Abel and Sommerhalder, 2015). Health literacy comprises different skills and abilities. In the specific context in which we discuss, someone with a basic understanding of nutrition information, who is nutrition literate, is not necessarily food literate. Likewise, a food literate person is not necessarily health literate in its broader definition. To advance the application of the concept of health literacy in nutritional interventions we suggest adopting food literacy as the single well defined term that encompasses the whole realm of competencies covered previously in two separate definitions. We argue that nutrition literacy should be folded into food literacy and that both can be seen as specific forms of health literacy. ",
          "Our study was strengthened by its systematic approach to literature search and analysis. Our backward and forward search on abstracts and reference helped us identify articles not listed in scientific databases. Five of the seven conceptual frameworks were drawn from grey literature sources. We may have missed other grey literature on nutrition literacy and food literacy because references to these publications are hard to retrieve, and also hard to access (Francois et al., 2014). Our study was also strengthened by our analytical grid, which we based on Nutbeam's widely accepted concept. Several authors of nutrition literacy and food literacy definitions and conceptual frameworks refereed to Nutbeam's model of functional, interactive and critical health literacy. His concept has been used as an analytical grid in several studies and is recommended to map different skills and abilities (Velardo, 2015). The grid allowed us to sort and analyse elements of nutrition literacy and food literacy definitions and conceptual frameworks. We could thus identify even rarely mentioned aspects of definitions, including interactive elements of nutrition literacy and food literacy. Although it is likely that another health literacy model that considers dimensions like cultural literacy (Zarcadoolas et al., 2005) or media literacy (Manganello, 2008) would make a difference in the number or kind of classifications for the components of nutrition literacy and food literacy, but we do not think it would have changed our conclusion that food literacy is the more comprehensive term.",
          "Regarding the major role of food in daily life and its importance in the development of chronic diseases, we believe that food literacy, as a specific form of health literacy can significantly contribute to guide future health promotion activities focusing on dietary behaviour. Our analysis suggests that more research on interactive skills is needed since they are so far underdiscussed in food literacy. Future research on food literacy should also explore the prominent role played by attitudes, motivation, and behaviour. The role of these factors is currently under debate in health literacy research and not all definitions of health literacy consider them to be integrated. Recently, Sorensen et al. (2012) presented an integrative model of health literacy that explicitly names as an important component the motivation to knowledge and competencies. We also identified this as an important component of food literacy. Since an understanding of the link or a possible pathway between different health literacy skills, motivational factors, and concrete health behaviour is still missing, we would encourage further research in this field. Moreover, quantitative data on food literacy is lacking and more empirical support is necessary to demonstrate that food literacy is an important prerequisite for health and well-being. There are a few instruments that measure nutrition literacy (Diamond, 2007;Gibbs and Chapman-Novakofski, 2013;Guttersrud et al., 2014), and fewer that assess food literacy (we found these latter only in the grey literature). Thus, we will need new instruments that measure all of the aspects of food literacy, and consider as well concepts like self-efficacy and attitudes towards healthy food.",
          "We offer conceptual clarification on the competing terms nutrition literacy and food literacy. We have shown that both nutrition literacy and food literacy are specific forms of health literacy. Our structured analysis of nutrition literacy and food literacy definitions shows that there is more than a subtle difference between them. Nutrition literacy focuses mainly on abilities to understand nutrition information, which can be seen as a prerequisite for a wider range of skills described under the term food literacy. Thus, nutrition literacy can be seen a subset of food literacy. We suggest using the term food literacy instead of nutrition literacy to describe the wide range of skills needed for a healthy and responsible nutrition behaviour. When measuring food literacy, we suggest the following core abilities and skills be taken into account: reading, understanding, and judging the quality of information; gathering and exchanging knowledge related to food and nutrition themes; practical skills like shopping and preparing food; and critically reflecting on factors that influence personal choices about food, and understanding the impact of those choices on society. (NRP69, grant number 406 940_145149), and by the Swiss Heart Foundation."
        ],
        "ground_truth_definitions": {
          "interactive health literacy": {
            "definition": "the abilities to act and interact successfully to improve health, and to utilize different forms of communication to obtain, provide, and apply relevant health information.",
            "context": "A secondary outcome of functional health literacy is that people know more about health issues. Interactive health literacy includes the abilities to act and interact successfully to improve health, and to util- ize different forms of communication to obtain, provide, and apply relevant health information. People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions.",
            "type": "implicit"
          },
          "functional health literacy": {
            "definition": "the ability to obtain, understand, and use factual health information.",
            "context": "We base our work on the de- scription of these forms by Smith et al. (2013): Functional health literacy includes the ability to ob- tain, understand, and use factual health information. A secondary outcome of functional health literacy is that people know more about health issues.",
            "type": "implicit"
          },
          "critical health literacy": {
            "definition": "the ability to critically assess and reflect on health information and advice.",
            "context": "People with bet- ter interactive health literacy skills are more likely to be proactive agents in everyday health-related actions. Critical health literacy includes the ability to critic- ally assess and reflect on health information and advice. This includes understanding and recognizing the wider social determinants of health.",
            "type": "implicit"
          },
          "conceptual framework": {
            "definition": "a theoretical structure that explained key factors, variables, ideas, and presumed relationships of the concept.",
            "context": "If a publication referred to an existing definition of nutrition literacy or food lit- eracy, we included only the definition from the original source. We defined a conceptual framework as a theor- etical structure that explained key factors, variables, ideas, and presumed relationships of the concept. (Miles and Hubermann, 1994). If publications contained a def- inition and a more detailed description of the associated competencies of nutrition or food literacy, and identified factors that influence the development of nutrition liter- acy or food literacy, or described the consequences of acquiring these competencies, we considered the publi- cation to have a conceptual framework.",
            "type": "explicit"
          },
          "nutrition literacy": {
            "definition": "individual’s capacity to obtain, process, and understand basic nutrition information necessary for making appropriate nutrition decisions.",
            "context": "Four of the six defin- itions of nutrition literacy (Blitstein and Evans, 2006; Neuhauser et al., 2007; Silk et al., 2008; Zoellner et al., 2009) adapted the U.S. Department of Health and Human Services definition of health literacy (National Research Council, 2004) by replacing the term ‘health’ with ‘nutrition’. They defined nutrition literacy as an in- dividual’s capacity to obtain, process, and understand basic nutrition information necessary for making appro- priate nutrition decisions. The remaining two publications (Guttersrud et al., 2014; Watson et al., 2013) referred to either Nutbeam’s (2000) or Peerson and Saunders (2009) definition of health literacy.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "09c660d438e8b372f3e77504c069255b85c00256",
        "sections": [
          "According to CLT, then, people traverse different psychological distances by using similar mental construal processes. Because the various distances have the same egocentric reference point, they should all be cognitively related to each other and similarly affect and be affected by level of construal. As psychological distance increases, construals would become more abstract, and as level of abstraction increases, so too would the psychological distances people envisage. Construal levels thus expand and contract one's mental horizon. The different distances should also similarly influence prediction, evaluation, and action, inasmuch as these outcomes are mediated by construal. The present article builds upon our earlier work on temporal construal theory, which focused in particular on the way that temporal distance from future events influences representation and judgment (Liberman & Trope, 1998;Trope & Liberman, 2003). Going beyond this earlier theory, we now treat temporal construal theory as a special case of a general theory of psychological distance. At the core of the proposed theory is a functional approach to construal levels, according to which mental construal processes serve to traverse psychological distances and switch between proximal and distal perspectives on objects. We describe the properties of the mental construal processes that enable them to fulfill this function and further explicate our approach by relating it to extant theories of how people respond to distant objects. Some of these ideas were presented in earlier literature reviews and book chapters (Liberman & Trope, 2008;Liberman, Trope, & Stephan, 2007;Trope, Liberman, & Wakslak, 2007). The present article presents a more advanced and comprehensive formulation of the theory and examination of related theories and research. The article consists of three main parts. In the first part, we present the basic assumptions of CLT. We explain what we mean by construal levels and why they are related to psychological distance (Section I) and examine the cognitive relationships among the four dimensions of psychological distance (Section II) and their bidirectional relationship to level of construal (Section III). In the second part, we turn to the construal-mediated consequences of psychological distance for prediction, preference, and self-regulation (Sections IV-VI). In the third part, we address open questions about psychological distance and discuss new directions for future research (Sections VII-IX).",
          "In the following three sections, we examine the basic assumptions of CLT regarding psychological distance, construal level, and the relationship between the two.",
          "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental. For example, by moving from representing an object as a \"cellular phone\" to representing it as \"a communication device,\" we omit information about size; moving from representing an activity as \"playing ball\" to representing it as \"having fun,\" we omit the ball. Concrete representations typically lend themselves to multiple abstractions. For example, a cellular phone could be construed also as a \"small object,\" and \"playing ball\" could be construed as \"exercising.\" An abstract representation is selected according to its relevance to one's goals. Thus, if one's goal is to contact a friend, then \"a communication device\" is relevant, but size is not. From the perspective of that goal, the cell phone could be replaced by (i.e., would be conceptually close to) a desktop computer with an Internet connection. If, however, one's goal is to pickpocket a valuable object, then size is a relevant attribute, and function is of less relevance. Given this goal, the cell phone could be replaced by (i.e., would be conceptually close to) a wallet. Like irrelevant details, details that are inconsistent with the chosen abstract representation are omitted from the representation or assimilated into it. For example, the detail that the ball game is slow would be omitted or modified once the activity is represented as \"exercising.\" Because abstract representations necessarily impose one of many alternative interpretations, and because irrelevant or inconsistent details are omitted or assimilated to it, these representations tend to be simpler, less ambiguous, more coherent, more schematic, and more prototypical than concrete representations (Fiske & Taylor, 1991, p. 98; E. R. Smith, 1998). It is important to note, however, that higher level construals are not simply more impoverished or vague than lower level construals. They often convey additional information about the value of the stimulus and its relations to other stimuli. For example, \"having fun\" entails many characteristics that are not an obvious part of \"playing ball outside\" (e.g., valence) and places the activity within a broader context by specifying its relations to other concepts (e.g., a party). Thus, the process of abstraction involves not only a loss of specific, idiosyncratic, and incidental information, but also ascription of new meaning deduced from stored knowledge and organized in structured representations. There are multiple levels of abstractness, as one could construct increasingly more inclusive categories of objects (e.g., poodle, dog, a mammal). Actions form hierarchies, too, as goals could be translated into more abstract, superordinate goals (Carver & Scheier, 2000;Miller, Galanter, & Pribram, 1960;Vallacher & Wegner, 1987). In such hierarchies, each action (e.g., study for an exam) has a superordinate, abstract level, which answers the question of why the action is performed (e.g., do well) and a subordinate, concrete level, which provides the details of how the action is to be performed (e.g., read a textbook). Higher levels of abstractness contain less concrete details about the specific type of action performed, the objects it involves, and its immediate context and more information about the general meaning and the valence of the action (Semin & Fiedler, 1988;Trope, 1986Trope, , 1989)). On the basis of this conceptualization, we posit two related criteria that can be used to distinguish which features of an item or an event are more high level and which are more low level (i.e., which features will be increasingly captured at a higher vs. lower level of construal). The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is. The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. The topic of the lecture, on the other hand, would be important regardless of the location's convenience. In this sense, details about location are subordinated to details about topic, and thus constitute a lower level of construal. Construal and distance-CLT contends that people use increasingly higher levels of construal to represent an object as the psychological distance from the object increases. This is because high-level construals are more likely than low-level construals to remain unchanged as one gets closer to an object or farther away from it. For example, the higher level goal to contact a friend is more stable over time than the more concrete goal to send her an e-mail, because an Internet connection might be unavailable when one is actually trying to contact the friend. From a temporally distant perspective, it is therefore more useful to construe this action in terms of the high-level goal rather than the low-level goal. The same holds true for the other distances. Thus, abstract categories tend to change less across social distance. For example, more people use communication devices than cell phones, and therefore, the former construal is more useful for relating to socially distant individuals. It is critical that, although we believe that these functional relationships underlie the association between construal and distance, we propose that the effects of distance on construal and of construal on distance are overgeneralized, making them persist even when the initial reasons that gave rise to the association are no longer present. That is, a large distance from an object would activate a high-level construal of the object even when lowlevel details are not particularly likely to change over distance, and high level of construal of an object would produce a sense of distance from the object even when such construal does not afford retrieval of particularly distal exemplars of the object. For example, students may know the room in which a guest lecture will take place well in advance and be quite certain that it will not change. Nevertheless, CLT predicts that these students will ignore the quality of the lecture room (a low-level detail) when it is temporally distant, when it takes place in a remote location, when it is planned for another person, and when it is unlikely to take place. Finally, it is worth emphasizing that although psychological distance and construal levels are related, they are not the same. Psychological distance refers to the perception of when an event occurs, where it occurs, to whom it occurs, and whether it occurs. Construal levels refer to the perception of what will occur: the processes that give rise to the representation of the event itself. Thus, psychological distance from an event should be more closely related to the spatiotemporal distance of the event from the self than to its inherent properties, whereas the construal of the event should be more closely related to its inherent properties than to its spatiotemporal distance from the self. Below, we develop more fully the assumption that the different psychological distances are interrelated (Section II) and that each distance affects and is affected by level of construal (Section III).",
          "We contend that the propensity to complete the sentence \"a long time ago, in a ____ place\" with \"far away\" rather than with \"nearby\" reflects not only a literary convention but also an automatic tendency of the mind. Indeed, people use spatial metaphors to represent time in everyday language and reasoning (Boroditsky, 2007). In social psychology, spatial distance is often used to measure social distance. For example, choosing a more distant seat from another person is taken to reflect social distancing from that person (e.g., Macrae, Bodenhausen, Milne, & Jetten, 1994;Mooney, Cohn, & Swift, 1992). More generally, if a common dimension of psychological distance underlies the various distance dimensions we have enumerated, then these distance dimensions should be mentally associated. For example, remote locations should bring to mind the distant rather than the near future, other people rather than oneself, and unlikely rather than likely events. In an investigation of this hypothesis, Bar-Anan, Liberman, Trope, and Algom (2007) used a picture-word Stroop task (Stroop, 1935) to examine the cognitive interrelations among psychological distances. Participants viewed landscape photographs containing an arrow that was pointing to either a proximal or a distal point in the landscape (see Figure 1). Each arrow contained a word denoting either psychological proximity (e.g., \"tomorrow,\" \"we,\" \"sure\") or psychological distance (e.g., \"year,\" \"others,\" \"maybe\"). Participants' task was to respond by pressing one of two keys as quickly and as accurately as possible. In one version of the task, they had to indicate whether the arrow pointed to a proximal or distal location. In another version, they had to identify the word printed in the arrow. In both versions, participants responded faster to distance-congruent stimuli (in which a spatially distant arrow contained a word that denoted temporal distance, social distance, or low likelihood, or a spatially proximal arrow contained a word that denoted temporal proximity, social proximity, or high likelihood) than to distance-incongruent stimuli (in which a spatially distal arrow contained a word denoting proximity, or a spatially proximal arrow contained a word denoting distance). These findings suggest that spatial distance, temporal distance, social distance, and hypotheticality have a common meaning and that people access this common meaning automatically, even when it is not directly related to their current goals. We think that the common meaning is psychological distance and that it is automatically assessed because of its important implications for the perceiver. For example, on a rainy day, it matters whether an umbrella one notices belongs to a friend or to a stranger (social distance); in the jungle, it is important whether a tiger is real or imaginary (hypotheticality); in making a financial investment, it is important whether a recession is anticipated in the near or distant future (temporal distance), here or somewhere else (spatial distance). Yet, unlike valence, distance is not an inherent aspect of the semantic meaning of objects. Umbrellas, tigers, and recessions are inherently good or bad, but they are not inherently proximal or distal. It is perhaps for this reason that distance has not been included among the basic dimensions of meaning: evaluation, potency, and activity (Osgood & Suci, 1955). The idea that distance dimensions are automatically associated further suggests that the distance of a stimulus on one dimension may affect its perceived distance on other dimensions. In this spirit, research by Stephan, Liberman, and Trope (2010) has investigated how social distance affects and is affected by spatial distance and temporal distance. This line of research was based on past work showing that polite language signifies and creates interpersonal distance: People address strangers more politely than they address friends, and the use of polite, formal language creates a sense of distance (Brown & Levinson, 1987). Consistent with this analysis, Stephan et al. found that the use of normative, polite language rather than colloquial, less polite language led participants to believe that the target of the communication was spatially and temporally more distant. For example, using normative rather than colloquial language to address a person (e.g., \"My brother is taking our family car, so the rest of us will stay at home\" vs. \"… will be stuck at home\") led participants to infer that the addressee was in a more remote location and that the conversation referred to an event in the more distant future. Another set of studies by Stephan et al. (2010) found evidence for the reverse direction of influence, namely, an effect of spatial and temporal distance from the target of communication on the use of polite language. In one study, participants wrote instructions for a person who was expected to read them either in the near future or the distant future (e.g., participants wrote sightseeing suggestions for a tourist that was supposed to arrive on the following day or a year later). The results showed that participants preferred to phrase instructions more politely when they were addressed to the distant future than to near future tourist. Similar findings were found with addressing a spatially proximal versus distal stranger. Williams and Bargh (2008, Study 4) showed, in a similar vein, that participants who were primed with spatial distance (relative to proximity) by marking close (vs. distant) points on a Cartesian plan subsequently reported greater social distances between themselves and members of their family and their hometown. Finally, Wakslak and Trope (2008) showed that hypotheticality also affects the perception of other distances, such that people expect unlikely events (which, in terms of CLT are distant on the dimension of hypotheticality), compared with likely events, to occur in situations that are relatively more distant in time, space, and social distance. For example, a rare cat blood type was expected to be found in cats in spatially remote rather than nearby places, whereas a common cat blood type was expected in a near rather than a remote location. Presumably, people map probability onto other distance dimensions and therefore expect improbable events to happen to distant others in distant places and times. Discussion-Recent research shows that the different psychological distance dimensions are associated and suggests that psychological distance is an aspect of meaning, common to spatial distance, temporal distance, social distance, and hypotheticality. This research further shows that cues of distance on one dimension affect the perceived distance of objects on other dimensions. For example, the spatial distance from an event may depend not only on its location relative to that of the perceiver but also on whether it is expected in the near or distant future, whether it occurred recently or a long time ago, whether it is expected to be experienced by oneself or another person, and whether it is probable or improbable. In this respect, the different psychological distances may be to some extent interchangeable (see Pronin, Olivola, & Kennedy, 2008). Our research has only started to explore the relationships among the various distance dimensions. Below we consider further questions about more complex relationships among the psychological distances than those we have identified thus far. The psychophysics of distance: How do objective distances in time, space, social target, and hypotheticality map onto generalized psychological distance? It seems reasonable to expect a relatively high sensitivity to changes in objective distance from the self here and now at the proximal end of the continuum. As distance increases, changes in objective distance might produce smaller corresponding changes in psychological distance. As vividly illustrated by Steinberg's painting View of the World from the 9th Avenue, the distance between 9th Avenue and 10th Avenue in Manhattan seems to Steinberg greater than the distance between New York City and Chicago. In other words, consistent with the Weber-Fechner law, psychological distance from an object might follow a concave, logarithmic function rather than a linear function. Recent research by Zauberman, Kim, Malkoc, and Bettman (2009) provides evidence showing that such a function fits subjective estimates of temporal distance from future events. Likewise, research on estimates of spatial distance uncovered increased sensitivity around an egocentric reference point (e.g., Holyoak & Mah, 1982). Investigating the functions that relate temporal distance, prospective and retrospective, as well as other distances, to psychological distance remains an important direction for future research. Another important but unexplored question is how different distances combine to affect psychological distance. One interesting possibility is that the Weber-Fechner law may apply across different psychological distance dimensions. That is, changes in distance of an object from the self on one dimension would have a greater impact on psychological distance when that object is proximal on another dimension than when it is distant on that other dimension. For example, the same changes in spatial distance of an object from the self might have a stronger impact on the psychological distance of that object when the changes are expected in the near future rather than in the distant future. To use Steinberg's drawing, this artist might think of a meeting as coming up sooner if he plans to have the meeting in Manhattan than in Chicago.",
          "According to CLT, different objects are mentally associated to the extent that they are psychologically equidistant from the self. In principle, the associations among proximal objects could be as strong as those among distal objects. However, because psychological distance is egocentric, anchored at the self here and now, objects at the proximal end may have more in common than those that are removed from that end. There is only one me and many others, and therefore, the things that are me and mine are more similar to each other than the things that are not me and mine. Only one place in space is \"here,\" but there are many places that are far away, and therefore, things that are here are more related to each other than things that are far away. Similarly, there is likely less variation among real events than possible events. The greater commonality among proximal objects than distal objects might give rise to stronger interrelationships among the former than the latter.",
          "Although we suggest that different types of distance are related, we do not wish to suggest that they are the same. It is possible that some distance dimensions are more basic or influential than others. For example, Boroditsky has recently proposed that spatial distance is primary and that temporal distance is understood as an analogy to it (Boroditsky, 2000(Boroditsky, , 2001;;Boroditsky & Ramscar, 2002). In support of this proposal, she showed that thinking about space before answering questions about time influenced the participants' responses, but thinking about time before answering questions about space did not affect participants' responses (Boroditsky, 2000, Experiments 2 and 3). In another series of studies, she presented participants with lines on a computer screen and showed that judgments of exposure duration were affected by the lines' length whereas judged length was not affected by duration of exposure (Casasanto & Boroditsky, 2008). Spatial distance may be more basic than temporal distance or other distances in that it is learned earlier, more clearly detected, less ambiguous, or easier to communicate about. It is possible, however, that the distinction between self and not-self, which underlies social distance and perspective taking (e.g., Pronin, 2008), might be a core psychological distance that exerts a particularly strong influence on the activation and perception of other distances. Perhaps hypotheticality, the distinction between real and imagined objects and between probable and improbable events, is least prominent and acquired at an older age, compared to other distance dimensions. The various distance dimensions may differ in other respects. Time is unidimensional and uncontrollable. We incessantly travel from the past to the future and have no control over time. Spatial distance has three dimensions, is relatively stable, and can be controlled by moving closer or farther away from things at wish. Social distance is only partly controllable (e.g., we can try to get closer to a person but success is not entirely dependent on our actions). Hypotheticality is also only partly controllable, because we cannot always change the probabilities of events. Another important difference among the distances is their relation to valence. Whereas social distance decreases positivity (e.g., in groups are perceived as more positive than out groups), temporal distance typically increases positivity (people are more positive about the more distant future). Exploring the psychological consequences of these differences in controllability, dimensionality, and valence is a fruitful direction for future research.",
          "The basic premise of CLT is that distance is linked to level of mental construal, such that more distant objects will be construed at a higher level, and high-level construal will bring to mind more distant objects. It may seem intuitive that from a distance we see the forest, and as we get closer, we see the trees. It may also seem intuitive that, to see the forest rather than individual trees, we need to step back. These effects should apply, however, not only to spatial distance but also to other distances, and not only to visual input, where it might seem a natural constraint of our perceptual system, but also to conceptual abstraction. We do not literally see either tomorrow or next year. Yet we may think about tomorrow in terms of trees and about next year in terms of the forest. We do not literally take a step back to forget the daily hassles and consider our life in general. Yet, when thinking of the general aspects of our life rather than about daily details, we may find ourselves looking farther into space. Evidence for these associations between distance and construal has been found at the level of both implicit associations and explicit judgments and decisions. Bar-Anan, Liberman, and Trope (2006), for example, examined the associations between level of construal and psychological distance using an Implicit Association Test. Similar to other assessments of implicit associations using this test (see Greenwald, McGhee, & Schwartz, 1998), participants in these studies were presented with stimuli from four categories: stimuli pertaining to high-level construal (e.g., category names such as \"drinks\"), stimuli pertaining to low-level construal (e.g., exemplar names such as \"coke\"), stimuli pertaining to low psychological distance (e.g., the word \"ours\" or the word \"friend\" for the social distance), and stimuli pertaining to high psychological distance (e.g., the word \"theirs\" or the word \"stranger\"). In CLT-congruent trials, high-level stimuli were paired with distant stimuli and low-level stimuli were paired with proximal stimuli, whereas on CLT-incongruent trials, high-level stimuli were paired with proximal stimuli and low-level stimuli were paired with distal stimuli. With all four dimensions of psychological distance-temporal distance, spatial distance, social distance, and hypotheticality-the participants were faster with congruent than with incongruent pairings, indicating that participants implicitly associate psychological distance with high-level construal and psychological proximity with low-level construal. This set of studies suggests that the association between psychological distance and construal level can be activated automatically without conscious deliberation. Beyond mere association, the reciprocal effects of distance on construal and of construal on distance have important implications for perception, categorization, action identification, and person perception. In the following, we discuss those implications. Visual Perception-The distinction between the (high-level) gestalt and (low-level) constituent details, such as the proverbial distinction between the trees and the forest, readily applies to visual perception. In a widely used task (Navon, 1977), participants are presented with global letters that are made of local letters (e.g., a large L made of 20 small Hs), and are asked to indicate whether a target letter appears on the screen (e.g., \"Is there an H on the screen?\"). The target letter can match a global letter or a local letter. Would psychological distance facilitate attending to global letters (e.g., the \"forest\") and impair attending to local letters (e.g., the \"trees\")? Would attending to global letters, as opposed to local letters, enhance perceived distance? Liberman and Förster's (2009 Liberman and Förster's (in press) studies suggest affirmative answers to both questions. In one of the studies (Liberman & Förster, in press), they primed participants with temporal distance or temporal proximity by making them write essays about their lives tomorrow or their lives on a day a year later and imagine taking an attention test on that day. Navon's (1977) task was then presented. Relative to a control group, in which participants did not write an essay, temporal distance facilitated processing of global letters and impaired processing of local letters, whereas temporal proximity produced the opposite effect. Similar effects were found for priming of spatial distance and social distance. To examine the reverse direction of influence, namely, that of global perception on estimated psychological distance, Liberman and Förster (2009) procedurally primed participants with either global or local perceptual processing, using a variation of Navon's (1977) task. In the global priming condition, the target letters were always global, and in the local priming condition, the target letters were always local, whereas in the control condition, the targets were global in half of the trials and local in the other half. Relative to the control condition, global processing led to greater estimates of temporal distance, spatial distance, social distance, and hypotheticality. Local processing had the opposite effect. For example, participants who were primed with global processing estimated temporal distance to a dental visit as longer and spatial distance between themselves and a designated point in the room as larger than participants primed with local processing. In a related vein, Wakslak and Trope (2009) found that priming global processing (vs. local processing) through the Navon task led participants to assign lower probability to a variety of everyday life occurrences. It is important to note that, given that psychological distance is egocentric, as CLT contends, if the distance between two objects is not mentally represented as social distance from oneself, temporal distance from now, or spatial distance from here, it should not necessarily depend on level of construal. Consistent with this prediction, in Liberman and Förster's (2009) studies, primed level of construal affected estimated spatial distance between the participant and a sticker in the room, but it did not affect the estimated spatial distance between the experimenter and a marked desk in the room. Likewise, level of construal did not affect estimates of temporal distance that were not anchored on now (e.g., \"How much time after receiving an invitation would you go to the dentist?\"). High-level construal of visual information often entails abstraction of coherent images from fragmented visual input. An example is the Gestalt Completion Task (see Figure 2; Street, 1931; see also Ekstrom, French, Harman, & Dermen, 1976), where performance depends on detecting the global pattern and where attending to details interferes with performance. In a series of studies, participants completed what they believed to be sample items of the Gestalt Completion Task, supposedly as a practice version before they perform the actual task. Participants' performance improved when they anticipated working on the actual task in the more distant future (Förster, Friedman, & Liberman, 2004), when they thought the actual task was less likely to take place (Wakslak, Trope, Liberman, & Alony, 2006), and when social distance was enhanced by priming of high social status (P. K. Smith & Trope, 2006). A psychologically distant perspective thus seems to enable people to better see the gestalt. Whereas distance improves the ability to perceive the gestalt in a visual array, it should have the opposite effect when the task requires attention to details. Distance should therefore have a detrimental effect on the ability to identify a missing low-level, local element within a coherent whole (e.g., a missing hand on a watch, a missing handle on a drawer chest). Wakslak et al. (2006) used the picture completion subtest of the Wechsler Intelligence Scale for Children (Wechsler, 1991) to test this prediction. As expected, participants did worse on sample items of this task when they believed they were less likely to later complete it. Pictures and Words-Pictures are concrete representations that bear physical resemblance to the referent objects, whereas words are abstract representations that carry the essence of that object (Amit, Algom, & Trope, 2009;Amit, Algom, Trope, & Liberman, 2008). Words therefore comprise a higher level of construal than do pictures. Supporting this assumption, Amit, Algom, and Trope (2009) found that participants classified items into more groups when the items were presented in pictures than when they were presented in words. In another study, they presented spatially, temporally, or socially near or distant items in either a pictorial or verbal format (see Figure 3). Participants' task was speeded identification of the object. For example, in an experiment on spatial distance, words or pictures were presented against a background that created an illusion of depth. As expected, participants responded faster to pictures of objects when they were made to seem spatially near than spatially distant, but they responded faster to words denoting those objects when they were made to seem spatially distant than spatially near. To manipulate temporal distance, they used words and pictures of modern objects and ancient objects (e.g., a cart and a car), and to manipulate social distance they used words and pictures of domestic and foreign objects (e.g., a coin of domestic and foreign currency). Responses were faster when pictures represented psychologically proximal objects and words represented psychologically distal objects than vice versa. It seems, then, that processing is most efficient when there is a congruency between the portrayed distance and the presentation medium. Moreover, using similar stimuli in free recall experiments, Amit, Trope, and Algom (2009) demonstrated better memory for proximal than for distal stimuli when the targets were represented in pictures but that it was better for distal than for proximal stimuli when the targets were represented in words. These findings were obtained with temporal, spatial, and social distances. Categorization-If the distant future is represented more abstractly, then individuals should use broader categories to classify objects for distant situations than for proximal situations. In support of this prediction, research has shown that people group objects into fewer, broader categories when they imagine the objects in a distant future scenario than a near future scenario (Liberman, Sagristano, & Trope, 2002, Study 1) or in an unlikely scenario than a likely scenario (Wakslak et al., 2006). Consistent with research on categorization, research on segmentation of ongoing events has found that people chunk behavior sequences into broader segments when the behavior is distal rather than proximal. For example, Henderson, Fujita, Trope, and Liberman (2006) asked New York University participants to partition an ongoing behavioral sequence into as many sections as they deemed appropriate. Participants viewed an animated film developed by Heider and Simmel (1944) that shows two triangles and a circle moving against and around each other and were told that the film depicted the action of three teenagers around a cabin at a well-known summer camp. In the spatially near condition, the camp was said to be located on the East Coast; in the spatially distant condition, the camp was said to be located on the West Coast. As expected, participants created fewer, broader sections out of the video when they believed the campers it depicted were in a spatially distant, as opposed to a spatially near, location. Similar effects were found for events that were described as less likely versus more likely (Wakslak et al., 2006). Do narrow, specific categories promote a sense of psychological proximity? A recent series of studies by Wakslak and Trope (2009) manipulated level of categorization in several distinct ways and found the predicted effect on event likelihoods. In one study, construal level was primed by asking participants to generate either superordinate categories or subordinate exemplars of 40 objects (e.g., table, sport, book). Next, participants completed a supposedly unrelated questionnaire where they made a series of probability judgments. As expected, participants who had been primed to have a high-level construal mindset indicated that the events were less likely to occur compared to those who had been primed to have a low-level construal mindset. In another study, participants compared either alignable or unalignable objects. In the alignable condition, participants compared two digital cameras, with information provided about the same seven features for each camera (e.g., battery life, digital zoom). In the nonalignable condition, participants were provided with information about seven different features for each camera (e.g., battery life for the digital camera; focus for the traditional camera). It was assumed that individuals who are asked to compare alternatives with nonalignable features would seek to make the attributes comparable by representing the alternatives at increasingly higher levels of abstraction (see M. D. Johnson, 1984;Malkoc, Zauberman, & Bettman, 2008). As expected, participants cued to think more abstractly (by having compared products with nonalignable attributes) judged an event involving the products as being less likely to occur than those cued to think more concretely (by having compared products with alignable attributes). Action Identification-Actions, like objects, may be construed in high-level terms, which link them to a superordinate purpose (why one performs them), or in low-level terms, which link them to subordinate means (how one performs them). Here, too, greater psychological distance promotes higher levels of construal (Liberman & Trope, 1998). In one of the studies, participants tended to describe more distant future activities (e.g., studying) in highlevel terms (e.g., \"doing well in school\") rather than in low-level terms (e.g., \"reading a textbook\"). Similar effects emerged when actions were to take place in a spatially distant location (Fujita, Henderson, Eng, Trope, & Liberman, 2006), when the actions were framed as unlikely to actually take place (Wakslak et al., 2006), and when the actor was dissimilar to the perceiver (Liviatan, Trope, & Liberman, 2008). If high-level construals serve to represent psychologically distant events, then activating high-level construals should lead people to think of events in psychologically more distant situations. Indeed, research shows that thinking about an activity in high-level, \"why\" terms rather than low-level, \"how\" terms leads people to think of the activity as taking place in more distant points in time (Liberman, Trope, Macrae, & Sherman, 2007;McCrae, Liberman, Trope, & Sherman, 2008) and of the actor as more socially distant (Stephan et al., 2010). Action-identification theory (Vallacher & Wegner, 1989) specifies how to determine the superordinate and subordinate levels of categorizing a given action. Semin and Fiedler's (1988) Linguistic Categorization Model assesses the abstractness of the action itself. At the most concrete level are descriptive action verbs (\"lift,\" \"take\"), which are typically directly observable. Interpretative action verbs (\"pretend,\" \"help\") are more abstract, as they involve interpretation and require some knowledge of a context larger than the immediately perceived. State verbs (\"prefer,\" \"understand\") are still more abstract, and adjectives (\"helpful,\" \"aggressive\") are the most abstract category. The Linguistic Categorization Model is therefore a useful tool for examining the relationships between psychological distance and abstractness of action construal. Indeed, various kinds of distance have been found to affect abstractness of language. For example, people were found to use more abstract language when describing another person's actions than their own actions (Semin & Fiedler, 1989; see also Fiedler, Semin, Finkenauer, & Berkel, 1995), when describing spatially distant interactions than spatially near interactions (Fujita, Henderson, et al., 2006), and when instructed to address another person politely than in colloquial language (Stephan et al., 2010). Would the reverse direction of influence hold? That is, would linguistic abstractness affect perceived psychological distance? Semin and Smith (1999, Studies 2 and 3) studied the effect of linguistic abstractness on the temporal distance of recalled events. They provided participants with retrieval cues of varying abstractness and examined the temporal distance of the events they recalled. For example, participants were asked to recall either an occasion on which they helped somebody (i.e., concrete retrieval cue) or an occasion on which they displayed a trait of helpfulness (i.e., abstract retrieval cue). As predicted, an abstract retrieval cue prompted older memories than memories that were prompted by a concrete retrieval cue. Person Perception-A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995;E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995;Semin & Fiedler, 1988;Semin & Smith, 1999). Distance should therefore enhance the correspondence bias. Furthermore, representing a person abstractly in terms of his or her dispositions would create a sense of distance. Naturally, social psychologists researched extensively the relations between social distance and dispositional inference. We first briefly review this literature and then describe comparable effects of other psychological distances.",
          "Considerable evidence for differences in construal between self and other comes from research on the actor-observer effect in attribution (E. E. Jones & Nisbett, 1972; for a review, see Gilbert, 1998). This research shows that a person's view of his or her behavior emphasizes the role of concrete situational factors that operate at the moment of action (e.g., \"I stepped on your toe because the bus was crowded\"), whereas his or her view of other people emphasizes the causal role of stable, general dispositional properties of the actor (\"he stepped on my toe because he is clumsy\"). Self-other differences might be explained as reflecting differences in knowledge (people know more about themselves and the variability of their behavior over situations than about others) and differences in the salience of behaviors versus situations (the latter is more salient from one's own perspective, the former from the observer's perspective). Would similar differences in construal emerge when knowledge is identical for near and distal social targets? Research relating abstractness of memories to the perspective in which they are recalled seems to offer an affirmative answer. It has been shown, for example, that personal memories of behaviors that were recalled from a third-person perspective (e.g., \"try to remember your first day at school, as if you are now watching the kid you were\") rather than from a first-person perspective (\"try to remember your first day at school, as if you are a kid again\") tended to use dispositional (as opposed to situational) terms (Frank & Gilovich, 1989;Nigro & Neisser, 1983). In a similar vein, Libby and Eibach (2002, Study 4) found that imagining performing an activity (e.g., rock climbing, playing drums) from a third-person perspective produced less vivid and rich reports of the activity than imagining the same activity from a first-person perspective. In terms of CLT, this means that a third-person perspective, which imposes more distance than a first-person perspective, induces a higher level of construal. Indeed, Pronin and Ross (2006) showed that taking a third person perspective rather a first-person perspective on one's own behavior promoted attribution of the behavior to personality traits rather than to specific situational factors. Temporal distance and spatial distance: Correspondent inference, the tendency to use high-level dispositional attributions, seems to increase not only with social distance but also with temporal distance. Over time, observers' attributions of an actor's behaviors become more dispositional and less situational (Frank & Gilovich, 1989; see also Funder & Van Ness, 1983;Moore, Sherrod, Liu, & Underwood, 1979;Nigro & Neisser, 1983;Peterson, 1980; but see Burger, 1986). These findings are consistent with memory consolidation, namely, the tendency for concrete details to fade more rapidly than general abstractions, thus making memories of the distant past more abstract than memories of the recent past (Bartlett, 1932;Wyer & Srull, 1986). However, assuming that memory is an active, reconstructive process, CLT proposes that increasing past temporal distance is associated not only with a loss of specific details but also with an increased use of high-level, schematic construals (Ross, 1989). Consistent with this proposal, McDonald and Hirt (1997) showed that, over time, student participants not only forgot a fellow student's grades but also were more likely to infer the grades from that student's general ability, resulting in a more coherent (and dispositional) impression of the target person over time. More recently, research has examined dispositional inferences from near future and distant future behavior (Nussbaum, Trope, & Liberman, 2003, Study 1) and from spatially close or spatially distant behavior (Henderson, Fujita, et al., 2006, Study 2). These studies found that student participants drew stronger corresponding attitude inferences from situationally constrained behavior when the behavior was expected in the distant future (vs. near future) or when it was believed to have occurred in a spatially remote location (vs. a near location). These findings demonstrate that the correspondence bias, the tendency to underweight lowlevel, situational constraints on observed behavior, increases with psychological distance from the behavior. Past research has shown that perceivers often form spontaneous trait inferences (STIs) when they observe trait-implying behaviors of other people. For example, upon reading the sentence, \"The secretary solved the mystery halfway through the book,\" people spontaneously inferred the trait \"clever\" (Winter & Uleman, 1984). Can temporal or spatial distance affect the extent to which abstract traits are spontaneously inferred at an early stage of information processing? Rim, Uleman, and Trope (2008) addressed this question in a series of studies that used rate of false recognition of implied (but not present) traits as a measure of STI. They found that New York University participants led to believe that actors were in a distant location (Florence, Italy), compared with a proximal location (Manhattan, New York), formed more STIs, even though behavioral information about targets was held constant across the two conditions. The same effect was found using temporal distance: Participants who were led to believe that actors were from the relatively distant past (year 1997), compared with the more recent past (year 2007), formed more STIs. Self-inferences: Distance may affect not only the inferences we draw about others but also the inferences we draw about ourselves. Pronin and Ross's (2006) research shows that people more likely view their future and past selves than their present selves in terms of general personality traits. In a related study, Wakslak, Nussbaum, Liberman, and Trope (2008) asked participants to imagine themselves in different situations either in the near future or in the distant future and to indicate the extent to which their behavior in those situations would reflect each of the Big Five personality traits. It was found that in the distant future, compared with the near future, participants expected to exhibit their traits more consistently across situations. Another study by Wakslak et al. showed that participants were faster in judging whether general personality traits describe the way they would be in the distant future than the near future. These findings suggest that the distant future self is represented more in terms of general, decontextualized traits than the near future self. People seem to have a clearer view of their personality characteristics when taking a temporally distant perspective rather than a temporally proximal perspective on the self. Indeed, using Linville's (1985Linville's ( , 1987) ) and Donahue, Robins, Roberts, and John's (1993) measures of self-complexity, Wakslak et al. found that self-descriptions were more structured and less complex when they referred to a distant future self than to a near future self. It would be interesting to examine in future research the effects of other distances on the self-concept. For example, would thinking of oneself in an unlikely situation or in a remote spatial location make one see oneself in a more coherent, schematic way? Is it possible that this effect also occurs while traveling (or even while planning traveling)? Does imagining the perspective of another person, especially an unfamiliar person, increase perceived selfintegrity? In general, the construal of the self, which is often more concrete than the construal of others, may be highly abstract and high level when the self is viewed from a distanced perspective, in remote times, places, imaginary situations, and from a third-person perspective. Discussion-Extensive research has verified that as psychological distance increases, construals become more abstract, and as level of construal increases, so too do perceptions of psychological distance. Much of past research has studied individual distance dimensions and types of construal within disparate theories. Without denying the uniqueness of the different distances and types of construal, CLT provides a unifying framework that integrates these disparate lines of research. It suggests that high-level construals and lowlevel construals serve different cognitive functions. High-level construals have evolved to represent distal objects because, with distance, one needs to conserve the essential, invariant properties of the referent object. In contrast, low-level construals preserve the object in minute detail for immediate use. High-level construals serve to transcend the here and now, whereas low-level construals instantiate the present. In the following, we expand our examination of construal levels by relating them to prospection and mental simulation, inferences of distance, and to heuristic low-effort processing.",
          "The present view is consistent with recent functional approaches to memory and cognition (Barsalou, 1999;Buckner & Carroll, 2007;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). These approaches suggest that a major function of episodic memory is prospection, namely, anticipating the future rather than merely reproducing the past. The theories of constructive simulation (Schacter & Addis, 2007) and embodied cognition (Barsalou, 1999;Niedenthal, Barsalou, Winkielman, Krauth-Gruber, & Ric, 2005;Winkielman, Niedenthal, & Oberman, 2008) specifically suggest that episodic memory enables prospection through a constructive process of mentally simulating future episodes. Such simulations are analogical and multimodal and serve to guide choice and action with respect to the future (Barsalou, 2008). CLT adds to this view in several respects. First, it is possible that simulations vary in level of construal, ranging from multimodal simulations that are rich in contextual detail and resemble the kind of analogical representation identified by embodied cognition researchers to general simulations that retain common elements and omit incidental detail. CLT predicts that the latter, higher level simulations are more likely to be used with increasing distance of the past episodes from which simulations are constructed and of the future target situation to which the simulation is applied. For example, a simulation of a future meeting with a friend is likely to be more abstract (i.e., contain less detail on the tone of her voice and the look of the room in which the meeting would take place) to the extent that it is constructed from meetings held with that friend in the relatively distant past or distant locations and to the extent that the meeting with the friend is expected in the relatively distant future or location. Second, it is possible that as distance increases, prospection is increasingly more likely to be based on amodal symbolic representations. For example, a representation of a more distant future meeting with a friend that works in a high-tech company may refer more to semantic knowledge about layoffs in the high-tech industry and include less detail related to perceptual properties, such as the tone of her voice. The research showing that people switch from pictorial to linguistic representations of objects as distance from the object increases is consistent with this possibility (Amit, Algom, & Trope, 2009). Third, symbolic representations might also differ in abstraction, ranging from broad abstractions (she is a woman) to more narrowly applied knowledge (she is a single mother to four and a high-tech manager who is afraid to lose her job). Fourth, as the research reviewed here suggests, these changes in the nature of mental representation of objects might be induced not only by prospective temporal distance from the object but also by spatial, social, and hypotheticality distances. In sum, according to CLT both analogical simulations and symbolic representations might vary in level of construal. Distance may determine whether an analogical or symbolic representation is constructed and the level of abstractness at which it will be constructed. Inferring distance from construal level: We argued that because high-level construals are broad, they bring to mind more distant instantiations of objects, and because low-level construals are narrow, they bring to mind more proximal instantiations of objects. It is also possible for construal level to affect the psychological distance of objects through metacognitive inferences (N. Schwartz & Clore, 1996). People may interpret their low-level construal of an object as indicating that the object is close and their high-level construal of an object as indicating that the object is distant. This metacognitive inference of distance from construal level might involve a more complex attributional calculus when one or more other distances are known. Specifically, the construal-based inference that an object is distant on any given dimension will be discounted when the object is known to be distant on another dimension. Correspondingly, the construal-based inference that an object is distant on any given dimension will be augmented when the object is known to be proximal on another dimension. For example, one would attribute a detailed construal of a meeting with a friend to a relatively close relationship with that friend when the meeting is known to take place in the distant future rather than the near future. Thus, direct implicit associations among different distance dimensions generally result in positive relationships among those dimensions. However, when inferring distance from construal, adjusting the inference of distance on one dimension for distance on other dimensions may result in a negative relationship among those distances.",
          "Could the relationship between distance and construal be accounted for by assuming that people are less motivated to make an accurate judgment regarding relatively distant objects? Does the shift toward higher level construals with increasing distance reflect a shift toward low-effort heuristic processing (see Petty & Cacioppo, 1984)? We think that extracting the general meaning and invariant characteristics of objects is not necessarily more or less effortful than fleshing out the minute details of the objects. For example, generating a superordinate category for the concept soda (e.g., drinks) does not necessarily differ in these respects from generating exemplars of the concept (e.g., Diet Coke). Indeed, our research has not found any systematic relationship between the effects of distance on construal and measures of involvement or accuracy motivation (e.g., P. K. Smith & Trope, 2006;Wakslak et al., 2006). Three additional sets of findings argue against the idea that distance-driven high-level construals necessarily reflect uneffortful, heuristic processing. First, this idea cannot account for performance findings showing that judgments about distal objects (compared with judgments about proximal objects) are more accurate when a correct response requires high-level construals of the objects (e.g., identifying a pattern) and are less accurate only when the correct response requires low-level construals (e.g., detecting a missing detail; e.g., Liberman & Förster, in press;Wakslak et al., 2006). Second, low involvement and uneffortful processing of distant (vs. proximal) objects may account for the underutilization of low-level information about distant objects, but it cannot account for the underutilization of high-level information about proximal objects (e.g., Liberman & Trope, 1998;Nussbaum, Liberman, & Trope, 2006). Third, Fujita, Eyal, Chaiken, Trope, and Liberman (2008) directly addressed the issue of heuristic processing by examining sensitivity to argument strength, a commonly used test of heuristic processing (Chaiken, Giner-Sorolla, & Chen, 1996;Petty & Cacioppo, 1984). Fujita et al. presented participants with either strong or weak arguments from a wildlife conservation organization that either referred to a superordinate category (e.g., orcas in Puget Sound) of an attitude object or subordinate, specific exemplar (e.g., Simon, an orca in Puget Sound) of the attitude object. The results showed that when the arguments referred to a high-level category, attitudes were more sensitive to argument strength when the attitude object was temporally distant than near. However, when the arguments referred to a subordinate exemplar, the reverse was true; That is, attitudes were more sensitive to argument strength in the near than distal condition. Involvement might account for the findings obtained with arguments referring to low-level construal of the attitude object but not those obtained with arguments referring to high-level construal of the attitude object. Finally, using the number of thoughts generated about an attitude object as a measure of systematic processing, Ledgerwood, Trope, and Chaiken (2008) found that the number of thoughts was unaffected by temporal distance from the attitude object. We return to the issue of depth of processing in the sections on prediction and evaluation. Construal-Mediated Effects of Psychological Distance-What are the implications of the link between distance and construal for prediction, evaluation, and self regulation? In the following three sections, we address this question.",
          "As argued above, the very function of high-level construals is to enable people to mentally transcend the here and now by forming a representation consisting of the invariant features of the available information and projecting those representations onto distal situations. Predictions of future experiences would therefore be more schematic than the actual experiences, giving rise to a variety of prediction biases that stem from underweighting contextual and incidental features (Gilbert & Wilson, 2007;Kahneman, Krueger, Schkade, Schwarz, & Stone, 2006;Wilson & Gilbert, 2003). For example, people tend to predict that they will react more extremely to positive events (e.g., getting tenure) and negative events (e.g., a rejected paper) than they actually do. This misprediction stems from underweighting the effect of diluting low-level contextual circumstances (Wilson, Wheatley, Meyers, Gilbert, & Axsom, 2000). According to CLT, these biases should be accentuated by distance, because predicting more distal events should direct attention to even more invariant, schematic features of those events and away from low-level situational details. In the following, we examine the implications of this idea for lay prediction of research findings, predictions about one's own performance, and the effect of global trends and local deviations on extrapolation.1 Theories Versus Noise in Scientific Prediction-In scientific prediction, high-level construals reflect the theory, which fosters confidence, whereas lower level construals include noise, which might undermine confidence. A study by Nussbaum et al. (2006, Study 1) examined the confidence of advanced psychology students in replicating classic findings in psychology in either the near future or the distant future. For example, participants imagined entering a class at the university, either the next day or a year later (depending on the experimental condition), handing the students a list of words to memorize, and then testing how well they remember it after moving some of the students to a different room. Participants estimated how likely it is that those tested in the same room would outperform, on average, those that were moved to a different room, thus replicating the encoding specificity effect. Participants were more confident that they would replicate this effect when they imagined conducting the experiment in the distant future than in the near future, especially when reminded of the theory underlying prediction. The same pattern of results was obtained with other classic findings in social, cognitive, and developmental psychology. Knowledge Versus Task Format in Predicting Performance- Nussbaum et al. (2006) have also examined confidence in predictions about one's own performance. One of the studies assessed participants' confidence in predicting their performance on a general knowledge quiz expected to take place either on the same day or 2 months later (Nussbaum et al., 2006, Studies 3 and 4). The questions were the same, but in either a relatively easy or hard question format, which we assumed is a low-level aspect of the quiz, compared with the content the quiz focuses on. Specifically, the quiz consisted of either multiple-choice questions (relatively easy format) or open-ended questions (relatively hard format). In another study, the quiz consisted of questions with either two response alternatives (relatively easy) or four response alternatives (relatively hard). The results showed that the more difficult question format appropriately reduced confidence in near future performance but failed to reduce confidence in distant future performance. We think that this was the case because question format was a low-level aspect of the task. The results also showed that participants' beliefs about their knowledge in each domain predicted how confident they were in predicting their performance in that domain in the distant future better than in the near future. We think that this was the case because such beliefs concern the essence of the quiz as a test of knowledge and thus constitute a high-level construal of the quiz.",
          "Local Deviations-In a study that investigated the effect of spatial distance on the tendency to base predictions on global rather than local information (Henderson et al., 2006), New York University participants viewed a series of graphs depicting information from the years 1999-2004 (e.g., average number of photocopies per student). The information was said to pertain to the New York University campus in Manhattan (spatially near condition) or to the New York University campus in Florence, Italy (spatially distant condition). Each graph showed either an upward or downward trend, with the final year (2004) always deviating from that global trend. Participants estimated the likelihood that the year 2005 would be consistent with the general trend or with the more recent local deviation. In terms of CLT, global trends convey a highlevel construal, whereas deviations, being local exceptions, should receive more weight in low-level construals. As expected, spatial distance enhanced the tendency to predict on the basis of the global trend rather than the local deviation. Discussion-Spatial and temporal distances increase the impact of high-level information (e.g., theories, self-beliefs, general trends) and decrease the impact of low-level information (e.g., irregular outcomes, specific situational and task characteristics) on prediction. Thus, two complementary processes may contribute to the unwarrantedly higher confidence levels associated with predictions about psychologically distant events: underweighting of the uncertainty associated with low-level information and overweighting of the certainty associated with high-level information. Although we may know less about distant than near situations, our greater reliance on high-level construals in predicting the more distant situations may lead us to make more confident predictions about distant than near situations. Past research has shown that individuals tend to be more optimistic about distant future than near future outcomes (Gilovich, Kerr, & Medvec, 1993;T. R. Mitchell, Thompson, Peterson, & Cronc, 1997;Nisan, 1972). According to CLT, greater optimism is a likely but not necessary result of temporal distance. That is, greater temporal distance promotes optimism only when high-level construals imply a relatively high likelihood of desired outcomes. When low-level construals imply a high likelihood of desired outcomes, optimism may not increase and may actually decrease with greater temporal distance. Indeed, Nussbaum et al.'s (2006) studies on predicting one's performance showed that temporal distance led to more optimistic predictions when the question format was difficult but not when it was easy. Thus, underweighting a low-level aspect of outcomes (question format) increased optimism or decreased optimism depending on whether that aspect implied a high likelihood or low likelihood of success. Distance from an outcome might also reduce personal involvement, thus giving rise to shallow processing and less cautious predictions (see e.g., Petty & Cacioppo, 1984). Again, the critical difference between this view and CLT is that the latter assumes that distance reduces the use of low-level information, not high-level information. Accordingly, confidence increases with distance from an outcome only when the confidence stems from high-level construal of the outcome. The Henderson et al. (2006) and Nussbaum et al. (2006) studies described previously support this prediction.",
          "We make choices and set preferences with respect to our construals of objects rather than the objects themselves. When we choose a car, we do not decide on the car itself but rather on our construal of the car. When we decide on a diet, we do so because the construal of its outcomes seems attractive to us. We argue that construals depend not only on the actual attributes of the objects but also on the object's psychological distance. We therefore predict that all distances should similarly affect evaluation and preferences inasmuch as they all promote higher level construals. A common assumption in psychology and behavioral economics is that the value of an outcome is discounted as temporal distance from the outcome increases (see, e.g., Ainslie, 1975;Loewenstein & Prelec, 1992;Rachlin, Brown, & Cross, 2000). CLT proposes, however, that temporal distance shifts the overall attractiveness of an outcome closer to its high-level construal value than to its low-level construal value. When the low-level value of an outcome is more positive than its high-level value, the outcome should be more attractive in the near future, thus showing time discounting. However, when the high-level value of an outcome is more positive, the outcome should be more positive in the distant future, thus showing time augmentation (see Liberman &Trope, 2003, andTrope &Liberman, 2003, for more extensive discussions). In the following, we apply this principle to central versus peripheral features, feasibility versus desirability concerns, arguments in favor versus against an action, alignable versus unalignable attributes as sources of value, and attributes versus alternatives in choice matrices. Central Versus Peripheral Features-According to CLT, central, goal-related features of outcomes constitute high-level construal of outcomes, whereas peripheral, relatively goalirrelevant features of outcomes constitute low-level construal of outcomes. Distancing an outcome should therefore increase the weight of central features relative to peripheral features. Trope and Liberman (2000) found support for this prediction in studies on evaluations of objects and events containing both a primary and secondary aspect. In one study, for instance, participants imagined buying a radio set either the next day or 1 year later, to listen to morning programs. In one version, participants read that the sound quality of the radio set was good, but that the clock that was incidentally included was relatively useless. In another version, participants read that the sound quality of the radio set was poor, but that the clock aspect was quite useful. As expected, thinking about the radio set in the more distant future increased satisfaction when the sound quality was good and the clock poor, but decreased satisfaction when the sound quality was poor and the clock good, indicating that time delay increased the weight of central features and decreased the weight of peripheral features (for related findings in persuasion contexts, see Fujita et al., 2008). Conceptually similar findings were obtained with social distance, operationalized as interpersonal similarity (Liviatan, Trope, & Liberman, 2008) and social power (P. K. Smith & Trope, 2006). For example, in one study (Liviatan et al., 2008, Study 4), participants evaluated an essay of either high or low quality (a central, high-level aspect of value) allegedly written by a student that performed well or poorly on an unrelated physics exam (a peripheral, low-level aspect of value). Some participants were led to believe that the writer was similar to themselves, whereas others were led to believe that the writer was dissimilar to them. The weight of the peripheral information about the physics ability (relative to that of the essay quality) was greater in participants' evaluations of the essay written by the similar student than by the dissimilar student. It is interesting that research which compared the decisions people make for themselves to the advice they give to others obtained similar findings. Kray and Gonzalez (1999) and Kray (2000) compared participants' own choices with the advice they gave to socially close and distant others. They found that in advising others, especially socially remote others, participants tended to give more weight to a single attribute which they designated as the most important and less weight to other, more peripheral attributes. For example, when advising another person about choosing between two jobs, participants gave more weight to personal satisfaction (which they viewed as the most important dimension) and less weight to salary and location (the less important dimensions) than when choosing for themselves (Kray, 2000, Study 2). In two other studies, Kray found that this preferential weighting of important attributes was stronger in advising a distant social target (a student in another department) than a closer target (a student in one's own class). Moreover, as advisers, participants rated central attributes as highly important and peripheral ones as unimportant, whereas as deciders they rated the various attributes as relatively similar in importance. It is worth noting that participants reported greater responsibility and potential regret, and generated a larger number of decision-relevant attributes when making decisions for others than for themselves. As Kray pointed out, it therefore seems unlikely that participants simply invested less effort in advising others than in deciding for themselves. In our terms, these findings demonstrate choosing according to more central, high-level aspects for socially distant than social close others. Feasibility Versus Desirability-Desirability concerns involve the value of the action's end state (a high-level construal feature), whereas feasibility concerns involve the means used to reach the end state (a low-level construal feature). Therefore, desirability concerns should receive greater weight over feasibility concerns as psychological distance increases. Consistent with this prediction, it was found that as temporal distance from an activity (e.g., attending a guest lecture) increased, the attractiveness of the activity depended more on its desirability (e.g., how interesting the lecture was) and less on its feasibility (e.g., how convenient the timing of the lecture was; Liberman & Trope, 1998). Similar results have been found for other distance dimensions, including hypotheticality and social distance (e.g., Liviatan et al., 2008;Todorov, Goren, & Trope, 2007; see review by Liberman et al., 2007). Arguments in Favor of and Against an Action-In deciding whether to undertake an action, cons are subordinate to pros. This is because the subjective importance of cons depends on whether or not pros are present more than the subjective importance of pros depends on whether or not cons are present. For example, if we know that a medical treatment has some health benefit, we would inquire about its potential side effects before making a decision. But if the treatment has no benefits, we would decide against taking it without further inquiry about its side effects. In contrast, we would inquire whether a medical treatment has health benefits whether or not it has side effects. Thus, the importance of side effects depends on whether the treatment is known to have benefits, but the importance of benefits is independent of whether the treatment is known to have side effects. If cons are subordinate to pros, then pros should become more salient as temporal distance from the action increases, whereas cons should become less salient as temporal distance from the action increases. To test this prediction, Eyal, Liberman, Trope, and Walther (2004) asked participants to generate arguments in favor and against new (i.e., nonroutine) near future or distant future actions, such as introducing a new exam procedure (e.g., switching to open-ended questions instead of multiple-choice questions; Study 2), social policies (e.g., restricting private cars in the city center; Study 3), and a variety of personal and interpersonal behaviors (e.g., approaching a fellow student and offering to write an assignment together; Studies 4-6). As predicted, in all the studies, participants generated more pros and fewer cons as temporal distance from the actions increased. In an extension of these findings, Herzog, Hansen, and Wänke (2007) suggested that if pros are more salient as temporal distance increases and cons are more salient as temporal distance decreases, then an increase in temporal distance should make it easier to generate pros and more difficult to generate cons. Further, because attitudes tend to be more in line with content when the retrieval is experienced as easy (Wänke & Bless, 2000), the ease of retrieval associated with generating pros and cons of near and distant future activities should influence attitudes toward those activities, even when the number of arguments is held constant. In a test of these ideas, participants read about a proposed action that was to happen in the near or distant future and were instructed to write down either four pros or four cons regarding the activity. As expected, participants (a) found it easier to generate pros and more difficult to generate cons when the issue concerned the distant rather than near future and (b) had more favorable attitudes toward the action when it was to occur in the distant future. Alternatives and Attributes in Choice Matrices-Choice and enactment of a course of action is often based on information people search for about the available alternatives. Decision theoretic work has distinguished between searching within attributes, across alternatives and searching within alternatives, across attributes (Tversky, 1972). In a typical study, participants are presented with a matrix of information in which rows represent alternatives (e.g., different apartments), columns represent attributes (e.g., price, location, noise), and cells include the standing of each alternative on the corresponding attribute. Participants search this matrix by exposing the information in each cell, one at a time (see Payne, Bettman, & Johnson, 1988, for a review of this paradigm). We think that searching attributes within alternatives reflects a lower level construal of the choice situation than searching alternatives within attributes. This is because attributes are instantiated in specific objects and when detached from specific objects they become abstractions. Attributes are general, whereas objects are unique and variable manifestations of specific combinations of attributes. We thus expect that within-alternative search would characterize processing of proximal decision situations, whereas within-attribute search would characterize processing of distal decision situations. Borovoi, Liberman, and Trope (2010) recently tested this prediction. In a study on temporal distance, participants considered a choice for either the near future (e.g., choosing an apartment to rent in the next 2 weeks) or the distant future (e.g., choosing an apartment to rent a year later). In a study on social distance, participants considered a choice either for themselves or for another student. As expected, there were more within-alternative steps and less within-attribute steps when making decisions for psychologically proximal situations than for psychologically distal situations. It is important to note that in both studies, participants opened an equal number of cells and invested a similar amount of time in both distance conditions, indicating that they were not less motivated in the distal condition than in the near condition. It seems then that when making choices for proximal rather than distal situations, people are more likely to organize information within alternatives rather than within attributes. Alignable Versus Nonalignable Attributes-As noted earlier, comparing nonalignable objects (with information provided about different attributes for each object) requires higher level construal than comparing alignable objects (with information provided about the same attributes for each object), because making nonalignable objects comparable requires representing them in terms of more abstract attributes. On the basis of this argument, Malkoc, Zauberman, and Ulu (2005) reasoned that decisions that have distant future consequences (relative to near future consequences) would involve an increased consideration of nonalignable attributes. In a test of this prediction, participants evaluated two brands of potato chips and selected one brand to receive either at the end of the session (near future) or at the end of the semester (distant future). The two brands were designed based on pretesting to be equally attractive overall; one of the options, however, was designed to be better on its alignable attributes, whereas the other brand was better on its nonalignable attributes. As expected, temporal distance shifted both evaluations and choice toward the nonalignable better option over the alignable better option, indicating an increased reliance on nonalignable attributes when making decisions with distant future consequences. Discussion-Considerable evidence suggests that the value of outcomes is discounted or augmented over psychological distance depending on how it is distributed between highlevel and low-level aspects of the outcomes. People seem to appreciate proximal outcomes for their low-level aspects and distal outcomes for their high-level aspects. Although they often know less about distant than near outcomes, their evaluation of the distant outcomes might be more extreme when high-level construal of the outcomes have strong evaluative implications. These findings are consistent with CLT and the more general view that valuation is a constructive process (see e.g., Payne, Bettman, & Johnson, 1993;Shafir, Simonson, & Tversky, 1993;Weber et al., 2007). It is important to point out that the effect of distance on the value of outcomes is distinct from its effect on the controllability of the outcomes (Liberman & Trope, 2003). Psychological distance from outcomes often reduces the extent to which the outcomes are contingent upon one's action. For example, people typically have less control over others than themselves, over strangers than friends or relatives, and over events that occur in spatially remote than near locations. Moreover, because the number of ways of attaining an outcome often diminishes with proximity to the outcome, one's immediate action is less critical for a psychologically distant outcome to occur. For example, a long time before an exam, failing to study may be compensated by studying more intensely later, but a short time before the exam, when only a few hours remain, such possibility no longer exists. Similarly, failing to help a stranger may be compensated by help from another stranger, but failing to help a close friend is less likely to be compensated by someone else, because people typically have fewer close friends than strangers. By making outcome less contingent on one's action, greater psychological distance may decrease the motivation to engage in that action. Students may be less motivated to study for a distant exam than for a close exam because failure to study is easier to compensate for at a distance, or people may be less motivated to help a stranger than a close friend, because in the former case, their failure to help is more likely to be compensated by others. These changes in motivation do not reflect changes in value (e.g., the value of success on an exam may not increase closer to it). However, in many situations, measures of value might be contaminated with motivation and therefore show discounting over psychological distance. This could be one of the reasons for the widespread belief in economics and the behavioral sciences that value is discounted over time and psychological distance.",
          "Like predictions and evaluations, behavioral intentions and self-regulation should be increasingly based on high-level construal aspects as psychological distance increases. In the following, we examine the implications of this idea for values and ideologies, self-control, and negotiation. Values as High-Level Behavioral Guides-Values are commonly viewed as abstract structures that provide continuity and meaning under changing environmental circumstances (Feather, 1995), as stable meaning-producing superordinate cognitive structures (Rohan, 2000), and as trans-situational guides for action (S. H. Schwartz & Bilsky, 1987). On the basis of CLT, we propose that values, because of their relatively abstract and decontextualized nature, will be more readily applied to and guide intentions for psychologically distant situations. Evidence for this proposal has been recently obtained by Eyal, Sagristano, Trope, Liberman, and Chaiken (2009). One study used S. H. Schwartz's (1992) value questionnaire to assess the importance participants assigned to a wide range of values (e.g., power, benevolence, hedonism) and then asked participants to imagine 30 behaviors (e.g., \"rest as much as I can\") and to indicate the likelihood of performing each behavior either in the near future or in the distant future. Eyal et al. correlated the rated importance of each value and the mean likelihood of performing the behaviors corresponding to that value. As expected, these correlations were higher when the behaviors were planned for the distant rather than the near future, suggesting that people's values are better reflected in their intentions for the distant future than in their intentions for the immediate future or their actual behavior. For example, being high (vs. low) in hedonism might mean planning hedonic activities for the distant future, but not necessarily for the upcoming week. It is interesting that Eyal et al. (2009) also found that although values predicted participants' intentions for the distant future, feasibility concerns were more predictive of their intentions for the near future. For example, the number of hours participants volunteered in the distant future condition was predicted by their benevolence values but not by the convenience of the timing. In contrast, the number of hours participants volunteered in the near future condition was not predicted by their benevolence values and instead depended on the convenience of the timing. Extending this line of thought, Eyal, Liberman, and Trope (2008) argued that people judge immoral acts as more offensive and moral acts as more virtuous when the acts are psychologically distant than near. They showed that transgressions against core values that are deemed harmless because of extenuating circumstances (e.g., eating one's dead dog) were judged more severely when imagined from a more distant temporal or social perspective. Conversely, moral acts which might have had ulterior motives (e.g., adopting a disabled child when a government pays high adoption pensions) are judged more positively from temporal distance. The findings suggest that moral criteria are more likely to guide people's judgments of distant rather than proximal behaviors. Ideology and Incidental Social Influence-Our attitudes shift, often outside of our awareness, in response to other people in our local social context, including communication partners, significant others, and even total strangers (Baldwin & Holmes, 1987;Davis & Rusbult, 2001;Higgins & Rholes, 1978;Kawakami, Dovidio, & Dijksterhuis, 2003;Lowery, Hardin, & Sinclair, 2001). CLT predicts that when an attitude object is psychologically near, evaluations will be attuned to a particular social context and therefore more likely to be affected by incidental attitudes of others in the social situation rather than by one's ideology. Conversely, when the attitude object is psychologically distant, it will be abstracted away from its local context, and evaluation will therefore be less affected by the incidental attitudes of salient others and, instead, reflect one's ideology. A series of studies by Ledgerwood et al. (2008) tested the hypothesis that attitudes will align with those of another person in the local social context more when psychological distance is low (vs. high). Using an anticipated interaction paradigm, participants read about a policy that would increase the deportation of illegal immigrants starting either next week (near future) or next year (distant future) and learned that their discussion partner was either in favor of or against deporting illegal immigrants. They then privately reported how likely they would be to vote in favor of the policy. Participants' voting intentions shifted toward the interaction partner's attitude when the policy was set to be implemented in the near future but not when it was to be implemented in the distant future. However, voting intentions more strongly reflected participants' previously assessed ideological values when the policy was to be implemented in the distant (vs. near) future. Specifically, the more participants valued preserving the societal status quo, the more they supported a distant future policy that would enforce the deportation of illegal immigrants. Self-Control-Situations that require self-control involve a conflict between two opposing motivations (e.g., a desire to go out with friends and a need to study for an exam). Fujita, Trope, Liberman, and Levin-Sagi (2006) proposed an analysis of self-control conflicts as conflicts between behavioral implications of high-level construal (i.e., valence attached to primary, central, goal-relevant, superordinate considerations) and behavioral implications of low-level construal (i.e., valence attached to secondary, incidental, goal-irrelevant, subordinated, features). Failure of self-control, according to this proposal, is succumbing to the motivation implied by the low-level value. For example, if studying for an exam is related to more superordinate goals than going out with friends, then the latter behavior would represent a failure of self-control. Consistent with this analysis, a series of studies by Fujita et al. demonstrated that higher level construal increased self-control. In one study, participants first completed a task in which they indicated either why or how they would maintain good physical health. This task was designed to induce a high-level or low-level construal mindset, respectively (see Freitas, Gollwitzer, & Trope, 2004). Participants were then asked to hold a handgrip while connected to bogus electrodes, ostensibly as part of a psychophysiological assessment of personality. Participants were told that the longer they held the handgrip, the more diagnostic was the information obtained from the apparatus. Thus, the situation presented a conflict between a desire to get diagnostic, self-relevant information (high-level value) and the inconvenience of holding the handgrip (low-level value). The results indicated, as predicted, that participants in the high-level construal condition held the handgrip longer than those in the low-level construal condition. Just like higher level of construal, greater temporal, spatial, or social distance from a situation that poses a self-control dilemma should also enhance self-control. Indeed, research on intertemporal preference reversals has shown that people are better able to commit to self-control a long time in advance than a short time in advance (e.g., Ainslie & Haslam, 1992). A study conducted by Freitas, Salovey, and Liberman (2001) specifically showed that people were better able to forgo flattering but useless feedback for the sake of unpleasant but useful feedback when the feedback was delayed than when it was imminent. Similarly, research on children's delay of gratification has shown that greater temporal and spatial distance from a tempting object enhances self-control (Metcalfe & Mischel, 1999;Mischel, Shoda, & Rodriguez, 1989). CLT would further predict that we are more likely to advise another person to exercise self-control than to make a similar decision for ourselves, and that deciding on self-control is easier when it is premised on less likely events. For example, we could predict that deciding to start a diet upon getting an A in an exam would be facilitated to the extent that getting an A is less likely. In general, psychologically distancing oneself from a situation might promote people's ability to exercise self-control in that situation (see Kross & Mischel, in press). Negotiation-As with values, issues within an interpersonal negotiation can differ in their centrality and worth. If a pair of negotiators can trade off their lowest and highest priority issues (e.g., give in on secondary issues in exchange for getting what they want on highpriority issues, a process called logrolling), they are more likely to succeed in \"expanding the pie,\" maximizing both individual and joint outcomes. Because negotiators should be expected to focus more on central concerns and less on peripheral concerns as distance increases, we would expect to see more logrolling agreements in a distant future than near future context. Examining this idea within the context of a live negotiation, Henderson, Trope, and Carnevale (2006) found that whereas 91% of dyads with a temporally distant perspective reached a full logrolling agreement, only 50% of dyads with a temporally near perspective did so. The enhanced reciprocal concessions made by dyads with the temporally distant perspective culminated in better negotiated individual and joint outcomes. Moreover, research on the role of construal levels in the negotiation process has shown that negotiators who construed issues abstractly rather than concretely were more likely to discover integrative agreements (Henderson & Trope, 2009). For example, having negotiators think abstractly about issues (by having them generate general descriptions) rather than concretely about issues (by having them generate specific descriptions) increased negotiators' logrolling and individual and joint outcomes. These findings have implications for how negotiators handle conflicts over minor versus major issues and specific interests versus broad values and ideological differences in situations that do not allow for tradeoffs (Druckman & Broome, 1991;Harinck & De Dreu, 2004). Specifically, CLT suggests that the resolution of minor issues and specific interests should be hindered when individuals adopt a psychological proximal perspective or a lower level construal and facilitated by having a more distal perspective and abstract construal. Conversely, the resolution of conflict over major issues, values, and ideological differences should be hindered when individuals adopt a psychologically distant perspective or a higher level construal and facilitated by a less abstract construal and a psychologically proximal perspective. Discussion-The research reviewed in this section suggests that adopting a distal versus proximal perspective changes the way people make behavioral plans, resolve value conflicts, negotiate with others, and cope with self-control problems. The choices people make for psychologically distant situations are guided by their general attitudes, core values, and ideologies. As people get psychologically closer to the situation, their choices are increasingly influenced by more specific attitudes, secondary values, and incidental social influences. It seems, then, that from a distant perspective, global concerns are prioritized and unequivocally pursued, whereas from a proximal perspective, those priorities are weakened and even reversed as local concerns become more prominent. People may have less information about distant situations. Nevertheless, they often act as if they know better what to do or what they should have done when they remove themselves from the situation and take a distant rather than proximal perspective on it. Implications and Extensions-To recapitulate, the research we have reviewed thus far suggests that psychological distance triggers high-level construals, which focus on the central and enduring features of an object while screening out incidental and peripheral details. These high-level construals promote attunement to what is consistent about an object across multiple contexts, allowing individuals to transcend the particularities of the present situation and act according to their global concerns. Conversely, psychological proximity triggers low-level construals, which include the concrete and contextualized aspects of an object. These in turn are attuned to the current context, immersing individuals in the immediate situation and enabling them to flexibly respond to local concerns. In this final part of the article, we explore new directions for research on more speculative implications and extensions of CLT. These include psychological phenomena that might map onto different levels of construal (Section VII) and phenomena that might map onto different psychological distances (Section VIII). We conclude with a discussion of psychological distance in the brain (Section IX).",
          "We have shown that the distinction between different levels of construal is applicable to many psychological phenomena (e.g., gestalts vs. details, trait vs. situation attributions, pros vs. cons). In this section, we suggest that two more distinctions might map onto high-versus low-level of construal: assimilation versus contrast and high-versus low-level emotions. Assimilation and Contrast-When perceiving two stimuli in relation to each other (compared with perceiving the same stimuli separately), a perceiver may assimilate the two stimuli to each other, thereby perceiving each of the stimuli as more similar to the other stimulus, or contrast them away from each other, thereby perceiving each of the stimuli as more distinct from the other stimulus (Parducci, Perrett, & Marsh, 1969;Schwarz & Bless, 1992, 2007). For example, when considering a paper of a student in relation to the best paper in class (vs. considering it in isolation), the focal paper may be assimilated to the excellent paper and thus seem better, or it may be contrasted away and thus seem worse. In view of the opposite effects of assimilation versus contrast, the question of what makes each of them more likely becomes crucial (Mussweiler, 2001;Schwarz & Bless, 2007;Stapel, 2007). Most germane to the present framework are Schwarz andBless's (1992, 2007) inclusion/ exclusion model and Förster, Liberman, and Kuschel's (2008) global/local model of assimilation versus contrast. According to these models, because global, high-level construals are more inclusive, using those construals is likely to result in including two stimuli in the same category and an assimilation effect. Using low-level construals, however, is likely to result in categorizing the two stimuli in separate categories and a contrast effect. Consistent with this view, it has been found that priming of high-level construal and greater temporal distance enhances assimilation and reduces contrast. For example, in one of the studies, (Förster et al., 2008, Study 4), participants compared their athletic skills with either a moderately high standard or a moderately low standard and then rated their expected athletic performance in an athletic competition that would take place the next day (proximal temporal perspective) or a year from then (a distal temporal perspective). In the control condition, time was not specified. The results showed that a distant time perspective enhanced assimilation (i.e., produced a high self-rating after comparison to a high standard and a low self-rating after comparison to a low standard), whereas a proximal time perspective enhanced contrast (i.e., produced a low self-rating after comparison to a high standard and a high self-rating after comparison to a low standard). Borovoi et al. (2008) reasoned that if distance enhances assimilation, then it would also enhance the positive effect that attractive but unattainable alternatives might have on evaluations of the choice set. Consistent with this prediction, they showed that adding a very attractive job that was no longer available to a set of job offers increased the attractiveness of the entire set of job offers when the jobs were expected in the distant future rather than the near future. It seems that the attractiveness of available options is assimilated to that of attractive but unattainable possibilities when thinking about the relatively distant future. Affect-It is commonly assumed that psychological distance from an object diminishes the intensity of affective responses to that object. People typically react more strongly to events that are closer to them in time and space, to events that happen to themselves than to others, and to events that are real rather than hypothetical. However, building on appraisal theories of emotion (see, e.g., Beer & Keltner, 2004;Clore, Ortony, & Foss, 1987;Ortony & Turner, 1990;Tangney & Fischer, 1995;Tracy & Robins, 2007), CLT suggests that it might be useful to distinguish between emotions that may ensue from a low-level construal of an emotion-eliciting event and emotions that ensue from a high-level construal of the event. For example, love might be based on a more general and abstract representation of the target person than lust (Förster, Özelsel, & Epstude, in press). It is interesting that some emotions involve, by their very definition, taking a distant perspective. For example, guilt and shame require taking a socially distant perspective, as they include considering the situation from the perspective of another social agent (Beer & Keltner, 2004), whereas sadness does not require taking such a socially distant perspective. Anxiety involves consideration of a potential, distal danger (for mice, anxiety is often induced by a smell of a cat), whereas fear is a reaction to an immediately present danger (e.g., for mice, the presence of a cat; Gray, 2000). Likewise, counterfactual emotions (e.g., regret) involve consideration of hypothetical alternatives to reality. According to CLT, emotions that involve taking a distant perspective require relatively abstract, high-level construal of the emotion-eliciting situation. It would be interesting to examine whether, indeed, some emotional experiences are more abstract than others. For example, would thinking of guilt and anxiety prime people to think in higher level terms in a subsequent, unrelated task? Once the classification of emotions to high level versus low level is established, CLT would predict that higher level emotions would decay less steeply and may even intensify over distance. Eyal and Fishbach (2008) recently accrued evidence in support of this hypothesis. They showed, for example, that pride decays over time less than happiness. Because of their distance transcending qualities, abstract emotions seem to assist in self-control more than concrete emotions. For example, Eyal and Fishbach showed that among dieters, priming participants with pride rather than happiness resulted in higher choice of a carrot (a self-control choice) rather than a chocolate bar (a self-control failure). CLT would further suggest that even the same emotion may be high or low-level, depending on whether it reflects high or low-level construals of the object. For example, in a funeral, sadness about the transience of life and compassion are central and thus constitute high-level emotions, but happiness upon seeing old friends is peripheral and thus constitutes a lowlevel, peripheral emotion. In a birthday party, however, the centrality of these emotions reverses. We would predict that in this respect too, secondary emotions would diminish over distance more than primary emotions. For example, the positive value that may be added to a funeral by the prospect of meeting friends would be more apparent when the funeral is closer in time and space, when we anticipate going ourselves rather than think about somebody else and when attending the funeral is likely rather than unlikely.",
          "",
          "Social power: Social power may engender a sense of distance from others. Indeed, individuals who have power see themselves as less similar to and thus more distant from other people than individuals who have less power (e.g., Hogg, 2001;Hogg & Reid, 2001;Lee & Tiedens, 2001;Snyder & Fromkin, 1980). This perception might be due to the fact that groups, organizations, and societies ordinarily have a pyramidal structure with fewer individuals occupying high-power positions than low-power positions. There is therefore greater similarity in the positions held by individuals with low power than by individuals with high power. If social power makes people feel distant from others, then it should also predispose them to construe information abstractly, focus on the central aspects of situations, disregard secondary aspects, and establish clear priorities. Several lines of research are consistent with these predictions. For example, Overbeck and Park (2001) observed that high-power participants were better at distinguishing between central and peripheral information, a hallmark of abstract processing. Guinote, Judd, and Brauer (2002) found that assigning participants to high-power positions promoted the use of abstract language. A series of studies by P. K. Smith and Trope (2006) examined the effect of power priming on a range of construal-level phenomena. Their findings show that power primed participants (a) were better able to abstract visual stimuli in an embedded figures task and Gestalt completion task, (b) were better at detecting covariation in a series of data, and (c) made more superordinate categorizations. Finally, Magee, Milliken, and Lurie (in press) have recently found that the level of abstraction in verbatim reactions to the September 11, 2001 attack was higher for individuals with high social power, compared to those with low social power. These findings suggest that the distal perspective activated by the possession of social power promotes going beyond the information given, detecting the underlying structure, and abstracting from it superordinate, central features. Power-related construal may expand people's mental horizons, enabling them to transcend the immediate circumstances and take into account the past, future, a broad range of people, and unlikely possibilities. Recent research by P. K. Smith, Wigboldus, and Dijksterhuis (2008) suggests that the relationship between power and construal level might be bidirectional. Their findings show that thinking abstractly enhances individuals' sense of their own power. Are there analogous effects of level of construal on the way we perceive others? Do we think of individuals as suitable for power positions, or as actually holding such positions, when they articulate and enact highlevel construals of the situation? Do our inferences and preferences as to the power individuals possess depend on whether they can articulate plans that transcend the present and extend to the distant future, remote places, diverse groups, and unusual circumstances? These questions await future research.",
          "The endowment effect suggests that when individuals are asked to sell an object they own, they ask for a significantly higher price than buyers tend to offer (Thaler, 1980). For example, in a typical endowment study, half of the participants receive a mug and are asked to set a price at which they would be willing to sell the mug to the other half of the participants, who in the meantime set the price at which they would be willing to buy the mug (e.g., Kahneman, Knetsch, & Thaler, 1990). Results of numerous such experiments and field studies show that average selling prices are two to three times higher than average buying prices. Given that the benefit of having the mug and, hence, its objective value, should not change with random ownership, these results present a perplexing phenomenon. The most widely accepted explanation of the endowment effect is loss aversion (Bar-Hillel & Neter, 1996;Kahneman et al., 1990; but see also Liberman, Idson, Camacho, & Higgins, 1999) that goes back to prospect theory (Kahneman & Tversky, 1979). Prospect theory suggests that individuals' perception of losses and gains differ, such that the subjective pain of a loss is more intense than the subjective pleasure of a gain of similar objective value. According to a prospect theory approach to the endowment effect, selling an object is perceived as a loss, compared with the seller's reference point of having the object; buying, in contrast, is perceived as a gain, compared with the buyer's reference point of not having an object. Because individuals are loss averse, they therefore tend to evaluate objects they consider selling more positively than they do objects they consider buying. Adopting a CLT approach to the endowment effect, Irmak, Wakslak, and Trope (2009) argued that sellers adopt a distant perspective on an object they are considering selling, thinking about the object from the perspective of the distant other to which the object will be moving; whereas buyers consider the object from a more proximal perspective, considering how they personally view this nearing object. As a result, sellers will construe the object on a high level, and selling prices will largely be influenced by the object's high-level features (and less by its low-level features). In contrast, buyers will construe the object on a lower level, and buying prices will therefore be influenced by the object's low-level features. In other words, we argue that sellers and buyers think about the object differently from one another, focusing, and thus basing their pricing decisions, on different types of object features, which, in turn, may lead to systematic differences in selling and buying prices. A series of studies by Irmak et al. (2009) provides initial support for this analysis. First, they showed that sellers indeed tended to think about an object in terms of its value for the buyer (indicating a distal perspective), whereas buyers tended to think about the object in terms of its value for themselves (indicating a proximal perspective). Second, sellers, compared with buyers, were more likely to construe actions related to those objects in high-level, endsrelated terms rather than low-level, means-related terms. Third, selling prices were higher than buying prices for objects whose attractiveness derived from high-level construal features, but this was not the case for objects whose attractiveness derived from low-level construal features. For example, selling prices were higher than buying prices when the object was superior on the desirability dimension and inferior in the feasibility dimension, but not vice versa, and when the primary (goal-relevant) aspects of the object were superior and the secondary (goal-irrelevant) aspects of the object were inferior, but not vice versa. These findings demonstrate that sellers and buyers derive value from different aspects of the object. Sellers take a psychologically distal perspective and thus extract value from the highlevel, primary aspects of the object, whereas buyers assume a psychologically proximal perspective and their perception of value is influenced less by primary aspects of the object and more by its secondary aspects.",
          "Proximal versus distal senses: We view the four psychological distance dimensions as being anchored on a single starting point (zero-distance point): me, here, and now. This view may be oversimplified, however, as some direct experiences may be more proximal than others. More specifically, the five senses-sight, hearing, smell, touch, and taste-may be mapped along spatial distance according to the maximum physical distance of the sensed object. An object has to be in one's mouth to be tasted, it has to be within one's reach to be touched, it may be farther away to be smelled, and it can be still farther away to be heard or seen. The distant senses, sight and hearing, enable people to extend the scope of perception far beyond the range of the near senses, which are limited to the person's immediate environment (Boring, Langfeld, & Weld, 1939; see also Rodaway, 1994). Do proximal senses produce a psychologically closer experience than distal senses? If they do, then the antecedents and consequences of psychological distance should apply to stimuli that are perceived with proximal versus distal senses, such that proximal senses would be associated with low-level construal and psychological proximity, whereas distal senses would be associated with high-level construal and psychological distance. For example, sharing an experience of tasting something with another person might produce more social closeness to that person than sharing an experience of looking at something. Similarly, touching a product might make a consumer give more weight to its feasibility properties and less weight to its desirability properties than only looking at that product. Novelty: Novel events are unfamiliar and often (but not always) subjectively improbable. Novel objects may therefore be perceived as more psychologically distant. The question, then, is whether novelty affects construal and construal-mediated outcomes in the same way as psychological distance does. Addressing this question, Förster, Liberman, and Shapira (2009) found that when stimuli were presented as novel they were construed at a higher level than when they were presented as old. Novelty and familiarity were manipulated by either framing a task as new versus familiar or by asking participants to reflect upon novel versus familiar events prior to the task (i.e., procedural priming). The studies showed that relative to the control, no priming or no framing group, novelty priming or framing enhanced global perception, broad categorization, and high-level action identifications. Familiarity priming or framing had the opposite effect. Another study showed that presenting a product as novel (e.g., \"a novel model of a cellular phone\") versus old (e.g., \"Version 3 of an existing model of a cellular phone\") made participants assign greater weight to the products' high-level, desirability features and less weight to their low-level, feasibility features. It would be interesting to examine in future studies the effect of novelty on other construalmediated outcomes, such as creativity and self-control. Because creativity has been shown to be related to high-level construal (Förster et al., 2004), we might predict that presenting a task as novel (vs. old) would enhance creative performance. It is also possible that because high-level construal is conducive to self-control (see Fujita, Trope, et al., 2006), presenting self-control programs (e.g., dieting or smoking cessation programs) as novel (vs. old) would promote success in those programs. Generalizing from experience: Our responses to a new object are often based on generalizations from a familiar, previously experienced object. The generalization may result from the co-occurrence or some resemblance between the familiar object and the new object. Of special interest are generalizations, often called projections, from ourselves to new people who become associated with us. By definition, the self is more proximal than another person. Therefore, we are likely to form low-level construals of ourselves, while generalizing our high-level construals of ourselves to others. High-level construals of the self are more likely to drive responses to others, whereas low-level construals of the self are more likely to drive responses to self. As a result, the more people value their high-level self characteristics relative to their low-level self characteristics, the more likely are they to form favorable impressions of unfamiliar others. Again, the bias toward projecting one's highlevel characteristics rather than low-level characteristics onto another person is likely to be augmented with greater psychological distance from that person.",
          "Do psychological distances and their links to construal level correspond to the structure and functions of the brain? A growing body of research suggests that they do. For example, recent research suggests that a common brain network involving the prefrontal cortex and the medial temporal lobe is implicated in prospection, retrospection, and taking another person's perspective (e.g., Buckner & Carroll, 2007;J. P. Mitchell, Macrae, & Banaji, 2006;Schacter & Addis, 2007;Suddendorf & Corballis, 2007). In particular, different subregions of the medial prefrontal cortex (mPFC) have been associated with psychological processes that vary in their levels of abstraction. In their review on this subject, Amodio and Frith (2006) noted that representations of low-level goals and actions tend to be associated with activity in more posterior and ventral regions of the mPFC, whereas more complex and hierarchically organized representations are typically associated with more anterior and dorsal regions. On the basis of anatomical studies of connectivity in this region, Amodio and Frith (2006) proposed that activity in the mPFC corresponds to two different axes associated with the degree of abstraction (running posterior to anterior) and to action (running ventral to dorsal). In a related vein, J. P. Mitchell et al. (2006) found that processing information about a similar other engages the ventral region of the mPFC, whereas processing information about a dissimilar other engages more dorsal subregions of the mPFC. Moreover, recent research by Mitchel, Ames, and Gilbert (2008) observed an association in the brain between social distance and temporal distance. Specifically, the ventral mPFC was implicated in processing information about similar others and the present self, whereas the dorsal mPFC was implicated in processing information about dissimilar others and the future self. More generally, there is research showing that the brain is hierarchically organized, with higher points in the cortical hierarchy representing increasingly more abstract aspects of stimuli (Grill-Spector & Malach, 2004;Lieberman, Gaunt, Gilbert, & Trope, 2002). For example, progressively anterior and dorsal subregions of the prefrontal cortex have been found to be associated with more abstract representations (Badre, 2008;Koechlin & Summerfield, 2007;Ramnani, & Owen, 2004). This organization of information in the brain might be related to distance from stimuli, such that activation systematically progresses to higher points in the hierarchy as psychological distance from the stimuli increases. In sum, there may be a \"construal axis\" in brain systems that maps onto a \"distance axis.\" Brain systems that process high-construal information may be biased toward distal information, and brain systems that process low construal information may be biased toward proximal information.",
          "In reality, the different distance dimensions are separable. The fact that something happened long ago does not necessarily mean that it took place far away, that it occurred to a stranger, or that it is improbable. Nevertheless, as the research reviewed here demonstrates, there is marked commonality in the way people respond to the different distance dimensions. CLT proposes that the commonality stems from the fact that responding to an event that is increasingly distant on any of those dimensions requires relying more on mental construal and less on direct experience of the event. The findings we reviewed in the present article substantiate this claim in showing that (a) the various distances are cognitively related to each other, such that thinking of an event as distant on one dimension leads one to thinking about it as distant on other dimensions, (b) the various distances influence and are influenced by level of mental construal, and (c) the various distances are, to some extent, interchangeable in their effects on prediction, preference, and self-control. Past psychological research has typically investigated the different distance dimensions within disparate theoretical frameworks and methodologies. The research reviewed here suggests that psychological distance, as conceptualized here, captures a fundamental aspect of meaning common to all distances and may provide a unifying framework for understanding a wide range of seemingly unrelated psychological phenomena. Finally, taking an even broader perspective on psychological distance, it is worth noting that both collective and personal human development are associated with traversing increasingly greater distances. The turning points of human evolution include developing tools, which required planning for the future; making function-specific tools, which required considering hypothetical alternatives; developing consciousness, which enabled the recognition of distance and perspective taking; developing language, which enabled forming larger and more complex social groups and relations; and domestication of animals and plants, which required an extended temporal perspective (Flinn, Geary, & Ward, 2005). Human history is associated with expanding horizons: traversing greater spatial distances (e.g., discovering new continents, space travel), forming larger social groups (families vs. cities vs. states vs. global institutions), planning and investing in the more distant future, and reaching farther back into the past. Human development in the first years of life involves acquiring the ability to plan for the more distant future, consider possibilities that are nonpresent, relate to and take the perspective of more distant people (from self-centeredness to acknowledging others, from immediate social environment to larger social groups; Suddendorf & Corballis, 2007). Although the areas of evolution, history, and child development have different time scales, research in these domains seems to converge on the notion that transcending the present requires and is enabled by the human capacity for abstract mental representation. We hope that the present research on psychological distance, its relationship to level of construal, and its consequences for thought, feeling, and action advances our understanding of how individuals and groups transcend the here and now. Example of the four Spatial Location × Word Meaning combinations in Bar-Anan, Liberman, Trope, and Algom (2007). The illustration demonstrates words related to hypotheticality. Sample items from the Street Gestalt Completion Test (Street, 1931), provided courtesy of Teachers College, Columbia University. Identifying the pictures (from top right to bottom left: a boat, a rider on a horse, a rabbit, a baby) requires visual abstraction. Example of four Social Distance × Medium (picture vs. word) combinations in Amit, Algom, and Trope (2009). For the Israeli participants in this study, a shekel was socially proximal, and a dollar was distal."
        ],
        "ground_truth_definitions": {
          "psychological distance": {
            "definition": "a subjective experience that something is close or far away from the self, here, and now.",
            "context": "They serve to transcend the immediate situation and represent psychologically distant objects. Psychological distance is a subjective experience that something is close or far away from the self, here, and now. Psychological distance is thus egocentric: Its reference point is the self, here and now, and the different ways in which an object might be removed from that point-in time, space, social distance, and hypotheticality-constitute different distance dimensions.",
            "type": "explicit"
          },
          "high-level construal": {
            "definition": "relatively abstract, coherent, and superordinate mental representations, compared with low-level construals.",
            "context": "On the basis of theories of categorization (Rosch, 1975), concept formation (Medin & Smith, 1984), and action identification (Vallacher & Wegner, 1987), we view high-level construals as relatively abstract, coherent, and superordinate mental representations, compared with low-level construals. Moving from a concrete representation of an object to a more abstract representation involves retaining central features and omitting features that by the very act of abstraction are deemed incidental.",
            "type": "explicit"
          },
          "correspondence bias": {
            "definition": "a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions",
            "context": "A large body of person-perception research has shown that people are biased toward attributing others' behavior to the corresponding personal dispositions, even when the behavior is situationally constrained (see Gilbert & Malone, 1995; E. E. Jones, 1979). In terms of CLT, this bias, called the correspondence bias, reflects a tendency toward a high-level construal of behavior in terms of abstract, decontextualized dispositions (see Fiedler et al., 1995; Semin & Fiedler, 1988; Semin & Smith, 1999). Distance should therefore enhance the correspondence bias.",
            "type": "explicit"
          },
          "centrality": {
            "definition": "Changing a high-level feature has a greaterimpact on the meaning of an object than does changing a low-level feature",
            "type": "explicit",
            "context": "The first criterion reflects centrality: Changing a high-level feature has a greater impact on the meaning of an object than does changing a low-level feature. For example, a lecture would change more when the speaker is changed than when the room is changed, suggesting that the speaker is a higher level feature of the lecture than the room is."
          },
          "subordination": {
            "definition": "The meaning of low-level features depends on highlevel features more than vice versa",
            "type": "explicit",
            "context": ". The second criterion reflects subordination: The meaning of low-level features depends on highlevel features more than vice versa. For example, when learning about a forthcoming guest lecture, location would become important only if the topic is interesting. T"
          }
        }
      },
      {
        "paper_id": "2106.16117v1",
        "sections": [
          "The concept of the 'energy security' was first introduced by industrialized market economies in 1947, when the United States adopted a document regulating the actions of the state in the field of national security. Although it has been investigated and developed by a wide range of scholars and specialists, still there is no universal understanding of what is meant by this notion.",
          "In international relations, the term 'energy security' acquired significance and became widely used after the global oil crisis of 1973-1974, when the world faced a sudden and noticeable increase in prices for fuel and energy resources caused by the following political events [1]: • collective embargo imposed by the Arab OPEC countries on the export of oil to the USA; • support of Israel in the October war by a number of European countries. Being faced with the first largest energy crisis, industrial countries acutely felt their vulnerability and consequently energy security became one of the top priorities, considered a part of the country's national security. As mentioned above, currently, despite the abundance of modern research and publications on energy topics, a single generally accepted definition is still not formed. In point of fact, the meaning of energy security differentiates from country's dependence to their energy imports. Accordingly, countries which are highly dependent on imported oil and gas adheres energy security to supply whereas, countries which export oil and gas adheres energy security to demand. This is due to the fact that different groups of countries, namely exporting countries, importing countries and transit countries interpret this concept in their own way, based on their national and economic interests and priorities. Therefore, in order to give a clear definition of the concept of \"energy security\", it is imperative to consider it from the perspective of all three parties. But, nevertheless, in modern scientific literature there are many concepts and interpretations of international energy security, and representatives of different schools or countries offer their own definitions of this notion. The International Energy Agency (IEA), which is an international body that unites 29 industrialized countries under the Organization for Economic Development and Cooperation, defines energy security as 'the uninterrupted availability of energy sources at an affordable price' [2]. The limitations of this approach are directly implied by the fact that in this context, it considers energy security from the point of view of importing countries that reflects their national priorities in the face of growing dependence on imports of oil and natural gas. Nevertheless, the IEA admits that the concept of 'energy security' can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces 'timely investments to supply energy in line with economic developments and environmental needs', while the latter deals with 'the ability of the energy system to react promptly to sudden changes in the supplydemand balance' [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply. Beijing defines 'energy security' as 'reliable uninterrupted supply of energy, which is necessary for economic development of the country' [1]. In other words, energy security is seen as a need for uninterrupted supply of energy resources in order to maintain stable economic growth. Meanwhile, the exporting countries, including Russia, do not share this position. They are primarily interested in diversifying the markets of energy exports, preserving their national sovereignty and of course guaranteeing sustainable demand for the energy they export. Because of diverging interests in the energy market, exporting countries interpret 'energy security' in another way. The OPEC group (a permanent intergovernmental organization of 15 oilexporting developing nations that coordinates and unifies the petroleum policies of its Member Countries [4]) defines energy security as 'reliable and environmentally friendly energy supplies at prices reflecting fundamental principles of market economy'. This approach is also limited, as the IEA one, because it reflects only the interests of exporting countries that are focused on determining a decent price for energy resources. The interests of transit countries consist mainly in maximizing transit rent of energy resources. This is the approach Turkey follows, -the country is a natural energy corridor between the Middle East, the Caspian basin and Europe and contributes significantly to the attempts of Europe to diversify its energy suppliers for natural gas. In the meantime, the concept of energy security framed as an access to fossil fuels created the so-called 'energy nationalism' [3] that created a reality in which the behavior and decisions made on energy markets and the delivery of resources ultimately depended not on the economic market factors, but on producers, which resulted in the transformation of the energy market into the subject of inter-state relations. Oil and natural gas were used as geopolitical weapons, and geopolitics and geo-economics became an important part of world politics and foreign policy of the main players on the energy market. Energy security undoubtedly depends on the national and international background. In the context of state-controlled markets, the main guardians of energy security are the governments. On the contrary, given the energy markets are liberalized and the main actors are private companies, the security of supply consists of an efficient risk management strategy by governments, companies and consumers. Therefore, it is important not to fall into purely ideological approaches. Since Adam Smith, the state is authorized to intervene in the economy in order to prove security, including its energy component, to its citizens. Baring that in mind, it should be noted that such intervention should be done as far as possible in a concerted manner along with the companies and consumers, following the principle of subsidiarity. At the same time, energy security is associated with the evolution of the energy markets, the geopolitical situation and long-term international scenarios. There are several scenarios drawn for the long term that extrapolate to some extent conceptual differences, for example, the 'Markets and Institutions' scenario and its alternative called 'Empires and Regions' [3], and the three scenarios drawn by Shell Global Scenarios [5]: 'Low Trust Globalization', 'Open Doors' and 'Flags'. The 'Low Trust Globalization' is based on the trade-off between efficiency and security, and is characterized by limited international integration, intrusive state intervention and institutional discontinuities. The 'Open doors' arises from the dilemma between efficiency and environmental sustainability, premium market incentives and the participation of the civil society. It highlights the urgent necessity of transnational integration and harmonization, and mutual recognition of standards. The 'Flags' responds to the mercantilist logic and involves regulatory fragmentation, nationalism and the conflict between the values of the different regions. To some extent, the differentiation of scenarios roots in the neoliberal and neorealist paradigms to international energy issues. It is an old debate between two alternative visions of the world: a world in which the market disturbances are resolved by cooperation, or a fragmented world conceived as a billiard table where conflicts are resolved through the exercise of the political, economic and military power. Under the neoliberal paradigm, that is so praised by the EU, energy security is achieved through the development of markets and the management of conflicts at the multilateral level through supranational institutions. The neorealist paradigm of energy security, instead, implies the development of bilateral relations and the subordination of markets to foreign policy. The potential risks and threats related to energy security arise mainly from two circumstances: • the projected upcoming peak in the production of hydrocarbon resources, which is vital for the modern economy, and • the security of their supplies. However, recently the energy sector has started to develop a few key trends that were caused by new, very strong factors: the global financial and economic crisis and the shale revolution in the production of oil and gas. Today, energy security policies require a paradigm shift and a new model of factors and conditions for its implementation. The first factor that radically changed the context of energy policy was the global economic crisis. Since 2008, experts have determined it as the financial crisis, the economic crisis, the crisis of democracy and governance, the crisis of the culture of public consumption and material culture in general, and the environmental crisis that will eventually lead to global natural disasters. It would be reasonable to say that the world experienced a multidimensional global crisis, or the first systemic crisis of the global era. In the energy sector, this crisis coincided with the start of the gradual transition from 'industrial' and 'hydrocarbon' to 'neoindustrial' and 'smart' energy, which includes the following aspects: smart grids, energy efficiency (in a broad sense), renewable energy, new principles of energy systems and focus shift from producers to consumers [3]. The second factor that dramatically changed the energy markets was the quiet shale revolution in oil and gas production. The shale revolution, which has become a reality in the United States and Canada, as estimated by the experts, will have serious consequences for the global energy market. The unconventionally produced natural gas fundamentally changed the world market. The most serious consequence of the shale gas revolution is a shift in the focus from producers to consumers. In the context of the old paradigm, energy security is directly related to energy independence. The idea was that if a country is self-sufficient with respect to energy resources, and has an efficient (energy-saving) economy, then this will be accompanied with lower prices for energy carriers. The reality of oil prices in the United States after the shale boom has shown that this is utopia.",
          "Meanwhile, achieving self-sufficiency in the energy sector is almost impossible. Even such countries as Russia, Saudi Arabia, Venezuela, Brazil and Canada, which are rich in hydrocarbons, import a part of the energy in the form of refined petroleum products due to insufficient refining opportunities. This dependence could theoretically be eliminate with a little effort and investment in the construction of new refineries, but in practice it is not happening. Revolutionary changes of the energy security require a paradigm shift, which should be reflected in the energy security policies. Currently we are on the verge of transition to a post-industrial, 'smart' energy system, which means 'smart' networks, alternative energy sources for transport, decentralization of energy, integration of energy into the technical sphere, accompanied by an increase in energy efficiency. To conclude, it can be said that the definition of 'energy security' can be complete only if the interests of all the energy market participants are taken into account, and namely importing countries, exporting countries and transit countries. Interests and priorities of all three parties are different, which presents a difficulty in agreeing on a common concept. At the same time, with the paradigm shift all of the actors of the energy market are interested in ensuring the reduction of geopolitical and environmental risks and thus creating new opportunities."
        ],
        "ground_truth_definitions": {
          "short-term energy security": {
            "definition": "the ability of the energy system to react promptly to sudden changes in the supplydemand balance",
            "context": "Nevertheless, the IEA admits that the concept of ‘energy security’ can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‘timely investments to supply energy in line with economic developments and environmental needs’, while the latter deals with ‘the ability of the energy system to react promptly to sudden changes in the supply- demand balance’ [2]. Nowadays one of the major energy importing country is China that is primarily preoccupied with the security of the energy supply.",
            "type": "explicit"
          },
          "long-term energy security": {
            "definition": "timely investments to supply energy in line with economic developments and environmental needs",
            "context": "Nevertheless, the IEA admits that the concept of ‘energy security’ can be divided into two major groups: long-term energy security and short-term energy security. The first one embraces ‘timely investments to supply energy in line with economic developments and environmental needs’, while the latter deals with ‘the ability of the energy system to react promptly to sudden changes in the supply- demand balance’ [2].",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "dx.doi.org/https://doi.org/10.1016/j.ipm.2021.102505",
        "sections": [
          "Media has a substantial impact on public perception of events, and, accordingly, the way media presents events can potentially alter the beliefs and views of the public. One of the ways in which bias in news articles can be introduced is by altering word choice. Such a form of bias is very challenging to identify automatically due to the high context-dependence and the lack of a large-scale gold-standard data set. In this paper, we present a prototypical yet robust and diverse data set for media bias research. It consists of 1,700 statements representing various media bias instances and contains labels for media bias identification on the word and sentence level. In contrast to existing research, our data incorporate background information on the participants' demographics, political ideology, and their opinion about media in general. Based on our data, we also present a way to detect bias-inducing words in news articles automatically. Our approach is feature-oriented, which provides a strong descriptive and explanatory power compared to deep learning techniques. We identify and engineer various linguistic, lexical, and syntactic features that can potentially be media bias indicators. Our resource collection is the most complete within the media bias research area to the best of our knowledge. We evaluate all of our features in various combinations and retrieve their possible importance both for future research and for the task in general. We also evaluate various possible Machine Learning approaches with all of our features. XGBoost, a decision tree implementation, yields the best results. Our approach achieves an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79, which outperforms current media bias detection methods based on features. We propose future improvements, discuss the perspectives of the feature-based approach and a combination of neural networks and deep learning with our current system.",
          "News articles in online newspapers are considered a crucial information source that replace traditional media such as television or radio broadcasts and print media, and new sources of information such as social media (Dallmann, Lemmerich, Zoller, & Hotho, 2015). Many people consider such articles a reliable source of information about current events, even though it is also broadly believed and academically confirmed that news outlets are biased (Wolton, 2017). Given the trust readers put into news articles and the significant influence of media outlets on society and public opinion, media bias may potentially lead to the adoption of biased views by readers (Spinde, Hamborg, Donnay, Becerra, & Gipp, 2020a). However, ''unrestricted access to unbiased information is crucial for forming a well-balanced understanding of current events'' (Hamborg, Donnay, & Gipp, 2018). Highlighting media bias instances may have many positive implications and can mitigate the effects of such biases (Baumer, Elovic, Qin, Polletta, & Gay, 2015). While complete elimination of bias might be an unrealistic goal, drawing attention to its existence can not only warn readers that content is biased but also allow journalists and publishers to assess their work objectively (Dallmann et al., 2015). Furthermore, such insights could be very interesting for research projects, e.g., in social science. We want to point out that it is uncertain if and how actual news consumers would like to obtain such information. Only a few systems are already employed to help readers mitigate the consequences of media bias impact. Most of them focus on the aggregation of articles about the same event from various news sources to provide different perspectives (Lim, Jatowt, & Yoshikawa, 2018a). For example, the news aggregator Allsides 1 allows readers to compare articles on the same topic from media outlets known to have different political views. Various media bias charts, such as the Allsides media bias chart, 2 or the Ad Fontes media bias chart 3 provide up-to-date information on media outlets' political slants. The main objective of this paper is to present a prototypical system for the automated identification of bias-inducing words in news articles. In the following chapter, we will give an overview of research on the topic and show major currently existing drawbacks on the issue. They lead us to more fine-grained research contributions. Mainly, we will: 1. create a labeled data set for media bias analysis on different levels; 2. analyze and engineer features potentially indicating biased language; 3. train a classifier to detect bias-inducing words and 4. evaluate the performance. This study holds both theoretical and practical significance. We summarize all existing research to give a full overview of possible classification features for media bias. We also show the relevance of all these features. We provide a data set of media bias annotations. It is the first such data set in the field, reporting word and sentence level annotations and detailed information on annotator characteristics and background. Our current data set already significantly extends available data in this domain, providing a unique and more reliable insight into bias perception. It also offers grounds for future extension. Lastly, we train and present a classifier for biased words that outperforms existing feature-based classifiers for bias. The rest of the paper is organized as follows. Section 2 presents the literature review on media bias and its automated detection. Section 3 details the methodology, and Section 4 presents the results of this study. Finally, in Section 5 we present a discussion of the project and an outlook on future work.",
          "",
          "Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975). On the other hand, bias can also be unintentional and even unconscious (Baumer et al., 2015;Williams, 1975). Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently. In turn, an author can manipulate a reader's perception by implying a particular opinion or perspective or inducing positive or negative emotions. The following two examples present instances of media bias by word choice, respectively: 1. Practicing pro-life litigators know that Trump judges are saving lives by permitting restrictions on abortion to go into effect. 4 2. Tens of millions of children under 12 months are potentially at risk for diseases such as diphtheria and polio as the Chinese coronavirus pandemic interrupts routine vaccinations, according to data published by global public health experts on Friday. 5 In the first example, the author chooses the vaguer word ''pro-life'' to describe the very concrete ''anti-abortion'' position as highly positive. In the second example, labeling the coronavirus pandemic with the word ''Chinese'' implies China's fault in the pandemic.",
          "To the best of our knowledge, there are no tools or systems for automatic identification of media bias based on word choice. The task is a challenging one for several reasons. Firstly, while a vast amount of text data from the news is available, articles naturally are created without such sophisticated labels that could allow us to detect bias inducing words; therefore, existing data are unlabeled. Existing annotated data sets are very small. That means that currently, there is no large-scale gold standard data set for the identification of media bias by word choice (Hamborg et al., 2018). Furthermore, it has been proven that bias identification is a non-trivial task for non-experts (Lim et al., 2018a;Recasens, Danescu-Niculescu-Mizil, & Jurafsky, 2013), which can cause problems while creating such a data set. Secondly, bias words are highly dependent on the context. Therefore, simple presence of specific words is not a reliable indicator of bias (Hube & Fetahu, 2018). 1. An abortion is the murder of a human baby embryo or fetus from the uterus, resulting in or caused by its death (Hube & Fetahu, 2018). 2. In 2008, he was convicted of murder (Hube & Fetahu, 2018). In the first example, the word ''murder'' describes abortion as something highly negative. In the second example, ''murder'' is used to describe a pure fact of murder. In the first case, the word induces bias, whereas, in the second, it does not. Another good example of context dependence of bias is the bidirectionality of the epistemological bias, i.e., this bias can occur in two cases: when a truthful proposition is questioned or when a false or controversial proposition is presupposed or implicated. The word that will not cause bias in the first case, e.g., factive verb, will cause it in the second, and vice versa (Recasens et al., 2013). Finally, media bias and bias in reference works are subtle, implicit, and intricate since, in general, the news is expected to be factual and impartial (Baumer et al., 2015;Hube & Fetahu, 2018;Lim et al., 2018a;Recasens et al., 2013). As fairly noticed by Williams (1975), remarkable bias is not extreme but rather reasonable and plausible: extreme bias is obvious and does not threaten values or institutions. Despite the challenging nature of the task, several researchers attempted to annotate media bias by word choice automatically. We also derive valuable insights for this project from the research attempting to identify the biased language in the related fieldreference sources such as encyclopedias, where neutral language is also desired and required. Lim et al. (2018a) use crowdsourcing to construct a data set consisting of 1235 sentences from various news articles reporting on the same event, namely, the arrest of a black man. The data set provides labels on the articles and word level. The authors then train a Support Vector Machine on the Part of Speech (POS) tags and various handcrafted linguistic features to classify bias on the sentence level, achieving the accuracy of 70%. In related work, Lim, Jatowt, Färber, and Yoshikawa (2020) propose another media bias data set consisting of 966 sentences and containing labels on the sentence level. The data set covers various news about four different events: Donald Trump's statement about protesting athletes, Facebook data misuse, negotiations with North Korea, and a lawmaker's suicide. Baumer et al. (2015) focus on the automated identification of framing in political news and construct a data set of 74 news articles from various US news outlets covering diverse political issues and events. They then train Naïve Bayes on handcrafted features to identify whether a word is related to framing, and achieve 61% accuracy, 34% precision, 70% recall, 0.45-0.46 𝐹 1 -score. Hamborg, Zhukova and Gipp (2019) constructed a data set using content analysis. They created a codebook describing frame properties, coding rules, and examples. The data set consists of 50 news articles from various US news outlets and covers ten political events. The authors distinguish the target concept and phrases framing this concept, and define a number of framing properties, e.g., ''affection'', ''aggression'', ''other bias'', etc. The authors then automatically extract the candidates for target concepts and identify frames by looking for words semantically similar to the previously defined framing properties via exploiting word embeddings properties. Then, identified framing properties are assigned to the candidates via dependency parsing. The authors achieve an F1-score of 45.7%. Fan et al. (2019) create the data set BASIL, annotated by two experts, covering diverse events and containing lexical and informational bias. The data set allows analysis at the token level and relatively to the target, but only 448 sentences are available for lexical bias. Then, they employ BERT lexical sequence tagger to identify lexical and informational bias at the token level and achieve an 𝐹 1 -score of 25.98%. Chen, Al-Khatib, Wachsmuth, and Stein (2020) create a data set of 6964 articles covering various topics and news outlets containing political bias, unfairness, and non-objectivity labels at the article level. They then train the recurrent neural network to classify articles according to these labels. Finally, the authors conduct a reverse feature analysis and find that, at the word level, political bias correlates with such LIWC categories (Pennebaker, Boyd, Jordan, & Blackburn, 2015) as negative emotion, anger, and affect. Recasens et al. (2013) create static bias lexica based on Wikipedia bias-driven edits due to NPOV (Neutral Point of View) violations. 6 The bias lexicon and a set of various linguistic features are then fed into the logistic regression classifier to predict which words in the sentences are bias-inducing. The authors reached 34.35% to 58.70% accuracy for predicting 1 to 3 potential bias-inducing words in a sentence, respectively. Hube and Fetahu (2018) propose the semi-automated approach to extract domain-related bias words lexicon, based on the word embeddings properties. The authors then feed obtained bias words and other linguistic features into a random forest classifier to detect language bias in Wikipedia at the sentence level. The authors achieve 73% accuracy, 74% precision, 66% recall, and an 𝐹 1score 0.69 on the newly created ground truth based on Conservapedia,foot_1 and state that the approach is generalizable for Wikipedia with a precision of 66%. In their later work, Hube and Fetahu (2019) train a recurrent neural network on a combination of word embeddings and a few handcrafted features to classify bias in Wikipedia at the sentence level and achieve 81.9% accuracy, 91.7% precision, 66.8% recall, and 0.773 𝐹 1 -score. Spinde, Hamborg, andGipp (2020b, 2020c) analyze media bias in German news articles covering the refugee crisis. The three components: an IDF component, a combined dictionary-based component, and a component based on a semantically created bias dictionary, are analyzed to identify bias on the word level. The combination of the dictionary component and the topic-dependent bias word dictionary achieves an 𝐹 1 -score of 0.31, precision of 0.43, and recall of 0.26. The authors point out that considering adjectives separately increased the performance.",
          "",
          "The general workflow of a prototypical system for automated identification of bias-inducing words in news articles is presented in Fig. 1. In our work, we start from collecting the sentences and gathering annotations via a crowdsourcing process (1). We then obtain various features (3) described in more detail in Section 3.5. One of the features is a bias lexicon built semi-automatically by computing words similar to potential bias words using outlet-specific word embeddings (2). We then train a supervised classifier on our engineered features and annotated labels (4). After the best model is selected and optimized, we evaluate the performance of the feature-based approach for detection of media bias. Furthermore, we evaluate all features individually (5).",
          "One of the challenges in the automated detection of media bias is the lack of a gold standard large-scale data set with labeled media bias instances. The existing data sets described in Section 2.2 either do not allow for the analysis of media bias on the word level or can induce drawbacks due to the following limitations: (1) they only include a few topics (Lim et al., 2020(Lim et al., , 2018a)), (2) they mostly focus exclusively on framing (Baumer et al., 2015;Hamborg, Zhukova et al., 2019), (3) annotations are target-oriented (Fan et al., 2019;Hamborg, Zhukova et al., 2019), (4) annotations are not on the word level (Lim et al., 2018a), or (5) training data are too small (Fan et al., 2019). Therefore, we decided to create a diverse, robust, and extendable data set to identify media bias. We hand-picked 1.700 sentences from around 1.000 articles. According to our collection strategy, most of the sentences should contain media bias instances, while the smaller number of sentences should be neutral. However, the final annotations are made by the crowd-source annotators. The sentences equally represent the full political spectrum since we used the articles from the major left and right-wing outlets, classified by their political ideology within the media bias ratings of www.allsides.com. We covered 14 different topics (selected randomly out of a variety of possible topics), from very contentious (e.g., abortion, elections) to less contentious topics (e.g., student debt, sport). To gather annotations of the sentences and the words, we developed our own survey platform, combining classical survey functionality with text annotations, and hired participants via Amazon Mechanical Turk to complete microtasks. Annotation quality ensured by experts is often preferable, but we expressively wanted to collect a large number of annotations from non-experts. It has been shown that many complex problems can be resolved successfully through crowdsourcing if the existing crowdsourcing platforms are used in combination with appropriate management techniques and quality control (Mitrović, 2013;Mladenović, Mitrović, & Krstev, 2016)  8 . Seven hundred eighty-four annotators participated in the survey, all located in the United States. The vast majority (97.1%) of the annotators were native English speakers, 2.8% were near-native speakers. The annotators from diverse age groups participated in the survey; people from 20 to 40 years old (67.4%) prevail over other age groups. The annotators' gender is balanced between females (42.5%) and males (56.5%). The annotators have a diverse educational background; more than half have higher education. The political orientation is not well balanced: liberal annotators prevail (44.3%) over conservative annotators (26.7%) and annotators from the center (29.1%). The vast majority of the annotators read the news sometimes, more than half -one (46.4%) or more (23.1%) times per day. Each annotator received 20 randomly reshuffled sentences. We showed each sentence to ten annotators. Within our platform,foot_3 we first instruct participants about the general goals of the study. We explain the tasks in detail and ask them to leave aside their personal views. We also give them a few examples of bias and ask a control question to check whether participants understood media bias's general concept. If the control question was not answered correctly, participants had to reread the instructions. Within the annotation task itself, we provide detailed instructions on the workflow. We then ask each annotator to highlight words or phrases that induce bias according to the provided instructions. After that, we ask them to annotate the whole sentence as biased or impartial, and whether they would describe it as opinionated, factual, or mixed. To the best of our knowledge, our data set is the first in the research area to collect detailed background demographic information about the annotators, such as gender, age, education, English proficiency, but also information on political affiliation and news consumption. Overall, our data set allows for performing three different tasks: bias identification on the word level, sentence level, and a classification of the sentence as being opinionated, factual, or a mixture of both. We discuss the results of the annotation in Section 4.1.",
          "As one of our features, we present a lexicon of biased words, built explicitly for the news domain (Hube & Fetahu, 2018). Interestingly, although such a lexicon cannot serve as an exhaustive indicator of media bias due to high context-dependence (Hube & Fetahu, 2019), it can potentially serve as a useful feature of a more complex media bias detection system. To extract a biased word lexicon of high quality, we replicate the method proposed by Hube and Fetahu (2018). The authors proposed a semi-automated way to automatically extract biased words from corpora of interest using word embeddings. We present the whole pipeline of the approach in Fig. 2. We first manually create a list of words that describe contentious issues and concepts. Then, we use this list to manually select ''seed'' biased words in the two separate word embedding spaces trained on news articles potentially containing a high number of biased words. We select seed biased words among the words that have high cosine similarity to the words describing contentious issues. We publish our list of seed biased words at https://bit.ly/36guHdu. 10We assumed that news outlets with presumably stronger political ideology would use bias words when describing contentious issues with a higher likelihood than neutral mediums. To capture both liberal and conservative biases, we train word embeddings separately on the corpora of news articles from HuffPost and Breitbart, respectively. In the choice of the outlets, we relied on the information provided by Allsides: both outlets are presented at the media bias chart,foot_5 and for both outlets, the confidence level of the assigned ratings is high.foot_6 Noteworthy, these two sources are also ones of the most popular media sources that left-and right-leaning communities share respectively in Soliman, Hafer, and Lemmerich (2019). The articles from both sources, published from 2010 to 2020, are scraped from Common Crawlfoot_7 using NewsPlease (Hamborg, Meuschke, Breitinger and Gipp, 2017). We split the initial text into lower-cased tokens, remove punctuation marks and numbers, and train Word2Vec word embeddings (Mikolov, Chen, Corrado, & Dean, 2013). The chosen hyper-parameters are summarized in Table 1. Since evaluation of such an unsupervised task as word embeddings creation is quite challenging (Bakarov, 2018), we choose the hyper-parameters based on the existing research (Spinde, Rudnitckaia, Hamborg, & Gipp, 2021a). The number of dimensions is set to 300 and is not increased further due to the scarcity of the training data (Mikolov et al., 2013). The window size is set to 8 since  larger window size can capture broad topical content (Levy, Goldberg, & Dagan, 2015). We increase the number of iterations to 10 since the size of training data is small and cannot be increased. In the scope of this project, it is important to avoid unstable low-qualitative vectors, therefore words appearing less than 25 times are excluded. Finally, treating n-grams as single units may lead to better training of a given model (Camacho-Collados & Pilehvar, 2018). We use the default scoring for n-grams generation and run two passes over training data. The thresholds for n-grams inclusion are based on manual analysis of the generated n-grams. The rest of hyper-parameters are set to the default values. As a next step, we divide the set of seed biased words into random batches consisting of ten words and repeat this process ten times to create batches with various combinations of words. Then, for each batch, the average vector in the word embedding space trained on a 100 billion Google news data setfoot_8 is calculated. For each average vector, we extract the top 100 words close to this average vector. Hube and Fetahu (2018) do not reshuffle words in batches and extract the top 1000 words. However, the average cosine similarity of farthest words among the top 1000 is 0.47, whereas the average cosine similarity of farthest words among the top 100 is 0.52. Besides, extracting the top 1000 words introduces noise. Finally, we add extracted words to the resulting bias word lexicon, and remove duplicates.",
          "We define bias-inducing words detection as a binary classification problem where we have only two mutually exclusive classes: whether a word is biased (class 1) or not (class 0). With our binary classifier, and in the context of media bias by word choice, no exhaustive set of precise media bias characteristics exist. Therefore, we combine different linguistic features of biased language proposed by Recasens et al. (2013) and a variety of other syntactic and lexical features (Lim et al., 2020). As the context is highly important when distinguishing between unbiased and biased words, we attempt to capture useful information from context by including collective features adding two previous and two following words into a word's feature vector. We admit that such a way T. Spinde et al.",
          "The complete set of features used in our approach for detecting biased words.",
          "POS tags POS tag indicating the syntactic role of each word, e.g., noun, adverb, etc. Honnibal and Montani (2017).",
          "Dependencies revealing how words in the text relate to each other, e.g., whether a word is a root, object, or subject (Bird, Loper, & Klein, 2009;Honnibal & Montani, 2017).",
          "Named entities, e.g., persons, organizations, locations, etc. Bird et al. (2009) and Honnibal and Montani (2017).",
          "Norms of GloVe word embedding vectors pre-trained on the Common Crawl a Honnibal and Montani (2017). TF-IDF Frequency of the term in a document and in the whole article collection (Lim, Jatowt, & Yoshikawa, 2018b;Pedregosa et al., 2011).",
          "Word is a report/implicative/assertive/factive/positive/negative word, is strongly or weakly subjective, or a hedge (Recasens et al., 2013).",
          "Classifications as kill verb (Greene & Resnik, 2009), hyperbolic term (Chakraborty, Paranjape, Kakar, & Ganguly, 2016), boosters, and attitude markers (Hyland, 2019). LIWC features LIWC features based on psychological and psychometric analysis (Pennebaker et al., 2015). Semi-automated bias lexicon Previously described semi-automatically created bias word lexicon (Hube & Fetahu, 2018). a https://commoncrawl.org, accessed on 2020-10-31. to account for context is not optimal and requires elaboration in future. We compare different combinations of the features, and also train different machine learning classification algorithms, such as logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), complement Naïve bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), decision tree (DT), random forest (RF), XGBoost and a simple neural network -multilayer perceptron (MLP). To the best of our knowledge, we present the first detailed comparison of classifiers for word-level bias detection.",
          "We present our entire feature list in Table 2. In continuation, we describe the individual features and the intuition behind using them for our task. For POS tags, syntactic dependencies, named entity types, word vector norms, and linguistic features, we refer to the previous work on these topics as described by Hube and Fetahu (2018) and Recasens et al. (2013). TF-IDF. Lim et al. (2018b) propose detection of bias using inverse document frequency (IDF) as one of the features to detect media bias, under the assumption that more rarely occurring terms are more likely to be extreme in any direction and are hence more likely to induce bias. As our data set consists of sentences where the terms are probably rarely repeated within one sentence, we adjusted Lim et al.'s assumption (Lim et al., 2018b) slightly by calculating the TF-IDF statistic basing on the whole text of articles the sentences were collected from. LIWC Features. Linguistic Inquiry and Word Count (Pennebaker et al., 2015) is a common approach to analyzing various emotional, cognitive, and structural components in language. It identifies linguistic cues related to psychological processes such as anger, sadness, or social wording. We consider all feature categories from LIWC, as this has shown to be the most effective usage of the resource to identify bias. Additional lexical features. It is not well known which features are the most efficient indicators of media bias (Baumer et al., 2015). Therefore, we test additional features that have been used by researchers to study similar constructs but have not been applied for the detection of media bias yet. According to Greene and Resnik (2009), the so-called ''kill verbs'' together with the relevant grammatical relation (governor or dependent term) cause different sentiment perception. In the following example, the second one is perceived as more negative since it implies an intention (Yano, Resnik, & Smith, 2010): 1. Millions of people starved under Stalin. 2016) study click baits in online news and find that click bait headlines usually include hyperbolic wordswords with highly positive sentiment. We assume that hyperbolic words used in click bait titles to attract readers' attention can be used to emphasize some concepts and induce bias in news articles. Hyperbolic words are, for example, ''absolutely'', ''brilliant'', or ''impossibly''. Hyland (2019) introduces linguistic features that help authors to express their views on the discussed proposition. One such subcategory are boosters -words that express certainty about a particular position, e.g., ''believed'', ''always'', ''no doubt''. In some regard, boosters are opposite to hedges, which, on the contrary, reduce the confidence of a statement. Another subcategory are attitude markers -indicators of the author's expression of affective attitude to statements, e.g., ''fortunately'', ''shockingly'', ''disappointed'', etc. ",
          "",
          "",
          "In the final data set, classes are highly imbalanced (Section 4.1). Since accuracy does not capture the capability of a model to predict rare class correctly (Branco, Torgo, & Ribeiro, 2015), we focus on such evaluation metrics as confusion matrix, precision, recall, F1-score, and receiver operating characteristic area under curve (ROC AUC). We compare the performance of our system to several possible baselines: • Baseline 1 (B1) -a purely random classifier; • Baseline 2 (B2) -occurrence of a word in the negative sentiment lexicon; • Baseline 3 (B3) -occurrence of a word in the negative or positive lexicon; • Baseline 4 (B4) -occurrence of a word in the semi-automated bias lexicon. In our final data set, each observation corresponds to one word, its feature vector (including the collective context features), and the label. We perform 10-fold cross-validation when compare performance of different classifiers, 5-fold cross-validation when optimize hyper-parameters of the selected model, and finally, estimate the final performance on a test set of words that did not participate in training and manually investigate correctly and wrongly classified instances.",
          "",
          "To gain insights into the characteristics of our data set, we analyze it quantitatively and qualitatively. The results of sentences classification are presented in Figs. 3 and 4. The annotation results confirm our data sampling strategy: biased and non-biased statements are not balanced in the data set: biased statements prevail over non-biased statements. Besides, most media bias instances are taken from liberal and conservative news sources, whereas sources from the center were used mainly to retrieve non-biased statements. Note that this does not imply that liberal and conservative news outlets generally experience media bias by word choice and provide opinionated news more often than news outlets from the center. It is valid only due to our data collection scheme. We assigned a biased or impartial label to a sentence if more than half of respondents annotated a sentence as biased or impartial, respectively. 149 sentences could not be labeled due to a lack of agreement between annotators. Many measures for assessing interannotator agreement have been used in similar computational linguistics projects. Based on the way in which we have organized our crowdsourcing workflow, i.e. having 10 annotators per task, we have decided to use Fleiss (1971) to assess the inter-annotator agreement. It represents the task's general difficulty: for example, Hube and Fetahu (2018) reported 𝛼 = 0.124 on word-level bias, and Recasens et al. (2013) reported a 40.73% agreement when looking at only the most biased word in Wikipedia statements. The value of 0.21 that we achieved can be considered as a fair agreement. We assigned an opinionated, factual, or mixed label to a sentence if most respondents annotated a sentence as opinionated, factual, or mixed, respectively. We could not label 174 sentences due to the lack of agreement between annotators. According to our crowdsourced annotations, the data set contains an almost equal number of factual, opinionated, and mixed statements. The annotation scheme for biased words allowed respondents to highlight not only the words but also short phrases. A word was considered biased if at least four respondents highlighted it as biased. On average, a sentence that contains biased words contains two biased words. Out of 31,794 words for training, only 3018 are biased, which constitutes only 9.5% of our current data. The types of words annotated as biased are presented in Table 3.  We observe that annotators select not only extreme and emotional words that can be considered biased even without context, but also context-dependent bias words. For instance, while the word ''Chinese'' is generally not biased it can be in specific contexts, such as ''House Democrats' Chinese coronavirus relief package bails out coastal millionaires and billionaires while ensuring big businesses are able to freely hire illegal aliens and visa overstayers over unemployed Americans''. 15Albeit emphasizing in the instructions that words that are connected to very controversial topics or have very negative sentiment are not necessarily biased, some of such words were still annotated as biased. For example, the term ''neo-Nazis'' in the sentence ''For years now, Fox News has been mainstreaming arguments that used to be the province of fringe websites run by neo-Nazis and other groups who believe the U.S. is meant to be a country of white people and for white people''. 16Furthermore, we find that annotators sometimes fail to mark words as biased if a sentence contains clearly extreme and emotional words. For example, a majority of annotators marked ''cray-cray'' as biased but did not notice ''totally'' in the sentence ''Over the past few decades, RFK Jr.'s famous name has helped him get in the door to talk to important people, and it probably is not long before the person who is all jacked up to meet a Kennedy realizes the guy is totally cray-cray''. 17  As expected, we find a positive correlation between marking sentences as biased and opinionated, and factual and non-biased. Furthermore, more controversial topics are annotated as non-biased, on average, 7.4 p.p. less than less controversial topics. Interestingly, in 49.3% of the sentences labeled as non-biased, annotators still labeled some words as biased. Annotators who estimate themselves as conservative mark 3.76 p.p. more sentences as biased than others who describe themselves as being liberal -except if the sentence stems from a conservative news outlet (Yano et al., 2010). Furthermore, annotators who report that they check news at least sometimes, label sentences as biased 6.85 p.p. more than those who report to check news very rarely, and 19.95 p.p. more than those who report that they never check news.",
          "In this section, we first present the characteristics of the articles we used to train our word embedding models and the performance of the trained word embeddings. We also provide the characteristics of the pre-trained Google News embeddings. We measure semantic word similarity and word analogy (Bruni, Tran, & Baroni, 2014;Finkelstein et al., 2001;Mikolov et al., 2013). Table 4 depicts the results of our measures. Two data sets -WordSim-353 (Finkelstein et al., 2001) and MEN (Bruni et al., 2014) -allow to estimate the Pearson correlation for the semantic similarity between pairs of words in respective word embeddings and as estimated by human assessors. The Google analogy test set (Mikolov et al., 2013) allows to evaluate accuracy. Even though those evaluation data sets are not perfectly suited for our task, the comparison shows that our data sets are large enough to give comparable results to the full Google News data set. We also manually inspected the embeddings' results and confirmed that they capture bias to a reasonable extent. Second, we qualitatively investigate the lexicon of biased words resulting from the semi-automated expansion (Section 3.3). We manually inspected a random sample of 100 words and find that the vast majority (Around 69% in a random sample of 100 words) are negatively connotated, are emotional, and convey strong opinion (Fig. 5). Furthermore, the dictionary consists of disproportionally many rather uncommon words (e.g., ''teapartiers'', ''obamian'', ''eggheaded'', ''mobocracy''), that are only interpretable when knowing about the circumstances they developed in. We find only one word (''similarly'') that cannot directly be related to bias, while we ourselves would classify all 99 other words as being very likely to induce bias. Among 96 words for which POS tag can be identified unambiguously, 41.7% are nouns, 24.0% are verbs, 21.9% are adjectives, and 11.5% are adverbs. Finally, we compare the method of batches developed by Hube and Fetahu (2018) to the naive approach where close words are retrieved for a single seed bias word instead of an average of a batch. We find the employed batched extraction to be superior. Specifically, while both approaches yield a high proportion of biased words, naive approach also yields many words that are not biased but co-occur with biased words, such as ''abortion'', ''personhood'', ''immigrants'', ''mexicans'', etc. Table 5 contrasts extraction for two methods. So far, the lexicon seems to be valuable especially in finding negatively connotated neologisms and words that convey strong opinion even by themselves. Despite being more efficient than the naive approach, the method of batches, nevertheless, still cannot avoid some degree of noisy words and words falsely included as biased. Among such words are misspellings, abbreviations, and words that describe a contentious or a negative issue or concept, e.g., ''xenophobia'', ''criticize'', ''anti-Semite'', ''solipsist''. We will further evaluate the resource in future work.",
          "We first train ''quick and dirty'' models (Gron, 2017) on all available features with default parameters (as implemented in Scikit-Learn (Pedregosa et al., 2011) and XGBoost (Chen & Guestrin, 2016) libraries) and compare the performance based on scores T. Spinde et al. averaged from ten-fold cross-validation (Pedregosa et al., 2011). We compare 𝐹 1 -score, precision, recall, and ROC AUC. Since data are imbalanced (only ≈ 10% are biased), weighting of classes is employed for all methods (where possible). Table 6 shows that no model yields a high 𝐹 1 -score. Instead, best performing models yield either high precision or high recall. Since results of our method are, for now, intended to be verified by a user, we prefer recall over precision while still aiming for moderately high 𝐹 1 -score. We choose XGBoost for further optimization since it achieved both the highest ROC AUC score and the highest 𝐹 1 -score. It also has a relatively high recall: the model predicts more True Positives (biased words as biased) than False Negatives (biased words as unbiased). The model suffers from predicting many False Positives (unbiased words as biased) but to the smaller extent than other models with higher recall (Logistic regression, QDA, NB, SVM). XGBoost is ''a scalable end-to-end tree boosting system'' (Chen & Guestrin, 2016). The algorithm is based on gradient boosting -an ensemble method that adds predictors sequentially to an ensemble, each new one is fit to the residual errors made by the previous one (Gron, 2017). Thus, the final model -a combination of many weak learners -is itself a strong learner. In addition to the fact that XGBoost already achieved best results on our data set, it provides several advantages: it accounts for sparsity caused by one-hot encoding (Chen & Guestrin, 2016), allows for a fine parameter tuning using a computationally efficient algorithm (Bentéjac, Csörgo, & Martínez-Muñoz, 2019), and allows to estimate feature importance since we do not have reliable prior information about the importance of the features (Baumer et al., 2015). We fine-tune five hyper-parameters that help to control for overfitting. 18  For the fine-tuned model, we quantitatively evaluate the performance and feature importance. Table 7 shows that fine-tuning yields an insignificant performance improvement of 𝐹 1 = 1p.p. We find that the model suffers from underfitting since performance 18 We find the following values to be optimal. max-depth = 6, min-child-weight = 18, subsample = 1, colsample-bytree = 1, and eta = .2. Other hyper-parameters are set to the default values. The maximum number of boosting rounds is set to 999, and early stopping is applied if the 𝐹 1 -score for validation set does not increase within ten rounds. The model is weighting the imbalanced classes. The evaluation metric is the 𝐹 1 -score averaged on five-fold cross-validation. is also low on training (𝐹 1 = 0.51) and validation sets (𝐹 1 = 0.50). Comparing XGBoost performance to the defined baselines, we see that XGBoost significantly outperforms the random baseline (B1) but fails to significantly outperform the naive usage of the negative sentiment lexicon (B2). However, when analyzing results in a confusion matrix, we see that using just the negative dictionary, in fact, predicts 53% of biased words incorrectly as non-biased words whereas XGBoost predicts only 23% incorrectly. High 𝐹 1 -score and ROC AUC of baseline B2 is mostly due to the low number of False Positives, but this is a behavior close to simply predicting all words as non-biased. Since we do not have the prior information on which features are the most contributing into media bias detection, we first trained the classifier on the all the available features. When analyzing feature importance, we find that the most important features are the occurrence of a word in a negative lexicon (gain = 1195) and being a proper noun (470). The bias lexicon that we created semi-automatically is among the top 10 important features (gain = 106). Among linguistic features proposed by Recasens et al. (2013) as indicators of bias, only sentiment lexica, subjectivity lexica, and assertive verbs are among the top 30 important features. While report verbs and hedges still have minor importance, factive and implicative verbs have zero importance. We train several models feeding them with features that have different importance. Excluding features with low feature importance does not improve the performance (Table 7). Besides, we test how the model performs when different feature groups are not included. Thus, we train a model with all features except one particular feature or group of features. We notice that the performance drops significantly only when linguistic features are not used, most likely because of the negative sentiment lexicon's high importance. Lastly, we qualitatively investigate automatically detected bias candidates. Examples of correctly classified biased words (TPs) include mostly emotional words that can be considered biased even without context. Words that can be described as causing negative emotions occur more often than those causing positive emotions. 12.5% of TPs correctly indicate less obvious bias, which is most likely generally more rare. The following examples illustrate (1) obvious negative bias, (2) obvious positive bias, and (3) slightly more subtle bias among correctly classified words. 1. Large majorities of both parties seem to like the Green New Deal, despite efforts by Fox News to paint it as disastrous. 19  2. Right-wing media sprung into action to try to discredit her, of course, by implying that a woman who graduated summa cum laude with an economics degree is a bimbo and with Twitchy using a screenshot to make the usually genial Ocasio-Cortez somehow look like a ballbuster. 20 3. As leading 2020 Dems advocate spending big on the Green New Deal, it turns out most Americans are worried about other issues. 21   Examples of biased words incorrectly classified as non-biased (FNs) include words that are (1) parts of phrases, (2) ambivalent as to whether they are biased, (3) not generally biased but only in a particular context, (4) mistakes in the annotation, and random misclassifications: 1. By threatening the kids and their families with deportation, the administration's U.S. Citizenship and Immigration Services was effectively delivering death sentences. 22 19 https://www.alternet.org/2019/04/just-a-cover-for-sexism-and-white-nationalism-paul-krugman-explains-why-the-rights-attacks-on-new-democratic-  lawmakers-are-bogus/, accessed on 2020-10-31. 20 https://www.alternet.org/2019/01/alexandria-ocaio-cortez-is-absolutely-right-there-shouldnt-be-any-billionaires/, accessed on 2020-10-31. 21 https://fxn.ws/370GuwZ, accessed on 2020-10-31. 22 https://www.msnbc.com/rachel-maddow-show/trump-admin-backs-plan-deport-critically-ill-children-msna1280326, accessed on 2020-10-31. T. Spinde et al. 2. When the Muslim ban was first enacted, it triggered chaos at airports and prompted widespread protest and legal challenges, and it continues to impose devastating costs on families and people who wish to come to the U.S. 23 3. The specter of ''abortion regret'' has been used by lawmakers and judges alike to impose or uphold rules making it harder for people to get abortions. 24 4. Gun enthusiasts cannot admit that they like firearms because they fear black people. 25   Examples of non-biased words misclassified by the model as biased (FPs) include words that (1) are ambivalent as to whether they are biased, (2) describe negative or contentious issues, (3) are due to erroneous annotation, and (4) random misclassifications: 1. Justice Sonia Sotomayor, in her dissent, accused the majority of weaponizing the First Amendment -an unconscionable position for a person tasked with ''faithfully and impartially'' discharging the duty to protect the inherent rights of all Americans. 26 2. He also denounced the policy of Chancellor Angela Merkel and the attitude of the German media, which ''are constantly pushing'' for Europe to welcome more and more migrants, in opposition to the will of the Hungarian people. 27 3. Michelle Williams won a Golden Globe for her role in ''Fosse/Verdon'' on Sunday night, but perhaps her biggest moment came during her acceptance speech when she defended abortion rights and encouraged women to vote ''in your own self-interest''. 28 4. The case was sent back to lower courts to determine whether the gun owners may seek damages or press claims that the amended law still infringes their rights. 29",
          "One of the main contribution of our work is the creation and annotation of a robust and diverse media bias data set. Other than already existing studies on the topic, our data contain background information about the annotators, increasing our results' transparency and reliability. We perform visual analysis and observe the following findings. Topics that are less controversial are annotated as non-biased slightly more often than very controversial topics. Conservative annotators perceive statements as biased slightly more often than liberal annotators, but for both, it is only true unless the statement is from a conservative media outlet. We also find that annotators who read news never or very rarely are less likely to annotate statements as biased. Besides, our annotation results show the connection of bias and opinion. Even though our data set was developed on a small scale and cannot serve as a gold standard for the field of media bias research, it offers a complete framework for further data extension, especially in combination with our specifically developed survey platform. We plan to extend the data set in the future. While using crowdsourcing gave us an insight into the perception of bias by a broad audience, some related issues could not be resolved, e.g., the submission of random words. Furthermore, even honest workers made mistakes because the identification of media bias is, in general, not a trivial task, especially for non-experts (Lim et al., 2018a;Recasens et al., 2013). We will follow a dual strategy in future work: While extending the existing data set, we will develop an expert annotation guideline and evaluate the same sentences in cooperation with experts in the field. We will also test the difference between the annotation of single, isolated sentences, and sentences within an article's scope. Lastly, we will evaluate from a psychological perspective how different types of questioning affect the perception of bias, and have already collected around 500 varying questions on the issue. Regarding the quality of the annotations, our main strategy is to exclude noise from the final labels by setting up a threshold of the number of annotators required to label a word as biased or not. However, after manual analysis of final annotations, we find that setting up any strict threshold will introduce some noise and result in some words being omitted. The threshold of four is the most reasonable, but we admit that some words are omitted, and some non-biased words are included. We will experiment with more annotators per sentence to see whether we can reduce the percentage of errors. Our semi-automatically created bias lexicon is indeed able to find emotional words and words that convey a strong opinion. However, we conclude that while capturing emotional and negative opinionated words, the lexicon is unlikely to be exhaustive. So far, the approach lacks an additional method on how to expand the lexicon without adding non-biased words. Overall, our prototypical system achieves an 𝐹 1 -score of 0.43, precision of 0.29, recall of 0.77, and ROC AUC of 0.79. Our data set is the largest and most transparent in the area to date and our classifier is the first built on these data, making a direct comparison to other methods unfeasible. On their respective data sets, researchers who detected media bias on the word level achieved an T. Spinde et al. F1-score of 0.26 (Fan et al., 2019) and 0.31 (Spinde et al., 2020b); researchers, who detected framing on the word level, achieved an F1-score of 0.45-0.46 (Baumer et al., 2015;Hamborg, Zhukova et al., 2019). We present the most complete collection of features for classification to date, extending the work of Hube and Fetahu (2018) and Recasens et al. (2013). Especially Boosters were not used in previous research, but are among the most important features. We will continue our detailed analysis of feature importance for the overall task with our larger crowdsourced data set and the expert data. We will also improve the quality of our features. For example, for implicative verbs, Pavlick and Callison-Burch (2016) introduced a method to automatically predict implicativeness of a verb based on the known constraints on the tense of implicative verbs. We could also expand our sentiment and subjectivity lexicons by using WordNet, a de facto lexico-semantic network (Fellbaum, 1998;Miller, 1995), or SentiWordNet 3.0, a lexical resource that assigns sentiment scores to each synset of WordNet (Baccianella, Esuli, & Sebastiani, 2010). While recognizing around 77% of biased words correctly, our approach misclassifies around 20% of non-biased words. Due to the classes' imbalance, 20% of the misclassified majority class significantly decreases overall performance. In this section, we discuss the drawbacks of the implementation that lead to low performance. Especially words that are biased only in a particular context are rarely classified correctly, highlighting how media bias is usually very subtle and context-dependent. However, so far, we only accounted for context by using one collective feature for the window of four words surrounding the word. Overall, we believe that the feature-based approach is especially valuable because of its explanatory character, relating bias to specific features, which is impossible with automated feature extraction. It is also not as dependent on the amount of data as a neural network. However, we will integrate deep learning into our approach, as such an architecture especially helps to account for inter-dependencies between words. Both methods can also be combined, with our specific features giving meaning to the identification of words with automatically identified features. We will also investigate whether the classifier works better on the political left, right, or center sources. We will also determine whether we can distinguish bias for any particular ideology in our overall vocabulary of biased words. In this paper, we propose an approach to identifying media bias using an automatic feature-based classification. To evaluate our method, we also present a 1700-sentence data set of crowdsourced biased word annotations, where we show each sentence to ten survey participants. For the first time in the research area, we report each person's background and make our data more transparent and robust. We extend existing feature sets for the task and especially evaluate each feature in detail. We also experiment with different classifiers, with our final choice returning an 𝐹 1 -score of 0.43, a precision of 0.29, a recall of 0.77, and a ROC AUC of 0.79. Our results slightly outperform existing methods in the area. To increase our results further, we show how we can improve our data and features in future work and how neural networks could be a suitable option to combine with our method, even though some drawbacks have to be overcome. We publish our code and current system at https://github.com/Media-Bias-Group/Automa-CRediT authorship contribution statement Timo Spinde: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Lada Rudnitckaia: Initiated the project and implemented/completed all technical components as well as all writing except the discussion. Jelena Mitrović: Helped to implement and completely financed the successful survey execution. Felix Hamborg: Wrote the discussion chapter. Michael Granitzer: Gave permanent expert feedback on our work. Bela Gipp: Gave permanent expert feedback on our work. Karsten Donnay: Helped to implement and completely financed the successful survey execution."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "slanted news coverage or internal bias, reflected in news articles.",
            "context": "2.1. Media bias Media bias is defined by researchers as slanted news coverage or internal bias, reflected in news articles (Hamborg et al., 2018). By definition, remarkable media bias is deliberate, intentional, and has a particular purpose and tendency towards a particular perspective, ideology, or result (Williams, 1975).",
            "type": "explicit"
          },
          "bias by word choice": {
            "definition": "when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept.",
            "context": "Different news production process stages introduce various forms of media bias. In this project, we will focus on the bias that arises when journalists or, more generally, text content producers label the same concepts differently and choose different words to refer to the same concept, namely, bias caused by word choice. Depending on which words journalists select to describe an event, inflammatory or neutral, a reader can perceive the information differently.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "22cb049f711570de72ea60a5f6b49fdf4459d589",
        "sections": [
          "Social movement actors seeking to influence public and political agendas are highly dependent upon the mass media to help them communicate their political messages to a wider audience. In his classic work, \"Protest as a Political Resource,\" Lipsky argues that leaders of groups lacking routine access to the policy process can appeal to their ultimate (legislative or governmental) targets only indirectly by attracting influential third parties to their causes. Because they lack direct contact to these influential publics, most relatively powerless groups can only communicate with these audiences through the mass media {Lipsky, 1969}. The message that is conveyed, and the government response it helps generate, are conditioned largely by the influence of the bystander publics that movements seek to reach. Since the time of Lipsky's seminal article, there has been a tremendous growth in the use of public protests as a form of political action, particularly in Washington D.C. [INSERT NUMBERS??] This growth in the frequency of public protest contributed to the development of policing institutions designed to manage public protests {McCarthy, 1998 #xxx}. As a consequence, some have argued that protest activity has become normalized, and is therefore less effective as a political resource for powerless groups {??Everett, 1991; OTHERS --CITE???. Whatever these claims, social movement actors continue to advocate change through the use of public demonstrations. Protests are an almost daily fixture in the United States's capital city, and their numbers show little sign of yielding, despite more effective policing and the fact that the social movement sector has developed other methods of communicating with a wider audience -such as door-to-door canvassing and direct mail --that do not depend on the sympathies of the mass media. Indeed, the importance of the mass media for social movement actors in the United States may be further enhanced by the greater reliance of citizens on mass media news sources --especially televised ones --and the declining influence of locally published newspapers {Iyengar, 1985}. Our research has attempted to illuminate how the mass media packages the images of protest they project to the general public, including those influential third parties Lipsky views as vital to protest success. Lacking direct contact with influential groups, social movements must rely on the mass media as an indirect means of communicating with potential sympathizers. But movements' heavy reliance on the mass media complicate their efforts to convey political messages, as media institutions operate under logics that are independent of --and often contradictory to --movement agendas. This can undermine social movements' impact on public policy. Understanding relationships between media routines and social movements is therefore crucial to illuminating processes of social change. Social movements try to transform the general notions in society about what problems deserve attention and about how these problems should be addressed. This means that they are often engaged in framing contests with other actors in society as they seek to raise issues to the social agenda and to suggest ways of thinking about these issues. Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem. But political actors, including social movement actors, are not simply concerned with influencing what people think about, but they also hope to shape how people think about social problems and their solutions. Thus, protesters are ultimately interested in shaping the agenda-building process by helping define how issues are framed in the mass media. In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story. This paper seeks to uncover the factors that shape the portrayal of public protests in the mass media by examining media reports on protest events that took place in Washington, D.C. during 1982 and1991. By doing so, we hope to illuminate the political dynamics of agenda-building processes as well as to provide an interpretive baseline for researchers using media records as evidence of protest activity.",
          "Our findings regarding media selection bias suggested that media agendas can drive the selection of protest events that are reported, independently of the numbers or activities of protesters themselves. In other words, \"objective\" matters such as the demonstration's size and characteristics have little bearing on whether or not the event is reported in the mass media. This appears increasingly true in more recent years, and is especially the case in television news reporting. Thus, social movement agents seeking to influence media agenda-setting cannot simply rely on generating large numbers of protesters or on engaging in unusual forms of collective action. Rather, they must also consider how their issue might be tied to media issue cycles. In other words, rather than simply reflecting \"objective\" occurrences, the operations and procedural logics of mass media institutions are important predictors of what events get reported. The practical need to select a limited number of issues on which to report leads to claims of selection bias in how editors and reporters go about the business of determining the news that is \"fit to print.\" Since they are, by definition, \"outsiders\" to political institutions and processes, social movements are typically marginalized by mainstream media routines. Once they achieve media attention to their protest event, demonstrators often find that the coverage of their protest fails to portray their cause as they would have liked. Often the more unruly activists are highlighted or the most vocal leaders advanced by reporters seeking images that they hope will appeal to their mass readership {Gitlin, 1980}. The specifics of protesters' policy concerns or the organizational details involved are largely irrelevant to those reporting on public protests. This may be a function of media gatekeepers, whose motivations, routines, and professional interests support the status quo. It may also result from the failure of many social movement challengers to engage in conscious efforts to wage an \"insider media game\" {Ryan, 1990 #xxx :5}. Plagued with limited staff, time, and money and with the inherently contradictory necessity of simultaneously appealing to constituents, publics, and targets, social movement actors often lack media strategies. Ryan (1990) summarizes the main theoretical models of news production routines according to: gatekeeper models which focus on daily operations and routines of media institutions; political economy or propaganda models, which explain media routines according to broad socioeconomic forces including media ownership patterns; and cultural studies, which emphasizes the role of mass media in maintaining the political and social hegemony of elites {Ryan 1990 #xxx : 22-25}. Clearly the operations of and influences on the mass media as well as the consequences of how the news is received are of interest to social movement researchers and activists. And recent media research suggests that a dynamic, process-oriented approach will best explain how the news is produced and to what consequence {#xxx see, e.g. Clayman, 1998; OTHER CITATIONS HERE?}. Gatekeeper Model. Gatekeeper models presume that in order to understand how social movements interact with the mass media, we must account for the institutional logics that shape how mass media operate. This approach might also be called the \"newsmaking approach,\" as it sees the mass media as not simple mirrors that reflect what goes on in society, but rather as actors in and of themselves that construct images and compile messages {Kielbowitz, 1986 #xxx :75}. Viewing media institutions as news makers leads one to account for how they determine what constitutes a newsworthy event. Social movements are faced with the dilemma of justifying why some chronic problem such as poverty or racial injustice is more worthy of attention today than it was yesterday. In other words, those seeking to raise routine issues onto media agendas must portray these issues as changing or somehow exceptionally interesting in order to mesh with media newsgathering routines. The identification and use of sources is another area where gatekeeper models help us understand media routines. Gans {#xxx 1979} and others have pointed out the institutional logic behind the decisions about deploying reporters and selecting sources --all of which work against the access of social movements to the mass media. Editors favor reporters who are generalists rather than specialists in the area on which they write, because generalists are cheaper, easier to control, and they are more reliant on their sources for information (thus removing responsibility from the media organization). A preference for government and official experts as sources derives from the ease of reaching their centralized press offices and the norm that has evolved of viewing government sources as credible and unbiased, requiring no routine fact-checking or corroboration. The reliance on government sources thus both reduces the costs of newsgathering and limits the risks {Herman, 1995}. It also tends to reproduce the agendas of these sources {Gans, 1979; Kielbowitz, 1986}. The need to understand media logics and to find ways to mesh with media news gathering routines means that movements whose leaders provide the most appealing media images and rhetoric will be the most likely to gain media coverage {Gitlin, 1980}. Movements, moreover, need to provide consistent streams of images that appeal to the mass media in order to sustain media focus on the issues. This leads activists to escalate their tactics in an ongoing effort to create \"new\" hooks for media attention {Kielbowitz, 1986}. Social movement actors have long recognized that generating media attention requires the creation of an event that is somehow \"newsworthy.\" Social movements have traditionally relied on public demonstrations --among other activities --to attract media attention that might further their aims and amplify their issues on social agendas. However, the tactics that are most likely to attract media attention are not often those that win respectability among the legislative and governmental targets that movement actors ultimately hope to influence {Gamson, 19xx}. Propaganda Model. Propaganda models generally assume that the structure of media ownership leads to an inherent bias in the mass media in favor of capitalist interests. Scholars adopting this perspective such as Edward Herman have argued that the privatization and commercialization of broadcasting during the 1970s and 1980s has severely constrained access to the mass media and limits the range of ideas conveyed through them {Herman, 1995 #xxx: 169}. The \"free market model\" outlined by Herman sees market forces \"naturally\" marginalizing dissent by crowding out any ideas for which there is no substantial economic backing, or \"effective demand.\" The commercialization of the airwaves restricts democratic access to the mass media by: 1) limiting media control to those with the large amounts of capital required to purchase and run large corporate structures that control media networks; 2) creating a market where reliance on advertising is essential to media operations; 3) driving the search for cost-effective news production, where the crucial need for cheap, \"credible\" sources leads editors to favor centralized government sources over hard to gather dissident sources that (unlike government sources) require fact-checking; 4) creating a fear of \"flak\" or \"negative feedback\" that imposes costs on or threatens the media, often leading to self-censorship i ; and 5) reinforcing ideological premises that favor capitalist institutions such as private property, the merits of free enterprise, the threat of state ownership and intervention, the benevolent role of the government in international affairs, and anti-communism. \"One of the great merits of this system of control is that it operates so naturally, without collusion or explicit censorship, merely by the decentralized pursuit of a set of micro-interests. It 'just works that way'\" {Hermann, 1995 #xxx :170}. One effect of the corporate control of media institutions is that news production becomes dependent upon what advertisers will support. When they buy air time, sponsors align their products with certain images. As consumers demanding services from news production agencies, advertisers can withold support of certain kinds of news programming. For instance, the portrayal of news about the Persian Gulf war was shaped to make it more appealing to advertisers: Pressure from advertisers led to the proliferation of upbeat \"home front\" stories of the war as network executives sought to assuage the worries of their sponsors. For instance, CBS executives \"offered advertisers assurances that the war specials could be tailored to provide better lead-ins to commercials' with 'patriotic views from the home front '\"{Andersen, 1995 #xxx:211, Quoted from Carter 1991}. While this kind of influence over programming may be good for business, it does little to advance peoples' understandings of complex social and political issues. And it divides the agendas of social change agents even further from those of the news media, which --in theory --serves as the public's \"watchdog\" against abuses by government, business and other elites. The propaganda model would lead one to expect little if any media coverage of social movements that threaten the capitalist order. And whatever coverage such movements might attain would be expected to marginalize or otherwise de-politicize their messages. One additional theme in the propaganda perspective is the focus on the increasing centralization of mass media institutions into corporate structures that control other aspects of social and economic life. The emergence of media monopolies has serious implications for the free exchange of ideas. With a media environment that is relatively free of competition, there are smaller news holes and fewer incentives to seek out innovative stories. This further threatens social movement access to the mass media {Kielbowitz, 1986}. Cultural Studies Model. Like propaganda models, cultural studies models of media operations explain the choices made in the selection and interpretation of the news in terms of broader power relations. The focus of these models, however, is more upon the subtle ways in which economic and political elites control images and meaning. According to these perspectives, the conveyance of information is used to reinforce ideas and interpretations of events that support existing power structures. Such models would lead us to expect that mass media coverage of social movements, particularly those that promote ideas that differ most strongly from dominant ideologies, will tend to marginalize social movement agendas. One way that this done is by framing stories in a way that suggests individualized or non-systemic causes and solutions to problems on which social movement activists work. Iyengar's {1991} field experiments led him to support the ideas in the cultural studies model; he concluded that how the media portray a problem affects public attributions of blame and their presumptions about appropriate solutions. Iyengar's study begins with the assumption that news stories can be categorized as either \"episodic\" or \"thematic\" in nature. Episodic news frames are event-oriented, and they focus on concrete issues such as an individual welfare mother or a terrorist bombing. In contrast, thematic news frames provide information that helps contextualize social problems, focusing more on general trends and conditions that set the stage for more dramatic events such as terrorist attacks. \"The essential difference between episodic and thematic framing is that episodic framing depicts concrete events that illustrate issues, while thematic framing presents collective or general evidence\" (Iyenger 1991:14). Thematic coverage in Iyengar's study was associated with systemic attributions of blame, while episodic coverage typically led subjects to see individuals as responsible for the problem at hand. Relating this idea to the relationship of social movements to the mass media, we note that social movements often seek to attract media attention to some broad social concern by generating an event that may be newsworthy in itself. But this demands that the coverage of the protest event focuses on the issues the movement seeks to address, rather than on the event used to attract media coverage. By creating a newsworthy event or episode, movement leaders hope to draw attention to underlying themes of injustice, inequality, or oppression. Social movements address chronic social problems that they attribute to some abstract, systemic cause. In order to appeal to the mass media and the general public, they must find ways to link these chronic problems with some more concrete, tangible event or focus. In other words, they must try to repackage what they see as an ongoing, systemic problem (e.g., poverty, racism) into an \"acute\" issue that demands urgent attention on a crowded social agenda {see Rochon, 1997 #xxx :178-9}. Movements generally seek support for their claims that the political and/or economic system is the source of some problem and that without systematic reforms the problem will persist. ii Thus, we would expect that more thematic --rather than episodic --coverage of their issues best advances the aims of social movement actors. Moreover, because they use protest events as \"pegs\" on which a news story might be based, we would expect to see a tendency towards more episodic reporting of social movement protests.",
          "Our understandings of media routines and institutions led us to expect that the print media would provide more thematic coverage of political issues related to protest events, given that its audience tends to be among the more politically active and attentive segments of the general public {Iyengar, 1985; OTHER CITES on participation & media attentiveness}. However, our earlier study of selection bias suggest that the relatively fewer stories on protest that are selected by the electronic media are chosen because they correspond to ongoing media issue agendas {McCarthy, 1996}. This may mean that television coverage tends to be more thematic than print coverage, which was less sensitive to issue agendas in its selection of protest events on which to report. Our first hypothesis accounts for these contradicting tendencies. H1: Television news is more selective in the range of social movement events it reports, but the coverage of these events is more thematic than print coverage. Media routines and gatekeeper motivations would also lead us to expect that extraordinary activities such as arrests and violence will attract more episodic coverage. This is partly due to the fact that these events themselves are considered newsworthy, especially in cases such as the demonstrations by and against the Ku Klux Klan, which triggered large-scale riots in downtown Washington D.C. Arrests and violence also signal some social controversy, which reporters and editors often seek out in their daily reporting routines. Certainly the presence of dramatic incidents and controversy on which to report does not preclude a reporter from incorporating some discussion of the underlying social tensions that motivate public demonstrations. Indeed, the main strategy behind public demonstrations is to trigger attention to some broad social problem through a dramatic event, and the character of the drama (e.g., size of demonstration or personal sacrifices made by protesters, etc.) should highlight the relative importance or scale of injustice the issue involves and thereby generate more in-depth consideration of the issue. But small newsholes, reporters' limited knowledge, and financial concerns of media gatekeepers constrain the extent to which the underlying concerns of protesters become the primary focus of stories on highly controversial protest events. In short, while demonstrators may seek to use extraordinary activities such as violence or civil disobedience to gain media attention, the attention they get in response to these activities may contribute less to their overall political objectives than was initially hoped. H2: Controversy such as the presence of counter-demonstrations, arrests and/or violence generate more episodic than thematic coverage of a protest event. Issue attention cycles are also likely to be important forces shaping media gatekeepers' reporting on public protests. As we noted earlier, the issues motivating protests were important factors in explaining which demonstrations received media attention. In some cases, protests provide a segue into reports on ongoing social problems just as reports on airline crashes often turn into discussions of the overall safety of air transportation infrastructures. So the daily updates on the progress of the war in the Persian Gulf were often accompanied by highlights from the ongoing demonstrations against and for the war. Or a protest by homeless advocates may trigger a more in-depth story on the causes of homelessness or national policies to mitigate its effects. This leads us to our third hypothesis: H3: Issues that are related to ongoing media issue agendas will attract more thematic coverage than episodic coverage. Propaganda and cultural models of media reporting on social movement activity would lead to expectations that most coverage of protest events would tend to favor authorities in their \"spin,\" and that they would rely on sources outside the movements for information about the protest and/or the issues motivating it. These models would also predict that coverage of protest demonstrations --particularly those promoting goals that contradict major economic interests --would tend to be episodic. Less controversial or threatening issues may attract more thematic coverage, and in these cases news reporting may even appear to favor demonstrators and even to rely on movement leaders for information. iii H4: Social movements that most threaten the existing socioeconomic order will attract more episodic than thematic coverage. H5: Media coverage of social movements will tend to favor the status quo; the \"spin\" of news stories will in general favor government and other authorities. Our account here of the relationships between episodic and thematic coverage and social movement protests builds upon the belief that social movement aims are best served by coverage that is more thematic, or that delves into the underlying social problems to which movements attribute some injustice. We would hold, then, that stories taking a thematic rather than an episodic frame would be most supportive of movement actors. This leads us to our sixth hypothesis about the relationship between the \"news spin\" and the framing of a story: H6: The \"spin\" of thematic stories is more likely to favor social movement actors than is that of episodic stories.",
          "Media stories on public protests in Washington, D.C. were collected for the entire years of 1982 and 1991 from the Washington Post and The New York Times, and from the Vanderbilt Network News Index and Abstracts summaries of television network news broadcasts. Media records were matched with evidence on the protest event drawn from police records. iv A detailed codebook was developed to assess various features of the coverage, focusing particularly on the extent to which the coverage reflects a thematic or episodic frame. v Coders read each media story and coded the reporting on each demonstration mentioned in the story. Thus, a single article may have been coded three separate times, each reflecting the details of a distinct protest event. Roughly two-thirds of all articles or news abstracts reported on more than one protest event. Often, news stories reported on protest campaigns or series of linked protests that are carried out over a number of days by the same groups and for the same issues. Our work to develop and test a codebook pointed to the difficulty of systematically assessing the extent of episodic or thematic coverage in each protest event report. We therefore developed multiple different measures of each concept, asking coders to record their level of confidence for judgements related to episodic or thematic coverage. [INTER-CODER RELIABILITY; CONFIDENCE RATINGS] We included specific questions asking for separate evaluations of how much (on a 7 point scale) of the text of an article was devoted to describing the goals of protesters, and to the details of the protest event. Following these more concrete questions, coders were asked to make judgements (again, using a 7 point scale) about how episodic an article appeared (not at all or \"highly episodic\") and how thematic the article appeared. We did not presume that these concepts were mutually exclusive, and we left it to the coder to judge each concept independently. Finally, the coder was asked to place each protest event report on a 3 point scale as to whether it was \"primarily episodic,\" \"primarily thematic,\" or a \"mixture of episodic and thematic.\" In addition to coding evidence about the framing of a particular event report, coders recorded information on the location of the story in the news lineup, the amount of time or space devoted to an event report, the \"spin\" of the coverage (whether it favored protesters or authorities), and whether there were reports of counter demonstrators, violence, or arrests. Using this trichotomous scale we find that, of 635 protest event reports for which relevant data were present, 34% were considered \"primarily episodic,\" 24% were \"primarily thematic,\" and 41% were a mixture of both thematic and episodic coverage. Table 1 presents correlations between this summary measure and other indicators we collected on the episodic and thematic nature of each event report. vi Table 1 About Here On the specific questions where coders were asked to determine how much of the article was devoted to the details of the protest or to the issues underlying protesters' activities, we found strong correlations which reflected the expected relationships. Reports with primarily episodic coverage (coded 1) tended to have greater emphasis on the details of the event, while those that were primarily episodic (coded 3) devoted more attention to the issues and specific goals addressed by protesters. Also, coder evaluations of the extent of episodic or thematic coverage correspond highly with their ranking of a story as primarily episodic or primarily thematic. The one measure that does not appear to adequately capture the concept we hoped it would is the question of how much the report emphasized incidental details of the event such as the weather or protesters' clothing, which we had presumed would be related to more episodic framing. Given the correspondence of our other measures, however, we believe that we have constructed a reliable and valid measure of news framing. Having determined that our measure of news framing is valid, we examined whether the framing of news coverage is related to variables associated with the source of the news report, the context of the demonstration, or the issues the protesters seek to address. In addition to the episodic/thematic variable, we look at the relationship of our predictor variables to two other measures of relationships between social movements and media coverage. These include news spin, or the extent to which the story favors demonstrators (coded 1) or authorities (coded -1) and a 7-point scale of the extent to which the story discusses the issues protesters are addressing (ranging from 0-\"none\" to 6-\"all\"). vii",
          "Table 2 shows positive relationships among the dependent variables we examine in this study. Stories whose spin favors demonstrators tend to devote more attention to discussion of the underlying issues (as opposed to the details of the protest event) and to encompass a more thematic frame. As expected, there is a strong tie between the amount of the story devoted to the issue and thematic framing. Source variables were related to the extent to which a report was thematic or episodic. Television news tended to be more thematic than print news coverage. The reliance on any demonstrator source (e.g., organizer, literature, posters, etc.) as opposed to a government or \"expert\" source for information on the story was related to more thematic coverage. Over time, the coverage of the issues was slightly more thematic, and more attention was devoted to the issue itself rather than the demonstration, but there was less tendency for stories to \"spin\" towards demonstrators in the later time period. viii Context measures focused on whether there was some kind of controversy involved in the protest event. The presence of a counter-demonstration tended to produce more episodic framing of the report. Also, the presence of arrests and/or violence was associated with more episodic frames. This finding is reinforced by the negative relationships between the amount of the story devoted to discussion of the issue and conflict measures. Bivariate correlations also show that where controversy or counter protests are present, the spin of the story tends to favor authorities rather than protesters. The propaganda and gatekeeper models of news production would lead us to expect the issues at stake in a public demonstration to be important predictors of the news frame. Indeed, we found that issues were important in determining whether or not a protest was reported in the media, so it would not be surprising to find that issue cycles also drive the nature of media coverage. We took the top issues around which demonstrations were held in 1982 and 1991 for which there were significant numbers of media reports. ix Many issues that attracted the largest numbers of protests did not receive enough media coverage to assess description bias in the reporting on those protests. x The bivariate correlations between particular issues and news frames reveal none or very weak relationships. The strongest pattern appears in the coverage of protests involving the Ku Klux Klan, which tended to favor authorities in their spin, to devote less space to discussion of the issues, and to be more episodic. These patterns are probably due to the frequent presence of violence and arrests at these events, which tends to focus coverage more on the protest event itself rather than on the motivations of demonstrators. The disruption and violence that helped attract media attention to the story also led to stories favoring authorities rather than movement actors. Other patterns of note are that the news spin in these instances reveal a tendency of the media to support nationalistic frames. The coverage of the protests against the Gulf war tended to support government authorities, while that of demonstrations marking the opening of the Vietnam Veterans memorial in Washington, D.C. supported demonstrators. The coverage of the Gulf war, moreover, tended to be more thematic than coverage of other issues. This ongoing, major media story attracted massive amounts of news coverage, and demonstrations provided a regular source of \"hooks\" for these reports which quickly became routine in the daily newspapers and nightly news reports. Table three presents regression estimates for models predicting the amount of a story devoted to the issue, and news frame (episodic/thematic scale) from variables related to the media source, the context of the protest, and issue. Table four presents the analysis of our news spin variable, examining what factors appear to be related to whether or not the stories' subtext supports demonstrators or authorities.",
          "The analyses in table three show consistent patterns of greater thematic emphases in television as opposed to print media reports. xi The models also reveal that stories relying on government or expert sources had characteristics that worked against social movement framing efforts: They were more episodic, and they devoted less space to a discussion of the issues at stake. Looking to table four, we can see that stories depending on government or expert sources also tended to favor the interests of authorities rather than demonstrators. Another pattern that was fairly strong and consistent across the models was the role of controversy in shaping coverage of protests. The presence of a counter-demonstration tended to detract media attention away from the underlying issues relevant to a demonstration and towards more episodic coverage. It also fostered coverage that tended to favor authorities. Similar patterns are seen when arrests and/ or violence was present as part of a demonstration event. Issues had less consistent and generally weaker relationships with the dependent variables we examine, and they contributed a relatively small amount to explaining variation in the dependent variables. One unexpected finding is that coverage of the Gulf war tended to be more thematic, but the negative coefficient for the model predicting amount of the story devoted to discussing the issue suggests that the coverage remained superficial. xii Coverage of demonstrations around the establishment of a Vietnam Veterans Memorial tended to be thematic and their spin favored demonstrators. In contrast, coverage of anti-Ku Klux Klan demonstrations (which degenerated into violent confrontation) tended to be episodic, to spin towards authorities, and to devote relatively less attention to the issues underlying protests. In table four we also examine how the news frame is related to the spin of the story. We hypothesized earlier that thematic coverage would generate news spin that favored demonstrators. The bivariate correlation between our thematic/ episodic scale and news spin suggest some support for this argument (r=.289). The last column of table four provides further support for this argument while controlling for other variables. Adding the episodic/thematic scale to the model explains an additional 6% to of the variance in news spin. Thus, our hypothesis that thematic coverage reinforces the goals of social movements is supported. The substantive importance of thematic coverage for social movement framing contests lead us to ask for more detail about the relationships between news framing across print and electronic sources. Iyengar's research led him to expect greater thematic emphases in the print media, given that this source generally has a larger potential newshole than national network news programs {Iyengar, 1991 #xxx :129}. Table 5 contrasts the coverage in the print media sources with that in the electronic media, attempting to uncover any systematic differences in episodic versus thematic framing. The first two columns display the analysis of all cases, and they reveal a strong difference in news frames between print and electronic media. Indeed, the strongest effect is found here, and electronic sources tended to provide more thematic coverage. The next strong effects, which hold throughout most of the other models, are those measuring whether protesters were relied upon as information sources for the report and whether or the \"spin\" of the report favors protesters or authorities. As the propaganda and cultural models would predict, when protesters and their banners or flyers constituted the main source of information for a report, news frames tended to be more episodic. Experts and government sources were more likely to be consulted in stories with a thematic emphasis. Also in cases where the \"spin\" of the story appeared to favor demonstrators, the coverage tended to be thematic, and where the \"spin\" favored authorities, the coverage of the protest was more episodic. xiii   ",
          "The presence of controversy around a demonstration --such as a counter-protest or the presence of violence or arrests --had a smaller impact on the nature of coverage, though it led consistently to more episodic reporting, as we had expected. Finally, in the model including all the cases, we found that issue agendas contributed relatively little to explaining variation in news frames. The Gulf war coverage tended to be thematic, but other issues had little overall effect. In contrast, the model including only the electronic media reports shows incredibly strong effects of issue agendas. Measures of the model's explanatory power increased dramatically with the addition of issue variables. In television reports, coverage of veterans' issues and of protests surrounding the Ku Klux Klan tended to be episodic, while Gulf war coverage, which made up a large proportion of the electronic reports, were typically thematic. In the print media reports, we found much smaller effects of issue agendas on the tendencies of news frames.",
          "We set out in this study to examine how social movement attempts to influence media coverage impact their efforts to affect public debates about social and political change. Based on the work of other social movement researchers, we know quite well that public demonstrations have uncertain and often contradictory effects on social movement campaigns. And yet they remain one of the main tactics available to social movement actors in their efforts to shape broader public agendas. The hypotheses stated above set out expectations about the relationships we should expect to see given what we know about social movements' relationships with the mass media. In short, we expected that media reports overall would not favor demonstrators' interests, particularly on issues that most threaten existing economic and political elites. Supporting our earlier findings about coverage of protests by television news media, we found strong tendencies in both 1982 and 1991 for reports on public demonstrations in the television media to be more thematic than print media reports. This finding reinforces the notion that television news is more driven by issue agendas than by some independent selection of events deemed newsworthy. Demonstrations selected for television news coverage tended to fit within ongoing issue agendas, and the content of the stories tended to focus on the issues themselves more than on the details of the protest event. xiv   In keeping with the expectations of the gatekeeper model, we found that controversy generated media coverage that was more episodic. The presence of counter-demonstrators, arrests, and/or violence produced more reporting on the demonstration event itself and less attention to the issues at stake. These conditions also tended to produce news spin that favored authorities rather than demonstrators. Also, we have limited evidence on which to base our claim that issues related to ongoing media agendas may receive more thematic coverage. The evidence that suggests support for this claim is the reporting on the anti-Gulf war demonstrations, which provides only one case that is suitable for testing this hypothesis. Additional research based on a larger range of issues should test this hypothesis further. We were not able to adequately test our hypothesis derived from propaganda models of media reporting, largely because our dataset did not include enough stories on demonstrations that could be considered to be about issues potentially threatening the socioeconomic order. xv One issue that Herman and Chomsky have directly linked with their claims about the pro-capitalist bias of the news media is the coverage of protests against U.S. intervention in Latin America. Although a large proportion of all Washington D.C. demonstrations were focused on this issue [INSERT % of ALL 1982 demos on LATIN], only 8 of these were covered. xvi This suggests that a pro-capitalist bias may be more strongly operating in the agenda-setting process than in the priming or agenda-building that the media does. The reports we examined on protests related to socioeconomic issues did not provide strong evidence one way or another here. Although the sign of the regression coefficients for our episodic/thematic scale and for the amount of the story describing the issue run counter to our expectations, the coefficients are weak and non-significant. Further examination is warranted to test the hypothesis that movements posing a threat to the capitalist order receive media attention --to the extent that they receive it at all --that marginalizes their messages. Our last two hypotheses relate to our expectations about the news spin of the reports on protest demonstrations. Our fifth hypothesis, that the \"spin\" of protest event reports will in general favor government and other authorities was not supported by the evidence we have assembled here. Only 27% of all the stories we coded revealed any discernible bias, and the bulk of those stories favored demonstrators rather than government authorities. Nevertheless, the news spin variable still showed patterns that are consistent with what we expected. The \"spin\" of thematic stories was more likely to favor social movement actors than was that of episodic stories. And stories that relied on government and expert sources tended to reinforce the agendas of those sources by displaying news spin that favored authorities. In conclusion, social movements seek to both to draw attention to issues they deem important. They also seek to influence how these issues are portrayed to the general public. A common strategy for influencing public and policy agendas is to stage mass demonstrations as a way to dramatize a problem. But this common social movement strategy is not an effective tool for either influencing media agendas (roughly 2% of all public demonstrations in Washington D.C. receive any media attention) or for influencing the media frames used to portray issues. Of all news stories on demonstrations in 1982 and 1991, only 24% were reported in a manner that emphasized primarily the social issues manifest in protesters' demands. An additional 41% of the stories included some discussion of the underlying issues along with reports on the protest event itself. Thirty-four percent of all reported demonstrations were treated primarily as events in their own right: the underlying demands were left unexplored by the media reports as the activities of protesters and police, incidental factors such as the weather, and other details of the protest event constituted the focus of the story. According to research on the impact of news frames on peoples' interpretation of social problems, the episodic frame through which many public protests are portrayed leads consumers of mass media away from systemic explanations for the problems protesters are seeking to address: [E]xposure to episodic framing tends to elicit individualistic (or nonsocietal) attributions of responsibility for most of the issues studied. . . . Because television news generally fails to activate (and may indeed depress) societal attributions of responsibility, however, it tends to obscure the connections between social problems and the actions or inactions of political leaders. By attenuating these connections, episodic framing impedes electoral leadership. . . . In the long run, episodic framing contributes to the trivialization of public discourse and the erosion of electoral accountability. {Iyengar, 1991 :141, 143} News frames that are thematic tend to portray social movements in a positive light. However, public demonstrations do not appear to be an effective means of communication for challengers. Changes in the mass media industry also threaten opportunities for wider public debate, including the consideration of social movements' agendas. Corporate ownership and centralization, combined with management strategies that have downsized the news staffs of major networks {#xxx see Kimball, 1994} generate news gathering routines that are increasingly less likely to include social movement sources or activities. -Implications for social movements--media strategy -Implications for research ii. This may be called system attribution, and it is seen as a key to the emergence of social movements. McAdam {#115 1982} refers to this as \"cognitive liberation.\" iii. Recall Iyengar's finding that thematic coverage leads viewers/ readers towards more systemic explanations for social problems. Thus, thematic coverage of issues would tend to support many social movements' objectives, since movements generally seek the transformation of broad social and political institutions and require a system critique in order to mobilize and motivate adherents. iv. These data are described in McCarthy, McPhail, and Smith (1996). v. The codebook is available from Smith or McCarthy. vi. Developing questions that would generate reliable and valid measures of thematic and episodic coverage proved more difficult than we had anticipated. Our codebook basically leads the coder to ask whether the coverage tends to focus on the details of the demonstration itself or on the issue more generally. Of course, we would have liked to ask whether the coverage of the issue tended to be episodic --e.g., focused on individualistic or localized interpretations rather than systemic ones --but this proved impossible given the range of issues covered in this study. The latter kind of question is most appropriate in a study that focuses on a smaller range of issues in greater detail. vii. Just over six percent of all cases were coded as having a spin that favored authorities, compared to 21% favoring demonstrators. The remaining cases (73%) were considered neutral. The mean score for the amount of the story devoted to discussing the relevant issues was 2.47 (st.dev.=1.42). viii.Further research is needed to determine whether these are trends or idiosyncratic. ix. In both years, this includes the top protest issue, which was the Israeli-Lebanon war in 1982 and the Gulf war in 1991. x. These issues included, for instance, demonstrations against U.S. Central America policy in 1982, demonstrations against the policies of foreign governments in both years, and pro-environmental demonstrations in 1991. xi. The large number of television reports devoted to coverage of the Gulf war in 1991 meant that there may have been an unusually high emphasis on thematic coverage of antiwar demonstrations. But the patterns of greater thematic emphases in the reporting on demonstrations in the television news was found when we repeated the analysis for only the 1982 cases. xii. This may be a function of the nature of this study, where we had to ask coders to base decisions about thematic vs. episodic coverage on the story's emphasis of the issue at stake versus the details of the protest itself rather than on some decision about the framing of the issue the demonstrators sought to advance . For instance, in the Gulf war case, there were likely to be news stories of the war regardless of whether or not protests occurred, and the stories that appeared seemed to use the demonstrations as a \"hook\" for routine coverage of the war. These stories involved much discussion of the war effort and administration policy, but were not generally concerned with detailing the critical views of antiwar protesters (see also note ??6?? above). xiii. Most reports (73%) were considered to have no readily observed \"spin\" that favored authorities or demonstrators. Roughly 6% of the reports were coded as favoring authorities and 21% favoring demonstrators. xiv. This points to a limitation of Iyengar's claim that television news coverage is less thematic than print news, although it is relevant for only a small portion of news stories devoted to covering protest events. Indeed, had we --as Iyengar did--reviewed all the coverage of the issues about which we were concerned, we might also find that print media were more thematic in their reporting. Instead, we look only at stories on public demonstrations in the nation's capital. xv. Our attempts to characterize demonstrations according to the extent to which each posed a threat to the socio-economic order were highly problematic, in part because few demonstrations posed any serious threat. This suggests that cross-national or historic research may be necessary to systematically assess this hypothesis. xvi. Of the eight stories on anti-interventionist protests regarding Central America, three were considered \"primarily episodic,\" one was \"primarily thematic,\" and four were \"a mixture of episodic and thematic coverage.\""
        ],
        "ground_truth_definitions": {
          "agenda-setting": {
            "definition": "the process of identifying and advocating issues for inclusion on public and governmental agendas.",
            "context": "This means that they are often engaged in framing contests with other actors in society as they seek to raise issues to the social agenda and to suggest ways of thinking about these issues. Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem.",
            "type": "explicit"
          },
          "agenda-building": {
            "type": "explicit",
            "definition": "efforts to influence the context in which social problems are considered",
            "context": "Graber {#xxx 1993} distinguishes between \"agenda-setting,\" which is the process of identifying and advocating issues for inclusion on public and governmental agendas, and \"agenda-building,\" or efforts to influence the context in which social problems are considered. When social movement actors gather in the streets and in other public spaces to demonstrate for their respective causes, they seek to set agendas by bringing new or greater attention to a particular problem"
          },
          "Selection Bias": {
            "type": "explicit",
            "definition": "agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report.",
            "context": "In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story."
          },
          "Description Bias": {
            "type": "explicit",
            "definition": "agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest eventis portrayed in a media story",
            "context": "In an earlier article {McCarthy, 1996 #xxx}, we identified two distinct forms of bias in the media's recording of protest events as selection bias and description bias. Selection bias involves agenda-setting, and it results from media gatekeepers' choices of a very small number of protest events from a large pool of possible events on which to report. Description bias reflects agenda-building processes, and it introduces another distinct way in which media personnel shape the political messages conveyed in their news reports by making choices about how a protest event is portrayed in a media story."
          }
        }
      },
      {
        "paper_id": "03c4bb2121b3dd2b5302d37c81e84092a501bc91",
        "sections": [
          "Social media platforms are under increasing public scrutiny because of inconsistencies in how they apply their policies with respect to cultural difference and hate speech. In April 2015, Facebook banned the trailer of a new Australian Broadcasting Corporation (ABC) comedy show that attempted to confront white frames on Aboriginality with humour, because it contained an image of two bare-chested women taking part in a traditional ceremony. The platform labelled the video \"offensive\" and in breach of its nudity policy (Aubusson, 2015). In response, the Indigenous activist and writer Celeste Liddle posted the video in her Facebook page with a written message to denounce Facebook's standards. What she did not imagine was that \"malicious people\" would repeatedly flag her post until she was temporarily locked out of Facebook and the video removed (Liddle, 2016). One year later, Liddle gave a keynote address discussing issues of colonialism and Indigenous feminism. New Matilda published her speech with a similar accompanying image of two Aboriginal women with painted bare chests. After sharing this link in her Facebook page, Liddle received a temporary ban from Facebook for publishing an image of a \"sexually explicit nature\" in breach of its guidelines (Liddle, 2016). Users who shared Liddle's article also faced suspensions. Only when the media reported on the case did Facebook issue a public statement in which the company defended its nudity restrictions because some audiences \"may be culturally offended\" by this type of content, and suggested that users share Liddle's speech without the accompanying image (Ford, 2016). Facebook has also been notorious for refusing to ban racist pages towards Aboriginal people. In 2012 and 2014, the Online Hate Prevention Institute (OHPI) went through extensive negotiations with Facebook to get the platform to remove several pages containing racist attacks on Indigenous Australians (Oboler, 2013). Facebook initially ruled that the pages did not breach its terms of service and instead compelled their creators to rename them to note that they were \"controversial content\" (Oboler, 2013). Not until the Australian Communications and Media Authority was involved did Facebook decide to block these pages, but even then only in Australia (Oboler, 2013). These incidents illustrate frictions derived from platforms' curation of content. Facebook's removal of the photograph of two Aboriginal women and its refusal to entirely ban racist content towards Indigenous Australians signals the platform's lack of understanding of images of Aboriginality and its tendency to favour Western ideals of free speech. It also shows how in this case Facebook's politics (Gillespie, 2010) favoured the offenders over Indigenous people. Facebook's editorial practices are complex and largely distributed, and involve the platform's technical infrastructure, policies, and users' appropriation of technology to moderate content. It also involves the labour of often outsourced workers who live in different parts of the world. This \"unseen work\" tends to favour platforms' profit seeking and legal demands rather than responding to social justice or advocacy-related goals (Roberts, 2016). The entanglement between the national specificity of racism and the mediumspecificity (Rogers, 2013) of platforms and their cultural values is the focus of this article. Specifically, I argue that this entanglement triggers a new form of racism articulated via social media, which I call 'platformed racism'. Platformed racism is a product of the libertarian ideology that has dominated the development of the Internet since its early beginnings (Streeter, 2011), and has a dual meaning; it (1) evokes platforms as tools for amplifying and manufacturing racist discourse both by means of users' appropriations of their affordances and through their design and algorithmic shaping of sociability and (2) suggests a mode of governance that might be harmful for some communities, embodied in platforms' vague policies, their moderation of content and their often arbitrary enforcement of rules. I take inspiration from Bogost & Montfort's (2009) \"platform studies\" approach to situate the computational nature of platforms as \"fully embedded in culture\", and I draw on a postcolonial framework to examine the cultural dynamics of racism in a specific national context, that of Australia. Accordingly, platformed racism is examined through the investigation of a race-based controversy, the booing of the Australian Football League (AFL) Indigenous star Adam Goodes, as it was mediated by Twitter, Facebook and YouTube.",
          "The issue of white Australian race relations is a complex one partly rooted in a long history of domination and dispossession of Indigenous people, a history that has defined and continues to propel the construction of Australian national identity (Hollinsworth, 2006). Moreton-Robinson (2015) calls it the \"white possessive\", which refers to the \"patriarchal white sovereignty\" logic of the nation-state that constantly disavows Indigenous people (pp. xi-xix). These complex race relations are mirrored in Australian sports, an arena of intense national pride and Australian cultural politics, where racism prevails but where Indigenous athletes have found a platform to perform their identities and counter prevailing nationalistic discourses (Hallinan & Judd, 2009). The controversy surrounding the AFL star Adam Goodes is a recent example of these tensions. Adam Goodes is an Andyamathanha and Norungga man who played for the Sydney Swans from 1999 to 2015. He is a dual Brownlow medallist, awarded to the best and fairest player in the AFL each season, and widely recognised as one of the best game's players. Goodes is also an advocate against racism in Australia, a political aspect to his public persona that contributed to his being named the 2014 Australian of the Year. However, Goodes was also involved in controversies which had racial dimensions: during a game in 2013, he pointed out to the umpire a girl in the crowd who had called him an \"ape\", and who was subsequently removed. While there was substantial support for Goodes, and for addressing racism in the AFL and Australian sporting culture, rival supporters would also boo Goodes when their teams played Sydney. During the 2015 AFL season's Indigenous Round, an annual celebration of the role of Aboriginal and Torres Strait Islander people in Australian Rules football, Goodes celebrated a goal against Carlton by performing a war dance; this included him mimicking the action of throwing a spear in the general direction of the crowd. This gesture served to reignite debate about race and racism in Australia: while celebrating Indigenous culture, the dance and spear-throwing were also perceived by some as antagonistic or offensive. Coupled with the already turbulent relationship between opposition supporters and Goodes, this momentum of Indigenous pride clashed head-on with the expectations of what Hage (1998) has called the \"white nation fantasy\": the link of whiteness in Australia with notions of space, empowerment and the perception of the \"others\" as mere objects whose place is determined by the will of the dominant culture (pp. 18-23). On the Internet, the controversy was played out along similar lines. Opponents used Twitter to ridicule Goodes (Wu, 2015), Facebook pages were created solely to vilify him (Online Hate Prevention Institute, 2015) and his Wikipedia page was vandalised, replacing pictures of him with images of chimpanzees (Quinn & Tran, 2015). Since the performance of the war dance, the increasing intensity of the booing every time Goodes played and the harassment campaign on social media forced him to take time off from the game, until he quietly retired in September 2015, and later deleted his Twitter account in June 2016 (The Age, 2016).",
          "The impact of the Internet on racialised identities and practices has been a complex and on-going field of research. Early work on race and the Internet pointed to unequal levels of access as a source of racial inequalities on the web (Hoffman & Novak, 1998), a line of study that later also stressed unevenness in digital literacies and skills (Hargittai, 2011) and algorithmic visibility (Introna & Nissenbaum, 2000) as important factors of digital inequality. From a discursive perspective, the Internet is both an opportunity to perform racial identity (Nakamura, 2002) and a forum to reproduce power relations and hierarchies (Kendall, 1998;McIlwain, 2016) or amplify racism (Daniels, 2009). Social media platforms, as current mediators of the majority of online sociability and creativity (van Dijck, 2013), are also tools for both prosocial and antisocial uses. For example, the movement organised around the hashtag #sosblackaustralia -created in 2015 by Indigenous activists to stop the foreclosure of Aboriginal remote communities -has found on Twitter and Facebook a space for advocating for the rights of Black people in Australia. However, Twitter is also an outlet where hate speech and harassment thrive (Shepherd, Harvey, Jordan, Srauy, & Miltner, 2015), including racial and sexist abuse (Hardaker & McGlashan, 2016;Sharma, 2013). Platforms also contribute to racist dynamics through their affordances, policies, algorithms and corporate decisions. The underlying infrastructure of platforms, their materiality, largely responds to their economic interests (Helmond, 2015). For example, by tracking users' activity -for example, pages liked and posts people engage with -Facebook has built a category called \"ethnic affinity\", which marketers can choose or exclude at the time to sell products to users. The fact that within the housing category marketers could exclude users with an African American or Hispanic \"ethnic affinity\" violated federal housing and employment laws, which prohibit discrimination on the basis of someone's race and gender (Angwin & Parris Jr., 2016). The business orientation of this technical functionality overlooked its potentiality to be discriminatory. While platforms still perform a rhetoric of neutrality (Gillespie, 2010) -for example, Facebook presents itself as a technology (D'Onfro, 2016), Twitter as a \"service\" for people to communicate (Frier, Gillette, & Stone, 2016) and YouTube as a \"distribution platform\" (YouTube, n.d.) -they \"intervene\" in public discourse (Gillespie, 2015) and often contribute, as has happened with other technologies, to sustaining whiteness (De la Peña, 2010). Accordingly, 'platformed racism' aligns with the body of literature that critically interrogates social media sites as actors that not only host public communication, but also coordinate knowledge through their technological affordances and under their logics and rules (Gerlitz & Helmond, 2013;Gillespie, 2010;Langlois & Elmer, 2013;Puschmann & Burgess, 2013;van Dijck, 2013). In the next sections I will elaborate further on the concept of 'platformed racism' following a \"platform-sensitive approach\" (Bucher & Helmond, 2017)  ",
          "Specific cultural forces have shaped personal computers, networked communication and the Internet in America, characterised by a libertarian inclination towards technology \"abstracted from history, from social differences, and from bodies\" (Streeter, 2011, pp. 11-12). While women were actively involved in the development of computing, computer culture had a masculine ethos (Turner, 2006) and was entangled with an idea of capitalism as the means to guarantee freedom of action, which helped to popularise the rights-based free market under a rhetoric of openness (Streeter, 2011). These cultural forces, which tend to be blind to identity politics and labour inequalities (Borsook, 1997), have been the object of study of the \"Values in design\" approach to technology, which contends that pre-existing societal bias influence technological progress (Nissenbaum, 2005). Technologies, as human designed objects, can embody cultural assumptions with regard to race (Brock, 2011;McPherson, 2012) and gender (Bivens, 2015). For example, the Emoji Set in Unicode 7.0 was criticised for its lack of black emoji and the stereotyping of other cultures (Broderick, 2013), and for constraining participation by their exclusion of some national icons, like the Aboriginal and Torres Strait Islanders' flag (Verass, 2016). This racial homogeneity did not respond to overt racism but to the \"aversion\" of the Unicode Consortium, the US body responsible for the emoji set, to recognise the politics of technical systems and that the monotone of emoji reproduced privilege in the first place (Miltner, 2015). However unintentional this unequitable representation could be, it exemplifies De la Peña's (2010) description of technological epistemologies, which protect whiteness and imagine the ideal subject as white (pp. 923-924). Similar debates have surrounded platforms' authenticity mechanisms (Duguay, 2017) and their advocacy for anonymity and pseudonymity, which facilitate abuse online (Barak, 2015). Although a degree of anonymity is desirable to avoid racial or sexist bias, platforms could improve by requiring more information about the users in their sign up processes yet allowing maintaining a fake personality online (Lanier, 2010). Other cultural assumptions embodied in the design of platforms are subtler. Although Facebook is a popular platform among the Aboriginal community in Australia (Carlson, 2013), Indigenous people have shown concern around how to delete a deceased friend's or relative's profile for cultural reasons (Rennie, Hogan, Holocombe-James, 2016). Facebook's architecture and procedures to permanently delete someone's account are complex, and it can take up to 90 days to delete everything someone has posted (Curtis, 2017). During this period, some information may still be visible to others, which may be problematic for Aboriginal people since in many areas of Indigenous Australia the reproduction of names and images of recently deceased persons is restricted during a period of mourning (Australian Government, n.d.). Avoiding privilege when designing technology can be a difficult task, but the capacity to readdress these biases is the first step to fight discrimination. Nextdoor, a social network that allows users to post messages to neighbours that have joined the site, reacted effectively to racial profiling practices in its platform. Nextdoor introduced a design change to counter discrimination: Before users can post a crime and safety message, the site displays a banner that reads: \"Ask yourself -is what I saw actually suspicious, especially if I take race or ethnicity out of the equation?\" (Levin, 2016). Cultural values and the \"purpose\" of platforms (Rieder, 2017) influence what social media offer to users. For instance, Nakamura (2014) argues that platforms' promotion of share-ability encourages users to circulate racist visual content in a decontextualized fashion. Social media buttons matter, and they both relate and differ across platforms (Bucher & Helmond, 2017). While on Facebook and Twitter it is not possible to \"dislike\" content, YouTube offers the possibility to give a \"thumbs down\" to a video. Facebook and Twitter's bias towards positivity makes it difficult to locate the negative spaces (John & Nissenbaum, 2016). By providing a \"dislike\" button, Facebook and Twitter could allow users to counter racism online, providing at the same time the possibility to measure this practice. However, a \"dislike\" button could also contribute to algorithmically generated information opposed to the interests of platforms' advertisers or amplify racist practices. Users' appropriation of technology has an effect on platforms and is crucial to the creation of meaning (Bucher & Helmond, 2017). The specific cultures of use associated with particular platforms, or what Gibbs et al. (2015) call their \"platform vernaculars\", play a significant role in the enactment of platformed racism in its dual medium and national specificity. Abusive users can use platforms' affordances to harass their victims, either by means of creating and circulating hateful content or by hijacking social media sites' technical infrastructure for their benefit (Phillips, 2015). For example, wrongdoers have used Twitter's promoted tweets to insert abuse towards transgender people (Kokalitcheva, 2015). On Facebook, cloaked public pages are used to disseminate political propaganda (Schou & Farkas, 2016) and extremist communities use YouTube to spread hate due to the platforms' low publication barrier and anonymity (Sureka, Kumaraguru, Goyal, & Chhabra, 2010). Users can also manipulate algorithms as exemplified by Microsoft's Tay chatbot fiasco. Within hours, users turned Tay into a misogynistic and racist bot, and Microsoft was criticised for not having anticipated this outcome (Vincent, 2016). Algorithms, as significant actors within platform dynamics, are outcomes of complex processes that entail \"a way of both looking at and acting in and on the world\" (Rieder, 2017, pp. 108-109). Rieder invites social science researchers to think about these processes as \"algorithmic techniques\" -for example, \"selected units and features, training and feedback setup, and decision modalities\" (p. 109) -and to pay attention to the \"moments of choice\" in the formalisation of these techniques (p. 115). How to make platforms more accountable for the performativity of their algorithms is at the centre of debate in recent scholarly work (Crawford, 2015;Gillespie, 2013;Pasquale, 2015). For instance, the visibility of race on Twitter triggered controversy since some topics relevant to the Black community in America rarely reach the needed thresholds to be recognised as \"trending topics\" (Gillespie, 2016). Similarly, Massanari (2015) has studied how the entanglement between Reddit's algorithms, features and geek culture contributes to propagating \"toxic technocultures\".",
          "Platforms adapt their interfaces to their users largely according to their economic interests (Bucher & Helmond, 2017). Changes in their technical infrastructures and policies, though, also respond to the demands of public opinion that increasingly claim changes in platforms' curation of content (Gillespie, 2017). In most cases, due to their protection under the \"safe harbour\" provision of Section 230 of the US Communication Decency Act (CDA), platforms are not liable for what users post (Gillespie, 2017). However, CDA 230 allows platforms to moderate content by means of their terms and services contracts and policies without attracting any increased liability. Facebook CEO Mark Zuckerberg (2009) has said that the terms of service would be \"the governing document that we'll all live by\", which embodies a constitutive power over sociability and implicitly implies a sense of equality to all users (Suzor, 2010). This is problematic: first, platforms have unclear rules with regard to hate speech. Second, there is a chain of liability in the moderation of content from platforms to other actors (human end-users and non-human algorithms). Third, there is certain arbitrariness in the enforcement of rules since platforms police content in an \"ad hoc\" fashion (Gillespie, 2017). Fourth, questions remain unanswered about whom and what gets to moderate content: Who are platforms' moderators and the users who flag? What \"algorithmic techniques\" (Rieder, 2017) are involved in these processes?",
          "The majority of platforms prohibit hateful content on the basis of someone's race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, ability or disease. However, different safeguards emanate from these policies. For instance, Twitter does not tolerate \"behaviour that crosses the line into abuse\" (Twitter rules, n.d.) but acknowledges the role of parody and humour in its parody account policy. On Facebook, humour is linked directly to its hate speech policy: \"We allow humour, satire, or social commentary related to these topics\" (Facebook, n.d.), and YouTube defends user's right to express \"unpopular points of view\" (YouTube Help, n.d.). What is considered \"humour\" or \"unpopular points of view\" is not further explained by Facebook or YouTube, which usually apply countryspecific blocking systems. The protection of humour as guarantor of freedom of expression is problematic for receivers of abuse, since defences of satire and irony to disguise racist and sexist commentary are a common practice online (Milner, 2013) that fosters discrimination and harm (Ford & Ferguson, 2004). Policies also respond to platforms' purposes, and Facebook, Twitter and YouTube's purpose is not to be editorial companies but multi-sided markets that respond to different actors (Gillespie, 2010;Rieder & Sire, 2014). Nevertheless, in order to safeguard their public image and protect their business model, platforms are increasingly engaging more actively in policing controversial content, especially with regard to terrorist propaganda and violence against women, such as revenge porn (Gillespie, 2017). As a general norm, though, they delegate the moderation of content to others: end-users, algorithms and moderators.",
          "Platforms afford users with different technological mechanisms to manage and report controversial content: from flags and reporting tools, to filters and blacklists of words and links. These mechanisms are per se limited, since they leave little room for transparent and public discussion about why something is considered offensive (Crawford & Gillespie, 2014). They also raise the question about who gets to flag -for example, just end-users or algorithms too? -and who has the final say in deciding what will be banned. Celeste Liddle's complaint about how abusive users repeatedly flagged her content to ban her activity on Facebook exemplifies how platformed racism as a form of governance is the outcome of end-users' practices and the platform's response to these actions, which in this case downplayed the performance of Aboriginality on Facebook. Although it is impossible to guess, we could add into consideration a third factor that contributed to platformed racism as a form of governance in this example, the subjectivity of the human editors that considered that the elders' picture was in breach of Facebook's nudity policy. Subjectivity is unavoidable in content moderation and some decisions can be attributed to the cultural background of platforms' moderators (Buni & Chemaly, 2016). Similarly, it is impossible to know if algorithms were involved in the identification of \"nudity\" in this picture. What we do know is that there is \"little consistency\" in the way other platforms owned by Facebook, like Instagram, use algorithms to censor hashtags and avoid the posting of images containing controversial content (Suzor, 2016). Overall, platformed racism in its dual meaning requires a careful exploration of the medium and national specificity of the actors involved, which will be explored further through the Adam Goodes case study.",
          "This study used an issue mapping approach to social media analysis to examine the Twitter, Facebook and YouTube activity around the Adam Goodes controversy (Burgess & Matamoros-Fernández, 2016;Marres & Moats, 2015). I followed the different actors, objects and themes involved in this controversy with a specific focus on how platforms' features and policies afford and constrain communicative acts. Although Facebook and Twitter are not primarily visual, visual objects (e.g. images, animated GIFs and videos) are central to social media and its practices (Highfield & Leaver, 2016) and represent an opportunity to understand public discussions about race and racism online (Nakamura, 2008). Twitter is a rich repository of links that users post from other platforms (Thelwall et al., 2016), and was used as the starting data source. I used the Tracking Infrastructure for Social Media Analysis (TrISMA) for the data collection, which utilises the Twitter Application Programming Interface to capture tweets of 2.8m Australian users on a continuing basis (Bruns, Burgess & Banks et al., 2016). Using the TrISMA dataset, I queried all tweets that matched the keyword \"Goodes\" between when Goodes performed the war dance until his retirement ( 29 YouTube links that were shared on Twitter during the period studied. To examine the extent to which Facebook and YouTube recommendation systems contributed to the circulation of information around this controversy I followed two approaches. I created a research Facebook profile with no previous history, liked a Facebook page that emerged from the Twitter dataset as relevant -entitled 'Adam Goodes for Flog of the Year' -and annotated the 25 pages that the algorithm suggested. On YouTube, three videos featuring anti-Adam Goodes content emerged from the Twitter dataset as relevant. I extracted their video networks based on YouTube's \"recommended videos\" algorithm and using the YouTube Data Tools (Rieder, 2015).",
          "The use of humour to cloak prejudice played an important role in amplifying racial vilification practices on Twitter, Facebook and YouTube in the Adam Goodes case study. On Twitter, attacks towards Goodes were articulated by means of sharing memes. This practice included the posting of overtly racist images that compare him with an ape and the use of \"sensitive media\" filters to disguise this abuse. Twitter enables users to apply a \"sensitive media\" filter to the content they post. This filter is meant to let users know prior to viewing that the media objects shared might be \"sensitive\" (e.g. contain sexually explicit material). However, some users find this filter a useful tool to cloak hate speech or avoid being flagged (Allure, 2016). On Twitter, humour was also mediated through the sharing of YouTube videos as 'funny' annotations. When content is reposted in its original form but in a new context, it adopts another layer of meaning (Baym & Shah, 2011), and these YouTube videos embedded in a tweet were offensive in the new context. For instance, one user tweeted a YouTube video of a song called \"Apeman\" and accompanied it with a message for Goodes saying that this was his retirement song. The networked nature of social media platforms makes hate speech thrive across platforms in a decontextualised nature (Nakamura, 2014). In terms of design, platforms like Instagram have chosen to not allow users to post external links, while others rely on this affordance as a way to increase user interaction (van Dijck, 2013). On Facebook, humour tended to concentrate in compounded spaces, like meme pages, or in comments. Similarly, on YouTube, parody was also located in the comment space rather than being mediated through videos uploaded specifically to make fun of Goodes. Sharing and liking practices on Twitter, Facebook and YouTube were also important in magnifying the racist discourse around Adam Goodes. Some of the images and videos critical of Goodes were shared and liked thousands of times across these platforms. These metrics give relevance to racist discourse, award it with certain legitimacy (Beer, 2016) and influence the ranking of this content by platforms' algorithms. For example, one of the links shared on Twitter was a video posted on Facebook by the Australian TV program Today featuring Jesinta Campbell, engaged to Goodes' Sydney teammate and fellow Indigenous player Buddy Franklin, in which she explains that her future children will be Indigenous and thanks Goodes for being a role model. The first comment under this video, with 2222 likes, reads: \"Racism goes both ways your kids won't be Indigenous. They will be Australian. Stop segregating\". Although this is not an overtly racist post, it denies the Aboriginal heritage of Campbell's future children and can be offensive for Indigenous people in its reference to the historical segregation of Indigenous populations by the colonial power and the institutional treatment of Indigenous people (Jayasuriya, Gothard, & Walker, 2003). The prevalence of racist comments on Facebook aligns with other studies that have found racist discourse on the comment space of online news and Facebook pages (Ben-David & Matamoros-Fernández, 2016;Faulkner & Bliuc, 2016). Moreover, from a vernacular approach to affordances (McVeigh-Schultz & Baym, 2015), users showed concern about these micro-social acts and perceived them as a symptom of general acceptance. For example, one user posted a comment on Facebook showing discomfort about the fact that the anti-Goodes posts were liked by thousands of people. Platforms' algorithmic management of sociability also contributed to the amplification of controversial humour and racist content. By liking the page 'Adam Goodes for Flog of the Year', Facebook's algorithm suggested other meme pages, such as 'AFL memes', and different football and masculinity-oriented pages (e.g. 'Angry Dad'). On YouTube, the related videos network unveiled new videos discussing the booing controversy and videos featuring the involvement of three media personalities critical of Goodes -radio presenter Alan Jones, television presenter (and president of rival AFL team Collingwood) Eddie McGuire and Sam Newman -in other controversial issues, such as the presence of Muslims in Australia. These recommendations are helpful to understand platformed racism in the national context of Australia. By liking and watching racist content directed to Adam Goodes on Facebook and YouTube, the platforms' recommendation algorithms generated similar content about controversial humour and the opinions of Australian public figures known for their racist remarks towards Aboriginal people.",
          "The distributed nature of platforms' curation of content was also evident through this case study. While users used features to curate content such as the sensitive media filter to disguise racist humour, they also utilised other affordances to moderate content. The sharing of screenshots to denounce hate online was a common practice in the discussion of the Adam Goodes controversy on Twitter. People took screenshots of tweets from other users that contained hate speech before they were deleted. Users reported 'tweet and delete' harassment tactics (Matias et al., 2015) and also circulated screenshots to denounce abuse that happened elsewhere. For instance, one user posted a screenshot of the Adam Goodes Wikipedia Page when it was flooded with images of chimpanzees. However, Twitter does not accept screenshots as evidence of harassment (Matias et al., 2015). By restricting this form of evidence, Twitter is allowing \"tweet and delete\" harassment tactics to circulate with impunity on its platform. Platforms' involvement in the curation of content in this case study was evident through an examination of the broken links that circulated in the Twitter dataset. A significant number of the links that were shared on Twitter (6%), Facebook (37%) and YouTube (6%) were no longer available as at January 2017, and each platform displays different messages when one tries to access this content. Twitter activates the same message for all the broken links: \"Sorry, that page doesn't exist!\" without specifying whether this content is no longer available because it had been removed by the platform or by users. Facebook displays similar messages when pages or posts are no longer available: \"the link may be broken or expired\", \"the page may have been removed\", \"it may be temporary unavailable\", \"you may not have permission to view this page\" or \"the page may only be visible to an audience you're not in\". YouTube has also different messages to indicate that videos are no longer available, which can be because the accounts associated with them have been terminated, the videos are private or other reasons that are not further explained. However, unlike Twitter and Facebook, YouTube gives further information when the takedown was because of copyright issues. In the context of copyright law, Internet intermediaries are shielded from liability for copyright infringement claims under the Digital Millennium Copyright Act (DMCA) as long as they establish effective notice and takedown schemes upon request of the copyright holder. Notice and takedown messages prove platforms' intervention upon content, although they do not provide clear information about why this content is no longer available, which obscures the scope and type of this abuse.",
          "This article has examined platformed racism as a new form of racism derived from users' practices and cultural values and platforms' politics (Gillespie, 2010). On the one hand, it evokes platforms as amplifiers and manufacturers of racist discourse by means of their affordances and users' appropriation of them (e.g. like button vs \"dislike\" button, sharing and liking practices that influence platforms' algorithms). On the other hand, it conceptualises platforms as a form of governance that reproduces inequalities (e.g. unclear policies, chain of liability and arbitrary enforcement of rules). Platformed racism unfolded in the Adam Goodes' controversy as it was mediated by Twitter, Facebook and YouTube. Platforms' protection of humour in their policies contributed to the circulation of overt racist memes, videos and racist comments. Other affordances like the Twitter's sensitive media filter were used to disguise hate speech, and liking and sharing practices contributed to amplify overt and covert speech across platforms. Users' appropriation of these affordances influenced other actors involved in the amplification of racism, such as algorithms. YouTube and Facebook's recommendation systems generated more racist content to be consumed, perpetuating racist dynamics on the platforms. Following Rieder's (2017) invitation to study how algorithms work, Facebook and YouTube recommendation outcomes could be further explored as a way to understand how algorithms perform based on users' input in different race-based controversies. In addition, the Adam Goodes case study made visible how the distributed nature of platforms' curation of content can lead to discrimination (Ben-David & Matamoros-Fernández, 2016). Users took the lead in denouncing abusive practices towards Adam Goodes through the sharing of screenshots of racist content. However, screenshots are not a valid evidence of abuse for Twitter, which only accepts links as proof of harassment (Matias et al. 2015). Platforms' contribution to the curation of content was explored through an examination of the broken links that circulated on Twitter. Platforms' notice and takedown automatic messages do not provide information about the reason behind why content is no longer available. While scholars argue that more transparency is needed to understand platforms' algorithmic contribution to the circulation of ideas (Pasquale, 2015), more transparency in the notice and take down message is also needed to act as a deterrent for those that engage in vilification practices online. For example, similar to Nextdoor's move to counter racial profiling, platforms could display a message to inform that certain content has been taken down due to their hate speech policies. In other words, they could follow their approach to copyright issues as a strategy to raise awareness on hate speech. The article has also shown theoretically and empirically the national specificity of platformed racism. Celeste Liddle's complaint about being flagged by abusive users and ignored by Facebook exemplifies how the racist dynamics in Australia are not only not fully understood by Facebook but perpetuated. Although Liddle explained the cultural relevance of posting a picture of two topless Aboriginal women performing a traditional ceremony, Facebook decided to continue banning the photograph and temporary block Liddle for repeatedly posting it. Moreover, Aboriginal people only account for the 3% of Australian population (Australian Bureau of Statistics). If issues important for the Black community in America rarely make it to Twitter's trending topics (Gillespie, 2016), issues pushed by the Aboriginal community in Australia are unlikely to reach the needed thresholds to be recognised by Twitter's algorithm. Empirically, covert racist arguments towards Indigenous Australians circulated and received large acceptance across platforms, which perpetuates dominant discourses on Australian identity, ideally imagined as white (Hage, 1998). In essence, platformed racism contributes to signifying \"white possession\" as embedded in technology and norms (Moreton-Robinson, 2015). Platformed racism is being increasingly normalised by platforms' logics (van Dijck & Poell, 2013) and requires scholarly attention. Further research could empirically examine platformed racism around other racial controversies in different national contexts. Similarly, the concept could be expanded to interrogate the material politics of platforms with regards to other sociocultural issues, such as sexism and misogyny."
        ],
        "ground_truth_definitions": {
          "platformed racism": {
            "definition": "a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them.",
            "context": "This article proposes the concept 'platformed racism' as a new form of racism derived from the culture of social media platforms--their design, technical affordances, business models and policies-- and the specific cultures of use associated with them. Platformed racism has dual meanings: first, it evokes platforms as amplifiers and manufacturers of racist discourse and second, it describes the modes of platform governance that reproduce (but that can also address) social inequalities",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "235c4f33d5bfc81bfa09a2458fcc0e42ef4454dc",
        "sections": [
          "works, by preaching and catechizing the countries lost to the Church in the debacle of the sixteenth century.\"1 Propaganda in this sense meant something along the lines of persuasion, preaching, or education. Although the term \"propaganda\" originated in the sixteenth century, the term was not widely used until the early part of the twentieth century. The development of various mass media, from the mass market magazines in the 1880s to film in 1895 from radio in the 1930s to television in the 1950s, allowed access to an ever-increasing audience for mass persuasion. Theories of propaganda analysis became popular as propaganda, associated mainly with totalitarian regimes and war efforts, was perceived as a threat to liberal democracies.2 Most theories of propaganda analysis begin with definitions of propaganda. As a starting place we will consider these definitions, and I will offer counterexamples to suggest that available definitions are insufficient to capture the sense that we now associate with the word. Perhaps the most general definition of propaganda is: \"any attempt to persuade anyone of any belief.\"3 Thus, propaganda is defined as mere persuasion. Of course, the original connotation of the term was just that: mere persuasion. However, this definition includes all acts of persuasion from the meteorologist attempting to influence beliefs about the weather to parents urging their children to go to bed. The Institute for Propaganda analysis was formed to educate the public about the styles and uses of propaganda. It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions. Further it does not offer any explanation of how propaganda could be contrary to the end of self-determination. The Institute for Propaganda Analysis is suggesting that the phenomena of propaganda is dangerous without accounting for this within its framework. The definition of propaganda as mere persuasion does not account for how we currently use the term, in other words, for the pejorative sense it has come to have. Alfred Lee also holds that propaganda involves persuasion, but adds further conditions that restrict its scope. He defines propaganda as \"an expression overtly set forth or covertly implied in order to influence the attitudes and through the attitudes, the opinions and action of a public. \"5 This is not just mere persuasion, but rather adds the condition that the target of the persuasion is a public. This is an improvement over a parent telling a child to go to bed being thought of as propaganda. However, there is no mention of the kind of public or the kind of message that propaganda presents so a mathematical theorem presented at an international math symposium would seem to meet these criteria. In other words, there are still not enough conditions to limit its application. F.C Bartlett, following along similar lines as Lee, claims that \"propaganda is an attempt to influence public opinion and conductespecially social opinion and conductin such a manner that the persons who adopt the opinions and behaviors do so without themselves making any definite search for reasons.6 His definition, by claiming that the recipient need not search for reasons, comes closer to identifying the pejorative sense of propaganda. However, commands given in a computer manual would have to be considered propaganda under this definition. Hans Speier has a far more restrictive definition of propaganda \"activities and communications from a government to its own citizens, other governmental personnel, or foreign audiences in general.\"7 We might call this definition a top-down definition. Speier as well as Edward Bernays, Walter Lippmann, and more recently Noam Chomsky maintain that propaganda must stem from those in government or power.8 By specifying a particular source of propaganda, this definition differs from the previous definitions. However, this might be too restrictive as it ignores propaganda on behalf of revolutionaries. These definitions are useful reference points, and I will use them throughout this paper. Of course, as we have seen in these definitions, the phenomenon of propaganda is not limited to art. Nonetheless, since art has been so widely used for political ends any definition of propaganda should be able to account for its use. However, the principle problem with these definitions is that even though all of the theorists listed above claim that propaganda is in some sense badoften using terms such as \"brain-washing\" to describe ittheir definitions do not accurately characterize wherein the pejorative character of propaganda lies. My definition will both pinpoint its negative aspects, and be applicable to art.9",
          "The definitions mentioned in the previous section all have a common thread that suggests propaganda can be analyzed by using the communication model of Sender-Message-Receiver. These theorists all hold that propaganda is in some sense intentional. That is, not only does propaganda involve persuading, but also the one who is persuading (Sender) is doing so intentionally, and moreover there is a target for such persuasion (Receiver) and a means of reaching that target (Message). This suggests the first condition for something's being propaganda: Propaganda involves the intention to persuade. This condition is in line with the original use of the word, which we recall is to propagate or intentionally spread Catholicism. However, as we have seen with the definitions in the first section, not any attempt to persuade is sufficient. We can narrow the field by examining the sender, the message, and the receiver of propaganda.",
          "Conventionally, theorists offer the following advice for analyzing propaganda: Consider the source. In other words, we should ask \"who is trying to persuade us?\" The second condition involves who sends the message. Speier argued that the sender of propaganda needed to be a government. This is too restrictive, but captures the political nature of propaganda. Revising Speier's condition, I contend that a second condition is that propaganda is sent on behalf of a political institution organization or cause. Many can agree that propaganda is political, but a disagreement may then arise over what constitutes the political realm. I suggest that being political, in the garden variety sense of the word, involves, minimally, partisanship and a preoccupation with a conception of justice or social order. There are conspicuous cases such as candidates running for political office or countries involved in a war effort. The institutions that constitute governments clearly meet these criteria. Partisanship that involves a preoccupation with a conception of justice or some sort of social order may also take the form of a cause or organization such as Planned Parenthood, Right to Life, English First, Greenpeace, or The Grey Panthers. However, in saying that the sender is a political institution, organization, or cause, I am not suggesting that propaganda has to be overtly propagated as historically some propaganda has been done covertly on behalf of these political entities.10 Nor am I suggesting that the person persuading has to be a member of the political institution or believe in the cause, for it is often the case the hired guns are used to construct and convey the message. This, of course, is not to say that I have settled definitively what should count within the realm of the political. My aim is to capture the intuition that propaganda is political. Even though what counts as political is somewhat contentious, there may be some consensus about clearly political institutions, organizations, or causes. However, those who have another conception of the political could still use the epistemic merit model, but it would be up to them to specify or stipulate how the term is being used.",
          "When considering other definitions of propaganda, we found that The Institute for Propaganda Analysis' definition was too broad since it included a parent's telling a child to go to bed as an instance of propaganda. In order to avoid this counterexample, I proposed above that the sender must be a political institution, organization, or cause. However, this does not completely deal with the elements present in the counterexample. It does not seem that propaganda is aimed at merely one person; rather, propaganda must be aimed at a group. To capture this intuition, I propose that the third condition is the recipient of propaganda be a socially significant group of people. At this point we should be clear about the differences between a socially significant group of people and political institutions, organizations, and causes. We must be careful to distinguish the sender and the receiver of propaganda. The sender of propagandaa political institution, organization, or causeis a somewhat formal group that might involve something like membership. A political group has an internal organization and is committed to some set of particular positions. The senders of propaganda often aim at creating an \"us\" against \"them\" mentality. In contrast, the receivers of propaganda are possible supporters of that group or cause who may not be linked formally with each other.11 A socially significant group of people could be as broad as a group dividing along the dimensions of race, sex, or age, or the relevant group might even be more narrow such as the readership of The National Review. It may be that the subject matter of a particular piece of propaganda is only of local interest and therefore the socially significant group that it addresses is restricted to certain geographical locations, such as Chicago. In targeting possible supporters of their cause, political groups are attempting to influence the beliefs, desires, opinions, and actions of the socially significant group of people.",
          "My objection in the first section was that while many theorists regard propaganda as dangerous, their definitions do not account for this assessment. Thus far, we have gathered that propaganda involves (1) an intention to persuade, (2) a socially significant group of people, (3) on behalf of a political institution, organization, or cause. This does not account for the pejorative sense that propaganda has come to have such that it is associated often with lies, appeals to emotion, and psychological manipulation. In short, it seems that the pejorative sense of propaganda stems from how its message is thought to function. And furthermore, identifying how propaganda actually functions is paramount to coming to terms with its power. However, even though we can agree that lies, appeals to emotions, and psychological manipulation do play some role in propaganda, they are not sufficient to characterize how propaganda is deleterious. Nevertheless, emotions can be a relevant part of propaganda's message. In order to capture the emotional component of some propaganda, we can say that some propaganda encourages inappropriate emotional responses. Birth of a Nation is designed to inspire hatred toward a race of people, false pride, and ignoble courage which are all examples of inappropriate emotional responses. It is not that pride qua pride is bad or inferior to reason but rather the object of pride is not an appropriate object. Sometimes this can take the form of connecting emotional responses to spurious states of affairs. However, even if we accept the qualification that propaganda often encourages inappropriate emotional responses, this will not account for all cases of propaganda. In writing about propaganda, Bertrand Russell maintains that too much has been made of the view that propaganda appeals to emotions. He writes \"[t]he line between emotion and reason is not so sharp as some people think. Moreover, a clever man could frame a sufficiently clever argument in favor of any position which has any chance of being adopted.\" 15 ",
          "Propaganda often attempts to use emotion to help mobilize action, but this ought not to be wherein the pejorative sense lies. By highlighting problems with locating propaganda's pejorative sense in appealing to emotions, I do not want to say that the emotions do not play any part in propaganda's effectiveness. My account later in this section will recognize that propagandists do appeal to emotions. Another common view is that propaganda consists of lies, and this is what accounts for its negative connotation. In its usual sense, lying entails x's uttering a statement that x believes to be false with the intention of leading an audience y to believe it to be true. There are two separate claims wrapped in the view that pieces of propaganda are lies. The first is that lying involves intent, and the second is that lying involves falsehoods. First, we might wonder whether an intent to lie is a necessary part of propaganda. There is a distinction between an intent to persuade and an intent to lie. I argue the first condition for something's being propaganda is there must be an intent to persuade. That is, there must be an attempt to influence or convince someone of something. In contrast, an intent to lie seems to imply that the person persuading believes one thing about a particular state of affairs, yet endeavors to convince others to hold a position contrary to it. A practical problem is that often propagandists do not try to persuade someone to believe a view contrary to what the propagandist actually holds. That is, by claiming that propaganda involves the intent to lie, this view denies that there are \"true believers\" who try to persuade others to join their cause. I think that an attempt to persuade as the first condition more aptly describes the activity of a propagandist. However, focusing on lies is helpful because it leads us to consideration of the truth status of the claim being made. If we think of propaganda as lies, then the purported falsity of the message could account for the pejorative sense propaganda has come to have. For we would like to think of ourselves as preferring truth over falsity, and propagandists sometimes use falsehoods in order to convey their message. Some of the most notorious cases of propaganda involve the misrepresentations of war atrocities, from the French's claims in World War I that the Germans ate babies to the claims that Iraqi soldiers dumped prematurely born infants out of their incubators.17 Nevertheless, falsity cannot fully account for propaganda's pejorative sense, as, strictly speaking, propaganda may not involve falsehoods. In practice there are difficulties if a propagandist relies upon falsehoods to convey her position, since the falsehood may be recognized as such, and one's credibility is then called into question. Josef Goebbels emphasized that lies should be told only about unverifiable facts and further the truth should not be told if it is deemed to be unprobable or lacking credibility. 18 The aim of the propagandist is to create the semblance of credibility. If we consider all propaganda to be false and, furthermore lies, we are not capturing the actions of those who use propaganda to achieve their political ends. To capture the spirit of the claims that propaganda functions by appeals to the emotions or lies, we should characterize propaganda as being epistemically defective or lacking epistemic merit where this criterion not only applies to propositions and arguments but also to conceptual schemas (ways of carving up the world) as well as moral precepts. 19  We can look to an example from advertising in order to illustrate uncontroversially how a true sentence can be used in an epistemically defective message. The makers of Bayer aspirin claim of their product that \"No aspirin has been proven more effective.\" This statement is not clearly false; however, it leads one to believe that this aspirin is the most effective pain reliever: it is the best aspirin. As a matter of fact, aspirin is aspirinproviding that there is the same amount in each tablet and that there are not any other active ingredientsall brands of aspirin are equally effective. This message is epistemically defective in virtue of its seemingly natural but unwarranted connection with other beliefs. Epistemic defectiveness is not only broader than truth and falsity such that true statements can sometimes be used in epistemically defective messages, it also encompasses communications and ways of understanding the world that are not normally thought to admit of truth-values such as commands, and conceptual frameworks. This attempts to capture the fact that we make judgments with respect to the fittedness of conceptual frameworks to the phenomena at hand, evaluate certain commands as being just or unjust to follow, and deem some metaphors to be apt while others are inapt. For example, the command of World War II pamphlet \"Jap...You're Next....Buy Extra War Bonds!\" trades on inappropriate stereotypes. Likewise, the metaphor of the rat/Jew in the Nazi film Jud Suess is an inapt metaphor. By describing propaganda as inherently epistemically defective, I have attempted to captured the role that psychological manipulation plays in its dissemination. This condition accords well with descriptions that propagandists offer of their work. George Orwell, who wrote propaganda for the BBC during World War II, claims that it is at best indifferent to the truth. His term \"doublespeak\" captures how ambiguous referents can create a true statement but an epistemically defective message. Obviously, it is beyond the scope of this project to delineate all the ways in which a message could be epistemically defective, if such a project were indeed possible. It will be up to the critic to argue that a particular message is epistemically defective; the burden will be to show how a particular message seems to be intended to function. Some examples will serve to illustrate how this notion can be applied. A message may be misleading given a particular context and background assumptions that an audience is likely to have. For example, toward the end of a fierce 1990 U.S. Senate campaign against African-American candidate Harvey Gantt, the Jesse Helms campaign ran what has come to be known as the \"white hands ad.\" The voice-over said, \"You needed that job. And you were the best qualified. But they had to give it to a minority because of a racial quota. Is that really fair?\" and on screen viewers saw masculine white hands crumpling a rejection notice. This commercial was believed to be crucial to Helm's re-election, and it was successful in spite of the fact that Harvey Gantt had gone on the record as being opposed to racial quotas. The \"white hands\" ad is not strictly speaking a lie, for nothing within the ad is false, nor is it effective simply by appealing to emotions, even though emotions about affirmative action run high. This ad is epistemically defective because it spuriously leads its audience to believe that Harvey Gantt (perhaps, because he's an African American) supports morally objectionable racial quotas. Epistemic defectiveness is the cornerstone of my definition of propaganda. I have argued that propaganda is an epistemically defective message used with the intention to persuade a socially significant group of people on behalf of a political institution, organization, or cause. While building upon previous definitions, this successfully accounts for the pejorative sense propaganda has come to have. Epistemic defectiveness captures the role that the emotions play in propaganda as well as commands, conceptual schemes and metaphors. In so doing, it can accommodate the phenomenon of art propaganda.",
          "Historically, political institutions, organizations, or causes have employed artists to convey their message to the people. Pablo Picasso's Guernica was commissioned by the Republic during the Spanish Civil War. Many British authors, including E.M. Forster and H.G Wells, were organized by England's",
          "Since art is frequently used as a medium of dissemination by propagandists, we need to be sure that any definition of propaganda can account for propagandistic art. There is also political art that is not propaganda. So, we need to be wary that our definition of propaganda is not so broad as to make all political art propaganda. That is, we need to distinguish merely political art from political art that is also propaganda. In this section, I will show how my definition functions to make this distinction. First, we will recall that I have delineated four necessary and jointly sufficient conditions for something's being propaganda. They are (1) an epistemically defective message (2) used with the intention to persuade (3) the beliefs, opinions, desires, and behaviors of a socially significant group of people (4) on behalf of a political organization, institution, or cause. I will deal with the first condition last as it is the most complex. It will also enable me to argue that even if all art is epistemically defective, my definition would still allow for a distinction between merely political art and propaganda. I have argued that propaganda involves an intention to persuade. This condition would seem to be met by most political art. Performance artist Holly Hughes does so in Clit Notes as does filmmaker Sergei Eisenstein in Potemkin. However, there are some cases of artists who provide social commentary and are aligned with political causes but whose work is unknown and not public until after their death.22 Emily Dickinson's poem I'm Wife-I've Finished Thatis a case of a work of art that was made without the intention to persuade, but is decidedly feminist in its message. This is a case of political art that is not propaganda, since there is no intention to persuade. This condition does not preclude a previously private artwork, or an artwork without political pretensions, from being used as propaganda. The work of art may be invested with new meanings by the propagandist, whose intentions then become relevant in the analysis of the piece of art as propaganda. Although Wagner did not write his operas with Nazi ideology in mind, his work was picked up by the Nazis and invested with new meanings. When we evaluate the artwork, we need to evaluate not only the conditions of its making, but also the conditions of its use. A piece of artwork might be labeled propaganda in some contexts, but not in others. This allows for the possibility that there is political art that is not propaganda. The intentions in a given use of art (or particular context) help to determine whether or not it's propaganda. Closely linked with the intention to persuade is the condition that propaganda targets a socially significant group of people. Personal diaries typically are not written to target socially significant groups of people, so those works will not count as propaganda. The Diary of Anne Frank is not an incidence of propaganda. I have also argued that propaganda must be done on behalf of a recognizable political institution, organization, or cause. Often, political satire offers an epistemically defective message in order to persuade a socially significant group of people, but it does not do so on behalf of a recognizably political institution or cause. Dana Carvey's satire of President George Bush on Saturday Night Live is not an example of propaganda, since neither Dana Carvey nor Saturday Night Live are arguably a political institution, organization, or cause. On the other hand, Jonathan Swift's A Modest Proposal is propaganda since he uses satire on behalf of the Irish in order to protest taxation laws that were oppressing poor people. This is where the advice \"Consider the source\" is relevant in propaganda analysis. My most controversial claim is that propaganda must be epistemically defective. Epistemic defectiveness nicely captures how works of art can function as propaganda. Messages presented through works of art are often not in the form of an argument but rather made through the use of icons, symbols, and metaphors. The condition of epistemic defectiveness also can explain how montage, dissolves, and other cinematographic features play a role within propaganda without making the claim that montage is necessarily a political form. 23 My definition requires that we look at the content of the message and how that content is supported by evidence within the work, and further how the message is connected with other beliefs. In order to argue that something is propaganda, we must offer details about the content of the work, and demonstrate how a message functions to mislead individuals. Once we have done that and indicated how the work meets the other conditions, we have a convincing case for something's being propaganda.",
          "At this point there are two types of concerns that we must consider. We must address certain objections to the epistemic merit model and show the importance of this model of propaganda to art education. There may be some who object that the epistemic merit model of propaganda is too broad and others who have doubts about the epistemic defectiveness criterion. After addressing these objections, we can better see how the model is valuable for art education. First, it may be objected that there are countless propositions, beliefs, commands, and conceptual schemas that are epistemically defective. For 9+14 = 19822 is false and thereby epistemically defective. Since so many things lack epistemic merit, the scope of propaganda will be immense. To this objection, I must reply that there is too much emphasis being placed on the name of the model and not enough emphasis on how the four conditions work together to limit the scope of propaganda. It will not be the case, for example, that Newtonian physics counts as propaganda, even if we want to say that it is epistemically defective. For it would be hard to argue that Newton was endeavoring to persuade his audience of something he himself would not rely on. That is, propaganda involves the intention to use an epistemically defective message. These two conditions in concert limit the scope of propaganda considerably. There may be others who object that the criterion of epistemic defectiveness is itself vague. I would argue that the fact that our judgments about what is epistemically defective are sometimes tentative does not diminish the value of this model. For it has isolated conceptually propaganda from other communications, and given us a framework within which we can discuss our disagreements about particular cases. Simply dismissing another's work as propaganda is not an acceptable analysis under this model, but rather the critic must say how the piece in question meets each of the conditions which in turn may lead to a productive disagreement, if the case is not clear-cut. The notion of a productive disagreement is at the heart of an education in the humanities and is particularly relevant for art education. And finally, perhaps the most important aspect of the epistemic merit model with respect to art education is that it can be easily applied to our everyday interactions with popular arts, movies, posters, music, and television. The film Dead Man Walking is an ideal candidate for this type of analysis. We need to evaluate the film in the social context it was presented to see if it meets the four conditions of the epistemic merit model of propaganda. (1) an epistemically defective message (2) used with the intention to persuade (3) a socially significant group of people (4) on behalf of a political institution, organization, or cause. We might note that the political cause set forth in the film is the abolition of the death penalty. Tim Robbins, the director of the film, has reported that he made the film with that cause in mind and the film was distributed worldwide.24 So it meets conditions (2), (3), and (4). The question then becomes whether the message of the film is epistemically defective. The message that the death penalty is unjust is, in part, conveyed through a powerful visual metaphor at the end of the movie. The central character Matthew Poncelet, played by Sean Penn, is being executed by lethal injection. He is portrayed as the sacrificial lamb of our society and its need to exact revenge for murder. The character is shown from a variety of camera angles strapped to a medical table which is visually similar to being tied to a cross. The final allusion of the film is that the person being executed is christ-like. This strikes me as an inapt metaphor, for even if the death penalty is immoral, the comparison of a convicted killer to Christ seems unfair. For they are in essence comparing a person who might be regarded as evil to someone who represents the divine. We will remember that inapt metaphors are a species of epistemically defective messages. The point here is not that it is simply offensive to those that regard themselves as Christians, but that someone convicted of a horrible crime is not aptly described as divine. The message is arguably epistemically defective because it seeks to tie our views about the justice of the death penalty to the claim that Christ was unjustly executed which spuriously leads us to believe that all such executions are wrong. The faulty comparison with Christ leads us to believe that, like Christ, Matthew is being unfairly persecuted. If this interpretation of the film is correct, there is a plausible case for its propaganda. However, some may argue that this case is not as clear-cut as I have presented it. To do so, they would offer an alternative interpretation of the significance of the final scene. Whether or not a particular instance of art should be considered propaganda will depend upon which is the better in- "
        ],
        "ground_truth_definitions": {
          "propaganda": {
            "definition": "an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.",
            "context": "It published workbooks and held seminars in the early 1930s aimed at promoting the ideal of \"self-determination,\" regarding propaganda as contrary to this end. They offer the following definition: \"Propaganda is an expression of opinion or action by individuals or groups deliberately designed to influence the opinions and actions of other individuals or groups with reference to a predetermined end.\"4 While this definition adds some details to the previous one, it does not add any restrictions.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "167f1d5e64b874bea86acdef6d5476da904fec6a",
        "sections": [
          "(e.g., Snyder, 1981;Snyder & Campbell, 1980;Snyder & Swann, 1978). The investigation of hypothesis testing has been concerned with both descriptive and prescriptive issues. On the one hand, researchers have been interested in understanding the processes by which people form, test, and revise hypotheses in social judgment, logical reasoning, scientific investigation, and other domains. On the other hand, there has also been a strong implication that people are doing things the wrong way and that efforts should be made to correct or compensate for the failings of human hypothesis testing. This concern has been expressed with regard to everyday reasoning (e.g., see Bruner, 1951;Nisbett & Ross, 1980) as well as professional scientific endeavor (e.g., Mahoney, 1979;Platt, 1964). In this article, we focus on hypotheses about the factors that predict, explain, or describe the occurrence of some event or property of interest. We mean this broadly, to include hypotheses about causation (\"Cloud seeding increases rainfall\"), categorization (\"John is an extrovert\"), prediction (\"The major risk factors for schizophrenia are . . .\"), and diagnosis (\"The most diagnostic signs of malignancy are. . .\"). We consider both descriptive and prescriptive issues concerning information gathering in hypothesis-testing tasks. We include under this rubric tasks that require the acquisition of evidence to determine whether or not a hypothesis is correct The task may require the subject to determine the truth value of a given hypothesis (e.g., Jenkins & Ward, 1965;Snyder & Campbell, 1980;Wason, 1966), or to find the one true hypothesis among a set or universe of possibilities (e.g., Bruner, Goodnow, & Austin, 1956;Mynatt, Doherty,& Tweney, 1977,1978;Wason, 1960Wason, ,1968)). The task known as rule discovery (Wason, 1960) serves as the basis for the development of our analyses, which we later extend to other kinds of hypothesis testing. We first examine what \"confirmation\" means in hypothesis testing. Different senses of confirmation have been poorly distinguished in the literature, contributing to misinterpretations of both empirical findings and theoretical prescriptions. We propose that many phenomena of human hypothesis testing can be understood in terms of a general positive test strategy. According to this strategy, you test a hypothesis by examining instances in which the property or event is expected to occur (to see if it does occur), or by examining instances in which it is known to have occurred (to see if the hypothesized conditions prevail). This basic strategy subsumes a number of strategies or tendencies that have been suggested for particular tasks, such as confirmation strategy, verification strategy, matching bias, and illicit conversion. As some of these names imply, this approach is not theoretically proper. We show, however, that the positive test strategy is actually a good all-purpose heuristic across a range of hypothesis-testing situations, including situations in which rules and feedback are probabilistic. Under commonly occurring conditions, this strategy can be well suited to the basic goal of determining whether or not a hypothesis is correct. Next, we show how the positive test strategy provides an integrative frame for understanding behavior in a variety of seemingly disparate domains, including concept identification, logical reasoning, intuitive personality testing, learning from outcome feedback, and judgment of contingency or correlation. Our thesis is that when concrete, task-specific information is lacking, or cognitive demands are high, people rely on the positive test strategy as a general default heuristic. Like any all-purpose strategy, this may lead to a variety of problems when applied to particular situations, and many of the biases and errors described in the literature can be understood in this light. On the other hand, this general heuristic is often quite adequate, and people do seem to be capable of more sophisticated strategies when task conditions are favorable. Finally, we discuss some ways in which our task analysis can be extended to a wider range of situations and how it can contribute to further investigation of hypothesis-testing processes.",
          "The Rule Discovery Task Briefly, the rule discovery task can be described as follows: There is a class of objects with which you are concerned; some of the objects have a particular property of interest and others do not. The task of rule discovery is to determine the set of characteristics that differentiate those with this target property from those without it. The concept identification paradigm in learning studies is a familiar example of a laboratory rule-discovery task (e.g. Bruner, Goodnow, & Austin, 1956;Levine, 1966;Trabasso & Bower, 1968). Here, the objects may be, for example, visual stimuli in different shapes, colors, and locations. Some choices of stimuli are reinforced, others are not. The learner's goal is to discover the rule or \"concept\" (e.g., red circles) that determines reinforcement. Wason (1960) was the first to use this type of task to study people's understanding of the logic of confirmation and disconfirmation. He saw the rule-discovery task as representative of an important aspect of scientific reasoning (see also Mahoney, 1976Mahoney, , 1979;;Mynatt et al., 1977Mynatt et al., , 1978;;Simon, 1973). To illustrate the parallel between rule discovery and scientific investigation, consider the following hypothetical case. You are an astro-physicist, and you have a hypothesis about what kinds of stars develop planetary systems. This hypothesis might be derived from a larger theory of astrophysics or may have been induced from past observation. The hypothesis can be expressed as a rule, such that those stars that have the features specified in the rule are hypothesized to have planets and those not fitting the rule are hypothesized to have no planets. We will use the symbol R H for the hypothesized rule, H for the set of instances that fit that hypothesis, and H for the set that do not fit it. There is a domain or \"universe\" to which the rule is meant to apply (e.g., all stars in our galaxy), and in that domain there is a target set (those stars that really do have planets). You would like to find the rule that exactly specifies which members of the domain are in the target set (the rule that describes exactly what type of stars have planets). We will use T for the target set, and R T for the \"correct\" rule, which specifies the target set exactly. Let us assume for now that such a perfect rule exists. (Alternate versions of the rule might exist, but for our purposes, rules can be considered identical if they specify exactly the same set T.) The correct rule may be extremely complex, including conjunctions, disjunctions, and trade-offs among features. Your goal as a scientist, though, is to bring the hypothesized rule R H in line with the correct rule R T and thus to have the hypothesized set H match the target set T. \\ou could then predict exactly which stars do and do not have planets. Similarly, a psychologist might wish to differentiate those who are at risk for schizophrenia from those who are not, or an epidemiologist might wish to understand who does and does not contract AIDS. The same structure can also be applied in a diagnostic context. For example, a diagnostician might seek to know the combination of signs that differentiates benign from malignant tumors. In each case, an important component of the investigative process is the testing of hypotheses. That is, the investigator wants to know if the hypothesized rule R H is identical to the correct rule R T and if not, how they differ. This is accomplished through the collection of evidence, that is, the examination of instances. For example, you might choose a star hypothesized to have planets and train your telescope on it to see if it does indeed have planets, or you might examine tumors expected to be benign, to see if any are in fact malignant. Wason (1960Wason ( , 1968) developed a laboratory version of rule discovery to study people's hypothesis-testing strategies (in particular, their use of confirmation and disconfirmation), in a task that \"simulates a miniature scientific problem\" (1960, p. 139). In Wason's task, the universe was made up of all possible sets of three numbers (\"triples\"). Some of these triples fit the rule, in other words, conformed to a rule the experimenter had in mind. In our terms, fitting the experimenter's rule is the target property that subjects must learn to predict. The triples that fit the rule, then, constitute the target set, T. Subjects were provided with one target triple (2,4,6), and could ask the experimenter about any others they cared to. For each triple the subject proposed, the experimenter responded yes (fits the rule) or no (does not fit). Although subjects might start with only a vague guess, they quickly formed an initial hypothesis about the rule (R H ). For example, they might guess that the rule was \"three consecutive even numbers.\" They could then perform one of two types of hypothesis tests (Htests): they could propose a triple they expected to be a target (e.g., 6, 8, 10), or a triple they expected not to be {e.g., 2,4,7). In this paper, we will refer to these as a positive hypothesis test (+Htest) and a negative hypothesis test (-Htest), respectively. Wason found that people made much more use of +Htests than -Htests. The subject whose hypothesis was \"consecutive evens,\" for example, would try many examples of consecutive-even triples and relatively few others. Subjects often became quite confident of their hypotheses after a series of+Htests only. In Wason's (1960) task this confidence was usually unfounded, for reasons we discuss later. Wason described the hypothesis testers as \"seeking confirmation\" because they looked predominantly at cases that fit their hypothesized rule for targets (e.g., different sets of consecutive even numbers). We think it more appropriate to view this \"confirmation bias\" as a manifestation of the general hypothesis-testing strategy we call the positive test (+ test) strategy. In rule discovery, the +test strategy leads to the predominant use of +Htests, in other words, a tendency to test cases you think will have the target property. The general tendency toward -(-testing has been widely replicated. In a variety of different rule-discovery tasks (KJayman & Ha, 1985;Mahoney, 1976Mahoney, , 1979;;Mynattetal., 1977Mynattetal., , 1978;;Taplin, 1975;Tweney et al., 1980;Wason & Johnson-Laird, 1972) people look predominantly at cases they expect will have the target property, rather than cases they expect will not As with nearly all strategies, people do not seem to adhere strictly to -(-testing, however. For instance, given an adequate number of test opportunities and a lack of pressure for a quick evaluation, people seem willing to test more widely (Gorman & Gorman, 1984;Klayman & Ha, 1985). Of particular interest is one manipulation that greatly improved success at Wason's 2,4,6 task. Tweney et al. (1980) used a task structurally identical to Wason's but modified the presentation of feedback. Triples were classified as either DAX or MED, rather than yes (fits the rule) or no (does not fit). The rule for DAX was Wason's original ascending-order rule, and all other triples were MED. Subjects in the DAX/MED version used even fewer -Htests than usual. However, they treated the DAX rule and the MED rule as two separate hypotheses, and tested each with -(-Htests, thereby facilitating a solution. The thrust of this work has been more than just descriptive, however. There has been a strong emphasis on the notion that a -Hest strategy (or something like it) will lead to serious errors or inefficiencies in the testing of hypotheses. We begin by taking a closer look at this assumption. We examine what philosophers of science such as Popper and Platt have been arguing, and how that translates to prescriptions for information gathering in different hypothesis-testing situations. We then examine the task characteristics that control the extent to which a -Hest strategy deviates from those prescriptions. We begin with rule discovery as described above, and then consider what happens if additional information is available (examples of known targets and nontargets), and if an element of probabilistic error is introduced. The basic question is, if you are trying to determine the truth or falsity of a hypothesis, when is a 4-test strategy unwise and when is it not? The Logic of Ambiguous Versus Conclusive Events As a class, laboratory rule-discovery tasks share three simplifying assumptions. First, feedback is deterministically accurate. The experimenter provides the hypothesis tester with error-free feedback in accordance with an underlying rule. Second, the goal is to determine the one correct rule (R T ). All other rules are classified as incorrect, without regard to how wrong R H may be, although the tester may be concerned with where it is wrong in order to form a new hypothesis. Third, correctness requires both sufficiency and necessity: A rule is incorrect if it predicts an instance will be in the target set when it is not (false positive), or predicts it will not be in the target set when it is (false negative). We discuss later the extent to which each of these assumptions restricts generalization to other tasks. Consider again Wason's original task. Given the triple (2, 4, 6), the hypotheses that occur to most people are \"consecutive even numbers,\" \"increasing by 2,\" and the like. The correct rule, however, is much broader, \"increasing numbers.\" Consider subjects whose hypothesized rule is \"increasing by 2.\" Those who use only +Htests (triples that increase by 2, such as 6, 8,10) can never discover that their rule is incorrect, because all examples of \"increasing by 2\" also fit the rule of \"increasing.\" Thus, it is crucial to try -Htests (triples that do not increase by 2, such as 2,4,7), This situation is depicted in Figure 1. Here, U represents the universe of instances, all possible triples of numbers. T represents the target set, triples that fit the experimenter's rule (\"increasing\"). H represents the hypothesized set, triples that fit the tester's hypothesized rule (say, \"increasing by 2\"). There are in principle four classes of instances, although they do not all exist in this particular example: 1. HOT: instances correctly hypothesized to be in the target set (positive hits). 2. HOT: instances incorrectly hypothesized to be in the target set (false positives). 3. HOT: instances correctly hypothesized to be outside the target set (negative hits). 4. HOT: instances incorrectly hypothesized to be outside the target set (false negatives). Instances of the types HnT and H n T falsify the hypothesis. That is, the occurrence of either shows conclusively that H ¥= T, thus RH i 1 RT? the hypothesized rule is not the correct one. Instances of the types H n T and H H f verify the hypothesis, in the sense of providing favorable evidence. However, these instances are ambiguous: The hypothesis may be correct, but these instances can occur even if the hypothesis is not correct. Note that there are only conclusive falsifications, no conclusive verifications. This logical condition is the backbone of philosophies of science that urge investigators to seek falsification rather than verification of their hypotheses (e.g., Popper, 1959). Put somewhat simplistically, a lifetime of verifications can be countered by a single conclusive falsification, so it makes sense for scientists to make the discovery of falsifications their primary goal. Suppose, then, that you are the tester in Wason's task, with the hypothesis of \"increasing by 2.\" If you try a -f Htest (e.g., 6, 8,10) you will get either a yes response, which is an ambiguous verification of the type H Pi T, or a no, which is a conclusive falsification of the type H Pi f. The falsification H n T would show that meeting the conditions of your rule is not sufficient to guarantee membership in T. Thus, +Htests can be said to be tests of the rule's sufficiency. However, unknown to the subjects in the 2, 4, 6, task (Figure 1) there are no instances of H n T, because the hypothesized rule is sufficient: Any instance following R H (\"increasing by 2\") will in fact be in the target set T (\"increasing\"). Thus, +Htests will never produce falsification. If you instead try a -Htest (e.g., 2,4,7) you will get_either a no answer which is an ambiguous verification (H fl T) or a yes answer which is a conclusive falsification (HOT). The falsification H fl T shows that your conditions are not necessary for membership in T. Thus, -Htests test a rule's necessity. In the 2, 4, 6 task, -Htests can result in conclusive falsification because R H is sufficient but not necessary (i.e., there are some target triples that do not increase by 2). In the above situation, the Popperian exhortation to seek falsification can be fulfilled only by -Htesting, and those who rely on +Htests are likely to be misled by the abundant verification they receive. Indeed, Wason deliberately designed his task so that this would be the case, in order to show the pitfalls of \"confirmation bias\" (Wason, 1962), The hypothesis-tester's situation is not always like this, however. Consider the situation in which the hypothesized set merely overlaps the target set, as shown in Figure 2, rather than being embedded within it, as  shown in Figure 1. This would be the case if, for example, the correct rule were \"three even numbers.\" There would be some members of H n T, instances that were \"increasing by 2\" but not \"three evens\" (e.g., 1, 3, 5), and some members of H fl T, \"three evens\" but not \"increasing by 2\" (e.g., 4, 6, 2). Thus, conclusive falsification could occur with either +Htests or -Htests. Indeed, it is possible to be in a situation just the opposite of Wason's, shown in Figure 3. Here, the hypothesis is too broad and \"surrounds\" the target set. This would be the case if the correct rule were, say, \"consecutive even numbers.\" Now a tester who did only -Htests could be sorely misled, because there are no falsifications of the type HOT; any instance that violates \"increasing by 2\" also violates \"consecutive evens.\" Only +Htests can reveal conclusive falsifications (HOT instances such as 1,3,5). Aside from these three situations, there are two other possible relationships between H and T. When H and T are disjoint (Figure 4), any +Htest will produce conclusive falsification, because nothing in H is in T; -Htests could produce either verification or falsification. This is not likely in the 2,4, 6 task, because you are given one known target instance to begin with. In the last case (Figure 5), you have finally found the correct rule, and H coincides with T. Here, every test produces ambiguous information; a final proof is possible only if there is a finite universe of instances and every case is searched. In naturally occurring situations, as in Wason's (1960) task, one could find oneself in any of the conditions depicted, usually with no way of knowing which. Suppose, for example, that you are a manufacturer trying to determine the best way to advertise your line of products, and your current hypothesis is that televi-sion commercials are the method of choice. For you, the universe, U, is the set of possible advertising methods; the target set, T, is the set of methods that are effective, and the hypothesized set, H, is television commercials. Suppose that in fact the set of effective advertising methods for these products is much broader: any visual medium (magazine ads, etc.) will work. This is the situation depicted in Figure 1. If you try +Htests (i.e., try instances in your hypothesized set, television commercials) you will never discover that your rule is wrong, because television commercials will be effective. Only by trying things you think will not work (-Htests) can you obtain falsification. \\bu might then discover an instance of the type HOT nontelevision advertising that is effective. Suppose instead that the correct rule for effectively advertising these products is to use humor. This is the situation in Figure 2. You could find a (serious) television commercial that you thought would work, but does not (H D T), or a (humorous) npntelevision ad that you thought would not work, but does (H Pi T). Thus, conclusive falsification could occur with either a +Htest or a -Htest. If instead the correct rule for these products is more restricted, say, \"prime-time television only,\" you would have an overly broad hypothesis, as shown in Figure 3. In that case, you will never obtain falsification if you use -Htests (i.e., if you experiment with methods you think will not work), because anything that is not on television is also not on prime time. Only + Htests can reveal conclusive falsifications, by finding instances of H n T (instances of television commercials that are not effective). What is critical, then, is not the testing of cases that do not fit your hypothesis, but the testing of cases that are most likely to prove you wrong. In Wason's task these two actions are identical, but as shown in Figures 2 through 5, this is not generally so. Thus, it is very important to distinguish between two different senses of \"seeking disconfirmation.\" One sense is to examine instances that you predict will not have the target property. The other sense is to examine instances you most expect to falsify, rather than verify, your hypothesis. This distinction has not been well recognized in past analyses, and confusion between the two senses of disconfirmation has figured in at least two published debates, one involving Wason (1960Wason ( , 1962) ) and Wetherick (1962), the other involving Mahoney (1979Mahoney ( , 1980)), Hardin (1980), andTweney, Doherty, andMynatt (1982). The prescriptions of Popper and Platt emphasize the importance of falsification of the hypothesis, whereas empirical investigations have focused more on the testing of instances outside the hypothesized set.",
          "Where's the Information? The distinction between -testing and seeking falsification leads to an important question for hypothesis testers: Given the choice between +tests and -tests, which is more likely to yield critical falsification? As is illustrated in Figures 1 through 5, the answer depends on the relation between your hypothesized set and the target set. This, of course, is impossible to know without first knowing what the target set is. Even without prescience of the truth, however, it is possible for a tester to make a reasoned judgment about which kind of test to perform. Prescriptions can be based on (at least) two considerations: (a) What type of errors are of most concern, and (b) Which test could be expected, probabilistically, to yield conclusive falsification more often. The first point hinges on the fact that +Htests and -Htests reveal different kinds of errors (false positives and false negatives, respectively). A tester might care more about one than the other and might be advised to test accordingly. Although there is almost always some cost to either type of error, one cost may be much higher than the other. For example, a personnel director may be much more concerned about hiring an incompetent person (H flT) than about passing over some potentially competent ones (H PI T). Someone in this position should favor +Htests (examining applicants judged competent, to find any failures) because they reveal potential false positives. On the other hand, some situations require greater concern with false negatives than false positives. For example, when dealing with a major communicable disease, it is more_serious to allow a true case to go undiagnosed and_untreated (H n T) than it is to mistakenly treat someone (H n T). Here the emphasis should be on -Htests (examining people who test negative, to find any missed cases), because they reveal potential false negatives. It could be, then, that a preference for +Htests merely reflects a greater concern with sufficiency than necessity. That is, the tester may simply be more concerned that all chosen cases are true than that all true cases are chosen. For example, experiments by Vogel and Annau (1973), Tschirgi (1980), and Schwartz (1981, 1982) suggest that an emphasis on the sufficiency of one's actions is enhanced when one is rewarded for each individual success rather than only for the final rule discovery. Certainly, in many real situations (choosing an employee, a job, a spouse, or a car) people must similarly live with their mistakes. Thus, people may be naturally inclined to focus more on false positives than on false negatives in many situations. A tendency toward +Htesting would be entirely consistent with such an emphasis. However, it is still possible that people retain an emphasis on sufficiency when it is inappropriate (as in Wason's task). Suppose that you are a tester who cares about both sufficiency and necessity: your goal is simply to determine whether or not you have found the correct rule. It is still possible to analyze the situation on the basis of reasonable expectations about the world. If you accept the reasoning of Popper and Platt, the goal of your testing should be to uncover conclusive falsifications. Which kind of test, then, should you expect to be more likely to do so? Assume that you do not know in advance whether your hypothesized set is embedded in, overlaps, or surrounds the target. The general case can be characterized by four quantities': The overall base-rate probability that a member of the domain is in the target set. This would be, for example, the proportion of stars in the galaxy that have planets. p(h) The overall probability that a member of the domain is in the hypothesized set. This would be the proportion of stars that fit your hypothesized criteria for having planets. The overall probability that a positive prediction will prove false, for example, that a star hypothesized to have planets will turn out not to. z\" = p(t|h) The overall probability that a negative prediction will prove false, for example, that a star hypothesized not to have planets will turn out in fact to have them. The quantities z + and z~ are indexes of the errors made by the hypothesis. They correspond to the false-positive rate and falsenegative rate for the hypothesized rule R H (cf. Einhorn & Hogarth, 1978). In our analyses, all four of the above probabilities are assumed to be greater than zero but less than one. 2 This corresponds to the case of overlapping target and hypothesis sets, as shown in Figure 2. However, other situations can be regarded as boundary conditions to this general case. For example, the embedded, surrounding, and coincident situations (Figures 1, 3, and5) are cases in which z + = p(t|h) = 0, z~ = p(t|h) = 0. or both, respectively, and in the disjoint situation (Figure 4), z + = 1. _ Recall that there are two sets of conclusive falsifications: H Q T (your hypothesis predicts planets, but there are none), and H n T (your hypothesis predicts no planets, but there are some ",
          "According to Equation 1, even if you have no information about z + and z~, you can estimate their relationship from estimates of the target and hypothesis base rates, p(t) and p(h). It is not necessarily the case that the tester knows these quantities exactly. However, there is usually some evidence available for forming estimates on which to base a judgment. In any case, it is usually easier to estimate, say, how many people suffer from schizophrenia than it is to determine the conditions that produce it. It seems reasonable to assume that in many cases the tester's hypothesis is at least about the right size. People are not likely to put much stock in a hypothesis that they believe greatly overpredicts or underpredicts the target phenomenon. Let us assume, then, that you believe that p(h) « p(t). Under these circumstances, Equation 1 can be approximated as (2) Thus, if p(t) < .5, then z* > z , which means that p(Fn|+Htest) > p(Fn|-Htest). In other words, if you are attempting to predict a minority phenomenon, you are more likely to receive falsification using +Htests than -Htests. We would argue that, in fact, real-world hypothesis testing most often concerns minority phenomena. For example, a recent estimate for the proportion of stars with planets is Vt (Sagan, 1980, p. 300), for the prevalence of schizophrenia, less than 1% (American Psychiatric Association, 1980), and for the incidence of AIDS in the United States, something between 10'' and 10\" 5 (Centers for Disease Control, 1986). Even in Wason's original task (1960), the rule that seemed so broad (any increasing) has a p(t) of only '/ 6 , assuming one chooses from a large range of numbers. Indeed, if p(t) were greater than .5, the perception of target and nontarget would likely reverse. If 80% of the population had some disease, immunity would be the target property, and p(t) would then be .2 (cf. Bourne & Guy, 1968;Einhorn& Hogarth, 1986). Thus, under some very common conditions, the probability of receiving falsification with +Htests could be much greater than with -Htests. Intuitively, this makes sense. When you are investigating a relatively rare phenomenon, p(i) is low and the set H is large. Finding a t in H (obtaining falsification with -Htests) can be likened to the proverbial search for a needle in a haystack. Imagine, for example, looking for AIDS victims among people believed not at risk for AIDS. On the other hand, these same conditions_also mean thatp(t) is high, and set H is small. Thus, finding a t in H (with +Htests) is likely to be much easier. Here, you would be examining people with the hypothesized risk factors. If you have a fairly good hypothesis, p(t|h) is appreciably lower than p(T), but you are still likely to find healthy people in the hypothesized risk group, and these cases are informative. (You might also follow a strategy based on examining known victims; we discuss this kind of testing later.) The conditions we assume above (a minority phenomenon, and a hypothesis of about the right size) seem to apply to many naturally occurring situations. However, these assumptions may not always hold. There may be cases in which a majority phenomenon is the target (e.g., because it was unexpected); then p(t) > .5. There may also be situations in which a hypothesis is tested even though it is not believed to be the right size, so that p(h) ^ p(t). For example, you may not be confident of your estimate for either p(t) or p(h), so you are not willing to reject a theoretically appealing hypothesis on the basis of those estimates. Or you may simply not know what to add to or subtract from your hypothesis, so that a search for falsification is necessary to suggest where to make the necessary change. In any case, a tester with some sense of the base rate of the phenomenon can make a reasoned guess as to which kind of test is more powerful, in the sense of being more likely to find critical falsification. The conditions under which -f Htests or -Htests are favored are summarized in Table 1. There are two main conclusions to be drawn from this analysis. First, it is important to distinguish between two possible senses of \"seeking disconfirmation\": (a) testing cases your hy- pothesis predicts to be non targets, and (b) testing cases that are most likely to falsify the hypothesis. It is the latter that is generally prescribed as optimal. Second, the relation between these two actions depends on the structure of the environment. Under some seemingly common conditions, the two actions can, in fact, conflict. The upshot is that, despite its shortcomings, the +test strategy may be a reasonable way to test a hypothesis in many situations. This is not to say that human hypothesis testers are actually aware of the task conditions that favor or disfavor the use of a +test strategy. Indeed, people may not be aware of these factors precisely because the general heuristic they use often works well.",
          "The 2,4,6 task involves only one-half of the proposed +test strategy, that is, the testing of cases hypothesized to have the target property (+Htesting). In some tasks, however, the tester may also have an opportunity to examine cases in which the target property is known to be present (or absent) and to receive feedback about whether the instance fits the hypothesis. For example, suppose that you hypothesize that a certain combination of home environment, genetic conditions, and physical health distinguishes schizophrenic individuals from others. It would be natural to select someone diagnosed as schizophrenic and check whether the hypothesized conditions were present. We will call this a positive target test (+Ttest), because you select an instance known to be in the target set. Similarly, you could examine the history of someone judged not to be schizophrenic to see if the hypothesized conditions were present. We call this a negative target test (-Ttest). Generally, Ttests may be more natural in cases involving diagnostic or epidemiological questions, when one is faced with known effects for which the causes and correlates must be determined. Ttests behave in a manner quite parallel to the Htests described above. A +Ttest results in verification (T f~l H) if the known target turns out to fit the hypothesized rule (e.g., someone diagnosed as schizophrenic turns out to have the history hypothesized to be distinctive to schizophrenia). A +Ttest results in falsification if a known target fails jo have the features hypothesized to distinguish targets (T l~l H). The jwobability of falsification with a +Test, designated x + , is p(h|t). This is equivalent to the miss rate of signal detection theory (Green & Swets, 1966). The_falsifying instances revealed by +Ttests (missed targets, T_n H) are the same kind revealed by -Htests (false negatives, H D T). Note, though, that the miss rate of +Ttests is calculated differently than the false negative rate of -Htests [x+ = p(h]t); z~ = p(t|h)]. Both +Ttests and -Htests assess whether the conditions in R H are necessary for schizophrenia. With -Ttests, verifications are of the type T n H (nonschizophrenks who do not have the history hypothesized for schizophrenics), and falsifications are of the type T n H (nonschizophrenics who do have that history). The probability of falsification with -Ttests, designated x~, is p(h|t). This is equivalent to the false alarm rate in signal detection theory. -Ttests and +Htests reveal the same kinds of falsifying instances (false alarms or false positives). The rate of falsification with -Ttests is x~ = p(hft) compared to 2* = />(t)h) for +Htests. Both -Ttests and +Htests assess whether the conditions in R H are sufficient. We can compare the two types of Ttests in a manner parallel to that used to compare Htests. The values x.* and jc\" (the miss rate and false alarm rate, respectively) can be related following the same logic used in Equation 1: (3) If we again assume thatp(t) < .5 andp(h) = p(l), then x + > x~. This means that +Ttests are more likely to result in falsification than are -Ttests. The full set of conditions favoring one type of Ttest over the other are shown in Table 2. Under common circumstances, it can be normatively appropriate to have a second kind of \"confirmation bias,\" namely, a tendency to test cases known to be targets rather than those known to be nontargets. It is also interesting to consider the relations between Ttests and Htests. In some situations, it may be more natural to think about one or the other. In an epidemiological study, for example, cases often come presorted as T or T (e.g., diagnosed victims of disease vs. normal individuals). In an experimental study, on the other hand, the investigator usually determines the presence or absence of hypothesized factors and thus membership in H or H (e.g., treatment vs. control group). Suppose, though, that you are in a situation where all four types of test are feasible. There are then two tests that reveal falsifications of the type H D T (false positives or false alarms), namely +Htests and -Ttests. These falsifications indicate that the hypothesized conditions are not sufficient for the target phenomenon. For example, suppose a team of meteorologists wants to test whether certain weather conditions are sufficient to produce tornadoes. The team can look for tornadoes where the hypothesized conditions exist (+Htests) or they can test for the conditions where tornadoes have not occurred (-Ttests). The probability of discovering falsification with each kind of test is as follows: p(Fn|+Htest)",
          "Thus, if we assume, as before, that p(t) < .5, and p(h) = p(t), then_z + > x~: the probability of finding a falsifying instance (h n T) is higher with +Htests than with -Ttests. _ There are also two tests that reveal falsifications of the type Thus, the probability of finding HOT falsifications is higher with +Ttests than with -Htests. These relationships reinforce the idea that it may well be advantageous in many situations to have two kinds of \"confirmation bias\" in choosing tests: a tendency to examine cases hypothesized to be targets (+Htests) and a tendency to examine cases known to be targets (+Ttests). Taken together, these two tendencies compose the general +test strategy. Under the usual assumptions [p(t) < .5 and p(t) <= p(h)], +Htests are favored over -Htests, and +Ttests over -Ttests, as more likely to find falsifications. Moreover, if you wish to test your rule's sufficiency, +Htests are better than -Ttests; if you wish to test the rule's necessity, +Ttests are better than -Htests. Thus, it may be advantageous for the meteorologists to focus their field research on areas with hypothesized tornado conditions and areas of actual tornado sighting (which, in fact, they seem to do; see Lucas & Whittemore, 1985). Like many other cognitive heuristics, however, this +test heuristic may prove maladaptive in particular situations, and people may continue to use the strategy in those situations nonetheless (cf. Hogarth, 1981;Tversky & Kahneman, 1974).",
          "Laboratory versions of rule discovery usually take place in a deterministic environment: There is a correct rule that makes absolutely no errors, and feedback about predictions is completely error-free (see Kern, 1983, andGorman, 1986, for interesting exceptions). In real inquiry, however, one does not expect to find a rule that predicts every schizophrenic individual or planetary system without error, and one recognizes that the ability to detect psychological disorders or celestial phenomena is imperfect. What, then, is the normative status of the +test heuristic in a probabilistic setting? Irreducible error. In a probabilistic environment, it is somewhat of a misnomer to call any hypothesis correct, because even the best possible hypothesis will make some false-positive and false-negative predictions. These irreducible errors might actually be due to imperfect feedback, but from the tester's point of view they look like false positives or false negatives. Alterna-tively, the world may have a truly random component, or the problem may be so complex that in practice perfect prediction would be beyond human reach. In any case, the set T can be defined as the set of instances that the feedback indicates are targets. A best possible rule, R B , can be postulated that defines the set B. B matches T as closely as possible, but not exactly. Because of probabilistic error, even the best rule makes falsepositive and false-negative prediction errors (i.e., p(t|b) > 0 and p(t|b) > 0). The probabilities of these errors, designated e + and t~, represent theoretical or practical minimum error rates. 3   Qualitatively, the most important difference between deterministic and probabilistic environments is that both verification and falsification are of finite value and subject to some degree of probabilistic error. Thus, falsifications are not conclusive but merely constitute some evidence against the hypothesis, and verifications must also be considered informative, despite their logical ambiguity. Ultimately, it can never be known with certainty that any given hypothesis is or is not the best possible. One can only form a belief about the probability that a given hypothesis is correct, in light of the collected evidence. Despite these new considerations, it can be shown that the basic findings of our earlier analyses still apply. Although the relationship is more complicated, the relative value of +tests and -tests is still a function of estimable task characteristics. In general, it is still the case that +tests are favored when p(t) is small and p(h) = p(t), as suggested earlier. Although we discuss only Htests here, a parallel analysis can be performed for Ttests as well.",
          "Assume that your goal is to obtain the most evidence you can about whether or not your current hypothesis is the best possible. Which type of test will, on average, be more informative? This kind of problem calls for an analysis of the expected value of information (e.g., see Edwards, 1965;Raiffa, 1968). Such analyses are based on Bayes's equation, which provides a normative statistical method for assessing the extent to which a subjective degree of belief should be revised in light of new data. To perform a full-fledged Bayesian analysis of value of information, it would be necessary to represent the complete reward structure of the particular task and compute the tester's subjective expected utility of each possible action. Such an analysis would be very complex or would require a great many simplifying assumptions. It is possible, though, to use a simple, general measure of \"impact,\" such as the expected change in belief (EAP). Suppose you think that there is some chance your hypothesis is the best possible, p(Rn = RB). Then, you perform a +Htest, and receive a verification (Vn). You would now have a somewhat higher estimate of the chance that your hypothesis is the best onep(R H = R B |Vn, +H). Call the impact of this test AP Vn , +H , the absolute magnitude of change in degree of belief. Of course, you might have received a falsification (Fn) instead, in which case your belief that R H = RB would be reduced by some amount, AP Fn ,+H-The expected change in belief for a +Htest, 3 For simplicity, we ignore the possibility that a rule might produce, say, fewer false positives but more false negatives than the best rule. We assume that the minimum c* and t~ can both be achieved at the same time. The more general case could be analyzed by defining a joint function olV and r which is to be minimized. given  RB). ( 10) This probabilistic analysis looks different from its deterministic counterpart in one respect. Before, the emphasis was strictly on falsification. Here, verification can sometimes be more informative than falsification. Using +Htests to illustrate, Equations 7 and 8 imply that if z* > .5, then AP V n,+H > AP Fn ,+H -A hypothesis with z* > .5 is a weak hypothesis; you believe the majority of predicted targets will prove wrong. Perhaps this is an old hypothesis that is now out of favor, or a new shot-in-thedark guess. The AP measure captures the intuition that surprise verification of a longshot hypothesis has more impact than the anticipated falsification. In considering the expected impact of a test, you must balance the greater impact of unexpected results against the fact that you do not think such results are likely to happen. With the EAP measure, the net result is that verifications and falsifications are expected to make equal contributions to changes in belief, overall (as shown in Equations 9 and 10). Verifications and falsifications have equal expected impact even in a deterministic environment, according to this definition of impact. The deterministic environment is merely a special case in which ( + = t~ = 0. Given this probabilistic view of the value of verification and falsification, where should one look for information? The answer to this question, based on the comparison between -t-Htests and -Htests, changes very little from the deterministic case. It would be a rational policy for a tester to choose the type of Htest associated with the greatest expected change in belief. In that case, according to Equations 1 1 and 1 2, you want to choose the test for which z -e is greatest: +Htests if (z + -« + ) > (z\" -e~). In other words, choose the test for which you believe the probability of falsification (z) is most above the level of irreducible error (c). This prescription is obviously very similar to the conditions specified for the deterministic environment. Indeed, if the two «s are equal (even if nonzero) the rule is identical: Choose the test with the higher z. Thus, the prescriptions shown in Table 1 hold in a probabilistic environment, as long as irreducible error is also taken into account. In the Appendix we also present an alternative measure of informativeness (a measure of \"diagnosticity\" often used in Bayesian analyses); the basic premises of our comparison remain intact. Qualitatively similar results obtain even when using a non-Bayesian analysis, based on statistical information theory (see Klayman, 1986).",
          "The foundation of our analysis is the separation of disconfirmation as a goal from discontinuation as a search strategy. It is a widely accepted prescription that an investigator should seek falsification of hypotheses. Our analyses show, though, that there is no correspondingly simple prescription for the search strategy best suited to that goal. The optimal strategy is a function of a variety of task variables such as the base rates of the target phenomenon and the hypothesized conditions. Indeed, even attempting falsification is not necessarily the path to maximum information (see also Klayman, 1986). We do not assume that people are aware of the task variables that determine the best test strategies. Rather, we suggest that people use a general, all-purpose heuristic, the positive test strategy, which is applied across a broad range of hypothesistesting tasks. Like any all-purpose heuristic, this +test strategy is not always optimal and can lead to serious difficulties in certain situations (as i n Wason's 2,4,6 task). However, our analyses show that +testing is not a bad approach in general. Under commonly occurring conditions, the +test strategy leads people to perform tests of both sufficiency and necessity (+Htests and +Ttests), using the types of tests most likely to discover violations of either.",
          "",
          "The main point of our analysis is not that people are better hypothesis testers than previously thought (although that may be so). Rather, the +test strategy can provide a basis for understanding the successes and failures of human hypothesis testing in a variety of situations. In this section, we apply our approach to several different hypothesis-testing situations. Each of the tasks we discuss has an extensive research literature of its own. However, there has been little cross-task generality beyond the use of the common \"confirmation bias\" label. We show how these diverse tasks can be given an integrative interpretation based on the general +test strategy. Each task has its unique requirements, and ideally, people should adapt their strategies to the characteristics of the specific task at hand. People may indeed respond appropriately to some of these characteristics under favorable conditions (when there is concrete task-specific information, light memory load, adequate time, extensive experience, etc.). We propose that, under less friendly conditions, hypothesis testers rely on a generally applicable default approach based on the +test strategy.",
          "At the beginning of this paper, we described the concept-identification task (Bruner et al., 1956) as a forerunner of Wason's rule-discovery task (Wason, 1960). In both tasks, the subject's goal is to identify the rule or concept that determines which of a subset of stimuli are designated as correct. In concept identification, however, the set of possible instances and possible rules is highly restricted. For example, the stimuli may consist of all combinations of four binary cues (letter X or T, large or small, black or white, on the right or left), with instructions to consider only simple (one-feature) rules (e.g., Levine, 1966). The hypothesis set, then, is restricted to only eight possibilities. Even when conjunctions or disjunctions of features are allowed (e.g., Bourne, 1974;Bruneret al., 1956), the hypothesis set remains circumscribed. A number of studies of concept identification have documented a basic win-stay, tose-shift strategy (e.g., see Levine, 1966Levine, ,1970;;Trabasso&Bower, 1968). That is, the learner forms an initial hypothesis about which stimuli are reinforced (e.g., \"Xs on the left\") and responds in accordance with that hypothesis as long as correct choices are produced. If an incorrect choice occurs, the learner shifts to a new hypothesis and responds in accordance with that, and so on. In our terms, this is +Htesting. It is what we would expect to see, especially since total success requires a rule that is sufficient for reward, only. In the concept-identification task + Htesting alone could lead to a successful solution. However, because there are only a finite number of instances (cue combinations), and a finite number of hypotheses, +testing is not the most effective strategy. A more efficient strategy is to partition the hypotheses into classes and perform a test that will eliminate an entire class of hypotheses in a single trial. For example, if a small, black X on the left is correct on one trial, the rules \"large,\" \"white,\" \"T,\" and \"right\" can all be eliminated at once. If on the next trial a large, black X on the right is correct, only \"black\" and \"X\" remain as possibilities, ignoring combinations. This \"focusing\" strategy (Bnmer et al.,19S6) is mathematically optimal but requires two things from subjects. First, they must recognize that having a circumscribed hypothesis set means it is possible to use a special efficient strategy not otherwise available. Second, focusing requires considerable cognitive effort to design an efficient sequence of tests and considerable memory demands to keep track of eliminated sets of hypotheses. Subjects sometimes do eliminate more than one hypothesis at a time, but considering the mental effort and memory capacity required by the normative strategy, it is not surprising that a basic +test heuristic predominates instead (Levine, 1966(Levine, , 1970;;Millward & Spoehr, 1973;Taplin, 1975).",
          "As suggested earlier, the +test strategy applies to both Htests and T tests. Thus, tasks that allow both are of particular interest. One example is the four-card problem (Wason, 1966(Wason, ,1968;;Wason & Johnson-Laird, 1972) and its descendants (e.g., Cox & Griggs, 1982;Evans & Lynch, 1973;Griggs, 1983;Griggs & Cox, 1982, 1983;Hoch & Tschirgi, 1983, 1985;Yachanin & Tweney, 1982). In these tasks, subjects are asked to determine the truth-value of the proposition \"if P then Q\" (P -• Q). For example, they may be asked to judge the truth of the following statement: \"If a card has a vowel on the front, it has an even number on the back\" (Wason, 1966(Wason, ,1968)). They are then given the opportunity to examine known cases of P, P, Q, and Q. For example, they can look at a card face-up with the letter E showing, face-up with the letter K, face-down with the number 4 showing, or face-down with the number 7. In our terms, this is a hypothesis-testing task in which \"has an even number on the back\" is the target property, and \"has a vowel on the front\" is the hypothesized rule that determines the target set. However, the implication P -* Q is not logically equivalent to the if-andonly-if relation tested in rule discovery: P is required only to be sufficient for Q, not also necessary. Subjects nevertheless use the same basic +test approach. From our point of view, to look at a vowel is to do a +Htest. The card with the consonant is a -Htest, the even number a +Ttest, and the odd number a -Ttest. If the +test heuristic is applied to problems of the form P -» Q, we would expect to find a tendency-to select the +Htest and the +Ttest (P and Q), or the -I-Htest only (P). Indeed, these choice patterns (P and Q, or P only) are the most commonly observed in a number of replications (Evans & Lynch, 1973;Griggs & Cox, 1982;Wason, 1966Wason, , 1968;;Wason & Johnson-Laird, 1972). However, there is a critical difference between the rule to be evaluated in the four-card problem and those in rule discovery. The implication P -»Q is subject to only one kind of falsification, P n Q. As a result, the +test strategy is inappropriate in this task. The only relevant tests are those that find false positives: +Htests and -Ttests (P and Q, e.g., E and 7). Earlier, we proposed that people would be able to move beyond the basic +test strategy under favorable conditions, and research on the four-card problem has demonstrated this. In particular, a number of follow-up studies have shown that a concrete context can point the way for subjects. Consider, for example, the casting of the problem at a campus pub serving beer and cola, with the proposition \"if a person is drinking beer, then the person must be over 19\" (Griggs & Cox, 1982). Here the real-world context alerts subjects to a critical feature of this specific task: The error of interest is \"beer-drinking and not-over-19\" (P n Q). The presence of people over 19 drinking cola (PflQ) is immaterial. In this version, people are much more likely to examine the appropriate cases, P and Q (beer drinkers and those under 19). Hoch andTschirgi (1983, 1985) have shown similar effects for more subtle and general contextual cues as well. Although there have been many explanations for the presence and absence of the P and Q choice pattern, a consensus seems to be emerging. The if/then construction is quite ambiguous in natural language; it often approximates a biconditional or other combination of implications (e.g., see Legrenzi, 1970;Politzer, 1986;Rumain, Connell, & Braine, 1983;Tweney & Doherty, 1983). A meaningful context disambiguates the task by indicating the practical logic of the situation. Some investigators have suggested that in an abstract or ambiguous task, people resort to a degenerate strategy of merely matching whatever is mentioned in the proposition, in other words, P and Q (Evans & Lynch, 1973;Hoch &Tschirgi, 1985;Tweney & Doherty, 1983). We suggest, however, that this heuristic of last resort is not a primitive refuge resulting from confusion or misunderstanding, but a manifestation of a more general default strategy (+testing) that turns out to be effective in many natural situations. People seem to require contextual or \"extra logical\" information (Hoch & Tschirgi, 1983) to help them see when this all-purpose heuristic is not appropriate to the task at hand.",
          "Snyder, Swann, and colleagues have conducted a series of studies demonstrating that people tend to seek confirmation of a hypothesis they hold about the personality of a target person (Snyder, 1981;Snyder & Campbell, 1980;Snyder & Swann, 1978;Swann & Giuliano, in press). For example, in some studies (Snydei;1981;Snyder & Swann, 1978), one group of subjects was asked to judge whether another person was an extrovert, and a second group was asked to determine whether that person was an introvert. Given a list of possible interview questions, both groups tended to choose \"questions that one typically asks of people already known to have the hypothesized trait\" (Snyder, 1981, p. 280). For example, subjects testing the extrovert hypothesis often chose the question \"What would you do if you wanted to liven things up at a party?\" This behavior is quite consistent with the +test heuristic. Someone's personality can be thought of as a set of behaviors or characteristics. To understand person A's personality is, then, to identify which characteristics in the universe of possible human characteristics belong to person A and which do not. That is, the target set (T) is the set of characteristics that are true of person A. The hypothesis \"A is an extrovert\" establishes a hypothesized set of characteristics (H), namely those that are true of extroverts. The goal of the hypothesis tester is, as usual, to determine if the hypothesized set coincides well with the target set. In other words, to say \"A is an extrovert\" is to say: \"If it is characteristic of extroverts, it is likely to be true of A, and if it is not characteristic of extroverts, it is likely not true of A.\" Following the +test strategy, you test this by examining extrovert characteristics to see if they are true of the target person (+Htests). The -t test strategy fails in these tasks because it does not take into account an important task characteristic: Some of the available questions are nondiagnostic. The question above, for example, is not very conducive to an answer such as \"Don't ask me, I never try to liven things up.\" Both introverts and extroverts accept the premise of the question and give similar answers (Swann, Giuliano, & Wegner, 1982). Subjects would better have chosen neutral questions (e.g., \"What are your career goals?\") that could be more diagnostic. However, it is not +Htesting that causes problems here; it is the mistaking of nondiagnostic questions for diagnostic ones (Fischhoff & Beyth-Marom, 1983;Swann, 1984). All the same, it is not optimal for testers to allow a general preference for +Htests to override the need for diagnostic information. A series of recent studies suggest that, given the opportunity, people do choose to ask questions that are reasonably diagnostic; however, they still tend to choose questions for which the answer is yes if the hypothesized trait is correct (Skov & Sherman, 1986;Strohmer& Newman, 1983;Swann & Giuliano, in press;Trope & Bassok, 1982, 1983;Trope, Bassok, & Alon, 1984). For example, people tend to ask a hypothesized introvert questions such as \"Are you shy?\" Indeed, people may favor +Htesting in part because they believe +Htests to be more diagnostic in general (cf. Skov & Sherman, 1986;Swann & Giuliano, in press). Interestingly, Trope and Bassok (1983) found this +Htesting tendency only when the hypothesized traits were described as extreme (e.g., extremely polite vs. on the polite side). If an extreme personality trait implies a narrower set of behaviors and characteristics, then this is consistent with our normative analysis of +Htesting: As p(t) becomes smaller, the advantage of +Htesting over -Htesting becomes greater (see Equations 1 and 2). Although only suggestive, the Trope and Bassok results may indicate that people have some salutary intuitions about how situational factors affect the +test heuristic (see also Swann & Giuliano, in press).",
          "So far we have only considered tasks in which the cost of information gathering and the availability of information are the same for +tests and -tests. However, several studies have looked at hypothesis testing in situations where tests are costly. Of particular ecological relevance are those tasks in which one must learn from the outcomes of one's actions. As mentioned earlier, studies by Tschirgi (1980) and Schwartz (1982) suggest that when test outcomes determine rewards as well as information, people attempt to replicate good results (reinforcement) and avoid bad results (nonreinforcement or punishment). This encourages +Htesting, because cases consistent with the best current hypothesis are believed more likely to produce the desired result. Einhorn and Hogarth (1978; see also Einhorn, 1980) provide a good analysis of how this can lead to a conflict between two important goals: (a) acquiring useful information to revise one's hypothesis and improve long-term success, and (b) maximizing current success by acting the way you think works best. Consider the case of a university admissions panel that must select or reject candidates for admission to graduate school. Typically, they admit only those who fit their hypothesis for success in school (i.e., those who meet the selection criteria). From the point of view of hypothesis testing, the admissions panel can check on selected candidates to see if they prove worthy (+Htests). It is much more difficult to check on rejected candidates (-Htests) because they are not conveniently collected at your institution and may not care to cooperate. Furthermore, you would really have to admit them to test them, because their outcome is affected by the fact that they were rejected (Einhorn & Hogarth, 1978). In other words, -Htests would require admitting some students hypothesized to be unworthy. However, if there is any validity to the admissions committee's judgment, this would have the immediate effect of reducing the average quality of admitted students. Furthermore, it would be difficult to perform either kind of Ttest in these situations. -l-Ttests and -Ttests would require checking known successes and known failures, respectively, to see whether you had accepted or rejected them. As before, information about people you rejected is bard to come by and is affected by the fact that you rejected them. The net result of these situational factors is that people are strongly encouraged to do only one kind of tests: +Htests. This limitation is deleterious to learning, because +Htests reveal only false positives, never false negatives. As in Wason's 2,4,6 task, this can lead to an overly restrictive rule for acceptance as you attempt to eliminate false-positive errors without knowing about the rate of false negatives. On the other hand, our analyses suggest that there are situations in which reliance on + Htesting may not be such a serious mistake. First, it might be the case that you care more about false positives than false negatives (as suggested earlier). You may not be too troubled by the line you insert in rejection letters stating that \"Regrettably, many qualified applicants must be denied admission.\" In this case, +Htesls are adequate because they reveal the more important errors, false positives. Even where both types of errors are important, there are many circumstances in which +Htests may be useful because false positives are more likely than false negatives (see Table 1). When P(t) = P(h) andp(t) < .5, for example, the false-positive rate is always greater than the false-negative rate. In other words, if only a minority of applicants is capable of success in your program, and you select about the right proportion of applicants, you are more likely to be wrong about an acceptance than a rejection. As always, the effectiveness of a +test strategy depends on the nature of the task. Learning from +Htests alone is not an optimal approach, but it may often be useful given the constraints of the situation.",
          "There has been considerable recent interest in how people make judgments of contingency or covariation between factors (e.g., see Alloy & Tabachnik, 1984;Arkes & Harkness, 1983;Crocker, 1981;Nisbett & Ross, 1980;Schustack & Steinberg, 1981;Shaklee & Mims, 1982), and one often-studied class of contingency tasks is readily described by the theoretical framework proposed in the present paper. These are tasks that require the subject to estimate the degree of contingency (or its presence or absence) between two dichotomous variables, on the basis of the presentation of a number of specific instances. For example, Ward and Jenkins (1965) presented subjects with the task of determining whether there was a contingency between the seeding of clouds and the occurrence of rainfall on that day. Subjects based their judgments on a series of slides, each of which indicated the state of affairs on a different day: (a) seeding + rain, (b) seeding + no rain, (c) no seeding + rain, or (d) no seeding + no rain. In our terms, the dichotomous-contingency task can be characterized as follows: The subject is presented with a target property or event and a set of conditions that are hypothesized to distinguish occurrences of the target from nonoccurrences. In the Ward and Jenkins (1965) example, the target event is rain, and the condition of having seeded the clouds is hypothesized to distinguish rainy from nonrainy days. This task is different from rule discovery in two ways. First, the hypothesized rule is not compared to a standard of \"best possible\" prediction, but rather to a standard of \"better than nothing.\" Second, the information search takes place in memory; the tester determines which information to attend to or keep track of rather than controlling its presentation. (A similar characterization is presented by Crocker, 1981.) Despite these differences, we propose that the basic +test strategy is manifested in covariation judgment much as it is in other, more external tasks. The event types listed above can be mapped onto our division of instances into H and H, T and T (see Table 3). The labels given the cells, A, B, C, and D, correspond to the terminology commonly used in studies of contingency. One possible evaluation strategy in such a problem is to think of cases in which the conditions were met (days with cloud seeding), and estimate how often those cases possessed the target property (rain). This is +Htesting: examining instances that Similarly, one could think of cases in which the target property occurred (it rained) to see whether the hypothesized conditions were met (clouds had been seeded). This is equivalent to +Ttesting, based on instances in cells A and C. We expect, as usual, that people will favor +Htests and +Ttests over -Htests and -Ttests. We also expect that there may be a tendency toward +Htesting in particular, because of greater attention to the sufficiency of rules than to their necessity (e.g., you do not mind if it rains sometimes without seeding). Also, many contingency tasks are framed in terms of the relation between causes and effects. Htests may be more natural then, because they are consistent with the temporal order of causation, moving from known causes to possible results (cf. Tversky & Kahneman, 1980). These hypotheses lead to some specific predictions about people's judgments of contingency. On a group level, judgments The data from a variety of studies support these predictions. Schustack and Sternberg (1981), for example, found that the contingency judgments of subjects taken as a group were best modeled as a linear combination of the number of instances of each of the four types, with the greatest emphasis placed on Acell, B-cell, C-cell, and D-cell data, in that order. Similar results were reported in an experiment by Arkes and Harkness (1983, Experiment 7), and in a meta-analysis of contingency-judgment tasks by Lipe (1982). A number of studies have also examined data from individual subjects. Although some studies indicate that people are influenced almost entirely by A-cell data (Jenkins & Ward, 1965;Nisbett & Ross, 1980;Smedslund, 1963), there is now considerable evidence for the prevalence of an A -B strategy (Arkes & Harkness, 1983;Shaklee&Mims, 1981,1982;Ward & Jenkins, 1965). This label has been applied to strategies that compare the number of H n T instances with the number of H n T (Cell A vs. CeUB) as well as strategies that compare T n H (Cell A) with T n H {Cell Q. The first comparison is consistent with our idea of +Htesting, the second with +Ttesting. These two kinds of comparison have not been clearly distinguished in the literature. For example, Arkesand Harkness (1983) sometimes label the condition-but-no-event cell as B, and sometimes the eventhut-no-condition cell as B. However, in one study, Shaklee and Mims (! 981) were able to distinguish A -B and A -C patterns in their data and found evidence of both. Further evidence of a +test approach is found in a recent study by Doherty and Falgout (1985). They presented the Ward and Jenkins (1965) cloud-seeding task on a computer screen and enabled subjects to save instances in computer memory for later reference. Although there were large individual differences, the most common pattern was to save a record of instances in cells A and B (the results of +Htests). The second most common pattern was to save A-, B-, and C-ceB instances (+Htests and +Ttests), and the third most common pattern was B and C (the falsifications from +Htests and +Ttests). Together, these 3 patterns accounted for 32 of 40 data-saving patterns in two experiments. In contingency judgment as in rule discovery, the +test strategy can often work well as a heuristic for hypothesis testing. However, this approach can deviate appreciably from statistical standards under some circumstances. Most statistical indexes (e.g., chi-square or correlation coefficient) put equal weight on all four cells, which -f testing does not Are people capable of more sophisticated strategies? Shaklee andMims (1981,1982) and Arkes and Harkness (1983) describe a sum-of-diagonals strategy that generally fares well as a rough estimate of statistical contingency. However, a simple combination of+Htests and +Ttests would result in a pattern of judgments very similar to the sum-of-diagonals strategy. A stimulus set could be carefully constructed to discriminate the two, but in the absence of such studies, we suspect that many sum-of-diagonals subjects may actually be using a combination of A versus B (+Htests) and A versus C (+Ttests). This may explain why individual analyses indicate frequent use of sum-of-diagonals strategies whereas group analyses often indicate that D-cell data is given little weight. On the other hand, we would expect that subjects might use more sophisticated strategies under favorable circumstances. There is some evidence that reduced memory demands have such an effect. Contingency judgments are more sophisticated when data are presented in summary form, rather than case by case (Arkes & Harkness, 1983;Shaklee & Mims, 1981, 1982;Shaklee & Tucko;1980;Ward & Jenkins, 1965). Also, the problem context and the wording of the question may direct attention to relevant sources of data (Arkes & Harkness, 1983;Crockei;1982;Einhorn& Hogarth, 1986).",
          "The concept of a general +test strategy provides an integrative interpretation for phenomena in a wide variety of hypothesis-testing tasks. This interpretation also prompts a number of new theoretical and empirical questions. There are several ways our analyses can be extended to explore further the nature of hypothesis-testing tasks and the strategies people use to accomplish them. We present a few examples here. In this article we discuss tasks in which the goal is to determine the correctness of a single hypothesis. This is a common situation, since people (including scientists) tend to view hypothesis testing in terms of verifying or falsifying one particular hypothesis (Mftroff, l974;Tweney, 1984Tweney, ,1985;;Tweney& Doherty, 1983;Tweney et al., 1980). On the other hand, it would be interesting to analyze the use of simultaneous alternate hypotheses in obtaining informative tests of hypotheses (see Figure 6). The importance of specific alternatives has been emphasized in laboratory hypothesis-testing studies (e.g., Wason & Johnson-Laird, 1972, chap. 16) and in philosophical discussions (e.g., Platt, 1964). An analysis like ours could be used to examine how alternate hypotheses can increase the expected information from tests, under what circumstances an alternative is not useful (e.g., with a straw-man hypothesis), and when it would be better to simultaneously verify or falsify two alternatives rather than perform a test that favors one over the other. From a theoretical perspective, it might also be interesting to examine a situation in which a larger set of alternate hypotheses are evaluated simultaneously. This may not be representative of ordinary scientific thought, but could provide an interesting normative standard (cf. Edwards, 1965;Raiffa, 1968). It is also akin to problems commonly faced by artificial intelligence researchers in designing expert systems to perform diagnostic tasks (see, e.g., Duda&Shortliffe, 1983;Fox, 1980). Another possible extension of these analyses is to consider standards of comparison other than \"correct\" or \"best possible.\" la many situations, it may be more appropriate to ask whether or not your hypothesis is \"pretty good,\" or \"good enough,\" or even \"better than nothing.\" Then, instead of comparing error rates to irreducible minima (t + and O, you are comparing them to other standards (s* and s~). Similarly, it would be possible to consider the testing of a rule for estimating a continuous variable rather than for predicting the presence or absence of a property. What you want to know then is the expected amount of error, rather than just the probability of error. Our theoretical analyses also suggest a number of interesting empirical questions concerning toe ways in which people adapt their strategies to the task at hand. For example, we indicate that certain task variables have a significant impact on how effective the -Hest strategy is in different situations. We do not know the extent to which people respond to these variables, or whether they respond appropriately. For example, do people use -Htests more when the target set is large? Will they do so if the cost of false negative guesses is made clear? Our review of existing research suggests that people may vary their approach appropriately under favorable conditions. However, there is still much to learn about how factors such as cognitive bad and taskspecific information affect hypothesis-testing strategies. Finally, there is a broader context of hypothesis formation and revision that should be considered as well. We have focused on the process of finding information to test a hypothesis. The broader context also includes questions about how to interpret yourfindings(e.g., seeDarley&Gross, 1983;Hoch&Ha, 1986;Lord et al., 1979). The astrophysicist must decide if the blur in the picture is really a planer, the interviewer must judge whether the respondent has given an extroverted answer. Moreover, questions about how hypotheses are tested are inevitably linked to questions about how hypotheses are generated. The latter sort of questions have received much less attention, however, possibly because they are harder to answer (but see, e.g., Gettys, 1983;Gettys & Fisher, 1979). Obtaining falsification is only a first step. The investigator must use that information to build a new hypothesis and must then do further testing. Thus, analyses of hypothesis testing and hypothesis generation will be mutually informative.",
          "Over the past 30 years, there have been scores of studies on the nature of hypothesis testing in scientific investigation and in everyday reasoning. Many investigators talk about confirmation bias, bat this term has been applied to many different phenomena in a variety of contexts. In our review of the literature, we find that different kinds of \"confirmation bias\" can be understood as resulting from a basic hypothesis-testing heuristic, which we call the positive test strategy. That is, people tend to test hypotheses by looking at instances where the target property is hypothesized to be present or is known to be present. This +test strategy, in its various manifestations, has generally been regarded as incompatible with the prescription to seek disconfirmation. The central idea of this prescription is that the hypothesis tester should make a deliberate attempt to find any evidence that would falsify the current hypothesis. As we show, however, +testing does not necessarily contradict the goal of seeking falsification. Indeed, under some circumstances, +testing may be the only way to discover falsifying instances (see Fig- ure 3). Furthermore, in probabilistic environments, it is not even necessarily the case that falsification provides more information than verification. What is best depends on the characteristics of the specific task at hand. Our review suggests that people use the +test strategy as a general default heuristic. That is, this strategy is one that people use in the absence of specific information that identifies some tests as more relevant than others, or when the cognitive demands of the task preclude a more carefully designed strategy. Our theoretical analyses indicate that, as an all-purpose heuristic, +testing often serves the hypothesis tester well. That is probably why it persists, despite its shortcomings. For example, if the target phenomenon is relatively rare, and the hypothesis roughly matches this base rate, you are probably better off testing where you do expect the phenomenon to occur or where you know the phenomenon occurred rather than the opposite. This situation characterizes many real-world problems. Moreover, +tests may be less costly or less risky than -tests when realworld consequences are involved (Einhorn & Hogarth, 1978;Tschirgi, 1980). Like most general-purpose heuristics, however, +testing can lead to problems when applied inappropriately. In rule discovery, it can produce misleading feedback by failing to reveal a whole class of important falsifications (violations of necessity). In propositional reasoning (e.g., the four-card problem), +testing leads to superfluous tests of necessity (+Ttests) and neglect of some relevant tests of sufficiency (-Ttests). In a variety of tasks, including concept indentification, intuitive personality testing, and contingency judgment, a +test strategy can lead to inefficiency or inaccuracy by overweighting some data and underweighting others. The consequences of using a +test strategy vary with the characteristics of the task. Our task analyses serve two major functions. First, they nighlight some of the structural similarities among diverse tasks in the broad domain of hypothesis testing. This permits integration of findings from different subareas that have so far been fairly isolated from each other. Second, our approach provides a framework for analyzing what each task requires of the subject, why people make the mistakes they do, and why changes in the structure and content of tasks sometimes produce significant changes in performance. These questions are central to understanding human hypothesis testing in the larger context of practical and scientific reasoning."
        ],
        "ground_truth_definitions": {
          "confirmation bias": {
            "type": "implicit",
            "definition": "a tendency to test cases known to be targets rather than those known to be nontargets",
            "context": "Under common circumstances, it can be normatively appropriate to have a second kind of \"confirmation bias,\" namely, a tendency to test cases known to be targets rather than those known to be nontargets. It is also interesting to consider the relations between Ttests and Htests"
          }
        }
      },
      {
        "paper_id": "4d65bab6f9b1fd61885fe187e6014a89592df187",
        "sections": [
          "Openness and transparency have long been considered key pillars of the scientific ethos (Merton 1973). Yet there is growing awareness that current research practices often deviate from this ideal, and can sometimes produce misleading bodies of evidence (Miguel et al. 2014). As we survey in this article, there is growing evidence documenting the prevalence of publication bias in economics and other scientific fields, as well as specification searching, and widespread inability to replicate empirical findings. Though peer review and robustness checks aim to reduce these problems, they appear unable to solve the problem entirely. While some of these issues have been widely discussed within economics for some time (for instance, see Leamer 1983;Dewald, Thursby, and Anderson 1986;DeLong and Lang 1992), there has been a notable recent flurry of activity documenting these problems, and also generating new ideas for how to address them. The goal of this piece is to survey this emerging literature on research transparency and reproducibility, and synthesize the insights emerging in economics as well as from other fieldsawareness of these issues has also recently come to the fore in political science (Gerber, Green, and Nickerson 2001;Franco, Malhotra, and Simonovits 2014), psychology (Simmons, Nelson, and Simonsohn 2011;Open Science Collaboration 2015), sociology (Gerber and Malhotra 2008a), finance (Harvey, Liu, and Zhu 2015), and other research disciplines as well, including medicine (Ioannidis 2005). We also discuss productive avenues for future work. With the vastly greater computing power of recent decades and the ability to run a nearly infinite number of regressions (Sala-i-Martin 1997), there is renewed concern that nullhypothesis statistical testing is subject to both conscious and unconscious manipulation. At the same time, technological progress has also facilitated various new tools and potential solutions, including by streamlining the online sharing of data, statistical code, and other research materials, as well as the creation of easily accessible online study registries, data repositories, and tools for synthesizing research results across studies. Data-sharing and replication activities are certainly becoming more common within economics research. Yet, as we discuss below, the progress to date is partial, with some journals and fields within economics adopting new practices to promote transparency and reproducibility and many others not (yet) doing so. The rest of the paper is organized as follows: Section 2 focuses on documenting the problems, first framing them with a simple model of the research and publication process (subsection 2.1), then discussing publication bias (subsection 2.2), specification searching (subsection 2.3), and the inability to replicate results (subsection 2.4). Section 3 focuses on possible solutions to these issues: improved analytical methods (subsection 3.1), study registration (subsection 3.2) and pre-analysis plans (subsection 3.3), disclosure and reporting standards (subsection 3.4), and open data and materials (subsection 3.5). Section 4 discusses future directions for research as well as possible approaches to change norms and practices, and concludes.",
          "Multiple problems have been identified within the body of published research results in economics. We focus on three that have come under greater focus in the recent push for transparency: publication bias, specification searching, and an inability to replicate results. Before describing them, it is useful to frame some key issues with a simple model.",
          "A helpful model to frame some of the issues discussed below was developed in the provocatively titled \"Why Most Published Research Findings Are False\" by Ioannidis (2005), which is among the most highly cited medical research articles from recent years. Ioannidis develops a simple model that demonstrates how greater flexibility in data analysis may lead to an increased rate of false positives and thus incorrect inference. Specifically, the model estimates the positive predictive value (PPV) of research, or the likelihood that a claimed empirical relationship is actually true, under various assumptions. A high PPV means that most claimed findings in a literature are reliable; a low PPV means the body of evidence is riddled with false positives. The model is similar to that of Wacholder et al. (2004), which estimates the closely related false positive report probability (FPRP). 2For simplicity, consider the case in which a relationship or hypothesis can be classified in a binary fashion as either a \"true relationship\" or \"no relationship\". Define Ri as the ratio of true relationships to no relationships commonly tested in a research field i (e.g., development economics). Prior to a study being undertaken, the probability that a true relationship exists is thus Ri/(Ri+1). Using the usual notation for statistical power of the test (1-β) and statistical significance level (α), the PPV in research field i is given by: (eqn. 1) PPVi = (1-β)Ri⁄((1-β)Ri + α) Clearly, the better powered the study, and the stricter the statistical significance level, the closer the PPV is to one, in which case false positives are largely eliminated. At the usual significance level of α = 0.05 and in the case of a well-powered study (1 -β=0.80) in a literature in which half of all hypotheses are thought to be true ex ante (Ri = 0.5), the PPV is relatively high at 89%, a level that would not seem likely to threaten the validity of research in a particular economics subfield. However, reality is considerably messier than this best case scenario and, as Ioannidis describes, this could lead much high rates of false positives in practice due to the presence of underpowered studies, specification searching and researcher bias, and the possibility that only a subset of the analysis in a research literature is published. We discuss these extensions in turn. We start with the issue of statistical power. Doucouliagos and Stanley (2013), Doucouliagos, Ioannidis, and Stanley (2016), and others have documented that many empirical economics studies are actually quite underpowered. With a more realistic level of statistical power for many studies, say at 0.50, but maintaining the other assumptions above, the PPV falls to 83%, which is beginning to potentially look like more of a concern. For power = 0.20, fully 33% of statistically significant findings are false positives. This concern, and those discussed next, are all exacerbated by bias in the publication process. If all estimates in a literature were available to the scientific community, researchers could begin to undo the concerns over a low PPV by combining data across studies, effectively achieving greater statistical power and more reliable inference, for instance, using meta-analysis methods. However, as we discuss below, there is growing evidence of a pervasive bias in favor of significant results, in both economics and other fields. If only significant findings are ever seen by the researcher community, then the PPV is the relevant quantity for assessing how credible an individual result is likely to be. Ioannidis extends the basic model to account for the possibility of what he calls researcher bias. Denoted by u, researcher bias is defined as the probability that a researcher presents a non-finding as a true finding, for reasons other than chance variation in the data. This researcher bias could take many forms, including any combination of specification searching, data manipulation, selective reporting, and even outright fraud; below we attempt to quantify the prevalence of these behaviors among researchers. There are many checks in place that attempt to limit this bias, and through the lens of empirical economics research, we might hope that the robustness checks typically demanded of scholars in seminar presentations and during journal peer review manage to keep the most extreme forms of bias in check. Yet we believe most economists would agree that there remains considerable wiggle room in the presentation of results in practice, in most cases due to behaviors that fall far short of outright fraud. Extending the above framework to incorporate the researcher bias term (ui) in field i leads to the following expression: (eqn. 2) PPVi = ((1 -β)Ri + uiβRi)/( (1 -β)Ri + α + uiβRi + ui(1-α)) Here the actual number of true relationships (the numerator) is almost unchanged, though there is an additional term that captures the true effects that are correctly reported as significant only due to author bias. The total number of reported significant effects could be much larger due to both sampling variation and author bias. If we go back to the case of 50% power, Ri = 0.5, and the usual 5% significance level, but now assume that author bias is low at 10%, the PPV falls from 83% to 65%. If 30% of authors are biased in their presentation of results, the PPV drops dramatically to 49%, meaning that nearly half of reported significant effects are actually false positives. In a further extension, Ioannidis examines the case where there are ni different research teams in a field i generating estimates to test a research hypothesis. Once again, if only the statistically significant findings are published, so there is no ability to pool all estimates, then the likelihood that any published estimate is truly statistically significant can again fall dramatically. In Table 1 (a reproduction of Table 4 from Ioannidis (2005), we present a range of parameter values and the resulting PPV. Different research fields may have inherently different levels of the Ri term, where presumably literatures that are in an earlier stage and thus more exploratory presumably have lower likelihoods of true relationships. This simple framework brings a number of the issues we deal with in this article into sharper relief, and contains a number of lessons. Ioannidis (2005) himself concludes that the majority of published findings in medicine are likely to be false, and while we are not prepared to make a similar claim for empirical economics research -in part because it is difficult to quantify some of the key parameters in the model -we do feel that this exercise does raise important concerns about the reliability of findings in many literatures. First off, literatures characterized by statistically under-powered (i.e., small 1-β) studies are likely to have many false positives. A study may be under-powered both because of small sample sizes, and if the underlying effect sizes are relatively small. A possible approach to address this concern is to employ larger datasets or estimators that are more powerful. Second, the hotter a research field, with more teams (ni) actively running tests and higher stakes around the findings, the more likely it is that findings are false positives. This is both due to the fact that multiple testing generates more false positives (in absolute numbers) and also because author bias (ui) may be greater when the stakes are higher. Author bias is also a concern when there are widespread prejudices in a research field, for instance, against publishing findings that contradict core theoretical concepts or assumptions. Third, the greater the flexibility in research design, definitions, outcome measures, and analytical approaches in a field, the less likely the research findings are to be true, again due to a combination of multiple testing concerns and author bias. One possible approach to address this concern is to mandate greater data sharing so that other scholars can assess the robustness of results to alternative models. Another is through approaches such as pre-analysis plans that effectively force scholars to present a certain core set of analytical specifications, regardless of the results. With this framework in mind, we next present empirical evidence from economics and other social science fields regarding the extent of some of the problems and biases we have been discussing, and then in section 3 turn to potential ways to address them.",
          "Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors. If the research community is unable to track the complete body of statistical tests that have been run, including those that fail to reject the null (and thus are less likely to be published), then we cannot determine the true proportion of tests in a literature that reject the null. Thus it is critically important to understand how many tests have been run. The term \"file drawer problem\" was coined decades ago (Rosenthal 1979) to describe this problem of results that are missing from a body of research evidence. The issue was a concern even earlier, see, for example, Sterling (1959) which warned of \"embarrassing and unanticipated results\" from type I errors if not significant results went unpublished. Important recent research by Franco, Malhotra, and Simonovits (2014) affirms the importance of this issue in practice in contemporary social science research. They document that a large share of empirical analyses in the social sciences are never published or even written up, and the likelihood that a finding is shared with the broader research community falls sharply for \"null\" findings, i.e., that are not statistically significant (Franco, Malhotra, and Simonovits 2014). Cleverly, the authors are able to look inside the file drawer through their access to the universe of studies that passed peer review and were included in a nationally representative social science survey, namely, the NSF-funded Time-sharing Experiments in the Social Sciences, or TESS3 . TESS funded studies across research fields, including in economics, e.g., Walsh, Dolfin, and DiNardo (2009) and Allcott and Taubinsky (2015), as well as political science, sociology and other fields. Franco, Malhotra, and Simonovits successfully tracked nearly all of the original studies over time, keeping track the nature of the empirical results as well as the ultimate publication of the study, across the dozens of studies that participated in the original project. They find a striking empirical pattern: studies where the main hypothesis test yielded null results are 40 percentage points less likely to be published in a journal than a strongly statistically significant result, and a full 60 percentage points less likely to be written up in any form. This finding has potentially severe implications for our understanding of findings in whole bodies of social science research, if \"zeros\" are never seen by other scholars, even in working paper form. It implies that the positive predictive value (PPV) of research is likely to be lower than it would be otherwise, and also has negative implications for the validity of meta-analyses, if null results are not known to the scholars attempting to draw broader conclusions about a body of evidence. Figure 1 reproduces some of the main patterns from Franco, Malhotra, and Simonovits (2014), as described in (Mervis 2014b). Consistent with these findings, other recent analyses have documented how widespread publication bias appears to be in economics research. Brodeur et al. (2016)  (i.e., above 1.96 for t-statistics). This is a strong indication that some combination of author bias and publication bias is fairly common. Using a variety of possible underlying distributions of test statistics, and estimating how selection would affect these distributions, they estimate the residual (\"the valley and the echoing bump\") and conclude that between 10 to 20% of marginally significant empirical results in these journals are likely to be unreliable. They also document that the proportion of misreporting appears to be lower in articles without \"eye-catchers\" (such as asterisks in tables that denote statistical significance), as well as in papers written by more senior authors, including those with tenured authors. A similar pattern strongly suggestive of publication bias also appears in other social science fields including political science, sociology, psychology, as well as in clinical medical research. Gerber and Malhotra (2008a) have used the caliper test, which compares the frequency of test statistics just above and below the key statistical significance cutoff, which is similar in spirit to a regression discontinuity design. Specifically, they compare the number of z-scores lying in the interval [1.96 -𝑋𝑋%, 1.96] to the number in (1.96, 1.96 + 𝑋𝑋%], where X is the size of the caliper, and they examine these differences at 5%, 10%, 15%, and 20% critical values.4  These caliper tests are used to examine reported empirical results in leading sociology journals (the American Sociological Review, American Journal of Sociology, and The Sociological Quarterly) and reject the hypothesis of no publication bias at the 1 in 10 million level (Gerber and Malhotra 2008a). Data from two leading political science journals (the American Political Science Review and American Journal of Political Science) reject the hypothesis of no publication bias at the 1 in 32 billion level (Gerber and Malhotra 2008b). Psychologists have recently developed a related tool called the \"p-curve,\" describing the density of reported p-values in a literature, that again takes advantage of the fact that if the null hypothesis were true (i.e., no effect), p-values should be uniformly distributed between 0 and 1 (Simonsohn, Nelson, and Simmons 2014a). Intuitively, under the null of no effect, a p-value <0.08 should occur 8% of the time, a p-value <0.07 occurs 7% of the time, etc., meaning a pvalue between 0.07 and 0.08, or between any other 0.01-wide interval, should occur 1% of the time. In the case of true non-zero effects, the distribution of p-values should be right-skewed (with a decreasing density), with more low values (0.01) than higher values (0.04) (Hung et al. 1997). 5 In contrast, in bodies of empirical literature suffering from publication bias, or \"phacking\" in their terminology, in which researchers evaluate significance as they collect data and only report results with statistically significant effects, the distribution of p-values would be leftskewed (assuming that researchers stop searching across specifications or collecting data once the desired level of significance is achieved). To test whether a p-curve is right or left skewed, one can construct what the authors call a \"pp-value,\" or p-value of the p-value-the probability of observing a significant p-value at least as extreme if the null were true-and then aggregate the pp-values in a literature with Fisher's method and test for skew with a χ 2 test. The authors also suggest a test of comparing whether a p-curve is flatter than the curve that would result if studies were (somewhat arbitrarily) powered at 33%, and interpret a p-curve that is significantly flatter or left skewed than this as lacking in evidentiary value. The p-curve can also potentially be used to correct effect size estimates in literatures suffering from publication bias; corrected estimates of the \"choice overload\" literature exhibit a change in direction from standard published estimates (Simonsohn,   Nelson, and Simmons 2014b). 6   Thanks to the existence of study registries and ethical review boards in clinical medical research, it is increasingly possible to survey nearly the universe of studies that have been undertaken, along the lines of Franco, Malhotra, and Simonovits (2014). Easterbrook et al. (1991) reviewed the universe of protocols submitted to the Central Oxford Research Ethics Committee, and both Turner et al. (2008) and Kirsch et al. (2008) employ the universe of tests of certain anti-depressant drugs submitted to the FDA, and all found significantly higher publication rates when tests yield statistically significant results. Turner et al. found that 37 of 38 (97%) of trials with positive, i.e., statistically significant, results were published, while only 8 of 24 (33%) with null (or negative) results were published; for a meta-meta-analysis of the latter two studies, see Ioannidis (2008). A simple model of publication bias described in McCrary, Christensen, and Fanelli (2015) suggests that, under some relatively strong assumptions regarding the rate of nonpublication of statistically non-significant results, readers of research studies could potentially adjust their significance threshold to \"undo\" the distortion by using a more stringent t-test statistic of 3.02 (rather than 1.96) to infer statistical significance at 95% confidence. They note that approximately 30% of published test statistics in the social sciences fall between these two cutoffs. It is also possible that this method would break down and result in a \"t-ratio arms race\" if all researchers were to use it, so it is mostly intended for illustrative purposes. As an aside, it is also possible that publication bias could work against rejection of the null hypothesis in some cases. For instance, within economics in cases where there is a strong theoretical presumption among some scholars that the null hypothesis of no effect is likely to hold (e.g., in certain tests of market efficiency) the publication process could be biased by a preference among editors and referees for non-rejection of the null hypothesis of no effect. This complicates efforts to neatly characterize the nature of publication bias, and may limit the application of the method in McCrary, Christensen, and Fanelli (2015). Taken together, a growing body of evidence indicates that publication bias is widespread in economics and many other scientific fields. Stepping back, these patterns do not appear to occur by chance, but are likely to indicate some combination of selective editor (and referee) decision-making, the file-drawer problem alluded to above, and/or widespread specification searching (the focus of the next subsection), which is closely related to what the Ioannidis (2005) model calls author bias.",
          "Scholars working in several specific literatures within economics have argued for the presence of considerable publication bias, including labor economics literatures on minimum wage impacts and on the value of a statistical life, and we discuss both briefly here, as well as several other bodies of evidence in economics. Card and Krueger (1995) conducted a meta-analysis of the minimum wage and unemployment literature, and test for the \"inverse-square-root\" relationship between sample size and t-ratio that one would expect if there was a true effect and no publication bias, since larger samples should generally produce more precise estimates (for a given research design). 7 They find that t-statistics from the 15 studies using quarterly data available at the time of writing are actually negatively correlated with sample sizes. A possible explanation is that a structural change in the effect of the minimum wage (a decline over time) has taken place, but the authors consider publication bias and specification searching a more likely explanation. Neumark and Wascher (1998) construct an alternative test for publication bias, which produces an attenuation of the effect size with larger sample sizes (as sample sizes increased over time) that is qualitatively similar to that in Card and Krueger (1995), but Neumark and Wascher thus place more emphasis on the structural change explanation (i.e., actual effects declined over time) and Several studies have also documented the presence of publication bias in the literature estimating the value of a statistical life (VSL). As government regulations in health, environment and transportation are frequently based on this value, accurate estimation is of great public importance, but there is growing consensus that there is substantial publication bias in this literature, leading to a strong upward bias in reported estimates (Ashenfelter and Greenstone 2004). Using the collection of 37 studies in Bellavance, Dionne, and Lebeau (2009), Doucouliagos, Stanley, and Giles (2012) find that correcting for publication bias reduces the estimates of VSL by 70-80% from that produced by a standard meta-analysis regression. Similar analysis shows that, correcting for publication bias, the VSL also appears largely inelastic to individual income (Doucouliagos, Stanley, and Viscusi 2014). An updated analysis of publication bias in the VSL literature (Viscusi 2015) shows that although publication bias is large and leads to meaningfully inflated estimates, he argues much of it may stem from early studies in the literature that used voluntary reporting of occupational fatalities, while more recent studies estimates employing the Census of Fatal Occupational Injuries (CFOI) suffer from less measurement error and tend to produce larger estimates. Evidence for publication bias has been documented in many other economics research literatures, although not in all. See Longhi, Nijkamp, and Poot (2005) and Knell and Stix (2005) for notable examples. Table 2 describes a number of related publication bias studies that might be of interest to readers, but for reasons of space they are not discussed in detail here. In the most systematic approach to date (to our knowledge), (Doucouliagos and Stanley 2013) carry out a meta-meta-analysis of 87 meta-analysis papers (many of which are reported in Table 2), and find that over half of the literatures suffer from \"substantial\" or \"severe\" publication bias, with particularly large degrees of bias in empirical macroeconomics and in empirical research based on demand theory, and somewhat less publication bias in subfields with multiple contested economic theories. The Journal of Economic Surveys has published many meta-regression papers, including a special issue devoted to meta-regression and publication bias (Roberts 2005). The statistical techniques for assessing publication bias are summarized in Stanley (2005), and many of these are applied in the articles listed in Table 2. One common data visualization approach is the use of funnel graphs, see Stanley and Doucouliagos (2010), Light and Pillemer (1984), and our discussion in section 3 below.",
          "Another important issue related to publication bias and null hypothesis testing is the reporting of the magnitude of effect sizes. Although it appears that economics may fare somewhat better than other social science disciplines in this regard, since economics studies typically report regression coefficients and standard errors while articles in some other disciplines (e.g., psychology) have historically only reported p-values, there is some evidence that under-reporting of effect magnitudes is still a concern. In a review in this Journal, McCloskey and Ziliak (1996) find that 70% of full-length American Economic Review articles did not distinguish between statistical and practical significance. Follow-up reviews in 2004 and 2008 conclude that the situation had not meaningfully improved (Ziliak and McCloskey 2004;Ziliak and McCloskey 2008). DeLong and Lang (1992) is an early contribution that addresses the issue of publication of null findings and effect sizes. They show that only 78 of 276 null hypotheses tested in empirical papers published in leading economics journals at the time were not rejected. However, using the uniform distribution of p-values under a true null hypothesis, and the startling lack of published p-values close to 1, they conclude it is likely that practically all economic hypotheses are indeed false. They also conclude that the null results that actually do get published in journals may also result from publication bias: a null result is arguably more interesting if it contradicts previous statistically significant results. DeLong and Lang go on to suggest that since almost all economic hypotheses are false, empirical evidence should pay more attention to practical significance and effect size rather than statistical significance alone, as is too often the case.",
          "While publication bias implies a distortion of a body of multiple research studies, bias is also possible within any given study (for instance, as captured in the author bias term u in Ioannidis (2005)). In the 1980's and 1990's, expanded access to computing power led to rising concerns that some researchers were carrying out growing numbers of analyses and selectively reporting econometric analysis that supported pre-conceived notions -or were seen as particularly interesting within the research community -and ignoring, whether consciously or not, other specifications that did not. One the most widely cited articles from this period is Leamer's (1983), \"Let's Take the Depending on which set of control variables, or covariates, were included (among state median income, unemployment, percent population nonwhite, percent population 15-24 years old, percent male, percent urban, percent of two-parent households, and several others), the main coefficient of interest-the number of murders estimated to be prevented by each executionranges widely on both sides of zero, from 29 lives saved to 12 lives lost. Of the five ways of classifying variables as important or doubtful that Leamer evaluated, three produced a range of estimates that included zero, suggesting that inference was quite fragile in this case. Leamer's recommendation that observational studies employ greater sensitivity checks, or extreme bounds analysis (EBA), was not limited to testing the effect of including different combinations of covariates, as in Leamer (1983). More detailed descriptions of EBA in Leamer (1978) and Leamer and Leonard (1983)  This flexibility was highlighted in Lovell (1983), who shows that with a few assumptions regarding the variance of the error terms, searching for the best k of c explanatory variables means that a coefficient that appears to be significant at the level 𝛼𝛼 � is actually only significant at the level 1 -(1 -𝛼𝛼 �) 𝑐𝑐/𝑘𝑘 . In the case of 𝑘𝑘 = 2 and 5 candidate variables, this risks greatly overstating significance levels, and the risk is massive if there are, say, 100 candidate variables. Lovell (1983) goes on to argue for the same sort of transparency in analysis as Leamer (1983). Denton (1985) expands on Lovell's work and shows that data mining can occur as a collective phenomenon even if each individual researcher tests only one pre-stated hypothesis, if there is selective reporting of statistically significant results, an argument closely related to the file drawer publication bias discussion above (Rosenthal 1979). Related points have been made in other social science fields in recent years. In psychology, Simmons, Nelson, and Simonsohn \"prove\" that listening to the Beatles' song \"When I'm Sixty-Four\" made listeners a year and a half younger (Simmons, Nelson, and Simonsohn 2011). The extent and ease of this \"fishing\" in analysis is also described in political science by Humphreys, Sierra, and Windt (2013)  The greater use of extra robustness checks in applied economics is designed to limit the extent of specification search, and is a shift in the direction proposed by Leamer (1983), but it is unclear how effective these changes are in reducing bias in practice. As noted above, the analysis of 641 articles from three top economics journals in recent years presented in Brodeur et al. (2016) still shows a disturbing two-humped distribution of p-values, with relatively few p-values between 0.10 and 0.25 and far more just below 0.05. Their analysis also explores the correlates behind this pattern, and finds that this apparent misallocation of p-values just below the accepted statistical significant level was less pronounced for articles written by tenured authors, and tentatively find it less pronounced among studies based on randomized controlled trials (suggesting that improved research design itself may partially constrain data mining), but they did not detect any discernible differences in the pattern based on whether the authors had publicly posted the study's replication data in the journal's public archive.",
          "One area of analytical flexibility that appears particularly important in practice is sub-group analysis. In many cases, there are multiple distinct interaction effects that could plausibly be justified by economic theory, and current datasets have a growing richness of potential covariates. Yet it is rare for applied economics studies to mention how many different interaction effects were tested, increasing the risk that only statistically significant false positives are reported. While there are few systematic treatments of this issue in economics, there has been extensive discussion of this issue within medical research, where the use of non-prespecified subgroup analysis is strongly frowned upon. The FDA does not use subgroup analysis in its drug approval decisions (Maggioni et al. 2007). An oft repeated, and humorous, case comes from a trial of aspirin and streptokinase use after heart attacks conducted in a large number of patients (N=17,187). Aspirin and streptokinase were found to be beneficial, except for patients born under Libra and Gemini, for whom there was a harmful (but not statistically significant) effect (ISIS-2 COLLABORATIVE GROUP 1988). The authors included the zodiac subgroup analysis because journal editors had suggested that 40 subgroups be analyzed, and the authors relented under the condition that they could include a few subgroups of their own choosing to demonstrate the unreliability of such analysis (Schulz and Grimes 2005).",
          "",
          "There have been longstanding concerns within economics over the inability to replicate the results of specific published papers. The pioneering example is a project undertaken by the Journal of Money, Credit, and Banking (JMCB) (Dewald, Thursby, and Anderson 1986). The journal launched the JMCB Data Storage and Evaluation Project with NSF funding in 1982, which requested data and code from authors who published papers in the journal. 9 Despite the adoption of an explicit policy of data sharing by the JMCB during the project, only 78% of authors provided data within six months after multiple requests, although this was certainly an improvement over the 34% data sharing rate in the control group, namely, those who published before the new journal policy went into effect. Of the papers that were still under review by the JMCB at the time of the requests for data, one quarter did not even respond to the request, despite the request coming from the same journal considering their paper. The data that was submitted was often an unlabeled and undocumented mess, a problem that has persisted with recent data sharing policies, as discussed below. Dewald, Thursby, and Anderson (1986) attempted to replicate nine empirical papers, and despite extensive assistance from the original authors, they were often unable to reproduce the papers' published results. Little changed for a long time after the publication of this landmark article. A decade later, in a follow-up piece to the JMCB Project published in the Federal Reserve Bank of St. Louis Review, Anderson and Dewald (1994)  failed to comply with the policy of providing data and code, editor Ben Bernanke made the data and code sharing policy mandatory in 2004 (Bernanke 2004;McCullough 2007). The current AER data policy states: \"It is the policy of the American Economic Review to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide to the Review, prior to publication, the data, programs, and other details of the computations sufficient to permit replication. These will be posted on the AER Web site. The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\"10  In addition to all the journals published by the American Economic Association in April 201611 . Table 3 summarizes journal policies regarding data sharing, publication of replications or comments, and funding or conflict of interest disclosures at twelve of the top economics and finance journals (according to Scientific Journal Rankings). There has clearly been considerable progress along all of these dimensions over the past decade, but journal policies remain a mixed bag. Among these leading journals, most but not all now have some data sharing requirements, and are officially open to publishing papers that could be considered \"replications.\"12 There is also greater use of disclosure statements. The AER conducted a self-review and found relatively good, though still incomplete, compliance with its data sharing policy (Glandon 2010). Despite this positive self-assessment, others observers believe that much work remains to ensure greater access to replication data in economics. Recent studies document that fewer than 15 of over 150 articles in the JMCB archive could be replicated; there is typically little to no verification that the data and code submitted to journals actually generate the published results; and the majority of economics journals still have no explicit data sharing requirements ( McCullough, McGeary, and Harrison 2006;Anderson et al. 2008;McCullough 2009). The uneven nature of progress along these dimensions across economics journals is ",
          "The American Economic Association's journal data sharing policy -which has been adopted by several other journals and organizations nearly verbatim, as shown in Table 3 -allows for some exceptions, importantly, for proprietary data. In particular, the policy reads: \"The Editor should be notified at the time of submission if the data used in a paper are proprietary or if, for some other reason, the requirements above cannot be met.\" In practice, this exemption is requested fairly often by empirical researchers, and the rate is increasing over time. During the past decade, the May American Economic Review Papers & Proceedings issue has featured a \"Report of the Editor\" which details the number of submissions to the journal, as well the number of papers published, those with data, and those that were granted exemptions. Figure 2 presents the percentage of papers in each issue of the AER since 2005 (when information becomes available) through 2015. A few patterns are noteworthy. First, proportion of papers that include data has risen over time, starting at roughly 60% and since increasing into the 70-80% range, capturing the shift towards empirical research in the discipline as a whole. During this period, the proportion of papers using data that received exemptions from 22 the data-sharing policy has risen rapidly, from roughly 10% to 40% over time. Thus replication data is in practice not available in practice for nearly half of all empirical papers published in the AER in recent years. There are many common sources of proprietary or otherwise non-sharable data driving this trend. One of the most common is U.S. government data. There are currently 23 Federal Statistical Research Data Centers (RDC), which provide researchers access to sensitive federal government data that cannot simply be shared publicly on a journal website, typically due to individual or corporate privacy concerns (e.g., IRS tax records). 16 We do not believe that research conducted with this data should be penalized in any way, and in fact, studies employing administrative data may be particularly valuable both intellectually and in terms of public policy decisions. However, despite the exemption from data sharing, it would still be useful for researchers (and journals) to make their work as reproducible as possible given the circumstances, for instance, by at least posting the associated statistical code and providing details about how other scholars could gain similar access to the data. Beyond government data, there are, of course, also an increasing number of proprietary datasets created by corporations or other entities that are willing to share sensitive commercial data with researchers, but not with the public at large where similar issues arise. Beyond commercially proprietary or legally restricted government data, there is also the important issue of norms regarding the sharing of original data collected by scholars themselves. Given the years of effort and funding that goes into creating an original dataset, what special intellectual property rights (if any) do scholars involved in generating data have? Economists should be aware of the incentives created by temporary monopoly rights to intellectual property, and in many ways the issues regarding original data collection are closely linked to traditional arguments around granting private patents. Such monopoly rights, even if temporary, could be socially beneficial if they help to drive the creation of innovative new data sources, such as the explosion of original new survey datasets in development economics over the past two decades. Yet we know of no empirical research that discusses the optimal length of such \"research dataset\" patents; this is an area that demands further attention, especially around the optimal length of exclusive access afforded to originators of new data.17  The increasingly common requirement to share data at the time of journal publication is a cause for concern in some fields. For example, in response to a proposal from the International Committee of Medical Journal Editors to require data sharing within six months after the publication of an article (Taichman et al. 2016) an editorial in the leading New England Journal of Medicine caused an outcry when they responded by describing those who do secondary analysis without the co-authorship and cooperation of the original data collecting author as \"research parasites\" (Longo and Drazen 2016). The journal re-affirmed their commitment to data-sharing (Drazen 2016) and published a supporting piece by Senator Elizabeth Warren (Warren 2016), but also a separate piece calling for a longer embargo period after publication: \"2 years after publication of the primary trial results and an additional 6 months for every year it took to complete the trial, with a maximum of 5 years before trial data are made available to those who were not involved in the trial\" (The International Consortium of Investigators for Fairness in Trial Data Sharing 2016). Presumably the increasing \"patent length\" here for each additional year it took to complete data collection is an attempt to reward research effort in collecting unusually rich longitudinal data. Yet these sorts of rules regarding timeframes seem quite ad hoc (to us, at least), further highlighting the need for a more serious examination of how best to balance the research community's right to replicate and extend existing research with scholars' incentives to invest in valuable original data. In political science, many journals have recently adopted policies similar to the AEA policy described above. For example, the current policy of the American Journal of Political Science states: \"In some limited circumstances, an author may request an exemption from the replication and verification policy. This exemption would allow the author to withhold or limit public access to some or all of the data used in an analysis. All other replication materials (e.g., software commands, etc.) still must be provided. The primary reasons for such exemptions are restricted access datasets and human subjects protection.\" 18 We lack data on how often this exemption is granted, however. Additionally, this journal goes much further than economics journals in one important way: instead of simply collecting and publishing data and code from authors, the editors use a third party research center (namely, the Odum Institute for Research in Social Science, at the University of North Carolina, Chapel Hill for quantitative analysis and the Qualitative Data Repository (QDR), at Syracuse University for qualitative analyses) to verify that the data and statistical code produce the published results.",
          "There given examples such as those discussed below in which there are discrepancies between the original published findings and later replication results, it remains possible that the high-profile cases of failed replication may simply be the tip of the iceberg. Thankfully, a few recent papers have begun to provide some evidence on this question, which we highlight below. We ourselves are no strangers to replication and re-analysis debates: papers by one of the authors of this article, described below, have been part of lively debates on replication and reanalysis using data that we shared publicly. These debates have led us to appreciate the great promise of replication research, as well as its potential pitfalls: exactly like original research studies, replication studies have their own particular strengths and weaknesses, and may serve to either advance the intellectual debate or could obscure particular issues. Yet there is no doubt in our minds that an overall increase in replication research will serve a critical role in establishing the credibility of empirical findings in economics, and in equilibrium, will create stronger incentives for scholars to generate more reliable results. Further complicating matters, an imprecise definition of the term \"replication\" itself often leads to confusion. A taxonomic proposal in Hamermesh (2007) distinguished between \"pure\", \"statistical\", and \"scientific\" replications, while a more recent effort (Clemens 2015) uses the terms \"verification,\" \"reproduction,\" \"reanalysis,\" and \"extension\" to distinguish between replications (the first two) and robustness exercises (the latter two). We first present some existing evidence on the replicability of economics and social science research in the next subsection, and then provide examples of each of Clemens' categories.",
          "The articles in the 1986 Journal of Money Credit and Banking project and the 1994 St. Louis Federal Reserve follow-up mentioned above provided some of the first attempts at systematic replication in economics, with fairly discouraging results. Have things improved in the last few decades? New evidence is emerging about the reliability of empirical economics research. One of the most important recent studies is Camerer et al. (2016), which repeated 18 behavioral economics lab experiments originally published between 2011 and 2014 in the American Economic Review and the Quarterly Journal of Economics to assess their replicability. Figure 3 below reproduces a summary of their findings. Their approach is similar in design to a large scale replication of 100 studies in psychology known as the \"Replication Project: Psychology\", which we discuss in detail below. The replication studies were designed with sample sizes that aimed to have 90% power to detect the original effect size at the 5% significance level. In all, the estimated effects were statistically significant with the same sign in 11 of the 18 replication studies (61.1% Other examples of replication failures abound. Clemens (2015) provides a useful taxonomy, and we provide an example of from each of the categories there to help distinguish between them, namely the two types of replication he discusses (verification and reproduction), and the two types of robustness exercises (reanalysis and extension). Of course not all papers fit easily into one of these categories as most tend to include elements from multiple categories.",
          "Perhaps the most straightforward type of replication in economics involves using the same specification, the same sample, and the same population. Essentially, this is running the same code on the same data and testing if you get the same results. Hamermesh (2007) referred to this as a \"pure replication.\" We believe this basic standard should be expected of all published economics research, and hope this expectation is universal among researchers. One tiny tweak to the definition of verification is that it also includes errors in coding. If an author describes a statistical test in the paper, but the code indisputably does not correctly carry out the test as described, this is also considered a verification failure. One of the earliest cases of quantitative economics research failing a verification test comes from an investigation of the effect of social security on private savings. Feldstein (1974) estimates a life cycle model showing that social security reduces private savings by as much as 50%. There were significant theoretical challenges to carrying out this exercise related to assumptions about the intergenerational transfer of wealth, but Leimer and Lesnoy (1982) discovered that a flaw in Feldstein's computer program that overestimated the growth rate of Social Security wealth for widows led to larger effects of Social Security wealth than when the mistake was corrected. Feldstein replied to the critique saying he was grateful for having the error corrected, but that the central conclusion of the study remains largely unchanged (namely, that Social Security decreased private savings by 44%) (Feldstein 1982). Much of the change in coefficients in the replication exercise resulted from Leimer and Lesnoy including an expanded time series of data-this is not a failure of verification, but rather an extension, which we discuss below.  3). 20 In many (but not all) cases discussed in Clemens, the original authors clearly admit to the failure of verification, but there is vigorous, and we think, healthy scholarly debate about how important those mistakes are and whether the results are still significant -statistically and/or practicallywhen the code or data are corrected. Of course, authors whose papers are subject to replication debates should be commended for providing other scholars with access to their data and code public in the first place, especially for these earlier articles published before journal data-sharing requirements were established.",
          "The other type of replication in Clemens' taxonomy is a reproduction. This approach uses the same analytical specification and the same population, but a different sample. Hamermesh (2007) refers to this as a statistical replication. In economics, this approach would be exhibited in a study that generated a certain set of results using a 5% sample of the census, while a different 5% census sample produced different results, or an experimental economics lab study that produced one set of results with a certain sample while the reproduction study analyzed a different sample from broadly the same population (e.g., U.S. university students). There is, of course, some  2007), the possibility is discussed that one factor contributing to lack of verification is that intermediary datasets constructed from raw data were over-written when the raw data was updated, as sometimes happens with U.S. government data. The work of one of the authors of this paper could be included on this list; see Miguel and Kremer (2004), Aiken et al. (2015) and Hicks, Kremer and Miguel (2015) on the impact of school-based deworming in Kenya. Camerer et al. (2016) could be better classified as failures of reproduction, as long as the samples were in fact collected in broadly the same manner (i.e., in person vs. online). Reproduction failures are perhaps more precisely defined in the hard sciences where experimenters routinely attempt to do the exact same physical process as another lab, albeit with a different sample of molecules, or in the biological sciences where experiments may employ a different sample of animal subjects. For instance, in defining reproduction, Clemens mentions the infamous case of the \"discovery\" of cold fusion by Fleischmann and Pons (1989), which failed to reproduce in Lewis et al. (1989).",
          "Robustness exercises come in two varieties, reanalysis and extensions. Reanalysis uses a different analytical specification on the same population (with either the same or a different sample). Many economics replication studies include both a verification aspect as well as some re-analysis. For instance, Davis (2013) conducts a successful verification of Sachs and Warner (1997), but concludes that reanalysis shows the estimates are somewhat sensitive to different statistical estimation techniques. Other well-known recent reanalysis debates in empirical economics include Miguel, Satyanath, and Sergenti (2004), Ciccone (2011) and Miguel and Satyanath (2011) on civil conflict and GDP growth using rainfall as an instrumental variable; and Acemoglu, Johnson, and Robinson (2001), Albouy (2012), and Acemoglu, Johnson, and Robinson (2012) on institutions and GDP growth with settler mortality as an instrumental variable. The debates over these and other studies makes it clear that reanalysis does not typically settle all key research questions, and the exercise often reveals that empirical economists have considerable flexibility in their analytical choices. This insight makes the development of methods to account for -and possibly constrain -this flexibility, which we discuss below in section 3, all the more important.",
          "Under Clemens' classification system, an extension uses the same analytical specification as an original study but a different population and a different sample. Most often this would be conducting the same analysis carried out in a different time or place. A well-known example of an extension involves Burnside and Dollar (2000), which showed that foreign aid seemed to be effective in increasing GDP if the recipient country was well-governed. However, using the exact same regression specification but including additional countries and years to the dataset, Easterly, Levine, and Roodman (2004) do not obtain the same result. Burnside and Dollar (2004) discuss the differences between the findings and conclude that they occur largely because of the additional countries, rather than lengthening the time series. One widely debated topic in economics that has features of both replication and robustness exercises is the topic of minimum wage impacts on unemployment. In early work, Welch (1974) concluded that early minimum wage legislation decreased teenage employment, increased the cyclicality of teenage employment with respect to the business cycle, and shifted teenage employment towards sectors not covered by the law. However, in the course of using Welch's data, Siskind (1977) discovered that Welch had used data for teenagers 16-19 years old instead of 14-19 years old for certain years, and once this was corrected, the minimum wage did not appear to reduce teenage employment. This was a fairly easy mistake to understand since the Current Population Survey was undergoing changes at the time, and table headings for unpublished data had not even been updated. Welch graciously acknowledged the error, and used the corrected data to extend the analysis to probe impacts by industry sector (Welch 1977). Scholars working on this important topic have, for several decades now, continued to find significant room for disagreement on key issues of sampling, data sources, and statistical analysis methods 21 , matters on which well-intended researchers may well disagree. In this and other similarly contentious debates, we believe that the use of pre-specified research designs and analysis plans could be useful for advancing scientific progress, a point we return to below.",
          "Though we believe (or at least, would prefer to believe) that most instances in which economics studies cannot be replicated are due to inadvertent human error or analytical judgment calls, fraud cannot be completely discounted in all cases. Popular books such as Broad and Wade's Betrayers of the Truth (1983) make it clear that scientists are not always saints. A survey of 234 economists at the 1998 ASSA/AEA meeting investigated falsification of research, inappropriate inclusion or omission of co-authors, and exchange of grades for gifts, money, or sexual favors (List et al. 2001). Both a randomization coin-toss technique to elicit true responses to sensitive questions, as well as a more standard question design, indicate that 4% of respondents admit to having at some time falsified research data, 7-10% of respondents admit to having committed one of four relatively minor research infractions, while up to 0.4% admitted to exchange of grades for gifts, money, or sexual favors. Given the seriousness of some of these offenses, an obvious concern is that these figures understate the actual incidence of fraudulent research practices. A more recent survey of members of the European Economics Association described in Necker ( 2014) asks individuals about the justifiability of certain practices as well as their behavior regarding those practices. Necker shows that 2.6% of researchers admit to having falsified data, while 94% admit to at least one instance of a practice considered inappropriate by the majority of the survey, and there is a clear positive correlation between justifiability and behavior, as well as between perceived professional publication pressures and questionable research practices. Similar surveys in other fields such as Anderson, Martinson, and Vries (2007), which surveyed researchers across disciplines funded by the U.S. National Institutes of Health, and John, Loewenstein, and Prelec (2012) in psychology, as well as a meta-analysis of 18 surveys of academic misbehavior, do not paint a very rosy picture, with 2% of respondents admitting to data fabrication, and 34% admitting to lesser forms of academic misconduct (Fanelli 2009). We are not aware of a recent case in economics that received media attention similar to the Michael Lacour fraud scandal uncovered by Broockman, Kalla, and Aranow (2015) in political science, or the case of Diedrick Stapel (see Carey 2011; Bhattacharjee 2013) in psychology. However, there is considerable evidence of plagiarism and other forms of research malpractice in economics. This Journal itself published the results of a survey sent to 470 economics journal editors, which revealed significant problems (Enders and Hoover 2004). Among the 127 editors who responded, only 19% claimed that their journal had a formal policy on plagiarism, and 42 cases of plagiarism were discovered in an average year, with nearly 24% of editors encountering at least one case. A follow-up survey of rank-and-file economists revealed a general lack of consensus on how to respond to cases of alleged plagiarism (Enders and Hoover 2006). 22Article retraction is another useful indicator of research misconduct. A search of four popular article databases for terms related to article retractions identified by Karabag and Berggren (2012)   and Stapel (2009), which was retracted in 2015 27 , clearly reads \"THIS PAPER HAS BEEN RETRACTED,\" the title has been altered to begin with \"Retracted: ,\" and the pdf features an obvious RETRACTED watermark on every page. This is also the case with all six of the retractions in Karabag and Berggren (2012), as well as other notable recent retractions such as LaCour and Green (2014), which was retracted by Marcia McNutt (2015). Innovation and New Technology for re-publication in Kyklos (Frey, Frey, and Eichenberger 1999). The most recent incident that seemed to attract significant attention was the submission of a substantively identical article to multiple journals within economics, which is also a serious lapse (\"Correspondence: David H Autor and Bruno S. Frey\" 2011). Even if plagiarism of this manner would seem significantly easier to catch in the Internet age, the proliferation of journals partially counteracts this ease. 23 https://plagiarism.repec.org/index.html 24 Although note that journals may present these policies online as opposed to formally publishing them in the journal; for instance, see the The bottom line is that there is little reason to believe that economists are inherently more ethical than other social scientists or researchers in other disciplines, so policies regarding fraud and retraction from other disciplines might potentially be beneficially applied to economics.",
          "This section discusses several new methods and tools that have emerged in economics research over the past two decades -and more forcefully over the past ten years -to address the concerns discussed in Section 2. These approaches have in common a focus on greater transparency and openness in the research process. They include improved research design (including experimental designs and meta-analysis approaches), study registration and pre-analysis plans, strengthened disclosure and reporting practices, and new norms regarding open data and materials. We discuss each in turn.",
          "There have been a number of different responses within economics to the view that pervasive specification searching and publication bias was affecting the credibility of empirical literatures. As mentioned above, there has been a shift towards a greater focus on prospective research design in several fields of applied economics work. Experimental (Duflo, Glennerster, and Kremer 2007) and quasi-experimental (Angrist and Pischke 2010) research designs arguably place more constraints on researchers relative to earlier empirical approaches, since there are natural ways to present data using these designs that researchers are typically compelled to present by colleagues in seminars and by journal referees and editors. Prospective experimental studies also tend to place greater emphasis on adequately powering an analysis statistically, which may help to reduce the likelihood of publishing only false positives (Duflo, Glennerster, and Kremer 2007). There is also suggestive evidence that the adoption of experimental and quasiexperimental empirical approaches is beginning to address some concerns about specification search and publication bias: Brodeur et al. (2016) present tentative evidence that the familiar spike in p-values just below the 0.05 level is less pronounced in randomized control trial studies than in studies utilizing non-experimental methods. Yet improved research design alone may not solve several other key threats to the credibility of empirical economics research, including the possibility that null or \"uninteresting\" findings never become known within the research community.",
          "In addition to improvements in research design, Leamer (1983) argued for greater disclosure of the decisions made in analysis, in what became known as \"extreme bounds analysis\" (described in Section 2). Research along these lines has dealt with model uncertainty by employing combinations of multiple models and specifications, as well as comparisons between them. Leamer himself has continued to advance this agenda (see Leamer 2016). We describe several related approaches here.",
          "A natural way to deal with statistical model uncertainty is through Bayesian model averaging. In this approach, each model in the space of plausible models is assigned a probability of being true based on researcher priors and goodness of fit criteria. Averaging the resulting estimates generates a statistic incorporating model uncertainty: The weighted average they generate implies a large but imprecisely estimated deterrent effect of executions on homicides in the U.S. Of course, even without employing explicit probability weights, simply visualizing the distribution of estimates across the entire space of statistical models can also be quite informative on its own. Two well-cited examples of model averaging engage in a thorough investigation of the determinants of cross-country economic growth. Sala-i-Martin's (1997) famous \"I Just Ran Two Million Regressions\" article uses model weights proportional to the integrated likelihoods of each model, picks all possible three-variable combinations out of 60 covariates that have been reported as being significantly related to economic growth, and finds that only about one-third of the 60 variables can be considered robustly positively correlated with economic growth across models. Sala-i-Martin, Doppelhofer, and Miller ( 2004) conduct what they call Bayesian Averaging of Classical Estimates (BACE), weighting estimates using an approach analogous to Schwarz's BIC, and find that just 18 of 67 variables are significantly and robustly partially correlated with economic growth, once suggesting that many findings reported in the existing empirical literature may be spuriously generated by specification searching and selective reporting.",
          "While specification searching or data mining often has a negative connotation in applied economic research, some scholars have taken a more favorable view of it, as long as the data mining is carried out appropriately (Pagan 1987;Phillips 1988). Advocates of this approach have sometimes been known as the \"LSE school\" of econometrics, or the general-to-specific modelling approach (Gilbert 1989;Hendry 1987;Hendry 1995). This approach is related in spirit to the idea of \"encompassing,\" which is the principle that one statistical model can account for, or explain, another (Mizon and Richard 1986;Bontemps and Mizon 2008). To our knowledge, this approach is more often applied in time series econometrics than in applied microeconomics, but perhaps a closer consideration is warranted. There have also been recent calls within applied economics for greater application of machine learning methods and other data science techniques that share some features with these approaches (Kleinberg, Ludwig, et al. 2015;Kleinberg, Lakkaraju, et al. 2015). For a wide set of views on data mining more broadly and the LSE approach specifically, see the Journal of Economic Methodology, which devoted a special issue to the topic (Backhouse and Morgan 2000).",
          "Simonsohn, Simmons, and Nelson (2015b) propose a method, which they call the \"specification curve,\" that is similar in spirit to Leamer's extreme-bounds analysis, but recommends researchers test the exhaustive combination of analytical decisions, not just decisions about which covariates to include in the model. If the full exhaustive set is too large to be practical, a random subset can be used. After plotting the effect size from each of the specifications, researchers can assess how much the estimated effect size varies, and which combinations of decisions lead to which outcomes. Using permutation tests (for treatment with random assignment) or bootstrapping (for treatment without random assignment), researchers can generate shuffled samples with no true effect by construction, and compare the specification curves from these placebo samples to the specification curve from the actual data. Many comparisons are possible, but the authors suggest comparing the median effect size, the share of results with predicted sign, and share of statistically significant results with predicted sign. A key comparison, which is analogous to the traditional p-value, is the percent of the shuffled samples with as many or more extreme results. Bakkensen and Larson 2014). The specification curve shows that 46% of curves from permuted data show at least as large a median effect size as the original, 16% show at least as many results with the predicted sign, and 85% show at least as many significant results with the predicted sign. This indicates that the results are likely to have been generated by chance. The Bertrand and Mullainathan (2004) specification curve, on the other hand, shows that fewer than 0.2% of the permuted curves generate as large a median effect, 12.5% of permuted curves show at least as many results with the predicted sign, and less than 0.2% of permuted curves show at least as many significant results with the predicted sign, providing evidence that the results are very unlikely to have been generated by chance.",
          "There have been significant advances in the methodological literature on quantifying the extent of publication bias in a given body of literature. Early methods mentioned above include Rosenthal's (1979) method (the \"fail-safe N\"), while Galbraith (1988) advocated for radial plots of log odds ratios, and Card and Krueger (1995) tested for relationships between study sample sizes and t-statistics. Statisticians have developed methods to estimate effect sizes in meta-analyses that control for publication bias (Hedges 1992;Hedges and Vevea 1996). The tools most widely used by economists tend to be simpler, including the widely used funnel plot, which is a scatter plot of some measure of statistical precision (typically the inverse of the standard error), versus the estimated effect size. Estimates generated from smaller samples should usually form the wider base of an inverted funnel, which should be symmetric around more precise estimates in the absence of publication bias. The method is illustrated with several economics examples in Stanley and Doucouliagos (2010), and two of these are reproduced in Figure 4. In addition to scrutinizing the visual plot, a formal test of the symmetry of this plot can be conducted using data from multiple studies and regressing the relevant t-statistics on inverse standard errors: (eqn. 4) The resulting t-test on 𝛽𝛽 0 , referred to as the Funnel Asymmetry Test (FAT) (Stanley 2008), captures the correlation between estimated effect size and precision, and thus tests for publication bias. Using the FAT, Doucouliagos and Stanley (2009) find evidence of publication bias in Card and Krueger's (1995) sample of minimum wage studies (𝛽𝛽 0 ≠ 0), consistent with their own interpretation of the published literature at that time. 𝛽𝛽 1 here can also be interpreted as the true effect (called the precision effect test, PET) free of publication bias, and Doucouliagos and Stanley (2009) find no evidence of a true effect of the minimum wage on unemployment. The authors also conduct the FAT-PET tests with 49 additional more recent studies in this literature and find the same results: evidence of significant publication bias and no evidence of an effect of the minimum wage on unemployment. Additional meta-analysis methods, including this \"FAT-PET\" approach, are summarized in Stanley and Doucouliagos (2012).",
          "Other applied econometricians have recently called for increasing the use of multiple testing corrections in order to generate more meaningful inference in study settings with many research hypotheses (Anderson 2008;Fink, McConnell, and Vollmer 2014). The practice of correcting for multiple tests is already widespread in certain scientific fields (e.g., genetics) but has yet to become the norm in economics and other social sciences. Simply put, since we know that pvalues fall below traditional significance thresholds (e.g., 0.05) purely by chance a certain proportion of the time, it makes sense to report adjusted p-values that account for the fact that we are running multiple tests, since this makes it more likely that at least one of our test statistics has a significant p-value simply by chance. There are several multiple testing approaches, some of which are used and explained by Anderson (2008), namely, reporting index tests, controlling the family-wise error rate (FWER), and controlling the false discovery rate (FDR). These are each discussed in turn below.",
          "One option for scholars in cases where there are multiple related outcome measures is to forego reporting the outcomes of numerous tests, and instead standardize the related outcomes and combine them into a smaller number of indices, sometimes referred to as a mean effect. This can be implemented for a family of related outcomes by making all signs agree (i.e., allowing positive values to denote beneficial outcomes), demeaning and dividing by the control group standard deviation, and constructing a weighted average (possibly using the inverse of the covariance matrix to weight each standardized outcome). This new index can be used as a single outcome in a regression model and evaluated with a standard t test. Kling, Liebman, and Katz (2007) implement an early index test in the Moving to Opportunity field experiment using methods developed in biomedicine by O'Brien (1984). This method addresses some concerns regarding the multiplicity of statistical tests by simply reducing the number of tests. A potential drawback is that the index may combine outcomes that are only weakly related, and may obscure impacts on specific outcomes that are of interest to particular scholars, although note that these specific outcomes could also be separately reported for completeness.",
          "The family-wise error rate (FWER) is the probability that at least one true hypothesis in a group is rejected (a type I error, or false positive). This approach is considered most useful when the \"damage\" from incorrectly claiming any hypothesis is false is high. There are several ways to implement this approach, with the simplest method being the Bonferroni correction of simply multiplying every original p-value by the number of tests carried out (Bland and Altman 1995), although this is extremely conservative, and improved methods have also been developed. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher discount factors (Holm 1979). A related and more efficient recent method is the free step-down resampling method, developed by Westfall and Young (1993), which when implemented by Anderson (2008) implies that several highly cited experimental pre-school interventions (namely, the Abecedarian, Perry, and Early Training Project studies) exhibit few positive long-run impacts for males. Another recent method improves on Holm by incorporating the dependent structure of multiple tests. Lee and Shaikh (2014) apply it to reevaluate the Mexican PROGRESA conditional cash transfer program and find that overall program impacts remain positive and significant, but are statistically significant for fewer subgroups (e.g., by gender, education) when controlling for multiple testing. List, Shaikh, and Xu (2016) propose a method of controlling the FWER for three common situations in experimental economics, namely, testing multiple outcomes, testing for heterogeneous treatment effects in multiple subgroups, and testing with multiple treatment conditions.28 ",
          "In situations where a single type I error is not considered very costly, researchers may be willing to use a somewhat less conservative method than the FWER approached discussed above, and trade off some incorrect hypothesis rejections in exchange for greater statistical power. This is made possible by controlling the false discovery rate (FDR), or the percentage of rejections that are type I errors. Benjamini and Hochberg (1995) details a simple algorithm to control this rate at a chosen level under the assumption that the p-values from the multiple tests are independent, though the same method was later shown to also be valid under weaker assumptions (Benjamini and Yekutieli 2001). Benjamini, Krieger, and Yekutieli (2006) describes a two-step procedure with greater statistical power, while Romano, Shaikh, and Wolf (2008) propose the first methods to incorporate information about the dependence structure of the test statistics. Multiple hypothesis testing adjustments have recently been used in finance (Harvey, Liu, and Zhu 2015) to re-evaluate 316 factors from 313 different papers that explain the cross-section of expected stock returns. The authors employ the Bonferroni; Holm (1979);and Benjamini, Hochberg, and Yekutieli (2006) methods to account for multiple testing, and conclude that tstatistics greater than 3.0, and possibly as high as 3.9, should be used instead of the standard 1.96, to actually conclude that a factor explains stock returns with 95% confidence. Index tests and both the FWER and FDR multiple testing corrections are also employed in Casey, Glennerster, and Miguel (2012) to estimate the impacts of a community driven development program in Sierra Leone using a dataset with hundreds of potentially relevant outcome variables.",
          "Another method that controls for, or acts as a reality check, for data snooping or data mining was developed in White (2000). The testing of multiple hypotheses, or repeated use of the same data, is a particularly central problem with time series data used over and over again by multiple scholars, such as data on stock returns, which makes this research quite important in empirical finance. Like the model averaging approach described above, the reality check requires a researcher to estimate the entire space of plausible models, but now compares the performance of the preferred model to a benchmark model (e.g., a model for stock market predictions based on the efficient market hypothesis), and does so repeatedly with bootstrapped samples. To assess whether a certain preferred model actually outperforms the benchmark after accounting for snooping with multiple models, the econometrician first calculates the performance of the preferred model (using mean squared error improvement over the benchmark, or relative profit of the strategy). She then selects a bootstrap sample (with replacement), and calculates the mean squared error improvement (or profit) with the new sample for all of the different plausible statistical models, recording the best mean squared error improvement (or profit) across all the models. This approach can then be repeated 1,000 (or more) times, gathering the 1,000 best mean squared errors (or profits). In the final step, one must compare the original preferred model's mean squared error to the best performance from each of the 1,000 bootstraps. The p-value is the fraction of bootstrapped best fits that outperform the preferred model. (A truly predictive model would have returns higher than 95% of the best performing models from each of the bootstrapped samples.) This method was implemented on a large number of trading rules in Sullivan, Timmermann, and White (1999) and a similar method that addresses the presence of poorly performing or irrelevant alternatives was developed in Hansen (2005).",
          "A leading proposed solution to the problem of publication bias is the registration of empirical studies in a public registry. This would ideally be a centralized database of all attempts to conduct research on a certain question, irrespective of the nature of the results, and such that even null (not statistically significant) findings are not lost to the research community. Top medical journals have adopted a clear standard of publishing only medical trials that are registered (De Angelis et al. 2004). The largest clinical trial registry is clinicaltrials.gov, which helped to inspire the most high-profile study registry within economics, the AEA Randomized Controlled Trial Registry (Katz et al. 2013), which was launched in May 2013. 29   While recent research in medicine finds that the clinical trial registry has not eliminated all under-reporting of null results or other forms of publication bias and specification searching (Laine et al. 2007;Mathieu et al. 2009), they do allow the research community to quantify the extent of these problems and over time may help to constrain inappropriate practices. It also helps scholars locate studies that are delayed in publication, or are never published, helping to fill in gaps in the literature and thus resolving some of the problems identified in Franco, Malhotra, and Simonovits (2014). Though it is too soon after the adoption of the AEA's trial registry to measure its impact on research practices and the robustness of empirical results, it is worth noting that the registry is already being used by many empirical researchers: since inception in 2013, over 850 studies conducted in over 100 countries have been registered, and the pace of registrations continues to rise rapidly. allowing users to create a frozen time-stamped web URL with associated digital object identifier (DOI) for any materials uploaded to OSF. Several popular data storage options (including Dropbox, Dataverse, and GitHub) can also be synced with the OSF and its storage, creating a flexible way for researchers to register their research and materials. As of October 2016, over 7,300 public registrations have been created on OSF since the service launched in 2013.",
          "In addition to serving as a useful way to search for research findings on a particular topic, most supporters of study registration also promote the pre-registration of studies, including preanalysis plans (PAPs) that can be posted and time stamped even before analysis data are collected or otherwise available (Miguel et al. 2014). Registration is now the norm in medical research for randomized trials, and registrations often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the U.S. template may be particularly useful to researchers themselves when developing their own preanalysis plans, and instructors may find it useful in their courses. Building on, and modifying, the FDA's 1998 checklist with insights from these other recent treatments of pre-analysis plans, there appears to be a growing consensus that pre-analysis plans in economics should consider discussing at least the following list of ten issues: Pre-analysis plans are relatively new to economics, and this list is likely to evolve in the coming years as researchers explore the potential, and possible limitations, of this new tool. For those concerned about the possibility of \"scooping\" of new research designs and questions based upon a publicly posted pre-analysis plan or project description, several of the social science registries allow temporary embargoing of project details. For instance, the AEA registry allows an embargo until a specific date or project completion. At the time of writing, the OSF allows a four-year embargo until the information is made public. 31",
          "Recent examples of economics papers based on experiments with PAPs include Casey, Glennerster, and Miguel (2012) and Finkelstein et al. (2012), among others. Casey, Glennerster, and Miguel (2012) discuss evidence from a large-scale field experiment on community driven development (CDD) projects in Sierra Leone. The project, called GoBifo, was intended to make local institutions in post-war Sierra Leone more democratic and egalitarian. GoBifo funds were spent on a variety of local public goods infrastructure (e.g., community centers, schools, latrines, roads), agriculture, and business training projects, and were closely monitored to limit leakage. The analysis finds significant short-run benefits in terms of the \"hardware\" aspects of infrastructure and economic wellbeing: the latrines were indeed built. However, a larger goal of the project, reshaping local institutions, making them more egalitarian, increasing trust, improving local collective action, and strengthening community groups, which the researchers call the \"software effects,\" largely failed. There are a large number of plausible outcome measures along these dimensions, hundreds in total, which the authors analyze using a mean effects index approach for nine different families of outcomes (with multiple testing 31 See http://help.osf.io/m/registrations/l/524207-embargoes. Accessed October 10, 2016. adjustments). The null hypothesis of no impact cannot be rejected at 95% confidence for any of the nine families of outcomes. Yet Casey et al (2012) go on to show that, given the large numbers of outcomes in their dataset, and the multiplicity of ways to define outcome measures, finding some statistically significant results would have been relatively easy\". In fact, the paper includes an example of how, if they had had the latitude to define outcomes without a pre-analysis plan, as has been standard practice in most empirical economics studies (and in other social science fields), the authors could have reported either statistically significant and positive effects, or significantly negative effects, depending on the nature of the \"cherry-picking\" of results. We reproduce their results here as Table 4, where Panel A presents the statistically significant positive impacts identified in the GoBifo data and Panel B highlights negative effects. This finding begs the question: how many empirical economics papers with statistically significant results are, unbeknownst to us, really just some version of either Panel A or Panel B? This is important because, as in Casey et al (2012), the researchers tested a large number of outcomes: hospital admissions through the emergency room (ER) and not through the ER; hospital days; procedures; financial strain (bankruptcy, judgments, liens, delinquency, medical debt, and non-medical debt, measured by credit report data); self-reported health from survey data, and so on. When running such a large number of tests, the researchers again could have discovered some \"significant\" effects simply by chance. The pre-analysis plan, in conjunction with multiple hypothesis testing adjustments, give us more confidence in the main results of the study: that recipients did not improve significantly in terms of physical health measurements, but they were more likely to have health insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Additional studies that have resulted from the experiment have also employed preanalysis plans, and they show that health insurance increased emergency department use (Taubman et al. 2014), had no effect on measured physical health outcomes after two years, but did increase health care use and diabetes management, as well as leading to lower rates of depression and financial strain (Baicker et al. 2013). The health care expansion had no significant effect on employment or earnings (Baicker et al. 2014). Other prominent early examples of economics studies that have employed pre-analysis plans include poverty targeting programs in Indonesia, an evaluation of the Toms shoe company donation program, and a job training program in Turkey, among many others (Olken, Onishi, and Wong 2012;Alatas et al. 2012;Wydick, Katz, and Janet 2014;Hirshleifer et al. 2015). The PAP tool is also spreading to other social sciences beyond economics. For instance, in psychology, a pre-specified replication of an earlier paper that had found a link between female conception risk and racial prejudice failed to find a similar effect (Hawkins, Fitzgerald, and Nosek 2015). One issue that arises for studies that did register a pre-analysis plan is the question of characterizing the extent to which the analysis conforms to the original plan, or if it deviates in important ways from the plan. To appreciate these differences, scholars will need to compare the analysis to the plan, a step that could be seen as adding to the burden of journal editors and referees. Even if the analysis does conform exactly to the PAP, there is still the possibility that authors are consciously or unconsciously emphasizing a subset of the pre-specified analyses in the final study. Berge et al. (2015) develop an approach to comparing the distribution of p-values in the paper's main tables versus those in the PAP in order to quantify the extent of possibly selective reporting between the plan and the paper. The Finkelstein et al (2012) study is a model of transparency regarding the presentation of results. To the authors' credit, all analyses presented in the published paper that were not prespecified are clearly labeled as such; in fact, the exact phrase \"This analysis was not prespecified.\" appears in the paper six times. Tables in the main text and appendix that report analyses that were not pre-specified are labelled with a \"^\" character to set them apart, and are clearly labeled as such.",
          "There remain many open questions about whether, when, and how pre-analysis plans could and should be used in economics research, with open debates about how useful they are in different subfields of the discipline. Olken (2015), for example, highlights both their \"promises and perils.\" On the positive side, pre-analysis plans bind the hands of researchers and greatly limit specification searching, allowing them to take full advantage of the power of their statistical tests (even making one-sided tests reasonable). A further advantage of the use of pre-analysis plans is that they are likely to help shield researchers from pressures to affirm the policy agenda of donors and policymakers, in cases where they have a vested interest in the outcome, or when research focuses on politically controversial topics (such as health care reform). This is especially the case if researchers and their institutional partners can agree on the pre-analysis plan, as a sort of evaluation contract. On the negative side, PAPs are often complex and take valuable time to write. Scientific breakthroughs often come at unexpected times and places, often as a result of exploratory analysis, and the time spent writing PAPs may thus lead less time to spend on less structured data exploration. Coffman and Niederle (2015) argue that there is limited upside from PAPs when replication (in conjunction with hypothesis registries) is possible. In experimental and behavioral economics, where lab experiments utilize samples of locally recruited students and the costs of replicating an experiment are relatively low, they argue that replication could be a viable substitute for pre-analysis plans. Yet there does appear to be a growing consensus, endorsed by Coffman and Niederle, that pre-analysis plans can significantly increase the credibility of reporting and analysis in large-scale randomized trials that are expensive or difficult to repeat, or when a study that relies on a particular contextual factor makes it impossible to replicate. For instance, Berge et al. (2015) carry out a series of lab experiments timed to take place just before the 2013 Kenya elections. Replication of this lab research is clearly impossible due to the unique context, and thus use of a pre-analysis plan is valuable. Olken (2015) as well as Coffman and Niederle (2015) discuss another potential way to address publication bias and specification search: results-blind review. Scholars in psychology have championed this method; studies that are submitted to such review are often referred to as \"registered reports\" in that discipline. Authors write a detailed study protocol and pre-analysis plan, and before the experiment is actually run and data is collected, submit the plan to a journal. Journals review the plan for the quality of the design and the scientific value of the research question, and may choose to give \"in-principle acceptance\". This can be thought of as a kind of revise and resubmit that is contingent on the data being collected and analyzed as planned. If the author follows through on the proposed design, and the data are of sufficiently high quality (e.g., with sufficiently low sample attrition rates in a longitudinal study, etc.), the results are to be published regardless of whether or not they are statistically significant, and whether they conform to the expectations of the editor or referees, or to the conventional wisdom in the discipline. Several psychology journals currently have begun using results-blind review, either regularly or in special issues (Chambers 2013;Chambers et al. 2014;Nosek and Lakens 2014). 32A forthcoming issue of Comparative Political Studies will be the first to feature results-blind review in political science (Findley et al. 2016), and it will include both experimental and observational research studies. In our view, it would also be useful to experiment with results-blind review and registered reports in economics journals. The rise in experimental studies and pre-analysis plans in economics, as evidenced by the rapid growth of the AEA registry, is likely to facilitate the eventual acceptance of this approach.",
          "An important open question is how widely the approach of study registration and hypothesis prespecification could be usefully applied in non-prospective and non-experimental studies. This issue has been extensively discussed in recent years within medical research but consensus has not yet been reached in that community. It actually appears that some of the most prestigious medical research journals, which typically publish randomized trials, are even more in favor of the registration of observational studies than the editors of journals that publish primarily in non-experimental research (see the dueling editorial statements in Epidemiology 2010; The Lancet 2010;Loder, Groves, and MacAuley 2010;Dal-Ré et al. 2014). A major logical concern with the pre-registration of non-prospective observational studies using pre-existing data is that there is often no credible way to verify that pre-registration took place before analysis was completed, which is different than the case of prospective studies in which the data has not yet been collected or accessed. In our view, proponents of the preregistration of observational work have not formulated a convincing response to this obvious concern. A more recent study bases its analysis on Neumark's exact pre-specified tests estimate the effect of minimum wages in Canada and found larger unemployment effects, but they had access to the data before estimating their models and did not have an agreement with the journal, so the value of this \"pre-specification\" is perhaps less clear (Campolieti, Gunderson, and Riddell 2006). In political science, a pre-specified observational analysis measured the effect of the immigration stances of Republican representatives on their 2010 election outcomes (Monogan 2013). It is difficult to see how a researcher could reach Neumark's level of pre-specified detail with a research question with which they were not already intimately familiar. It seems more likely that in a case where the researcher was less knowledgeable they might either pre-specify with an inadequate level of detail, or choose an inappropriate specification; this risk makes it important that researchers should not be punished for deviating from their pre-analysis plan in cases where the plan omits important details or contains errors, as argued in Casey et al (2012). It seems likely to us that the majority of observational empirical work in economics will continue largely as is for the foreseeable future. However, for important, intensely debated, and well-defined questions, it would be desirable in our view for more prospective observational research to be conducted in a pre-specified fashion, following the example in Neumark (2001). Although pre-specification will not always be possible, the fact that large amounts of government data are released to the public on regular schedules, and that many policy changes are known to occur well in advance (such as in the case of the anticipated federal minimum wage changes discussed above, with similar arguments for future elections), will make it possible for the verifiable pre-specification of research analysis to be carried out in many settings.",
          "Another frontier topic in this realm is the use of pre-specified algorithms, including machine learning approaches, rather than exact pre-analysis plans for prospective studies. For instance, the exact procedure to be used to determine which covariates should be included in order to generate the most statistically precise estimates can be laid out in advance, even if those covariates are unknown (and unknowable) before the data has been collected. This approach has not yet been widely adopted in economics (to our knowledge), but has begun to be used in medical trials and biostatistics (van der Laan et al. 2007;Sinisi et al. 2007). A proposal related to, but slightly different than, pre-analysis plans is Nobel Prize winning physicist Saul Perlmutter's suggestion for the social sciences to use \"blind analysis\" (MacCoun and Perlmutter 2015). In blind analysis, researchers add noise to the data while working with it and running the analysis, thus preventing them from knowing which way the results are turning out, and thus either consciously or unconsciously biasing their analysis, until the very end, when the noise is removed and the final results are produced. This technique is apparently quite common in experimental physics (Klein and Roodman 2005), but we are not aware of its use in economics or other social sciences. Major differences are also beginning to emerge in the use of pre-analysis plans, and in the design and interpretation of experimental evidence more broadly, among economists versus scholars in other fields, especially health researchers, with a much greater role of theory in the design of economics experiments. Economists often design experiments to shed light on underlying theoretical mechanisms, to inform ongoing theoretical debates, and measure and estimate endogeneous behavioral responses. These behavioral responses may shed light on broader issues beyond the experimental intervention at hand, and thus could contribute to greater external validity of the results. As a result, pre-analysis plans in economics are often very detailed, and make explicit reference to theoretical models. For example, Bai et al. (2015) preregistered the theoretical microeconomic model and detailed structural econometric approach that they planned to apply to a study of commitment contracts in the Indian health sector. This distinction between the types of studies carried out by medical researchers versus economists (including those working on health topics) has a number of important implications for assessing the reliability of evidence. One has to do with the quality standards and perceptions of the risk of bias in a particular design. For medical trialists accustomed to the CONSORT standards or other medical efficacy trial reporting guidelines (described below), studies that do not feature double-blinding, and thus run the risk of endogeneous behavioral responses to the medical intervention, are considered less reliable than those studies that employ double-blinding (for a detailed discussion, see Eble, Boone, and Elbourne 2014). While a few studies conducted by economists do feature double-blinding (e.g., Thomas et al. 2003;Thomas et al. 2006), in nearly all settings blinding participants to their status is either logistically difficult (for instance, if government partners are unwilling to distribute placebo treatments to some of their population) or even impossible. To illustrate, how would you provide a placebo treatment in a study investigating the impact of the distribution of cash transfers on household consumption patterns? Even in settings that might seem promising for placebo treatments, such as the community-level deworming treatments discussed in Miguel and Kremer (2004), blinding participants to their status is basically impossible: deworming generates side effects (mainly gastrointestinal distress) in roughly 10% of those who take the pills, so community members in a placebo community would quickly deduce that they were in fact not receiving real deworming drugs if there are few or no local cases of side effects. As noted above, endogeneous behavioral responses are often exactly what we economists (and other social scientists) set out to measure and estimate in our field experiments, as described in our pre-analysis plans, and thus are to be embraced rather than rejected as symptomatic of a \"low quality\" research design that is at \"high risk of bias\". Taken together, it is clear to us that the experimental literature in economics (and increasingly in other social sciences such as political science) often has very different objectives than medical, public health and epidemiological research, and thus different research methodologies are often called for. Despite the value of learning from recent experience in biomedical research, and the inspiration that the experience of medical research has played to the rise of new experimental research methods in the social sciences, economists have not simply been able to import existing medical trial methods wholesale, but are developing new and tailored approaches to pre-registration, preanalysis plans, reporting standards, and transparency more broadly.",
          "Another approach to promoting transparency is to establish detailed standards for the disclosure of information regarding study design, data, and analysis. These could serve to limit at least some forms of data mining and specification searching, or at least might make them more apparent to the reader. Detailed reporting standards have become widespread in medical research for both experimental and observational research. Most notably for clinical trials, the Consolidated Standards of Reporting Trials (CONSORT) was developed (Begg C et al. 1996). A before-andafter comparison showed improvement in some measures of study reliability (Moher D et al. 2001), and the standards have been twice revised (Moher, Schulz, and Altman 2001;Schulz et al. 2010) and since extended to at least ten specific types of research designs, interventions, or data. Among others, and possibly particularly relevant for some types of economics research, these include cluster randomized trials (Campbell, Elbourne, and Altman 2004;Campbell et al. 2012), non-pharmacological treatment interventions (Boutron et al. 2008), and patient-reported outcomes (Calvert et al. 2013) In psychology, researchers have created an extension of CONSORT for social and psychological interventions (CONSORT-SPI) (Montgomery et al. 2013;Grant et al. 2013). Others psychologists have proposed that an effective way to reform reporting and disclosure norms within their discipline is for referees to enforce desirable practices when reviewing articles (Simmons, Nelson, and Simonsohn 2011). These authors recommended six conditions for referees to consider include the following: 1. Authors must decide the rule for terminating data collection before data collection begins and report this rule in the article. 2. Authors must collect at least 20 observations per cell or else provide a compelling costof-data-collection justification. Recently, we, the authors of this article, were part of an interdisciplinary group of researchers that developed a detailed set of journal guidelines called the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015). This modular set of guidelines for journals features eight categories, namely: citation standards, data transparency, analytic methods (code) transparency, research materials transparency, design and analysis transparency, preregistration of studies, preregistration of analysis plans, and replication -with four levels (0-3) of transparency that journals could choose to endorse or require. For example, with regards to data transparency, the level zero standard is that the journal either encourages data sharing or says nothing, while the level three standard is that \"data must be posted to a trusted repository, and reported analyses will be reproduced independently prior to publication\"; levels 1 and 2 fall somewhere in between. Journals could choose to adopt higher standards in some categories than others, as they feel most appropriate for their research community. In the six months after the guidelines were published in Science, 538 journals and 57 organizations across a wide variety of scientific disciplines, including many in the social sciences, expressed their support for the standards and agreed to evaluate them for potential adoption. Science has now announced that it will be implementing the standards, effective  2016), which argues that differences in implementation between original and replication experiments were inappropriate and introduces noise in addition to the expected sampling error. When taking this into account, one should actually expect the relatively low reported replication rate, and they thus argue there is no replication crisis. Some of the original RPP authors respond that differences between original and replication studies were in fact often endorsed by original study authors and take issue with the statistical analysis in Gilbert et al. (Anderson et al. 2016). Simonsohn (2015) engages in further discussion of how one should evaluate replication results, suggesting that powering a replication based on the effect size of the original study is problematic, and to distinguish the effect size from zero, replications (at least in psychology, with their typically small sample and effect sizes) should have a sample at least 2.5 times as large as the original. An optimistic take by Patil, Peng, and Leek (2016) suggests that researchers should compare the effect in the replication study to a \"prediction interval\" defined as 𝑟𝑟ô𝑜 𝑜𝑜𝑖𝑖𝑜𝑜 ± 𝑧𝑧 0.975� 1 where 𝑟𝑟̂𝑜𝑜 𝑜𝑜𝑖𝑖𝑜𝑜 is the correlation estimate in the original study, 𝑛𝑛 𝑜𝑜𝑜𝑜𝑖𝑖𝑜𝑜 and 𝑛𝑛 𝑜𝑜𝐸𝐸𝑟𝑟 are the sample sizes in the original and replication studies, respectively; and 𝑧𝑧 0.975 is the 97.5% quantile of the normal distribution, which incorporates uncertainty in the estimates from both the original and replication study. Applying this approach leads to much higher estimates of study replication (75%) for the RPP. Economists may be interested to know that the researchers behind the RPP also included a prediction market in their project, and the market did a fairly good job of predicting which of the effects studies would ultimately be reproduced (Dreber et al. 2015). Unlike the prediction market in Camerer et al. (2016), the RPP prediction market outperformed a survey of researcher beliefs. 43 43 For related research on expert predictions, see DellaVigna and Pope (2016) . Other psychology researchers have tried another way to crowdsource replication: instead of bringing different research groups together to all independently run the same classic experiment, other researchers have independently analyzed the same observational dataset and attempted to answer the same question, in this case, the question of whether darker skintoned soccer players receive more red cards as a result of their race, conditional on other factors (Silberzahn and Uhlmann 2015). For instance, it has been shown in multiple instances that it is often trivially easy to identify individuals in purportedly \"de-identified\" and anonymous datasets using publicly available information. In one dramatic illustration, MIT computer science Ph.D. student Latanya Sweeney sent then-Massachusetts Governor William Weld his own complete personal health records only days after anonymized state health records were released to researchers (Sweeney 2002). A new focus of computer science theorists has been developing algorithms for \"differential privacy\" that simultaneously protect individual privacy while allowing for robust analysis of datasets. They have established that there is inherently a trade-off between these two objectives (Dwork and Smith 2010;Heffetz and Ligett 2014), though few actionable approaches to squaring this circle are currently available to applied researchers, to our knowledge.",
          "The rising interest in transparency and reproducibility in economics reflects broader global trends regarding these issues, both among academics and beyond. As such, we argue that \"this statements, and open data and materials lead to improved research quality in a way that can be credibly measured and assessed? To this point, the presumption among advocates (including ourselves, admittedly) is that these changes will indeed lead to improvements, but rigorous evidence on these effects, using meta-analytic approaches or other methods, will be important in determining which practices are in fact most effective, and possibly in building further support for their adoption in the profession. There are many potential avenues for promoting the adoption of new and arguably preferable practices, such as the data sharing, disclosure and pre-registration approaches described at length in this article. One issue that this article does not directly address is how to  Notes: Reproduced from Table 4 of Ioannidis (2005) 65 Notes: Table shows a sample of recent papers conducting meta-analyses and testing for publication bias in certain literatures in economics. Positive evidence for publication bias indicated by '+', evidence for no publication bias with '-', and mixed evidence with '~'. The number of papers and total estimates used in the meta-analysis are also shown.  Longhi, Nijkamp, and Poot (2005) Immigration and wages -18 (348) Publication bias is not found to be a major factor. The negative effect of immigration is quite small (0.1%) and varies by country. Knell and Stix (2005) Income elasticity of money demand -50 (381) Publication bias does not significantly affect the literature. Income elasticities for narrow money range from 0.4 to 0.5 for the US and 1.0 to 1.3 for other countries. Doucouliagos and Laroche (2003) Union productivity effects + 73 (73) Publication bias is not considered a major issue. Negative productivity associations are found in the UK, with positive associations in the US. Gorg and Strobl (2001) Multi-national corporations and productivity spillovers + 21 (25) Study design affects results, with cross-sectional studies reporting higher coefficients than panel data studies. There is also some evidence of publication bias. Ashenfelter, Harmon, and Oosterbeek (1999) Returns to Education + 27 (96) Publication bias is found, and controlling for it significantly reduces the differences between types of estimates of returns to education.  Duvendack, Palmer-Jones, and Reed (2015) or The Replication Network list (http://replicationnetwork.com/replication-studies/) as well as journal websites. Since \"replication\" is an imprecise term, this categorization is perhaps subject to some debate.      S1 shows a non-normalized version). Fourteen studies have a significant effect in the same direction as the original study in the meta-analysis [77.8%;95% CI = (56.5%,99.1%)]."
        ],
        "ground_truth_definitions": {
          "publication bias": {
            "definition": "certain types of statistical results are more likely to be published than other results, conditional on the research design and data used.",
            "context": "2.2 Publication Bias Publication bias arises if certain types of statistical results are more likely to be published than other results, conditional on the research design and data used. This is usually thought to be most relevant in the case of studies that fail to reject the null hypothesis, which are thought to generate less support for publication among referees and journal editors.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "0209e602acaeab882fee84e244caf574cf345ef9",
        "sections": [
          "large clinical study, 3% of patients taking a sugar pill or placebo had a heart attack compared to 2% of patients taking Lipitor.\" People have unprecedented access to information-available online, in print, and through other media-that they can use to improve their mental and physical health. Much of that information is expressed numerically. For example, the effectiveness of cancer treatments is expressed as survival rates (e.g., the percentage of treated patients who survive for 5 years), the benefits of lifestyle changes as reductions in cardiovascular risk, and the side effects of medications as probabilities of death, discomfort, and disability (Baker, 2006;Woloshin, Schwartz, & Welch, 2005). Indeed, numerical information about health is almost impossible to avoid, ranging from the cereal box at breakfast touting a four-point reduction in total cholesterol to direct-toconsumer advertisements in magazines reporting a 36% reduction in the risk of heart attack in the latest study of a cholesterol-lowering drug. The ubiquity and complexity of health-related numerical information place demands on people that, our review suggests, they are ill-prepared to meet. Two recent trends in health care have exacerbated these demands. First, medical decision making has shifted from a mainly provider-centered to a shared or patient-centered model (e.g., Apter et al., 2008;Sheridan, Harris, & Woolf, 2004). Thus, there is an increased burden on patients to understand health-related information in order to make fully informed choices about their medical care. Second, there is an increased emphasis on applying research findings to achieve evidence-based health practices (Nelson, Reyna, Fagerlin, Lipkus, & Peters, 2008). Thus, people are routinely exposed to research findings with health implications, and health care providers must effectively convey these research findings to patients, findings that are often described numerically (Reyna & Brainerd, 2007). Unfortunately, numerical information is a particularly difficult form of information for both patients and health care providers to understand. As this review shows, low numeracy is pervasive and constrains informed patient choice, reduces medication compliance, impedes access to treatments, impairs risk communication (limiting prevention efforts among those most vulnerable to health problems), and, based on the scant research conducted on outcomes, appears to adversely affect medical outcomes. To minimize the damaging effects of low numeracy, research on how people process numerical information, and how such processing can be improved, is essential. These questions-about how information is processed and can be improved-are fundamentally causal. However, most work on health numeracy has been descriptive rather than concerned with causal mechanisms, and we therefore lack sufficient understanding of how to improve numeracy in people facing various medical decisions. Thus, to resolve the dilemma of health numeracy-that people are swamped with numerical information that they do not understand, and yet they have to make life-and-death decisions that depend on understanding it-theory-driven research that tests causal hypotheses is of the first importance. Therefore, a major goal of this review is to spur interest in conducting such research.",
          "Systematic research on numeracy has been growing steadily over the last several years, but there has not been a comprehensive published review of this literature. In addition to summarizing key findings, this review identifies gaps in our knowledge and suggests paths for future research in the field. The primary goal of this article is to review current directions in numeracy research and, in particular, to examine the relationship between numeracy and decision making in health and selected nonhealth domains with a view to establishing a foundation for future research on causal mechanisms. In the first section of this review, we detail specific conceptualizations of numeracy that are referred to in the remainder of the article. Then, we consider the measurement of numeracy and the implications of different assessments for different conceptualizations of numeracy. In the next section, we describe national assessments of numeracy: how numeracy stacks up against other essential information-processing skills such as prose literacy; how numeracy differs in vulnerable subgroups of the population, such as the old and the poor; and how aspects of numeracy, such as understanding fractions, pose special challenges. In the latter sections of the article, we discuss instruments that assess numeracy in individuals or samples of research subjects, as opposed to national surveys; these assessments also reveal low levels of understanding. We discuss how these assessments relate to risk perception, patient values for health outcomes, other judgments and decision making, health behaviors, and, finally, medical outcomes. Then we review selected research from the cognitive and developmental literatures that elucidates psychological mechanisms in numeracy as well as theories of mathematical cognition that bear on judgment and decision making, including affective approaches, fuzzy trace theory, and other dual-process perspectives, and evolutionary and neuroscience frameworks. Last, we summarize the current state of knowledge concerning numeracy and discuss possible future directions for the field.",
          "Several methods were used to search the literature for potentially relevant research reports. Electronic databases (e.g., PsycINFO, Medline) were used to capture an initial set of potentially relevant research reports. The initial search terms were relatively broad (numeracy, numerical ability, number ability, etc.), resulting in a large number of potential reports. We scanned the abstracts of all the articles identified in the electronic databases for inclusion in the review. After the initial search, we used the Web of Science database to identify additional reports that had referenced many of the pivotal numeracy articles. Finally, the reference lists of all articles identified by the first two methods were examined for additional articles that were missed by the electronic searches.",
          "We focused primarily on empirical reports published in peer-reviewed journals or published books. We also excluded articles that reported single-case studies, introspective studies, and articles with very small sample sizes (e.g., results from interviews with two or three participants). A few unpublished working articles or other reports were included, but we did not make a specific effort to retrieve unpublished literature. We think the decision not to specifically seek unpublished reports is justified, as the primary purpose of this review was to get a broad sense of our current knowledge concerning numeracy and to propose directions for further research. This decision avoids such problems as overinterpretation of null effects (failures to detect effects that can be due to inadequate measures and methods), but it does leave open problems of publication bias (also called the \"file-drawer problem\"; Rosenthal, 1979).",
          "Increasing amounts of health information are being made available to the public, with the expectation that people can use it to reduce their risks and make better medical decisions. For example, patients are expected to take advantage of information about drug options available through Medicare Part D, assess the benefits and drawbacks of each option, and ultimately make wise choices regarding their care (Reed, Mikels, & Simon, 2008). The torrent of health information is likely to persist because it is generated by multiple trends, such as the public's increasing demand for health information related to preventing diseases and making medical decisions; ongoing efforts of government agencies to create and disseminate health information; the proliferation of technologies that support rapid dissemination of research discoveries; and continuing efforts of the health care industry to promote adoption of various medical interventions, exemplified in direct-to-consumer advertising (e.g., Hibbard, Slovic, Peters, Finucane, & Tusler, 2001;Reyna & Brainerd, 2007;Woloshin, Schwartz, & Welch, 2004). Rising health care costs have also encouraged a more consumer-driven approach to health care, in which patients share in both decision making and associated costs, adding to the need for health information (Hibbard & Peters, 2003; but see Shuchman, 2007). Researchers have long recognized the importance of literacy for making informed health decisions (Rudd, Colton, & Schacht, 2000). Individuals with limited literacy skills are at a marked disadvantage in this information age. Low literacy is associated with inferior health knowledge and disease self-management skills, and worse health outcomes (Baker, Parker, Williams, & Clark, 1998;Baker, Parker, Williams, Clark, & Nurss, 1997;Gazmararian, Williams, Peel, & Baker, 2003;Schillinger et al., 2002;Wolf, Gazmararian, & Baker, 2005). A basic understanding of numerical concepts is arguably as important for informed decision making as literacy. In addition to basic reading and writing skills, people need an understanding of numbers and basic mathematical skills to use numerical information presented in text, tables, or charts. However, numeracy, the ability to understand and use numbers, has not received the same attention as literacy in the research literature. We describe national results in detail in a subsequent section, but it is instructive to note here that simple skills cannot be taken for granted. National surveys indicate that about half the U.S. population has only very basic or below basic quantitative skills (Kirsch, Jungeblut, Jenkins, & Kolstad, 2002). Respondents have difficulty with such tasks as identifying and integrating numbers in a lengthy text or performing two or more sequential steps to reach a solution. Although recent surveys have reported some improvement, a significant percentage of Americans continue to have below basic quantitative skills (22% in the 2003 National Assessment of Adult Literacy [NAAL], sponsored by the National Center for Education Statistics; Kutner, Greenberg, Jin, & Paulsen, 2006; for international comparisons, see Reyna & Brainerd, 2007). Furthermore, it is not just the general population that has difficulty with numerical tasks. Studies have shown that even highly educated laypersons and health professionals have an inadequate understanding of probabilities, risks, and other chance-related concepts (Estrada, Barnes, Collins, & Byrd, 1999;Lipkus, Samsa, & Rimer, 2001;Nelson et al., 2008;Reyna, Lloyd, & Whalen, 2001;Sheridan & Pignone, 2002). These difficulties are reflected in poor risk estimation regardless of presentation format (i.e., in percentages or survival curves; Lipkus et al., 2001;Weinstein, 1999), improper calculation of the implications of diagnostic test results for disease probability (Reyna, 2004;Reyna & Adam, 2003), and inconsistent treatment decisions when outcomes are expressed in terms of absolute versus relative risk reduction (Forrow, Taylor, & Arnold, 1992). When surveyed, physicians generally indicate that it is important to provide quantitative risk estimates to their patients. However, they also report feeling more comfortable providing verbal estimates of risk than numerical ones, perhaps because of a lack of confidence and knowledge concerning the quantitative risk estimates or because they are aware that patients do not understand such estimates (Gramling, Irvin, Nash, Sciamanna, & Culpepper, 2004). Before we discuss the extent and ramifications of low numeracy, however, it is important to consider the fundamental question of how numeracy has been defined.",
          "Broadly defined, as we have noted, numeracy is the ability to understand and use numbers. Within this broad definition, however, numeracy is a complex concept, encompassing several functional elements. At the most rudimentary level, numeracy involves an understanding of the real number line, time, measurement, and estimation. Fundamental skills associated with numeracy include the ability to perform simple arithmetic operations and compare numerical magnitudes. At a higher level, numeracy encompasses basic logic and quantitative reasoning skills, knowing when and how to perform multistep operations, and an understanding of ratio concepts, notably fractions, proportions, percentages, and probabilities (Montori & Rothman, 2005;Reyna & Brainerd, 2008). Educators and researchers have defined numeracy in various ways that reflect differences in their domains of study (see Table 1). The word numeracy was coined in 1959 by Geoffrey Crowther of the U.K. Committee on Education in the context of educating English schoolchildren. In its original sense, numeracy encompassed higher level mathematical reasoning skills that extended far beyond the ability to perform basic arithmetical operations (G. Lloyd, 1959): There is the need in the modern world to think quantitatively, to realize how far our problems are problems of degree even when they appear as problems of kind. Statistical ignorance and statistical fallacies are quite as widespread and quite as dangerous as the logical fallacies which come under the heading of illiteracy. (pp. 270-271) Advancing a similarly expansive conception of numeracy, Paulos (1988) brought popular attention to the pervasive impairments in everyday functioning created by \"innumeracy,\" which he described as mathematical illiteracy. He emphasized the \"inability to deal comfortably with the fundamental notions of number and chance\" (p. 3), as well as difficulties in apprehending the magnitudes of extremely large and small numbers. The concept of numeracy is often subsumed within the broader concept of literacy (Davis, Kennen, Gazmararian, & Williams, 2005). Experts have recognized that literacy is multifaceted and extends beyond simply reading and writing text to include mathematical reasoning and skills. Numeracy has thus been referred to as quantitative literacy, or \"the ability to locate numbers within graphs, charts, prose texts, and documents; to integrate quantitative information from texts; and to perform appropriate arithmetical operations on text-based quantitative data\" (Bernhardt, Brownfield, & Parker, 2005, p. 6). The conception of literacy as a multidimensional construct, and of numeracy as an integral subcomponent of literacy, is evinced by how the U.S. Department of Education defines literacy in its national literacy surveys, such as the National Adult Literacy Survey (NALS; Kirsch, Jungeblut, Jenkins, & Kolstad, 2002) and the NAAL (Kutner et al., 2006). In these surveys, literacy is a composite construct consisting of prose literacy (understanding and using information from texts), document literacy (locating and using information in documents), and quantitative literacy (applying arithmetical operations and using numerical information in printed materials). Numeracy in the health context is often referred to as health numeracy and similarly conceptualized as a subcomponent of health literacy. As defined by Baker (2006), health literacy is an ordered skill set underlying the ability to understand written health information and to communicate orally about health. Baker's definition includes prose, document, and quantitative literacy, as others do, but also \"conceptual knowledge of health and health care\" (p. 878). Quantitative literacy is assumed to be critical in these definitions because numberseither in text or graphic format-pervade nearly all aspects of health communication. Other broad definitions of health literacy that have been proposed by various organizations include quantitative reasoning skills as an integral component, in addition to basic computational skills and knowledge (see Table 1). Health numeracy, however, is itself a broad concept because numerical reasoning in the health domain involves several different tasks and skills. One important task is to judge the relative risks and benefits of medical treatments; this task requires the ability to assess risk magnitude, compare risks, and understand decimals, fractions, percentages, probabilities, and frequencies, as these are the formats in which risk and benefit information is most often presented (Bogardus, Holmboe, & Jekel, 1999;Burkell, 2004). Other important tasks include interpreting and following medical treatment plans and navigating the health care system; such tasks require lower level, but still critical, numerical abilities including interpreting and following directions on a medication prescription label, scheduling follow-up medical appointments, and completing health insurance forms (Parker, Baker, Williams, & Nurss, 1995). Thus, health numeracy refers to various specific aspects of numeracy that are required to function in the health care environment (see Table 1). It is not simply the ability to understand numbers but rather to apply numbers and quantitative reasoning skills in order to access health care, engage in medical treatment, and make informed health decisions. In an effort to develop an overarching framework for health numeracy that incorporates the varied skills that we have discussed, Golbeck, Ahlers-Schmidt, Paschal, and Dismuke (2005) conceptualized health numeracy as falling into four categories: basic (the ability to identify and understand numbers, as would be required to identify the time and date on a clinic appointment slip), computational (the ability to perform simple arithmetical calculations, such as calculating the number of calories from fat in a food label), analytical (the ability to apply higher level reasoning to numerical information, such as required to interpret graphs and charts), and statistical (the ability to apply higher level biostatistical and analytical skills, such as required to analyze the results of a randomized clinical trial). These four categories together compose the first level of Ancker and Kaufman's (2007) conceptual model. As in Baker's (2006) approach, Ancker and Kaufman's (2007) model incorporates elements beyond the level of individuals' skills, most especially the health care environment. They proposed that health numeracy, or \"the effective use of quantitative information to guide health behavior and make health decisions\" (p. 713), depends on the interaction of three variables: (a) the individual-level quantitative, document, prose, and graphical literacy skills of the patient and provider; (b) the oral communication skills of both patient and provider; and (c) the quality and ease of use of information artifacts (such as decision aids and websites). Schapira et al. (2008) also described numeracy as a multifaceted construct that incorporates more than individuals' skills to include interpretive components influenced by patient affect. The definitions that we have discussed introduce useful distinctions, such as contrasting basic computational versus reasoning abilities, and they are designed to highlight aspects of numeracy that have practical importance in the health care setting. However, none of the definitions is derived from an empirically supported theory of mathematical cognition. As we discuss, assessments of numeracy are similarly uninformed by theory. Assessments, in fact, are more narrowly construed than definitions of numeracy. Although conceptual definitions of health numeracy have stressed the health care environment, assessments have focused squarely on the skills of individuals, as we discuss in the following section.",
          "How proficient are U.S. residents at understanding and working with numbers? Several national and international surveys of mathematical achievement suggest that although most Americans graduate from high school with basic mathematical skills, they are not proficient and compare unfavorably with residents of other countries (Reyna & Brainerd, 2007). Moreover, most 12th graders lack skills that are essential for health-related tasks, falling short of what Golbeck et al. (2005) would describe as the analytical level at which numbers are used and understood. The National Assessment of Educational Progress (NAEP), or nation's report card, provides a comprehensive assessment of mathematical knowledge and skills. The NAEP comprises two types of assessments: a long-term trend assessment that has charted performance since 1973 and a \"main\" assessment that is periodically updated. In the most recent trends assessment, the average score for 12th-grade students was not appreciably different from the average score of 12th graders in 1973 (Perie, Moran, & Lutkus, 2005). Therefore, despite the increasing amount and complexity of health-related numerical information, students enter young adulthood no better prepared to process it than they were a generation ago. The 2007 main NAEP assessed understanding of mathematical concepts and application of those concepts to everyday situations (Lee, Grigg, & Dion, 2007). Content areas included number properties and operations, measurement, geometry, data analysis and probability, and algebra. Achievement level was classified as basic (demonstrating partial mastery of gradelevel skills), proficient (demonstrating solid grade-level performance), or advanced (demonstrating superior performance). The most recent data for 12th-grade mathematics performance were obtained from a nationally representative sample of more than 9,000 high school seniors (Grigg, Donahue, & Dion, 2007). Fully 41% of students performed at a belowbasic level, 37% performed at a basic level, 20% performed at a proficient level, and 2% performed at an advanced level. This means that a substantial proportion of 12th graders did not have the basic mathematical skills required to, for example, convert a decimal to a fraction. In a theme that echoes across multiple national assessments, scores differed among subgroups. For example, Asian and Caucasian students performed better than African American, Hispanic, and American Indian students. Similar findings were reported for the 2003 Program for International Student Assessment (PISA), which assesses mathematical literacy and problem-solving skills. Questions on the PISA reflect real-world situations requiring mathematical skills (e.g., converting currency for a trip abroad) and, thus, might be expected to be especially relevant to health numeracy. Like Golbeck et al.'s (2005) analytical level, PISA's emphasis is on using numerical knowledge and skills. In 2003 the performance of U.S. students was mediocre compared with that of students from other nations, with U.S. students scoring significantly below their peers in 23 countries. Average scores on each of the four mathematical literacy subscales (space and shape; change and relationships; quantity; and uncertainty) were significantly below the average scores for industrialized countries. Americans lagged behind their peers in mathematical problem solving as well: They ranked 29th of 39 countries tested and again scored significantly below the average for industrialized nations (although difficulties with mathematics spanned international borders; Lemke et al., 2004). Not surprisingly, the mathematical proficiency of adults, as assessed by national surveys, is also lacking. The NALS, first carried out in 1992, surveyed a nationally representative sample of more than 26,000 adults (Kirsch et al., 2002). Each of the three literacy scales-prose, document, and quantitative-is divided into five proficiency levels. Twenty-two percent of adults performed at the lowest level of quantitative literacy, indicating that a substantial portion of the population has difficulty performing simple arithmetical operations. Twenty-five percent of adults performed at the next lowest level, which requires the ability to locate numbers and use them to perform a one-step operation. Nearly half the adult U.S. population could not identify and integrate numbers in a lengthy text or perform a numerical task requiring two or more sequential steps. Therefore, many adults lack the skills necessary to read a bus schedule to determine travel time to a clinic appointment or to calculate dosage of a child's medication based on body weight according to label instructions. The 2003 NAAL (http://nces.ed.gov/NAAL/index.asp?file=KeyFindings/Demographics/ Overall.asp&PageId=16), the most comprehensive assessment of the nation's literacy since the NALS, measured the literacy of a nationally representative sample of approximately 19,000 adults (Kutner et al., 2006). Included in this assessment was a scale designed to measure health literacy. Like the NALS, the NAAL evaluated prose, document, and quantitative literacy, and test items reflected tasks that people would likely encounter in everyday life. Adults were classified according to four literacy levels: below basic, basic, intermediate, and proficient. Those individuals functioning at the below-basic level would be expected to have only the simplest skills, such as being able to add two numbers, whereas those at a basic level would be expected to be able to perform simple, one-step arithmetical operations when the operation was stated or easily inferred. At a more advanced intermediate level, adults should be able to locate and use less familiar numerical information and solve problems when the operation is not stated or easily inferred. Overall, 36% of adults, or more than 93 million people, are estimated to perform at a below-basic or basic level. Those who scored lower on prose or document literacy also tended to score lower on quantitative literacy, but quantitative items elicited the lowest level of performance: Significantly more adults scored in the below-basic level on the quantitative scale (22%) than on the prose scale (14%) or document scale (12%; Kutner et al., 2006). Subgroup analyses provide an even more disturbing picture of the nation's health literacy (Gonzales et al., 2004;Kutner et al., 2006;Lemke et al., 2004;Perie, Grigg, & Dion, 2005;Perie, Moran, & Lutkus, 2005;Reyna & Brainerd, 2007). Vulnerable subgroups with traditionally lower access to health care were, unfortunately, those with the lowest scores: Poverty and being a nonnative speaker of English were associated with lower scores. Among racial and ethnic subgroups, Hispanics and African Americans had the lowest average health literacy: Sixty-six percent of Hispanics and 58% of African Americans performed at a belowbasic or basic level of health literacy. Adults age 65 and older had lower health literacy than younger adults: More than half the adults in the oldest age group had below-basic or basic health literacy. The latter figures are noteworthy in the context of health numeracy because older adults are more likely to have health problems. Although, as we have discussed, high school students performed poorly, adults who did not graduate from high school were worse off than those who did. Nearly one half of adults who did not complete high school functioned at a below-basic level. In sum, representative national assessments of mathematical performance indicate that a slim majority of Americans have basic knowledge and skills. Performance of 12th graders has not changed in decades, despite rising requirements for numeracy. National performance levels for adults in mathematics generally raise questions that are borne out by low performance in assessments of health literacy, most notably quantitative literacy. This concern is heightened when we consider that millions of Americans score below average and that differences in performance are found across racial, ethnic, and socioeconomic groups. People with more health problems, and who had fewer resources to draw on to deal with those problems, had the lowest scores: Older, poorer, and less educated adults had lower health literacy than their younger, richer, and more educated counterparts. Thus, national assessments of mathematics achievement, of quantitative problem-solving performance, and of health literacy (including quantitative health literacy or numeracy) suggest that the average person is poorly equipped to process crucial health messages and medical information.",
          "A variety of instruments have been developed that specifically assess health numeracy. These instruments are typically used in research studies and are not administered to nationally representative samples. They do allow, however, a more fine-grained and formal assessment of mathematical skills. Without such assessment, it is difficult to determine whether an individual is sufficiently literate and numerate to function effectively in the health care environment (Nelson et al., 2008). One reason is that physicians' ability to identify low-literate patients is limited. In three studies conducted at university-based medical clinics, physicians overestimated their patients' literacy skills (Bass, Wilson, Griffith, & Barnett, 2002;Lindau et al., 2002;Rogers, Wallace, & Weiss, 2006). Simply asking patients about their skills is unlikely to be useful because of the shame and stigma associated with low literacy and numeracy (Marcus, 2006;Parikh, Parker, Nurss, Baker, & Williams, 1996). Moreover, even if patients were willing, it is unlikely that selfassessments would be accurate (Dunning, Heath, & Suls, 2004). According to the NALS, most of the adults who performed at the lowest literacy level felt that they could read \"well\" and did not consider reading to be a problem (Kirsch et al., 2002). Similarly, Sheridan, Pignone, and Lewis (2003) found that although 70% of subjects perceived themselves to be good with numbers, only 2% answered three numeracy questions correctly. As we discussed earlier in the context of national surveys, health numeracy often lags behind literacy. Thus, educational attainment does not ensure grade-level skills, and this is particularly true for mathematical skills (Doak & Doak, 1980;Kicklighter & Stein, 1993;McNeal, Salisbury, Baumgardner, & Wheeler, 1984;Rothman et al., 2006;Safeer & Keenan, 2005;Sentell & Halpin, 2006). Educated, literate people have difficulty understanding important numerical concepts such as relative risk reduction, number needed to treat, and conditional probabilities (e.g., probability of disease given a genetic mutation; Gigerenzer, Gaissmaier, Kurz-Milcke, Schwartz, & Woloshin, 2007;Reyna et al., 2001;Weiss, 2003). A surprising number of such people also have difficulty with elementary numerical concepts, such as whether a .001 risk is larger or smaller than 1 in 100 (Reyna & Brainerd, 2007). Therefore, although educational attainment is correlated with prose, document, and quantitative literacy, years of schooling cannot be assumed to translate into levels of numeracy (Rothman, Montori, Cherrington, & Pigone, 2008). The findings we have discussed-that providers cannot reliably identify patients with low numeracy, self-report is suspect, and level of education is misleading -indicate that specific instruments that assess numeracy are required. Given the need for assessment of numeracy, it is not clear what form such assessment should take. Extant health numeracy measures can be broadly classified as either objective (respondents make numerical judgments or perform calculations, and their performance is evaluated objectively) or subjective (respondents express their level of confidence in their numerical ability). Objective measures ascertain a variety of abilities, such as how well people perform arithmetical operations, convert from one metric to another (e.g., express a frequency as a percentage), understand probability, and draw inferences from quantitative data. Subjective measures, which were conceived of as a less stressful and intimidating way to estimate level of numeracy, assess people's perceptions of their numerical competence (Fagerlin, Zikmund-Fisher, et al., 2007). Objective numeracy measures can be further subdivided into those that assess numeracy only or those that assess both literacy and numeracy, and into general or disease-specific measures. Numeracy measures have been related to measures of cognition, behaviors, and outcomes. In this section, we describe test characteristics and discuss relations to other measures in a subsequent section. We begin with objective composite measures that incorporate separate dimensions of competence, that is, literacy and numeracy.",
          "The Test of Functional Health Literacy in Adults (TOFHLA), the only health literacy measure to explicitly incorporate a numeracy component, represents a disease-general and composite measure in that it tests both reading comprehension and numeracy separately (Davis et al., 2005; see Table 2). The TOFHLA reading comprehension section tests how well people understand instructions for a surgical procedure, a Medicaid application form, and an informedconsent document. The TOFHLA numeracy items pertain to tasks commonly encountered in health settings. They test the ability to follow instructions on a prescription medicine label, judge whether a blood glucose value is within normal limits, interpret a clinic appointment slip, and determine eligibility for financial assistance based on income and family size. Although the TOFHLA tests reading comprehension and numeracy separately, it evaluates these sections psychometrically as a single unit. This feature, as well as the validation performed on the instrument, limits the utility of the TOFHLA for ascertaining numeracy per se. For example, concurrent validity of the TOFHLA was tested by correlating the TOFHLA with the Rapid Estimate of Adult Literacy in Medicine (REALM; Davis et al., 1991) and the reading subtest of the revised Wide Range Achievement Test (Jastak & Wilkinson, 1984), both of which test the ability to read and pronounce words (see Table 2). The numeracy section of the TOFHLA was not validated against a recognized measure of mathematical ability, such as the mathematics subtest of the Wide Range Achievement Test. Although reliability of a related measure in dentistry (TOFHLiD; Gong et al.) was determined for the reading comprehension and numeracy sections separately, construct validity was assessed with two reading tests, the REALM and the REALD-99, and the TOFHLA. The numeracy section of the TOFHLiD was also not validated against a specific numeracy measure. Despite these limitations, the TOFHLA provides an indirect measure of key numeracy skills that contribute to functional health literacy (Parker et al., 1995). However, one drawback is that the TOFHLA can take up to 22 min to administer; for this reason, a short version (S-TOFHLA) containing two prose passages and four numeracy items, and requiring 12 min to administer, was developed (Baker, Williams, Parker, Gazmararian, & Nurss, 1999). Although the S-TOFHLA had adequate internal consistency (Cronbach's alphas for the numeracy and prose sections were .68 and .97, respectively) and was significantly correlated with the REALM (.80), the correlation of the numeracy items with the REALM (.61) was considerably lower than that of the reading comprehension section with the REALM (.81). Because the two prose passages of the S-TOFHLA were highly correlated with the full TOFHLA (.91), the four numeracy items were deleted. The S-TOFHLA was thus reduced to 36 reading comprehension items that required only 7 min to administer. The TOFHLA and original S-TOFHLA have been used to assess health literacy and numeracy in a range of health studies, including studies of geriatric retirees (Benson & Forman, 2002), Medicare patients (Baker, Gazmararian, Sudano, et al., 2002;Gazmararian et al., 1999;Gazmararian et al., 2003;Scott, Gazmararian, Williams, & Baker, 2002;Wolf et al., 2005), community-dwelling patients (Baker, Gazmararian, Sudano, & Patterson, 2000;Montalto & Spiegler, 2001), rheumatoid arthritis patients (Buchbinder, Hall, & Youd, 2006), spinal cord injury patients (Johnston, Diab, Kim, & Kirshblum, 2005), HIV-infected patients (Kalichman, Ramachandran, & Catz, 1999;Mayben et al., 2007), cardiovascular disease patients (Gazmararian et al., 2006), chronic disease patients (Williams, Baker, Parker, & Nurss, 1998), public hospital patients (Baker et al., 1997;Nurss et al., 1997;Parikh et al., 1996;Williams et al., 1995), emergency department patients (Baker et al., 1998), and Veterans Administration hospital patients (Artinian, Lange, Templin, Stallwood, & Hermann, 2003). Like composite measures, integrative measures incorporate multiple dimensions of verbal and numerical processing (see Table 2). However, integrative measures involve tasks that require multiple skills for successful performance, such as both reading comprehension and numeracy. Unlike composite measures, literacy, numeracy, or other subscale scores cannot be separated; a single overall score is assigned. For example, in the Newest Vital Sign and the Nutrition Label Survey, people view a nutrition label and answer questions that require reading comprehension skills as well as arithmetical computational and quantitative reasoning skills. They test both document literacy and quantitative literacy, respectively, in that they require the ability to search for and \"use information from noncontinuous texts in various formats\" and the ability to use \"numbers embedded in printed materials\" (Kutner et al., 2006, p. iv). To complete the tasks on the Newest Vital Sign and the Nutrition Label Survey measures, people must be able to read and identify numbers contained in nutrition labels, ascertain which numbers are relevant to the specific question, determine the arithmetical operation required, and apply that operation. For example, in the Nutrition Label Survey, people are asked to view a soda nutrition label and determine how many grams of total carbohydrate are contained in a bottle. To answer this question, people must find where total carbohydrate content is listed on the label, determine the total carbohydrate content per serving (27 g), determine the number of servings per container (2.5), and apply the appropriate arithmetical operation to yield the correct answer (67.5 g; Rothman et al., 2006). Although such integrative tests involve realistic tasks, it is impossible to determine how much numeracy contributes to overall performance, and their reliability is lower than other measures (see Table 2).",
          "A major shortcoming of existing composite and integrative scales is that they do not assess understanding of risk and probability. Adequate understanding of risk and probability is critical for decision making in all domains of health care, ranging from disease prevention and screening to treatment to end-of-life care (Nelson et al., 2008;Reyna & Hamilton, 2001). Risks and probabilities are examples of ratio concepts in mathematics (Reyna & Brainerd, 1994). After surveying performance on national and international assessments, Reyna and Brainerd (2007) concluded that ratio concepts such as fractions, percentages, decimals, and proportions are especially difficult to understand, and most adults perform poorly on these items. The National Mathematics Advisory Panel ( 2008) recently reached a similar conclusion after reviewing over 16,000 published studies of mathematics achievement. In addition to the processing complexities inherent in ratio concepts, risks and probabilities are associated with challenging abstract concepts such as chance and uncertainty (which refers to ambiguity as well as probability; Politi, Han, & Col, 2007). One of the first efforts to assess people's understanding of risk information was undertaken by Black, Nease, and Tosteson (1995), who assessed numeracy by asking participants how many times a fair coin would come up heads in 1,000 tosses. Respondents were considered numerate if they answered the question correctly and provided logically consistent responses to other questions regarding the probability of developing or dying from breast cancer (e.g., estimating the probability of acquiring a disease as being greater than or equal to the probability of dying from the disease). Many numeracy measures feature such class-inclusion judgments (i.e., some probabilities are nested within other, more inclusive probabilities), a fact that has theoretical significance and, consequently, is discussed below in Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy. Another simple numeracy measure was developed by Weinfurt and colleagues (Weinfurt et al., 2003(Weinfurt et al., , 2005)), who used a single question to assess how well patients understood the relative frequency of benefit from a treatment. They asked 318 oncology patients the meaning of the statement \"This new treatment controls cancer in 40% of cases like yours\" in the context of a physician's prognosis. Seventy-two percent of patients indicated that they understood this meant that \"for every 100 patients like me, the treatment will work for 40 patients.\" However, 16% of patients interpreted this statement to mean either that the doctor was 40% confident that the treatment would work or that the treatment would reduce disease by 40%, and 12% of patients indicated that they did not understand the statement (Weinfurt et al., 2005). Similarly, the L. M. Schwartz, Woloshin, Black, and Welch (1997) three-item numeracy scale tests familiarity with basic probability and related ratio concepts (e.g., proportions), which represents a departure from the TOFHLA's emphasis on simple arithmetical operations, basic understanding of time, and the ability to recognize and apply numbers embedded in text. The Schwartz et al. measure tests understanding of chance (\"Imagine that we flip a fair coin 1,000 times. What is your best guess about how many times the coin would come up heads?\") and the ability to convert a percentage to a frequency (e.g., \"In the BIG BUCKS LOTTERY, the chance of winning a $10 prize is 1%. What is your best guess about how many people would win a $10 prize if 1000 people each buy a single ticket to BIG BUCKS?\"), and vice versa. In their original study, L. M. Schwartz et al. (1997) examined the relationship of general numeracy to the ability to understand the benefits of screening mammography among 287 female veterans. Although 96% of the participants were high school graduates, more than half the women answered no or one question correctly, and only 16% answered all three questions correctly. Compared with less numerate women, more numerate women were better able to understand risk reduction information. This numeracy assessment was subsequently used to examine the relationship of numeracy to the validity of utility assessment techniques (Woloshin, Schwartz, Moncur, Gabriel, & Tosteson, 2001). Compared with low-numerate women, high-numerate women provided more logically consistent utility scores. Similar findings were reported by S. R. Schwartz, McDowell, and Yueh (2004), who used a slightly modified version of the numeracy assessment to examine the effect of numeracy on the ability of head and neck cancer patients to provide meaningful quality of life data as measured by utilities for different states of health. Compared with low-numerate patients, highnumerate patients demonstrated greater score consistency on utility measures. Sheridan and Pignone (2002) also administered the L. M. Schwartz et al. (1997) numeracy assessment to 62 medical students and found that numeracy was associated with the ability to accurately interpret quantitative treatment data. The Schwartz et al. numeracy assessment has been used in an adapted or expanded format in other health research contexts (Estrada et al., 1999;Estrada, Martin-Hryniewicz, Peek, Collins, & Byrd, 2004;Parrott, Silk, Dorgan, Condit, & Harris, 2005). Lipkus et al. (2001) sought to extend the L. M. Schwartz et al. (1997) numeracy assessment and test it in a highly educated population. To expand the numeracy assessment, they added eight questions framed in a nonspecific health context to the original three-item measure. They made a minor change to one of the three Schwartz et al. scale items: Rather than assess understanding of probability in the context of flipping a fair coin, the question was phrased in terms of rolling a fair six-sided die. Each of the eight new questions referred generally to either a \"disease\" or an \"infection.\" As with the Schwartz et al. measure, the new items required an understanding of probability and ratio concepts (i.e., working with fractions, decimals, proportions, percentages, and probability). For example, the following question taps understanding of percentages: \"If the chance of getting a disease is 10%, how many people would be expected to get the disease out of 100? Out of 1000?\" Like other general numeracy measures that we have reviewed, which share similar items, the Lipkus et al. (2001) numeracy scale has acceptable reliability, but extensive psychometric validation and national norming data are lacking. However, the reported correlations between this measure and health-relevant judgments, such as risk perceptions, support its validity (see Effect of Numeracy on Cognition, Behaviors, and Outcomes, below). In any case, this numeracy scale is instructive in that it clearly demonstrated that even college-educated people have difficulty with basic ratio concepts (i.e., probability, percentages, and proportions) and perform poorly when asked to make relatively simple quantitative judgments. In fact, when one compares the performance of the less well educated participants in the L. M. Schwartz et al. (1997) study with that of the more highly educated participants in the Lipkus et al. (2001) study, the results are remarkably similar. In the Schwartz et al. study, 36% of the 287 participants had some college education, compared with 84%-94% of the 463 participants in the Lipkus et al. study. Despite this difference in educational attainment, 58% of the participants in both studies answered no or one question correctly. Sixteen percent of subjects in the Schwartz et al. study and 18% in the Lipkus et al. study answered all the questions correctly. (As these comparisons suggest, studies that control for effects of education, income, and other factors have shown that numeracy accounts for unique variance-e.g., Apter et al., 2006;Cavanaugh et al., 2008-though controls for ethnicity and socioeconomic status are inconsistent.) It is troubling that even college-educated people have difficulty with ratio concepts because ratio concepts are critical for understanding and interpreting risk, which in turn is required to make effective medical judgments (Reyna, 2004). Building on the Lipkus et al. (2001) numeracy scale, Peters and colleagues (Greene, Peters, Mertz, & Hibbard, 2008;Hibbard, Peters, Dixon, & Tusler, 2007;Peters, Dieckmann, Dixon, Hibbard, & Mertz, 2007) added four items to create an expanded numeracy scale. The new items, which make the Lipkus et al. numeracy scale more challenging, test familiarity with ratio concepts and the ability to keep track of class-inclusion relations (Barbey & Sloman, 2007;Reyna, 1991;Reyna & Mills, 2007a). For example, the following question from this test requires processing of nested classes and base rates and then determining the positive predictive value of a test (i.e., the probability that a positive result indicates disease): Suppose you have a close friend who has a lump in her breast and must have a mammogram. Of 100 women like her, 10 of them actually have a malignant tumor and 90 of them do not. Of the 10 women who actually have a tumor, the mammogram indicates correctly that 9 of them have a tumor and indicates incorrectly that 1 of them does not. Of the 90 women who do not have a tumor, the mammogram indicates correctly that 81 of them do not have a tumor and indicates incorrectly that 9 of them do have a tumor. The table below summarizes all this information. Imagine that your friend tests positive (as if she had a tumor), what is the likelihood that she actually has a tumor? (Peters, Dieckmann, et al., 2007, p. 174) The correct answer is .50. The Medical Data Interpretation Test calls on even more advanced skills, compared with the general numeracy scales just reviewed (L. M. Schwartz, Woloshin, & Welch, 2005;Woloshin, Schwartz, & Welch, 2007). Whereas most general numeracy measures assess a range of arithmetic computation skills, basic understanding of probability and risk, and simple quantitative reasoning skills, the Medical Data Interpretation Test \"examines the ability to compare risks and put risk estimates in context (i.e., to see how specific data fit into broader health concerns and to know what additional information is necessary to give a medical statistic meaning)\" (L. M. Schwartz et al., 2005, p. 291). The instrument tests skills needed to interpret everyday health information, such as information contained in drug advertisements or healthrelated news reports. In addition to the skills needed to complete the other general numeracy measures, the Medical Data Interpretation Test requires a more sophisticated understanding of base rates, absolute risk, relative risk, knowledge of the kinds of information needed to assess and compare risks, and the ability to apply inferential reasoning to health information. The test also taps understanding of epidemiological concepts and principles, such as incidence, the distinction between population-level and individual-level risk, and clinical trial design (e.g., why comparison groups are needed for clinical trials). For example, one of the test questions pertains to a description of a clinical trial of a new drug for prostate cancer. In this trial, only three subjects taking the study drug developed prostate cancer. On the basis of this information, the test taker is asked to select the most critical question for understanding the results of the clinical trial from among these options: (a) Who paid for the study? (b) Has the drug been shown to work in animals? (c) What was the average age of the men in the study? (d) How many men taking the sugar pill developed prostate cancer? Another series of questions tests reasoning skills. People are first asked to estimate a person's chance of dying from a heart attack in the next 10 years and then to estimate that same person's chance of dying for any reason in the next 10 years. To answer correctly, a person would need to recognize that the risk of dying from all causes is greater than the risk of dying from a single cause (another class-inclusion judgment; Reyna, 1991). The Medical Data Interpretation Test has been translated into Dutch and validated among Dutch university students (Smerecnik & Mesters, 2007). Like other disease-general numeracy measures, the Medical Data Interpretation Test has face validity, as it seems to require skills involved in medical decisions. In sum, performance on several disease-general numeracy tests has been linked to healthrelated risk perceptions, understanding of treatment options, measurement of patient utilities and other relevant cognitions, behaviors, and outcomes. However, as we discuss in greater detail below, the content of the tests has been determined by using prior measures and commonsense assumptions; none of these tests explicitly taps research or theory in mathematical cognition.",
          "Disease-specific numeracy instruments have also been developed to assist with the management of chronic conditions that require self-monitoring (see Table 2). These tests have yet to garner the extent of empirical support that disease-general measures have, but they allow researchers (and potentially clinicians) to focus on skills relevant to specific diseases and treatment regimens. Apter et al. (2006) developed a four-item numeracy questionnaire that assesses understanding of basic numerical concepts required for asthma self-management. The questionnaire tests a patient's understanding of basic arithmetic (e.g., determining how many 5-mg tablets are needed if your daily dose of prednisone is 30 mg) and percentages, as well as the ability to calculate and interpret peak flow meter values. Estrada et al. (2004) expanded the L. M. Schwartz et al. (1997) three-item numeracy assessment to test the ability of patients taking warfarin (an anticoagulant) to handle basic numerical concepts needed for anticoagulation management. They added three items that assess basic knowledge of addition, subtraction, multiplication, and division that apply specifically to warfarin (e.g., \"You have 5 mg pills of Coumadin [warfarin] and you take 7.5 mg a day. If you have 9 pills left, would you have enough for one week?\"). Finally, the Diabetes Numeracy Test (Cavanaugh et al., 2008;Huizinga et al., 2008) is a 43-item instrument that taps multiple numeracy domains relevant to diabetes nutrition, exercise, blood glucose monitoring, oral medication use, and insulin use. An abbreviated 15-item version of the Diabetes Numeracy Test, which demonstrates a .97 correlation with the 43-item instrument, is also available (Huizinga et al., 2008; see also Montori et al., 2004). Although of recent vintage, these disease-specific numeracy scales show promise in predicting medical outcomes that are tied to measured skills, as discussed further below (Estrada et al., 2004).",
          "Unlike the objective measures of numeracy that we have reviewed so far, subjective numeracy measures attempt to assess how confident and comfortable people feel about their ability to understand and apply numbers without actually having to perform any numerical operations. A primary rationale underlying researchers' interest in subjective measures has been to increase the feasibility and acceptability of measuring numeracy for respondents, because objective measures are arduous and potentially aversive. The aim has been to develop a measure that would allow subjective numeracy to be used as a proxy for objective numeracy. The first subjective numeracy measures to be developed were the STAT-Interest and STAT-Confidence scales, created by Woloshin et al. (2005) to assess people's attitudes toward healthrelated statistics. The three items on the STAT-Confidence scale cover perceived ability to understand and interpret medical statistics; the five items on the STAT-Interest scale pertain to level of attention paid to medical statistics in the media and in the medical encounter. Although study participants reported generally high levels of interest and confidence in medical statistics, the interest and confidence scales were weakly correlated with a validated measure of objective numeracy, the Medical Data Interpretation Test (r = .26 and r = .15, respectively), suggesting that people are poor judges of their ability to use medical statistics. This finding is not entirely unexpected, as it is well documented that people tend to be poor judges of their abilities, particularly in the educational domain (Dunning et al., 2004). The ability to self-assess is subject to such systematic biases as unrealistic optimism, overconfidence, and the belief that one possesses above-average abilities. However, in contrast to the findings of Woloshin et al. (2005), the Subjective Numeracy Scale (Fagerlin, Zikmund-Fisher, et al., 2007;Zikmund-Fisher, Smith, Ubel, & Fagerlin, 2007) demonstrated a moderate correlation (rs = .63-.68) with the Lipkus et al. (2001) numeracy scale, suggesting that subjective measures may be a potentially viable means of estimating numeracy. Naturally, the most persuasive evidence of validity for subjective measures would be that they are able to predict objective performance, but little evidence been gathered on this point (but see Fagerlin, Zikmund-Fisher, et al., 2007). Further research is also needed to determine the potential clinical utility of subjective measures such as these (Nelson et al., 2008).",
          "In sum, to date various measures have been developed to assess health numeracy, yet no single measure appears to capture the totality of this construct. Rather, the objective health numeracy measures can be thought of as representing a continuum of competencies, ranging from rudimentary numeracy skills (such as the ability to tell time and perform one-and two-step arithmetic problems) to intermediate level skills (including the ability to apply basic ratio concepts involved in understanding risks and probabilities) to advanced numeracy skills requiring higher level inferential reasoning skills (such as the ability to determine the positive predictive value of a test). Examples of measures that test basic, low-level skills include the TOFHLA and TOFHLiD. Measures that fall between basic and intermediate (analytical) level skills include the Newest Vital Sign and the Nutrition Label Survey. All the general and diseasespecific measures that we have examined require at least some intermediate-level skills. The Medical Data Interpretation Test and the Peters, Dieckmann, et al. (2007) expanded numeracy scale both require higher level reasoning skills to assess risk. Yet, as we discuss in the next section, progress in assessment has outpaced progress in basic understanding of numeracy on a causal level, that is, in understanding the cognitive mechanisms that underlie numeracy and how numeracy affects health behaviors and outcomes.",
          "For clinicians and policymakers, the importance of numeracy in health care is not as an end in itself but as a means of achieving health behaviors and outcomes that matter for patients. Because effective health care depends so critically on adequate patient understanding, numeracy has the potential to affect a variety of important outcomes, ranging from health decision making, health services utilization, and adherence to therapy to more distal outcomes including morbidity, health-related quality of life, and mortality. As our subsequent review of this research details, there is evidence for the expected associations between numeracy and various cognitive milestones along the causal path to such outcomes, ranging from effects on comprehension to effects on judgment and decision making; in a few studies, associations with health behaviors and outcomes have been demonstrated. Figure 1 portrays some of the points on this path. We begin with perceptions of risks and benefits, followed by measurement of patient utilities (e.g., values for health states such as disability as opposed to death), information presentation and formatting, and, last, health behaviors and medical outcomes.",
          "An understanding of the risks and benefits associated with particular choice options is important for many health decisions. For example, patients are expected to understand and weigh the risks and benefits of various treatment options shortly after being diagnosed with an illness. In this section, we review literature showing that people lower in numerical ability have consistent biases in their perceptions of risk and benefit. Many of the studies examining risk perceptions have been conducted in the context of breast cancer research. Black et al. (1995) asked women between the ages of 40 and 50 (N = 145) several questions about the probability that they would develop or die of breast cancer in the next 10 years. They measured numeracy with a single question (the number of times a fair coin would come up heads in 1,000 tosses). The entire sample overestimated their personal risk of breast cancer, compared with epidemiological data, and those lower in numeracy made even larger overestimations than those higher in numeracy. L. M. Schwartz et al. (1997) also asked women (N = 287) to estimate the risk of dying from breast cancer both with and without mammography screening. They presented the women with risk reduction information (i.e., risk reduction attributable to mammography) in four formats and calculated accuracy by how well they adjusted their risk estimates in light of the new information. After controlling for age, income, level of education, and the format of the information, they found that participants higher in numeracy were better able to use the risk reduction data to adjust their risk estimates. In another study, Woloshin, Schwartz, Black, and Welch (1999) asked women (N = 201) to estimate their 10-year risk of dying from breast cancer as a frequency out of 1,000. In addition, they asked the women to estimate how their risk compared with that of an average woman their age. Numeracy was measured with the three-item scale used by Schwartz et al. (1997). After controlling for education and income, they found that numeracy was not related to participants' comparison judgments, but participants lower in numeracy overestimated their risk of dying from breast cancer in the next 10 years. This study showed that participants lower in numeracy might still be able to make accurate risk comparisons, even though they are not able to make unbiased risk estimates. In another study of breast cancer risk (Davids, Schapira, McAuliffe, & Nattinger, 2004), a sample of women estimated their 5-year and lifetime risk of breast cancer and completed the L. M. Schwartz et al. (1997) scale. Similar to the findings above, participants (N = 254) as a whole overestimated their risk of breast cancer (compared with epidemiological data), with those lower in numeracy making larger errors in their estimates than those higher in numeracy (when controlling for age, race, education, and income). In a separate report, these authors also showed that numeracy was related to consistent use of frequency and percentage risk rating scales (Schapira, Davids, McAuliffe, & Nattinger, 2004). After controlling for age, health literacy, race, and income, they found that higher numeracy was shown to be predictive of using the percentage and frequency scales in a consistent manner (i.e., giving the same responses on both frequency and percentage scales for the 5-year and lifetime breast cancer risk estimates, respectively). There have also been a few studies that have not found a relationship between numeracy and breast cancer risk estimates. Dillard, McCaul, Kelso, and Klein (2006) investigated whether poor numeracy skills could account for the finding that women consistently overestimate their risk of breast cancer even after receiving epidemiological information about the risk. In this study (N = 62), numeracy as measured by the L. M. Schwartz et al. (1997) scale was not related to persistent overestimation of breast cancer risk. Another group of researchers asked a sample of Black and White women (N = 207) to estimate their 5-year survival after a diagnosis of breast cancer, along with an estimate of the relative risk reduction due to screening mammography (Haggstrom & Schapira, 2006). Also using the Schwartz et al. measure, they found no effect of numeracy after controlling for other demographic variables (e.g., race, family history of breast cancer, income, insurance type, and level of education). These null results amount to failures to detect relationships rather than evidence of their absence. There have also been studies of perceptions of risks and benefits outside the breast cancer domain. In a large survey of cancer patients (N = 328), Weinfurt et al. (2003) asked participants to estimate the chances that they would benefit from an experimental cancer treatment. Numeracy was measured with a single multiple-choice question about a treatment that controlled cancer in \"40% of cases like yours\" (the correct answer: the treatment will work for 40 out of 100 patients like me). Patients who did not answer the numeracy question correctly perceived greater benefit from experimental treatment. In another study, participants were presented with several hypothetical scenarios that described a physician's estimate of the risk that a patient had cancer (Gurmankin, Baron, & Armstrong, 2004b). The authors asked participants to imagine that they were the patient described and to rate their risk of cancer. Numeracy was measured with a scale adapted from Lipkus et al. (2001). They found that patients lower in numeracy were more likely to overestimate their risk of cancer. Similar results have been obtained in nonhealth domains. For example, Berger (2002) presented news stories describing an increase in burglaries, along with frequency information. Participants lower in numeracy were more apprehensive about the increase in burglaries. In addition, Dieckmann, Slovic, and Peters (2009) presented a narrative summary, along with numerical probability assessments, of a potential terrorist attack. Participants lower in numeracy reported higher perceptions of risk and recommended higher security staffing. Consonant with studies reviewed earlier, those lower in numeracy were less sensitive to numerical differences in probability and focused more on narrative evidence. In conclusion, several studies have found that numeracy is related to perceptions of healthrelated risks and benefits. Participants lower in numeracy tend to overestimate the risk of cancer and other risks, are less able to use risk reduction information (e.g., about screening) to adjust their risk estimates, and may overestimate benefits of uncertain treatments. Note that low numeracy does not lead to randomly wrong perceptions of risks and benefits, as hypotheses about lack of skills or about imprecision might expect, but rather to systematic overestimation. Woloshin et al. (1999), among others, suggested that the form of the risk question may be part of the problem. In other words, participants low in numerical ability might have trouble expressing their risk estimates on the scales generally used in this domain (but see Reyna & Brainerd, 2008). As discussed, the low numerate seem to have difficulty using frequency and percentage risk scales consistently. However, difficulty using risk scales does not in itself predict overestimation. Instead, uncertainty about the meaning of numerical information, resulting from lower numeracy, may promote affective interpretations of information about risks (i.e., fearful interpretations) and about benefits (i.e., hopeful interpretations). Alternatively, overestimation may reflect the domains studied; cardiovascular risk, for example, might be underestimated for women because it is perceived to be a disease of men. Future research should focus on disentangling response scale effects from affective, motivational, and conceptual factors that may influence how those low in numeracy interpret risk and benefit information.",
          "Much research has focused on measuring the values, or utilities, that patients place on different health states and health outcomes. Obtaining reliable and valid assessments of how patients value different health states is important for modeling their decision making as well as improving health care delivery. The two primary methods for eliciting these utilities are the standard gamble and time-trade-off methods (Lenert, Sherbourne, & Reyna, 2001;Woloshin et al., 2001). Both methods require a participant to make repeated choices between different hypothetical health states until they are indifferent between options-namely, a choice in which they do not favor one health state over the other. For example, imagine that a patient with health problems has a life expectancy of 20 years. The patient is faced with a series of choices between living in his or her current state of health for the remainder of life or living with perfect health for some shorter amount of time. A utility for the patient's current health state can be calculated based on the point at which the patient finds both choices equally attractive (e.g., 10 years of perfect health vs. remainder of life with current health = 10/20; thus, utility = .5). The methods used to elicit utilities from patients often require them to deal with probabilities and/or make trade-offs between states. Because of the quantitative nature of the task, some researchers have questioned the validity of the standard approach to eliciting utilities and their dependence on the numerical abilities of patients. Woloshin et al. (2001) used three methods for eliciting utilities about the current health of a sample of women (N = 96). The participants completed a standard gamble and a time-tradeoff task and rated their current health on an analog scale, as well as completed the L. M. Schwartz et al. (1997) numeracy scale. Low-numerate participants showed a negative correlation between a question about current health and utilities generated from the standard gamble and time-trade-off tasks (i.e., valuing worse health higher than better health), indicating that these participants had difficulty with these quantitative utility elicitation tasks. The highnumerate participants showed the expected positive correlation (between self-reported health and utility for current health) for the same two tasks. It is interesting to note that all participants showed a positive correlation with the analog rating scale, which demanded less quantitative precision. Similar studies have been conducted with head and neck cancer patients. Utility scores were more consistent for the numerate patients, and their scores were more strongly correlated with observed functioning (S. R. Schwartz et al., 2004). These findings also suggest that the standard methods for assessing utility may be untrustworthy in patients with limited numerical ability. Similar conclusions have been reached when using a subjective numeracy measure (Zikmund-Fisher et al., 2007). These studies indicate that patients lower in numeracy have difficulty with the standard procedures for assessing utilities, especially those that are more quantitatively demanding. Low-numerate patients have difficulties dealing with probabilities, but they also appear to have trouble making trade-offs. Making trade-offs between hypothetical states involves additional reasoning skills that do not seem to be necessary when simply comparing probabilities. Future work should investigate the interplay between the different skills that are needed to complete these tasks, which could lead to new methods of eliciting utilities that are appropriate for patients at all levels of numerical ability.",
          "Given the gap between the intended meaning of health information and what people construe that information to mean, researchers have tried to identify optimal methods of presenting numerical information to improve understanding (e.g., Fagerlin, Ubel, Smith, & Zikmund-Fisher, 2007;Lipkus & Hollands, 1999;Maibach, 1999;Peters, Hibbard, Slovic, & Dieckmann, 2007;Reyna, 2008;Reyna & Brainerd, 2008). Several experiments have examined how individuals varying in numerical ability are affected by information framing and by presenting probabilities in different formats. Framing effects have proven to be a relatively robust phenomenon in psychological research (e.g., Kuhberger, 1998). For example, presenting the risk of a surgical operation as an 80% chance of survival versus a 20% chance of death (i.e., gain vs. loss framing) has been shown to change perceptions of surgery (McNeil, Pauker, Sox, & Tversky, 1982). In one study conducted by Stanovich and West (1998), participants with lower total Scholastic Aptitude Test (SAT) scores were found to be more likely to show a framing effect for risky choices about alternative health programs. (Because gain and loss versions of framing problems are mathematically equivalent, ideally, choices should be the same; framing effects occur when choices differ across frames.) In a more recent study, Stanovich and West (2008) found that the effect of frame type failed to interact with cognitive ability. If anything, inspection of the means revealed a trend in the wrong direction (the high-SAT group displayed a slightly larger framing effect); according to standard theories, higher ability participants should treat equivalent problems similarly and not show framing effects. Regarding these conflicting findings, Stanovich and West (2008) pointed out that within-subjects framing effects seem to be associated with cognitive ability but between-subjects effects are not. However, in these studies, effects were probably due to general intelligence rather than to numeracy because results for verbal and quantitative measures of cognitive ability (i.e., verbal and quantitative SAT scores) did not differ. Controlling for general intelligence (using self-reported total SAT scores), Peters et al. (2006) compared framing effects for low-and high-numerate subjects (N = 100). These groups were defined based on a median split using their scores on the Lipkus et al. (2001) numeracy scale (low numeracy was defined as two to eight items correct on the 11-item scale). Naturally, college-student participants who were relatively less numerate were not necessarily \"low\" in numeracy in an absolute sense. Nevertheless, less and more numerate participants rated the quality of hypothetical students' work differently when exam scores were framed negatively (e.g., 20% incorrect) versus positively (e.g., 80% correct). That is, less numerate participants showed larger framing differences. Peters and Levin (2008) showed in a later study that the choices of the more numerate were accounted for by their ratings of the attractiveness of each option in a framing problem (i.e., the sure thing and the risky option). The choices of the less numerate showed an effect of frame beyond the rated attractiveness of the options, demonstrating effects for both single-attribute framing (e.g., exam scores) and risky-choice framing. Peters et al. (2006) also examined whether numerical ability affected the perception of probability information. Participants (N = 46) rated the risk associated with releasing a hypothetical psychiatric patient. Half read the scenario in a frequency format (\"Of every 100 patients similar to Mr. Jones, 10 are estimated to commit an act of violence to others during the first several months after discharge\"), and the other half received the same information in a percentage format (\"Of every 100 patients similar to Mr. Jones, 10% are estimated to commit an act of violence to others during the first several months after discharge\"). More numerate participants did not differ in their risk ratings between the two formats. Less numerate participants, however, rated Mr. Jones as being less of a risk when they were presented with the percentage format. Peters, Dieckmann, et al. (2007) also explored the relationship between numeracy and the format of numerical information. In each experiment, participants (N = 303) were presented with measures of hospital quality and asked to make an informed hospital choice. Numeracy was measured with the Peters, Dieckmann et al. expanded numeracy scale. In the first study, participants saw a number of hospital-quality indicators (e.g., percentage of time that guidelines for heart attack care were followed) as well as nonquality information (e.g., number of visiting hours per day) for three hospitals. Information about cost was also provided. Participants were asked to choose a hospital that they would like to go to if they needed care, and they also responded to a series of questions about the information presented (e.g., which hospital is most expensive?). The information was displayed in one of three ways: in an unordered fashion, with the most relevant information (cost and quality) presented first, or with only the cost and quality information displayed (other nonquality information was deleted). Those lower in numeracy showed better comprehension when information was ordered and when information was deleted, compared with the unordered condition. (Those higher in numeracy also benefited from deleting information.) In a second study, participants were told to imagine that they needed treatment for heart failure and were asked to choose among 15 hospitals based on three pieces of information: cost, patient satisfaction, and death rate for heart failure patients. The formatting of information was varied by including black and white symbols or colored traffic light symbols to help participants evaluate the goodness or badness of each piece of information. The traffic light symbols were thought to be easier to understand. However, the low-numerate participants made better choices (i.e., used cost and quality indicators) with the \"harder\" black-and-white symbols compared with the \"easier\" traffic light symbols, whereas the reverse was true for the high-numerate participants-a result that is difficult to interpret. In a third hospital choice study, the authors found that low-numerate participants were particularly sensitive to the verbal framing of the information. Low-numerate participants showed greater comprehension when information was presented such that a higher number means better (the number of registered nurses per patient) compared with when a lower number means better (the number of patients per registered nurse). Numeracy has also been related to reading graphs. Zikmund-Fisher et al. (2007) presented participants (N = 155) with a survival graph that depicted the number of people given two drugs who would be alive over a 50-year period. They then asked four questions about information displayed in the graph (e.g., regarding what year the difference in total survival between Pill A and Pill B was largest). They measured numeracy with a subjective numeracy measure. The ability to correctly interpret the survival graphs was strongly related to numeracy, with those higher in numeracy better able to interpret the graphs. In another study, effects of format on trust and confidence in numerical information were examined. Gurmankin, Baron, and Armstrong (2004a) conducted a web-based survey in which they presented subjects (N = 115) with several hypothetical risk scenarios. The scenarios depicted a physician presenting an estimate of the risk that a patient had cancer in three formats (verbal, numerical probability as a percentage, or numerical probability as a fraction). Participants then rated their trust and comfort with the information, as well as whether they thought the physician distorted the level of risk. Numeracy was measured by adapting the Lipkus et al. (2001) scale. Even after adjusting for gender, age, and education, Gurmankin et al. found that those subjects with the lowest numeracy scores trusted the information in the verbal format more than the numerical, and those with the highest numeracy scores trusted the information in the numerical formats more than the verbal. Sheridan and colleagues (Sheridan & Pignone, 2002;Sheridan et al., 2003) conducted two studies in which they assessed the relationship between numeracy and ability to interpret risk reduction information in different formats. Participants in both studies were presented with baseline risk information about a disease and then given risk reduction information in one of four formats: relative risk reduction, absolute risk reduction, number needed to treat, or a combination of all methods. Number needed to treat is an estimate of the number of patients who must be treated in order to expect that one patient will avoid an adverse event or outcome over a period. Mathematically, it is the inverse of absolute risk reduction (the decrease in disease due to treatment) and was introduced because of difficulties in understanding other risk reduction formats. Numeracy was measured with items from the L. M. Schwartz et al. (1997) scale. In a sample of first-year medical students (N = 62) and a sample of patients from an internal medicine clinic (N = 357), participants lower in numeracy had more difficulty using the risk reduction information and, in particular, had trouble with the number-needed-to-treat format. Finally, controlling for gender and ethnicity, Parrott et al. (2005) presented statistical evidence concerning the relationship between a particular gene and levels of LDL cholesterol. The statistical information was presented in either a verbal form with percentage information or a visual form that showed a bar graph of the mortality rates. They were interested in whether perceptions of the evidence differed between the formats and whether numeracy was related to these perceptions (four numeracy items were adapted from L. M. Schwartz et al., 1997). They did not find any relationships between numeracy and comprehension, perceptions of the quality of the evidence, or perceptions of the persuasiveness of the evidence. The authors noted, however, that the restricted range of numerical abilities may have contributed to the null effects. In sum, low-numerate participants tend to be worse at reading survival graphs, more susceptible to framing effects in some experiments, more sensitive to the formatting of probability and risk reduction information, and more trusting of verbal than numerical information. Many of these studies do not control for general intelligence, although some do and still obtain effects of numeracy (e.g., Peters et al., 2006). Regardless of whether numeracy per se is the problem, those who score low on these assessments can be helped by presenting information in a logically ordered format and displaying only the important information, presumably decreasing cognitive burden. Additional research is needed to further elucidate the presentation formats that are most beneficial for individuals at different levels of numerical ability (but for initial hypotheses based on research, see Fagerlin, Ubel, et al., 2007;Nelson et al., 2008;Reyna & Brainerd, 2008). Variations in formatting have generally not been theoretically motivated; variations in numerical ability further complicate theoretical predictions about formatting. Effective prescriptions for formatting, therefore, await deeper understanding of the locus of effects of presentation format.",
          "Given the research that we have reviewed summarizing the deleterious effects of low numeracy on perceptions of crucial health information and the vulnerability of low-numerate individuals to poor formatting of that information, it would not be surprising if numeracy were related to health behaviors and medical outcomes (see Figure 1). The limited data that are available support such a conclusion. As we have discussed in some detail, several studies have demonstrated a relationship between numeracy and disease risk perceptions, which are themselves known to be critical determinants of health behaviors (e.g., for reviews, see Brewer, Weinstein, Cuite, & Herrington, 2004;Klein & Stefanek, 2007;Mills, Reyna, & Estrada, 2008). Therefore, numeracy, through its effect on perceptions of risks and benefits, would be expected to change health behaviors and outcomes. Consistent with this suggestion, studies that found low numeracy to be associated with a tendency to overestimate one's cancer risk also showed that such overestimation affected the perception of the benefits of cancer screening as well as screening behaviors, generally encouraging screening but, in the extreme, perhaps leading to fatalistic avoidance of screening (e.g., Black et al., 1995;Davids et al., 2004;Gurmankin et al., 2004b;L. M. Schwartz et al., 1997;Woloshin et al., 1999). These data support the conclusion that numeracy may influence distal health outcomes through effects on risk perceptions and other mediating processes, as shown in the model depicted in Figure 1. Notably, however, the data on the relationship between risk perceptions and outcomes have not always been consistent (see Mills et al., 2008;Reyna & Farley, 2006). For example, numeracy was found to be unrelated to estimates of breast cancer survival and survival benefit from screening mammography in a study that controlled for the effect of sociodemographic variables (Haggstrom & Schapira, 2006). As discussed above, studies have also explored the relationship between numeracy and people's ability to provide utility estimates of health states, another intermediate factor of importance in health-related decisions (see Figure 1). An important question is whether health utilities can serve as proxy measures for objective medical outcomes. Indeed, some who emphasize quality of life suggest that perceived utilities are superior to objective health states as ultimate measures of outcomes (a suggestion that we would not endorse). In any case, this conclusion does not follow if health utilities are inaccurate, as they are for those low in numeracy (e.g., S. R. Schwartz et al., 2004;Woloshin et al., 2001;Zikmund-Fisher et al., 2007). It is not known whether numeracy influences understanding of the information presented with these techniques, performance of the trade-off tasks themselves, or people's ability to communicate their preferences. However, the overall implication is that limited numeracy may interfere with patients' ability to express their preferences and clinicians' ability to elicit them. These factors also represent important potential mediators of the effects of low numeracy on intermediate health outcomes such as patient-centered communication and informed decision making. Most of what we know and suspect about the health outcomes of numeracy is inferred from descriptive studies in the related and better developed domain of health literacy. A number of studies using the TOHFLA have linked health literacy to several important outcomes and provide indirect evidence of the effects of numeracy, as the TOHFLA measures both health literacy and functional quantitative skills. Health literacy as measured by the TOHFLA has been associated with poor knowledge and understanding of various chronic diseasesincluding hypertension, diabetes mellitus, congestive heart failure, and asthma-among patients with these conditions (Gazmararian et al., 2003;Williams, Baker, Honig, Lee, & Nowlan, 1998;Williams, Baker, Parker, & Nurss, 1998). Further down the potential causal path from patient understanding to health behaviors and outcomes (as depicted in Figure 1), health literacy as measured by the TOHFLA has also been associated with lower utilization of preventive medical services, such as routine immunizations, Pap smears, and mammograms (Scott et al., 2002), and lower adherence to highly active antiretroviral therapy in HIV patients (Kalichman et al., 1999). Low numeracy per se has also been found to be associated with poor patient self-management of chronic disease. In a prospective study, Estrada et al. (2004) examined numeracy with respect to anticoagulation control among patients taking warfarin. This labor-intensive therapy requires patients to monitor quantitative laboratory test results and respond to these results by calculating and making adjustments in medication doses. As expected, low numeracy was found to be associated with poor anticoagulation control (ascertained in terms of the extent to which patients' laboratory test results were within the therapeutic target range). In a crosssectional study, Cavanaugh et al. (2008) examined the association between numeracy and diabetes self-management skills using the Diabetes Numeracy Test (Cavanaugh et al., 2008). Higher diabetes-related numeracy was associated with higher perceived self-efficacy for managing diabetes. However, higher numeracy was only weakly associated with a key outcome, hemoglobin A 1c , a measure of glycemic control in diabetic patients. If low numeracy leads to poor understanding of health information, which in turn leads to lower utilization of health services and poor treatment adherence and disease self-management, then the next expected outcome in the causal chain would be greater morbidity (see Figure 1). Controlling for covariates, descriptive studies using the TOHFLA add further indirect evidence of a numeracy effect. For example, Baker and colleagues (Baker, Gazmararian, Williams, et al., 2002;Baker et al., 2004) found an association between low health literacy and greater utilization of emergency department services and risk of future hospital admission among urban patient populations. Health literacy has also been shown to be associated with lower self-rated physical and mental health functioning as measured by the Medical Outcomes Study 36-Item Short-Form Health Survey, or SF-36, which is predictive of both morbidity and mortality (Wolf et al., 2005). In one study of patients prescribed inhaled steroids for treatment of asthma, low numeracy was found to be correlated with a history of hospitalizations and emergency room visits for asthma (Apter et al., 2006). Thus, in combining results for disease management with direct and indirect evidence for disease outcomes (e.g., hospitalizations), there is limited evidence that low numeracy affects morbidity and mortality. Unfortunately, studies have not examined outcomes such as long-term morbidity and mortality, clear needs for future research. Our review of assessments indicates that measures of literacy and numeracy tend to be correlated and that, if anything, numeracy is lower than literacy; however, as we have noted, many studies do not distinguish these abilities. Research has also not examined the full range of health-related outcomes potentially associated with numeracy. A conspicuous gap in this respect pertains to the relationship between numeracy and patient experiences with care, an outcome domain that is receiving greater attention with the growing emphasis on patient-centered care and communication (R. M. Epstein & Street, 2007). Numeracy might be expected to influence several outcomes in this domain, including patient satisfaction with care, the nature and quality of the patient-physician relationship, and the extent of shared and informed decision making. Numeracy may have the strongest and most direct connection to the latter outcome of informed decision making, to the extent that this outcome is based on the underlying concept of substantial understanding or gist, for which numeracy represents a critical prerequisite (Reyna & Hamilton, 2001; see Theories of Mathematical Cognition: Psychological Mechanisms of Numeracy, below). For preference-sensitive decisions, that is, decisions involving uncertainty about the net benefits and harms of a medical intervention, some degree of numeracy is necessary for patients to appropriately understand and weigh these benefits and harms (O'Connor, et al., 2007). However, presently there is no empirical evidence demonstrating how numeracy relates to informed decision making or other outcomes in the domain of patient experiences with care. Although the studies we have reviewed suggest ways in which numeracy may moderate the effects of other psychological factors on different health outcomes, less is known about what factors might moderate the effects of numeracy. The influence of numeracy on health outcomes is likely to be highly context dependent (Montori & Rothman, 2005), varying substantially according to numerous factors that determine the extent and types of numerical reasoning that are required of patients. For example, problems such as patient self-management of anticoagulation therapy clearly require basic arithmetical skills, whereas other problems, such as the interpretation of individualized cancer risk information, involve higher order probabilistic reasoning. However, it is not known how much other routine clinical problems require such skills; in some contexts, numeracy may have no demonstrable effect on health outcomes. Even when numeracy effects exist, they may be difficult to demonstrate in health outcomes because of the influence of other confounding factors. For example, as noted previously, numeracy was found to have only a modest effect on glycemic control in diabetic patients (Cavanaugh et al., 2008). Although this seems counterintuitive given that self-management of diabetes involves various tasks that are clearly computational in nature (e.g., measuring blood sugar and adjusting insulin and oral medication doses), glycemic control also depends on numerous noncomputational factors, such as diet, weight control, genetic factors, and access to quality health care. In the context of these other factors, the influence of numeracy may be necessary but not sufficient. In other words, good medical outcomes depend on the occurrence of a series of linked events; any break in the causal chain prior to reaching the outcome can mask essential positive effects at earlier stages. Potential moderators of numeracy effects include characteristics of the individual and of the health care environment (see Figure 1). Individual differences in personality variables, such as need for cognition, might conceivably moderate numeracy effects by determining the extent to which patients rely on numerical reasoning in the first place, regardless of the extent to which clinical circumstances demand such reasoning. Individuals are known to differ in their preferences for information and participation in health decisions, factors that may further influence the extent to which patients engage in computational tasks (e.g., Reyna & Brainerd, 2008). This engagement may be influenced even more by differences in clinicians' communication and decision-making practices in their encounters with patients. Research has only begun to identify the important moderators and mediators of the effects of numeracy on health outcomes, and coherent theories that account for the causal mechanisms linking these factors have not been implemented. The critical challenge for future research is not only to identify the unique associations between numeracy-as opposed to literacy-and various proximal and distal health outcomes, but to develop a solid theoretical grounding for this inquiry. A more explicit application of theories of health behavior and decision making to research on numeracy would facilitate identification of a broader range of potentially important moderators and mediators of numeracy effects and the generation of empirically testable hypotheses about the causal mechanisms underlying these effects. We now turn to a discussion of theories that can provide such guidance for future research.",
          "Existing theoretical frameworks make predictions about numeracy, and recent research has begun to exploit these frameworks. There are four major theoretical approaches that are relevant to numeracy: (a) psychophysical approaches in which subjective magnitude is represented internally as a nonlinear function of objective magnitude, (b) computational approaches that stress reducing cognitive load and that emphasize \"natural\" quantitative processing, (c) dual-process approaches that contrast intuitive (or affective) and analytical processing in which errors are due mainly to imprecision and faulty intuitions, and (d) fuzzy trace theory, a dual-process theory that stresses gist-based intuition as an advanced mode of processing and contrasts it with verbatim-based analytical processing. We cannot review these theories in exhaustive detail, given the scope of the current article, but we outline the approaches, review their implications for numeracy, and point to avenues for future research.",
          "Decades of research have shown that humans and animals represent number in terms of language-independent mental magnitudes (Brannon, 2006). The internal representation of number obeys Weber's law, in which the discriminability of two magnitudes is a function of their ratio rather than the difference between them (Gallistel & Gelman, 2005). Thus, the psychological difference between $40 and $20 (a ratio of 2.00) is larger than that between $140 and $120 (a ratio of 1.16), although the absolute difference is identical ($20). If the internal representation were linear, a difference of $20 would always feel like the same amount. The deviation from linearity, in which the same objective difference is not perceived to be identical, is referred to as a subjective distortion (or an approximate representation) of objective magnitude. Human infants represent number in this way, showing the same property of ratio-dependent discrimination shared by adult humans and animals (Brannon, 2006). Young children's internal representation of number may be more distorted (or nonlinear) than that of educated adults (Siegler & Opfer, 2003). In addition, Dehaene, Izard, Spelke, and Pica (2008) found that numerical judgments by both children and adults who were members of an indigenous Amazonian group (with a reduced numerical lexicon and little or no formal education) were best fitted with a logarithmic curve, similar to those observed for young children in Western cultures (Siegler & Booth, 2004). In contrast, the responses of adults who had been through a longer educational period were best fitted with a linear function. The evolutionarily ancient nonverbal numerical estimation system exists alongside a learned verbal system of counting and computational rules. (Number words, such as two, can access the nonverbal estimation system, but words are not required in order to process number.) The intraparietal sulcus has been implicated as a substrate for the nonverbal system that represents the meaning of number (Brannon, 2006). The neural correlates of these nonverbal numerical abilities are distinct from those of language-dependent mathematical thinking (Dehaene, Piazza, Pinel, & Cohen, 2003). For example, when adults solve precise, symbolic mathematical problems, their performance is encoded linguistically and engages left inferior frontal regions that are active during verbal tasks. These language areas are not active in neuroimaging studies of nonverbal number processing. Research in this psychophysical tradition is relevant to explaining causal mechanisms underlying numeracy (e.g., Cantlon & Brannon, 2007;Dehaene, 2007;Furlong & Opfer, 2009;Gallistel & Gelman, 2005;Reyna & Brainerd, 1993, 1994;Shanteau & Troutman, 1992;Siegler & Opfer, 2003). A straightforward implication is that people without brain injury who are low in numeracy retain a nonverbal estimation system for representing number, regardless of their level of formal mathematical knowledge. Moreover, distortions in the perception of numbers should influence judgment and decision making involving numbers (Chen, Lakshminaryanan, & Santos, 2006;Furlong & Opfer, 2009). Indeed, effects reviewed earlier, such as framing, were originally predicted by assuming that the perception of magnitudes (e.g., of money) was distorted in accordance with the psychophysical functions of \"prospect theory\" (Kahneman & Tversky, 1979). Building on research on the psychophysics of magnitude, Peters, Slovic, Västfjäll, and Mertz (2008) showed that greater distortions in number perception (i.e., greater deviations from linearity) were associated with lower numeracy, consistent with results for children and less educated adults. They also showed that distortions in number perception predicted greater temporal discounting (choosing a smaller immediate reward over a later larger one) and choosing a smaller proportional (but larger absolute) difference between outcomes (see also Benjamin, Brown, & Shapiro, 2006;Stanovich, 1999). These authors argued that imprecise representations of number magnitudes may be responsible for difficulties in health decision making observed among those low in numeracy. The Peters et al. (2008) study represents an important first step in linking psychophysical measures of how individuals perceive numbers to measures of health numeracy, and the authors showed that perceptions of number are in turn related to decisions. However, closer inspection of the direction of differences raises questions about what these results signify. All participants tended to choose $100 now rather than wait for $110 in a month, regardless of the precision of their number perception. However, those with more precise representations (i.e., those with more linear representations and thus larger perceived differences between numbers) were more likely to wait to receive $15 in a week rather than accept $10 now. Ignoring the difference between 4 weeks (a month) and 1 week, those with a more precise representation treated a difference of $5 as though it were larger than a difference of $10. This pattern of preferences is consistent with a ratio-dependent discrimination of number because the ratio of $15 to $10 is larger than the ratio of $110 to $100. Moreover, choosing $15 over $10 but then choosing $100 over $110 could be justified by the different time delays: The $110 is delayed a month, making it less desirable. The preferences in Peters et al.'s (2008) second experiment are not so easily justified, however. When asked to choose which charitable foundation should receive funding, participants with more precise representations of number were more likely to choose a foundation that reduced deaths from disease from 15,000 a year to 5,000 a year, compared with a foundation that reduced deaths from 160,000 a year to 145,000 a year or even one that reduced deaths from 290,000 a year to 270,000 a year. As in the first experiment, preferences were consistent with a ratio-dependent discrimination of number; a greater proportion of lives saved (67% over 6.9%) was preferred. However, participants with more precise representations of number were more likely to choose the numerically inferior option, to save 5,000 lives rather than save 20,000 (or 15,000) lives. As Peters et al. acknowledge, this choice is not the normatively correct one. Surprisingly, numeracy was not significantly related to preferences in this task for younger adults (there was a marginal interaction among age, numeracy, and precision of representation, suggesting that higher numeracy and higher precision together increased preference of the inferior option for older adults). If we consider the implications for medical decision making, these results are troubling. People with superior number discrimination would be more likely to choose the worst option in terms of number of lives saved. Unfortunately, this is not an isolated result. As we discuss below, more numerate individuals (who tend to have more precise representations of number) sometimes choose the numerically inferior option in other tasks, too.",
          "Computational approaches emphasize reducing cognitive burdens associated with information processing. As an example, working memory limitations are assumed to interfere with cognitive processing, including processing of numerical information. Therefore, reducing memory load (i.e., reducing demands on working memory) is predicted to improve performance (as long as sufficient information is processed for accurate performance; for reviews, see Reyna, 1992Reyna, , 2005)). In this view, poor decision making is the result of information overload and the failure to sufficiently process information, as many have assumed in research on formatting effects reviewed earlier. Improving decision making, then, requires reducing the amount of information to be processed, especially irrelevant information, which drains working memory capacity, while thoroughly processing relevant information (Peters, Dieckmann, et al., 2007). Consistent with this approach, strategies aimed at making numerical information more organized and accessible have been shown to improve decision making. For example, asking people to actively process information by enumerating reasons for their preferences or by indicating the exact size of a risk on a bar chart is predicted to enhance the use of numbers and reduce reliance on extraneous sources of information (Mazzocco, Peters, Bonini, Slovic, & Cherubini, 2008;Natter & Berry, 2005). In this connection, Mazzocco et al. (2008) found that asking decision makers to give reasons for choices encouraged greater weighting of numerical relative to nonnumerical information (e.g., emotion and anecdotes). Decision analysis and public health programs emphasize this kind of precise and elaborate processing of numerical information (e.g., Fischhoff, 2008). Dual-process theories, discussed below, have incorporated this computational approach into their assumptions about the analytical side of processing (e.g., S. Epstein, 1994;Peters et al., 2006;Reyna, 2004Reyna, , 2008;;see Nelson et al., 2008). The natural frequency hypothesis is another computational approach (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994;Hoffrage, Lindsey, Hertwig, & Gigerenzer, 2000). Predictions have not been made about numeracy as differences across individuals; rather, they concern which kinds of numerical displays are more \"transparent\" than others, given the way that most people process numbers (e.g., Brase, 2002;Cosmides & Tooby, 1996;Gigerenzer, 1994). As an example of how natural frequencies differ from probabilities, consider the following information: The probability of colorectal cancer is 0.3%. If a person has colorectal cancer, the probability that the Hemoccult test is positive is 50%. If a person does not have colorectal cancer, the probability that he still tests positive is 3%. The same information expressed in terms of natural frequencies would be as follows: Out of every 10,000 people, 30 have colorectal cancer. Of these, 15 will have a positive Hemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. Natural frequencies were thought to facilitate reasoning because they reduce the number of required computations. They are \"natural\" in the sense that they are assumed to correspond to the way in which humans have experienced statistical information over most of their evolutionary history (e.g., Gigerenzer & Hoffrage, 1995). The hypothesis that frequencies or counts are more natural and easier to process than percentages or decimals (e.g., probabilities) appeared unassailable in the 1990s (e.g., Gigerenzer, Todd, & the ABC Group, 1999). For example, problems framed using natural frequencies were said to elicit fewer biases and errors than problems using probabilities (e.g., Cosmides & Tooby, 1996). However, these predictions have been challenged by a growing body of evidence (for reviews, see Barbey & Sloman, 2007;Reyna & Brainerd, 2008). In particular, the hypothesis that frequencies are easier to understand than probabilities was not confirmed (e.g., Evans, Handley, Perham, Over, & Thompson, 2000;Koehler & Macchi, 2004;Sloman, Over, Slovak, & Stibel, 2003; see also Macchi & Mosconi, 1998). In studies of risk communication and medical decision making, frequency and probability versions of identical information have been compared, and results have also disconfirmed this frequency hypothesis. For example, Cuite, Weinstein, Emmons, and Colditz (2008) studied 16,133 people's performance on multiple computational tasks involving health risks and found that performance was very similar for frequency (55% accurate) and probability (57% accurate) versions of the same information (see also Dieckmann et al., 2009). Furthermore, biases and heuristics were not reduced by presenting information using frequencies, once confounding factors were eliminated (see Barbey & Sloman, 2007;Reyna & Brainerd, 2008;Reyna & Mills, 2007a). For example, Windschitl (2002) found biasing effects of a context question on subsequent target judgments of cancer risk, but the bias was not less severe when frequency rather than probability representations were used. Complex decisions were also not made easier with frequencies. Waters, Weinstein, Colditz, and Emmons (2006) compared frequencies with percentages to determine which might increase the accuracy of judgments about trade-offs for different cancer treatments. Among 2,601 respondents, those who received the percentages performed significantly better than those who received the identical information in the form of frequencies. Thus, there is little evidence to support the idea that frequencies per se (when not confounded with other factors) are more natural or easier to comprehend than percentages or other \"normalized\" formats. However, it should be noted that the claim that all frequencies facilitate judgment should be distinguished from the natural frequencies hypothesis as characterized, for example, by Hoffrage, Gigerenzer, Krauss, and Martignon (2002). First, natural frequencies only refer to situations in which two variables are involved-they are nonnormalized joint frequencies (e.g., as in the example above of colorectal cancer and Hemoccult test results). Moreover, proponents argue that natural, but not relative, frequencies facilitate judgment. These proposals have much in common with the nested-sets or class-inclusion hypothesis, which holds that overlapping or nested relations create confusion (e.g., Reyna, 1991;Reyna & Mills, 2007a). The natural frequencies format clarifies relations among classes, but frequencies per se appear to be neither a necessary nor a sufficient means of disentangling classes (e.g., Brainerd & Reyna, 1990;Wolfe & Reyna, 2009). The frequency hypothesis of Cosmides, Gigerenzer, and colleagues (e.g., Cosmides & Tooby, 1996;Gigerenzer, 1994) should be distinguished from the frequency effect studied by Slovic and colleagues (discussed below; e.g., Slovic, Finucane, Peters, & MacGregor, 2004).",
          "Other theories take a dual-process approach to explain numerical processing (see Gigerenzer & Regier, 1996, for arguments against standard dual-process approaches). Extrapolating from psychodynamic dualism, S. Epstein and colleagues (e.g., see S. Epstein, 1994) have developed a series of measures of analytical or rational thinking versus intuitive or \"experiential\" thinking. This distinction between analytical and intuitive thought resembles other dual-process distinctions (e.g., Sloman, 1996;Stanovich, 1999) and has been applied by Kahneman (2003), Slovic et al. (2004), and others (e.g., Peters et al., 2006) to account for heuristics and biases. That is, heuristics and biases, which typically violate rules of probability theory or other quantitative rules, are ascribed to a more primitive intuitive way of thinking (System 1) that can sometimes be overridden or censored by advanced analytical thought (System 2). Some versions of dual-process theory are vulnerable to the criticism that they are, at best, a post hoc typology that does not lend itself to novel prediction, the true hallmark of a scientific theory. However, S. Epstein, Pacini, Denes-Raj, and Heier's (1996) dual-process theory is not post hoc because a valid and reliable instrument has been fashioned to characterize analytical versus intuitive thinking, which can then be used to predict heuristics and biases. As we discuss, however, although the instrument is a satisfactory measure from an empirical standpoint, its predicted relations to heuristics and biases are not obtained consistently. That is, although reliable individual differences in thinking style are detected when using the instrument, the Rational-Experiential Inventory (REI), these differences do not map onto judgments and decision making in ways that this dual-process theory predicts (S. Epstein et al., 1996). Specifically, the original version of the REI (S. Epstein et al., 1996) consisted of two scales, Need for Cognition and Faith in Intuition, which correspond to analytical and intuitive thinking styles, respectively (drawing on Cacioppo & Petty's, 1982, Need for Cognition scale). The original REI has been improved by adding items, and the reliability of the experiential scale has been increased (Pacini & Epstein, 1999a, 1999b). The new REI retains the good psychometric properties of the old measure, such as producing two orthogonal factors in factor analyses and exhibiting convergent and divergent validity with respect to other aspects of behavior and personality. Thus, the new REI seems to measure what the theory indicates that it ought to measure. The basic assumption of this dual-process approach as applied to numeracy (e.g., Peters et al., 2006) is that intuitive thinking is the source of biases and errors in numerical (and other) processing. Analytical thinking, in contrast, is the source of accurate and objective numerical processing. Although intuition is not assumed to lead invariably to biases, a key rationale for standard dual-process theory is that systematic biases are caused by intuitive thinking. A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget andInhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl. Bowl A contains nine colored and 91 white jelly beans, and Bowl B contains one colored and nine white jelly beans. Consequently, the rational or analytically superior choice is Bowl B: The chance of picking a colored jelly bean is objectively greater if you pick from Bowl B (10% chance of winning) than if you pick from Bowl A (9% chance of winning). The intuitive choice or ratio bias effect, however, is to pick Bowl A because it contains more winning jelly beans than Bowl B (i.e., nine vs. one). The theory predicts that when individual differences favor rational or analytical thought (measured by the Need for Cognition scale), people ought to pick Bowl B. However, those lower in rationality and/or higher in intuition (measured by the Faith in Intuition scale) should exhibit the ratio bias effect by picking Bowl A. Unfortunately, several critical tests of this prediction (including those using the improved REI measure) conducted by the theorists themselves yielded weak and inconsistent results (e.g., Pacini & Epstein, 1999a, 1999b; see also Alonso & Fernández-Berrocal, 2003;Reyna & Brainerd, 2008). Another example of heuristic processing that has been examined by using this dual-process theory is framing (e.g., Porcelli & Delgado, 2009). Predictions for framing effects are the same as those for the ratio bias, namely, that people high in analytical thinking but low in intuition should be less susceptible to framing effects than those low in analytical thinking and high in intuition. Framing effects occur when decision makers treat quantitatively equivalent options differently, such as rating a person as more intelligent when told that the person received a test score of 80% correct, as opposed to receiving a score of 20% incorrect. Shiloh, Salton, and Sharabi (2002) presented framing problems to college students and analyzed the data using three factors as independent variables: high or low analytical, high or low intuitive, and positive or negative frame. The results showed that participants fitting nonpredicted combinations of thinking styles-high analytical, high intuitive thinking and low analytical, low intuitive thinking-were the only ones to exhibit framing effects. The findings were interpreted as supporting \"the individual-differences perspective on heuristic processing, and as a validation of main assumptions\" of dual-process theory (Shiloh et al.,p. 415). Again, however, core predictions of dual-process theory were not confirmed: Neither low reliance on analysis nor high reliance on intuition was associated in any consistent fashion with framing effects, contrary to the theory. Despite these null or inconsistent effects, the dual-process theory's predictions regarding numeracy are straightforward and have met with greater success. According to Peters et al. (2006), for example, those higher in numeracy should approach numerical problems more analytically, whereas those lower in numeracy would be subject to intuitive biases, such as ratio bias and framing effects. In two of four experiments, they found the predicted pattern: Those higher in numeracy were less likely to exhibit ratio bias in one experiment and framing effects in the other experiment. According to Peters et al., the superior results of the more numerate are due to the greater clarity and precision of their perceptions of numbers (see also Peters & Levin, 2008). For instance, high-numerate participants were assumed to select Bowl B because they perceived the objective probabilities more clearly than low-numerate participants. The results of a third experiment were only partially supportive of dual-process predictions. Consistent with the theory, frequency and percentage formats did not differ for those higher in numeracy, but they differed for those lower in numeracy. In judging the probability of violence, highly numerate people judged 10 out of 100 patients committing a violent act as equivalent to 10% of patients committing a violent act. However, the low-numerate participants were predicted to rely on affect, considered part of intuition, as opposed to mathematics. According to the theory, relying on affect should lead to higher levels of risk being reported for the frequency (compared with the percentage) format because more vivid images of violent acts are generated (see Peters et al., 2006). Hence, those lower in numeracy should be more susceptible to the emotionally arousing frequency format, compared with those higher in numeracy, who rely on cold numbers. However, inconsistent with this theory, differences between low-and high-numerate participants were observed for the percentage format, not for the frequency format. Both groups seemed to have relied on similar processes in the emotionally arousing frequency condition (Slovic et al., 2004). In a final experiment, Peters et al. (2006) found that high-numerate participants were more prone than low-numerate participants to an intuitive bias in processing quantitative information. Different groups rated the attractiveness of playing a bet, either 7/36 chances to win $9 and 29/36 chances to win nothing (the no-loss bet) or 7/36 chances to win $9 and 29/36 chances to lose 5 cents (the loss bet). In an earlier study, Slovic et al. (2004) found that the noloss bet received an average rating of 9.4 on a 21-point desirability scale, but ratings jumped to 14.9 for the loss bet, which added the possible loss of 5 cents. Thus, the objectively worse bet was rated as more attractive. Peters et al. found that those higher in numeracy showed this effect; they gave higher ratings to the objectively worse bet. Those lower in numeracy rated them the same, a result consistent with the fact that the bets are objectively similar. Peters et al. (2006) acknowledged that rating the worse bet more highly is a less \"rational\" response. According to Peters et al., the highly numerate may sometimes make less rational responses than the less numerate \"precisely because they focus on the detail of numbers\" (p. 411). Nevertheless, dual-process theory predicts the opposite, that the highly numerate should show less bias (i.e., their judgments should better reflect objective quantities) than the less numerate. The same theoretical principles that explain the absence of ratio bias and framing effects for the highly numerate appear to be violated when the opposite result, greater bias, is found for the loss bet. Dual-process theory also predicts that mood will have biasing effects on those low in numeracy. The less numerate, who are less likely to attend to and understand numbers, should be more influenced by extraneous information, such as mood or affect. This effect was demonstrated in a study that examined how people made judgments about hospital quality (Peters et al., in press). Although most numerical quality indicators remained unused by all respondents, the highly numerate were more likely to use one of the indicators to rate the hospitals. As expected, compared with those of high-numerate patients, preferences expressed by low-numerate patients were less influenced by objective probabilities and more influenced by their mood. In sum, individual differences in dual processes do not consistently predict biases in processing numerical information in ratio bias and framing tasks. Differences in numeracy that are supposed to reflect such dual processes, however, are associated with ratio bias and framing effects as well as with effects of mood. Other effects of numeracy run counter to theoretical predictions: Those higher in numeracy rated a numerically worse bet as superior (those lower in numeracy did not), and numeracy did not produce expected differences in affective processing of numbers in a frequency format. Taken together, these theoretical tests suggest that the hypothesized differences in dual processes do not fully explain effects of numeracy. Future research should be aimed at delineating the specific processes that underlie biases and heuristics in people who differ in numeracy.",
          "Building on research in psycholinguistics, fuzzy trace theory distinguishes between verbatim and gist representations of information, extending this distinction beyond verbal information to numbers, pictures, graphs, events, and other forms of information (e.g., Reyna & Brainerd, 1992, 1995). Verbatim representations capture the literal facts or \"surface form\" of information, whereas gist representations capture its meaning or interpretation (based on a person's culture, education, and experience, among other factors known to affect meaning; e.g., Reyna, 2008;Reyna & Adam, 2003). Gist representations are also less precise than verbatim ones; they are the \"fuzzy traces\" in fuzzy trace theory. Verbatim and gist representations of information are encoded separately, and each forms the basis for different kinds of reasoning, one focused on memory for precise details (verbatimbased reasoning) and the other on understanding global meaning (gist-based reasoning). Thus, fuzzy trace theory is a dual-process theory but one in which gist-based intuition is an advanced mode of reasoning. Although standard dual-process theories have been criticized for lacking evidence for distinct processes (Keren & Schul, in press), there is extensive evidence for the independence of verbatim and gist processes, including findings from formal mathematical tests (e.g., Brainerd & Reyna, 2005;Reyna & Brainerd, 1995). Specifically, research has shown that people encode verbatim representations as well as multiple gist representations of the same information. When presented with various numbers or numerosities, people encode verbatim representations of numbers and gist representations that capture the order of magnitudes, whether they are increasing or decreasing (e.g., over time), which magnitudes seem large or small, among other qualitative (inexact) relations (Brainerd & Gordon, 1994;Reyna & Brainerd, 1991a, 1993a, 1994a, 1995;Reyna & Casillas, 2009; see also Gaissmaier & Schooler, 2008). For instance, given quantitative information that the numbers of deaths worldwide are 1.3 million deaths a year for lung cancer, 639,000 for colorectal cancer, and 519,000 for breast cancer, people encode such gists as \"lung cancer deaths are most,\" \"lung cancer deaths are more than breast cancer,\" and so on. People prefer to operate on the fuzziest or least precise representation that they can use to accomplish a task, such as making a judgment or decision (e.g., Reyna & Brainerd, 1991b, 1994, 1995;Reyna, Lloyd, & Brainerd, 2003). They begin with the simplest distinctions (e.g., categorical) and then move up to more precise representations (e.g., ordinal and interval) as the task demands. For example, fuzzy trace theory accounts for framing effects by assuming that people use the most basic gist for number, the categorical distinction between some quantity and none. Thus, a choice between saving 200 people for sure and a one-third probability of saving 600 people (and two-thirds probability of saving no one) is interpreted as saving some people for sure versus maybe saving some and maybe saving none. Because saving some people is better than saving none, the sure option is preferred. Analogous interpretations of the loss frame (as a choice between some people dying for sure vs. maybe some dying and maybe none dying) produces preferences for the gamble because none dying is better than some dying. More generally, framing effects occur because numbers are interpreted semantically in terms of vague relations, such as good versus bad, low versus high, some versus none, or more versus less (Mills et al., 2008;Reyna, 2008;Reyna et al., 2003). Often these gist interpretations reflect affect (see Brainerd, Stein, Silveira, Rohenkohl, & Reyna, 2008;Rivers, Reyna, & Mills, 2008). The specific explanation for framing effects described above has been confirmed by experiments (e.g., Kühberger & Tanner, 2009;Reyna & Brainerd, 1991b, 1995). Psychophysical accounts of framing are not sufficient to explain the results of these experiments. For example, framing effects persist even when some or all of the numbers in framing problems are deleted, and contrary to psychophysical predictions, framing effects are actually larger under these circumstances. Conversely, focusing attention on the numbers that are supposed to generate framing effects (in the psychophysical accounts) makes the effects smaller. As people gain experience making certain judgments or decisions, they tend to rely more on gist rather than verbatim representations, known as a fuzzy-processing preference (e.g., Nelson et al., 2008;Reyna & Ellis, 1994;Reyna & Lloyd, 2006). For example, framing effects have been predicted and found to increase from childhood to adulthood (Reyna, 1996;Reyna & Ellis, 1994); other heuristics and biases show a similar, counterintuitive trend (see Reyna & Farley, 2006, Table 3). In adulthood, experts have been found to base their decisions more on simple gist, compared with novices with less experience and knowledge (e.g., Reyna & Lloyd, 2006). Relying on gist may be especially beneficial for older people whose verbatim memories are less robust (Reyna & Mills, 2007b;Tanius, Wood, Hanoch, & Rice, 2009). Age differences in choice quality between younger and older adults are reduced when decisions are based on affect or bottom-line valence (Mikels et al., in press; but see Peters et al., 2008). These and other developmental trends suggest that more advanced numerical processing is not necessarily more precise or elaborate, as assumed in standard dual-process theories (e.g., Peters et al., 2006), but rather that it involves the extraction of bottom-line meanings or relations. Hence, knowing the best estimate of a lifetime risk of dying from breast cancer (such as that provided by online calculators) or knowing the exact probability of complications from surgery does not constitute informed consent or informed medical decision making, according to fuzzy trace theory (e.g., Reyna, 2008;Reyna & Hamilton, 2001). People can receive a precise estimate of risk tailored for them and yet not understand the essential gist of whether their risk is low or high, whether they should be relieved or alarmed. In fact, focusing on exact numbers has been found to exacerbate some biases. Removing numerical information so that participants must rely, instead, on memory for its gist improves performance (e.g., in class-inclusion problems; see Brainerd & Reyna, 1995;Reyna & Brainerd, 1995). The ratio bias effect is an example of a class-inclusion illusion; assumptions about retrieval and processing, as well as about representations, are required to explain this effect (Reyna, 1991;Reyna & Mills, 2007a;Wolfe & Reyna, 2009). Briefly, any ratio concept, including probability, is inherently confusing because the referents of classes overlap. Owing to this confusion, people focus on the target classes in numerators (e.g., the nine colored jelly beans in Bowl A and the one colored jelly bean in Bowl B) and neglect the classes in the denominator (e.g., the 100 total jelly beans in Bowl A and the 10 total jelly beans in Bowl B), producing the ratio bias effect (Reyna & Brainerd, 1994, Figure 11.3). Like the participants who favored saving proportionately more lives in Peters et al.'s (2008) experiment, people who favor Bowl A are making comparisons of relative magnitude, but they are the wrong comparisons (Reyna, 1991;Reyna & Brainerd, 2008). Experiments manipulating the salience of the wrong relative magnitude-a competing gist-confirm that this factor contributes to the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995). As fuzzy trace theory also predicts, manipulations that reduce confusion by discretely representing classes or drawing attention to denominators can reduce the ratio bias effect (e.g., Brainerd & Reyna, 1990, 1995;F. J. Lloyd & Reyna, 2001;Wolfe & Reyna, 2009). Because processing problems due to overlapping classes are not fundamental logical errors (i.e., participants understand the role of numerators and denominators in principle), class-inclusion errors persist even for advanced reasoners (see also Chapman & Liu, 2009). For example, physicians and high school students performed equally poorly on a base-rate neglect problem, which is another type of class-inclusion problem (Reyna, 2004;Reyna & Adam, 2003). They estimated the probability of disease for a patient with a positive test result, given the base rate of disease, and confused that positive predictive value with the test's sensitivity (probability of a positive test result for a patient with disease), as expected by fuzzy theory, because only their denominators differ (Reyna & Mills, 2007a). Therefore, icon arrays or other formats that disentangle classes (clarifying their relations), especially those that also make the relevant gist salient, reduce these biases. In sum, fuzzy trace theory distinguishes intuitive reasoning based on simple gist representations from detailed quantitative analysis using verbatim representations (a distinction supported by much independent evidence). The theory can account for heuristics and biases that have been the foci of earlier theories, such as framing and ratio bias effects. Although earlier approaches emphasized precision, accuracy, and analysis, fuzzy trace theory holds that more advanced reasoners (i.e., those who understand the meaning of numbers) should rely on the simple, qualitative gist of numbers whenever the task permits. Although research on numeracy is consistent with fuzzy trace theory, especially with its core assumption of fuzzy processing as a default mode of reasoning, alternative theoretical explanations for specific findings have rarely been compared (cf. Reyna & Brainerd, 2008). Current health numeracy measures seem to capture the ease or automaticity with which ratios are computed, an explanation that would account broadly for performance across problems for which the computed ratios were and were not appropriate. Tests of numeracy have been not yet been devised that capture individual differences in appropriate gist processing of numbers.",
          "We began this article with a description of the dilemma of low health numeracy. Despite the abundance of health information from commercial and noncommercial sources, including information about major new research discoveries that can be used to prevent and treat disease, most people cannot take advantage of this abundance. Few problems can be said to affect up to 93 million people, based on reliable assessments of nationally representative samples. Low numeracy is such a problem. The ideal of informed patient choice, in which patients share decision making with health care providers, is an elusive goal without the ability to understand numerical information about survival rates, risks of treatments, and conditional probabilities that govern such domains as genetic risk (e.g., the probability of disease given a genetic mutation). Those who are disadvantaged by poverty, lack of education, or linguistic barriers are also unlikely to have numerical skills that would empower them to access health care and to make informed decisions. Definitions of health numeracy-encompassing computational, analytical, and statistical skills, among other abilities-are impressively broad, and yet, on assessments of all varieties, people cannot accomplish much less ambitious tasks, such as judging whether a .001 risk of death is bigger or smaller than 1 in 100. Moreover, scores on numeracy assessments have been linked to key cognitions that predict morbidity and mortality, to health behaviors, and, in a few cases, to medical outcomes, the latter sometimes only indirectly through measures of literacy that include, and are correlated with, numeracy. Evidence of effects of numeracy exists along a causal chain from initial perceptions of risks and benefits to health-related judgments and decisions, which have been found to be biased and inaccurate for people with low numeracy. Low numeracy has been shown to impair understanding of risks and benefits of cancer screening, to reduce medication compliance in anticoagulation therapy, to limit access to preventive treatments for asthma, and to affect known predictors of death and disability, such as patients' self-rated functional status. However, there are many gaps and shortcomings in current research on health numeracy. The health domains that have been studied (e.g., breast cancer risk perception and screening) have been limited. For example, despite its importance, we could find no research on the effects of numeracy in mental health (e.g., on medication compliance in treatment for depression). Research has documented strong effects of numeracy on perceptions of risks and benefits, on elicitation of values or utilities, and on formatting effects such as framing and frequency effects, but only a handful of studies connect such perceptions, values, and effects to health behaviors or outcomes. Finally, and most important, much of the work is merely descriptive, rather than explanatory or, as scientific theory ought to be, predictive based on knowledge of causal mechanisms. Although evocative and practical, none of the definitions of numeracy is based on empirically informed, theoretically sound conceptions of numeracy. Assessments are similarly pragmatic rather than explanatory, despite evidence of their \"validity\" and reliability. On the basis of studies that have controlled for education, intelligence, literacy, and other factors, we can be reasonably sure that numeracy is a separate faculty. What that faculty consists of is the province of theory. Several theorists have characterized it as the ability to draw meaning from numbers, although they disagree about whether that meaning is affective, frequentistic, precisely quantitative, or fuzzy gist. The idea that people vary in the quality of the meaning that they extract from numbers is central to characterizing them as low or high in numeracy. Clearly, more sophisticated and coherent conceptual definitions and measures of numeracy are needed to account for the diverse, sometimes inconsistent ways in which numeracy has been found to relate to decision making and other outcomes. The pervasive theme that those low in numeracy score lower on just about every other dimension studied makes sense, and it is consistent with dual-process theories that contrast intuitive and analytical reasoning and attribute biases and fallacies mainly to the former. However, these theories do not explain surprising and robust exceptions to this rule, including nonnumerical framing effects, inconsistent relations between intuitive versus analytical thinking and biases, and greater preference for numerically inferior options (e.g., saving fewer rather than more lives or a loss bet over a no-loss bet) among those higher in numeracy. Furthermore, standard dual-process theories emphasize affect, which fails to account for some effects, such as frequency, but is implicated in others, such as mood. The surprising findings generated by dual-process theories are informative precisely because they challenge conventional assumptions about numeracy, precision, and accurate reasoning. These anomalies should be a focus of future research in order to better understand the mechanisms of numerical processing. Each of the theories we reviewed has been applied to pitfalls in numerical processing or to heuristics and biases. Psychophysical approaches fall short in this respect. They explain the ratio dependence of number perception, which can influence decision making involving numbers, but they do not explain ratio bias. This is a serious shortcoming because ratio concepts -fractions, decimals, percentages, and probabilities-are especially difficult to process, as observed in national and international surveys, as well as in many kinds of numeracy assessments. This difficulty is expected, according to fuzzy trace theory, because classinclusion judgments of all kinds (e.g., in logical reasoning and in judgments of nested probabilities, such as 5-year vs. lifetime risks of cancer) are subject to denominator neglect, explaining ratio bias, frequency effects, and confusion of conditional probabilities, among other findings. The theory also identifies specific interventions to reduce denominator neglect, which have been evaluated with populations ranging from children to physicians and been found effective. Contemporary theory seems to be coalescing around the conclusion that computational simplicity-that is, clarifying relations among classes-is important for understanding. However, little work on individual differences in numeracy has been done from a computational perspective. Although many of the most important questions for future research on numeracy have implications for theory, some questions do not hinge on any particular theoretical perspective, such as how to better distinguish numeracy from automatic computation or general reasoning ability. However, the most informative research would test specific hypotheses about how people who are low versus high in numeracy process information differently. Are specific results produced by affect or gist, by frequencies or denominator neglect, and what kinds of meaning do highly numerate people extract from important health information? Most imperative, how can such meaning be communicated more broadly to those who need it to make life-and-death decisions? Without a deeper theoretical understanding of numeracy, especially of deficiencies in numeracy, it is difficult to know which policy recommendations to make. However, one important question raised by the association between numeracy and outcomes is whether clinical screening for low numeracy should be implemented in health care settings. The data that we have reviewed suggest the potential utility of numeracy screening as a means of helping clinicians to identify low-numerate patients at risk for poor understanding of health information and to avert more distal adverse health outcomes through interventions targeted to these patients. For a number of reasons, however, the prospect of clinical screening for low numeracy is not straightforward. As we have noted, the evidence linking low numeracy and poor health outcomes is newly emerging and much less developed than the evidence on health literacy. There is currently no evidence that either numeracy screening or targeted interventions to improve numeracy or otherwise assist low-numerate patients will improve health outcomes. Although it stands to reason that this should be the case, one can argue that more evidence is needed before such a practice is implemented, particularly given the substantial resources that numeracy screening would likely entail in the clinical setting. Some researchers have advanced the same argument regarding clinical health literacy screening, which also lacks direct empirical support in spite of the larger evidence base linking health literacy and outcomes (Paasche-Orlow & Wolf, 2008). Other important considerations in assessing the prospect of clinical screening for low numeracy include the performance characteristics of the screening tests, and the potential harms of numeracy screening. Currently, there are several tools that could be used to screen for low health numeracy, although none has been widely accepted or validated for clinical purposes. Screening for low numeracy also has unknown acceptability and psychological effects on patients' experiences with health care, and these factors require further exploration before screening programs are implemented. Although similar concerns have been expressed about health literacy screening, limited evidence suggests that patients have favorable attitudes toward screening (Ryan et al., 2007); more work needs to be done to determine whether these findings generalize to health numeracy. A larger question relates to the optimal approach of the health care system to the problem of low numeracy. Clinical screening for low health numeracy represents an individual-based approach, aimed at detecting the risk factor of low numeracy and, in theory, targeting interventions toward high-risk individuals. An alternative population-based approach, however, would be to design communication and care interventions that would benefit all patients, regardless of their individual numeracy levels. For example, clinical interventions to improve the understandability of numerical information and to evaluate and ensure comprehension of this information might benefit all patients, even those with high numeracy. Supporting this possibility, research on health literacy suggests that educational interventions designed to target low-literacy individuals also benefit those with high literacy (DeWalt et al., 2006). If this is also true for numeracy, then one can ask whether the more worthwhile strategy would be to implement more broadly applicable interventions to improve numerical understanding. These approaches, however, are not mutually exclusive, and the optimal strategy is an empirical question. Once sufficient evidence is gathered, it may be feasible to add effectiveness in overcoming innumeracy as a quality indicator in the evaluation of procedures used in hospitals (e.g., for surgical consent) and in clinical practice.   Lloyd (1959, para. 398) Numeracy We would wish the word \"numerate\" to imply the possession of two attributes. The first of these is an \"at-homeness\" with numbers and an ability to make use of mathematical skills which enables an individual to cope with the practical mathematical demands of his everyday life. The second is an ability to have some appreciation and understanding of information which is presented in mathematical terms, for instance in graphs, charts or tables or by reference to percentage increase or decrease. Cockroft (1982, para. 34)",
          "The term numeracy describes the aggregate of skills, knowledge, beliefs, dispositions, and habits of mind-as well as the general communicative and problem-solving skillsthat people need in order to effectively handle real-world situations or interpretative tasks with embedded mathematical or quantifiable elements. Gal (1995, para. 9) Numeracy Numeracy, in the sense of knowledge and mastery of systems for quantification, measurement and calculation, is a practice-driven competence rather than abstract academic knowledge of \"mathematics.\" Proficiency in numeracy varies with people's backgrounds and experience. Adelswärd andSachs (1996, p. 1186)",
          "The specific aspect of literacy that involves solving problems requiring understanding and use of quantitative information is sometimes called numeracy. Numeracy skills include understanding basic calculations, time and money, measurement, estimation, logic, and performing multistep operations. Most importantly, numeracy also involves the ability to infer what mathematic concepts need to be applied when interpreting specific situations. Montori and Rothman (2005, p. 1071)",
          "The knowledge and skills required to apply arithmetic operations, either alone or sequentially, using numbers embedded in printed materials. Kirsch et al. (2002, pp. 3-4) Health literacy The capacity of individuals to obtain, interpret and understand basic health information and services and the competence to use such information and services in ways which are health-enhancing. Joint Committee on National Health Education Standards (1995, p. 5) Health literacy A constellation of skills, including the ability to perform basic reading and numerical tasks required to function in the health care environment. Patients with adequate health literacy can read, understand, and act on health care information. Ad Hoc Committee on Health Literacy for the Council on Scientific Affairs (1999, p.",
          "Health literacy [Those] cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health…. Health literacy implies the achievement of a level of knowledge, personal skills and confidence to take action to improve personal and community health by changing personal lifestyles and living conditions. World Health Organization (1998. p. 10)",
          "The degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions. Ratzan and Parker (2000, p. vi) a",
          "The degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions.  "
        ],
        "ground_truth_definitions": {
          "ratio bias/numerosity bias": {
            "definition": "the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default.",
            "context": "A core prediction of S. Epstein et al.'s (1996) theory, therefore, is that a predominance of intuitive over analytical thinking, as measured by the REI, will account for an effect that is sometimes called ratio bias (the same effect is called the numerosity bias in the probability judgment literature; for a review, see Reyna & Brainerd, 1994). The ratio (or numerosity) bias is the finding that people who understand that probability is a function of frequencies in both the numerator and the denominator still tend to pay less attention to the denominator as a default. In the classic ratio bias task derived from Piaget and Inhelder (1951/1975), participants are offered a prize if they draw a colored jelly bean from a bowl.",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "885c3154386e3dd1ff6a12e566152846186f9569",
        "sections": [
          "hypotheses based on a careful mapping of psychological profiles of over 2,300 American Twitter users linked to behavioral sharing data and sentiment analyses of more than 500,000 news story headlines. The findings contradict the ignorance perspective but provide some support for the disruption perspective and strong support for the partisan polarization perspective. Thus, individuals who report hating their political opponents are the most likely to share political fake news and selectively share content that is useful for derogating these opponents. Overall, our findings show that fake news sharing is fueled by the same psychological motivations that drive other forms of partisan behavior, including sharing partisan news from traditional and credible news sources. With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018). Political observers have linked the spread of fake news to recent key political events, like the \"Brexit\" vote in Great Britain and the 2016 U.S. presidential election (e.g. Rose 2017, Silverman 2016). Accordingly, media organizations, governments, and academics have poured hefty investments into practices and technologies to stem the tide of fake news on social media. Notable efforts include fact-checking services like FactCheck.org and Snopes, as well as platform-based fake news detection algorithms and bots (Crowell 2017). In light of these sizable investments, it is surprising how little we know about why people share fake news on social media. Studies suggest the spread of false information cannot be attributed only to bots or \"sock puppet\" accounts (Guess, Nagler andTucker 2019, Vosoughi, Roy andAral 2018). But researchers have largely failed to conduct adequate tests of why ordinary citizens take part in the distribution of fake news (Wagner and Boczkowski 2019). This is unfortunate because potential solutions to the spread of fake news rest on psychological assumptions about the root causes of the problem. For example, fact-checking should reliably work only if citizens actually want to share true information but need help to weed out falsehoods. Presently, we do not know whether this portrayal of citizens' motivations is correct. As summarized in a recent study, \"most existing studies rely solely on publicly available social media data and there is little research that seeks to understand people's motivations for (...) sharing news\" (Chadwick, Vaccari and O'Loughlin 2018, 4258). In this study, we ask: What are the individual-level psychological motivations behind fake news sharing? The first major challenge in addressing this research question is data unavailability (King andPersily 2019, Stier et al. 2019). Data on citizens' personality and political profiles are rarely available in combination with behavioral data on their sharing activities on social media (for notable exceptions, see Guess, Nagler and Tucker 2019, Guess, Nyhan and Reifler 2020, Grinberg et al. 2019). Here, we circumvent these data limitations by linking unique behavioral data on news sharing activity and sentiment analyses of more than 500,000 news story headlines to individual-level survey data with detailed psychological measures for a large sample of American Twitter users. This approach allows us to conduct a comprehensive study of competing psychological theories of fake news sharing. The second challenge is that multiple psychological motivations could potentially influence the sharing of false and factually correct news; these motivations may differ across individuals and across news content. Building on theories about information sharing in general, we first outline these potential psychological motivations before applying them to our research topic: the sharing of political fake news on social media, a domain characterized by intense conflict. 1Then, we deduce testable implications about relevant individual differences and differences in political news content.",
          "Cognitive psychologists have demonstrated the existence of two fundamentally different motivational goals that structure how individuals engage with information: Accuracy-oriented and goal-oriented motivations (the latter sometimes referred to as directional motivations) (Kunda 1990). Accuracy-oriented motivations spur individuals to engage with and accept only true and reliable information to form an accurate picture of the world. Goal-oriented motivations, in contrast, prompt people to focus on information that is useful given their pre-existing commitments to particular beliefs, groups, or outcomes. For decades, political scientists and communication researchers have applied this dual-motivation framework for studying how citizens engage with legacy media (e.g., newspapers and TV news shows). As audiences had no direct means to influence the content of legacy media channels, the bulk of this work has focused on selective exposure -the idea that political predispositions propel people to read and watch news from sources that share their political beliefs (Stroud 2010) -and the downstream effects it can have on polarizing readers and viewers (Levendusky 2013). The emergence of the internet, and particularly of social media has fundamentally changed the relationship between media and audience. Whereas once audiences were more or less passive consumers, they are now actively participating in the distribution, and sometimes even the production of news. People today can start their own blogs, live-stream from political events they participate in (e.g. protests), and may even contribute to small media organizations often catering to niche market segments. To be sure, just as with legacy media, online audiences still face a choice of exposure: which news do they choose to consume. But on top of that, they also face a new choice of which news to share with their friends and followers. These trends, we contend, have implications for the psychological dynamics underlying the way audiences engage with this media, the novel step of news sharing potentially recruiting new psychological mechanisms. These trends have pushed to the forefront questions about information veracity. In advanced democracies, strong editorial norms and standards for good reporting meant that citizens could -at least most of the time -trust legacy media outlets to publish stories that accurately reflected the world. For most citizens and for most legacy media, then, it made sense to pay more attention to the political usefulness of news stories and, thus, it is not surprising that this news dimension has been critical in driving selective exposure. Yet, on the internet, editorial norms have lost power, and information circulating on social media will to a much larger extent vary on both dimensions of usefulness and veracity. Truth cannot be taken for granted and veracity judgments are thus, potentially, more important than ever. Yet, these two information dimensions -veracity and usefulness -are not unique for social media users. In everyday conversations when, for example, discussing or relaying a rumor, people must consider both whether they believe it and how useful they think it is. Psychologists have studied the psychology of rumor transmission for decades, highlighting the importance of both accuracy-oriented and goal-oriented motivations (for an overview, see DiFonzo and Bordia 2007). For example, people may share rumors they believe are true to be cooperative, to signal competence, or to get others to affirm the accuracy of the rumors. Sharing a rumor that turns out to be wrong, in contrast, may damage people's reputations (Altay, Hacquin and Mercier 2019). Nonetheless, goal-oriented processes may still take precedent, notably under intense social conflict. Some of the most well-studied, and most extreme, examples come from ethnic conflicts, where rumors are heavily used to mobilize the in-group against the out-group. In his comprehensive review of this role of rumors, Horowitz (2000, p. 74) concludes: \"Rumors are (. . . ) embedded in the riot situation, because they are satisfying and useful to rioters and their leaders. (. . . ) Rumor is likely to prevail over accurate information. (. . . ) Rumor prevails because it orders and organizes action-in-process.\" Below we apply these general considerations about the psychology of information sharing to the object of our empirical inquiry: The sharing of news -fake or real -about politics on social media.",
          "The prevalence of false information on social media may strengthen the role of accuracyoriented motivations and the dimension of veracity for understanding news sharing behavior. People may worry about damaging their reputation by accidentally sharing fake news (Altay, Hacquin and Mercier 2019). To the extent such reputational worries induce accuracy-oriented social media behavior, we should expect that sharing fake news stories is a rare phenomenon relative to sharing real news, but also that the predictors of sharing fake versus real news will differ. Successfully discerning between fake and real news is difficult, first, because fake news is specifically designed to appear real and, second, because the fast-paced social media platforms deplete attention and cognitive resources (Pennycook et al. 2020). Accordingly, individual differences in the ability to detect cues about information veracity should largely determine whether people share fake news (Pennycook and Rand 2019b). In essence, from this perspective, fake news sharing happens because people are ignorant that the stories they share are false and, accordingly, below we refer to this argument as the ignorance theory. Goal-oriented motivations, however, may also critically shape sharing of political news on social media. As noted above, goal-oriented motivations affect information sharing during periods of social conflict. Politics is conflictual, but political conflict in the United States in recent years has been deepened by an increasing political polarization (Mason 2018). Goaloriented motivations put a premium on the usefulness of information. Sharing a political news story (fake or real) can be used to signal one's political affiliation, by denigrating political opponents or by mobilizing politically like-minded peers. In this regard, fake news stories can be particularly useful: as fake news is not constrained by reality, they can be excessive in their negative portrayal of political opponents (Acerbi 2019). Accordingly, to the extent goal-oriented motivations dominate social media users' interactions with news stories, the same psychological motivations should drive the sharing of real and fake news. People will share useful news that fits their political goals, and, in the extreme case, they will not pay attention to the veracity dimension of the information at all. In current American politics, two types of social or political goals could be served by sharing extreme information such as fake news. First, sharing news stories could reflect motivations to help one's preferred party against opposing parties in the increasingly polarized electoral competition (Iyengar, Sood andLelkes 2012, Lelkes andWestwood 2017). We will refer to this as the polarization theory as it focuses on partisan feelings rather than ideological disagreements between partisans of different stripes. The theory is prominent in current discussions about political events, like elections and referenda, and it has a long research history in political science (Mason 2018, Taber andLodge 2006). The theory has also informed studies on partisans' beliefs in political conspiracies (Miller, Saunders and Farhart 2015). Second, sharing news stories could be fueled by deep-seated discontent with the status quo and serve a desire to disrupt the existing social and political order. For example, Tucker et al. (2017, 53) convincingly argue that \"social media can lend a voice to ... anti system forces that actively seek to undermine liberal democracy.\" Below, we will refer to this as the disruption theory. Finally, there are also multiple scenarios for the intermediate (and likely) case where both veracity and usefulness considerations are relevant for social media users' sharing behavior. People may first evaluate the veracity of news and then, conditional on it being accurate, consider its usefulness. Or people may prioritize usefulness but still pay some attention to the veracity. Hence, the sharing of blatantly false information may, for example, complicate mobilization processes by derailing discussions about whether events really happened (Petersen, Osmundsen and Tooby 2020). Thus, when assessing usefulness, people may factor in veracity and selectively share information that is as extreme as they believe they can get away with. To elucidate whether and how the ignorance, polarization, and disruption theories account for the sharing of fake news, we use two approaches: First, we examine the role of individual differences, deducing for each theory who should be most likely to share fake news and, second, we examine the role of news story content, deducing what is most likely to be shared.",
          "The accuracy-oriented ignorance theory assumes people want to share accurate information, but end up sharing falsehoods because they lack the cognitive reflection or motivation to discern between true and false information. One line of evidence for the ignorance theory comes from Pennycook and Rand (2019b), who show that people who perform better on a cognitive reflection test can better tell if news story headlines are true or false. Another line of evidence comes from Guess, Nagler and Tucker (2019), who demonstrate that older people are more likely to share fake news on Facebook, presumably because they are less digitally literate than their younger peers. Building on this, we test the ignorance theory by focusing on four individual-level predictors of fake news sharing on Twitter: age, cognitive reflection, factual political knowledge, and digital literacy. The goal-oriented polarization theory focuses on partisanship as a key driver of fake news sharing. Currently, there is mixed evidence on how partisan motivations influence fake news sharing. In general, partisans seem to share information from ideologically similar sources (Barberá et al. 2015) and to treat articles shared by political opponents with greater suspicion (Lee, Kim and Coe 2018), suggesting that considerations about the political usefulness of information matters. Guess, Nagler and Tucker (2019, 3) find that partisanship also affects fake news sharing in that conservatives and Republicans were more likely to share fake news on Facebook in 2016 than liberals and Democrats, a partisan asymmetry attributed to \"the overwhelming pro-Trump orientation in both the supply and consumption of fake news during that period.\" Pennycook and Rand (2019b), however, tone down the effect of partisanship. They conclude that \"people fall for fake news because they fail to think; not because they think in a motivated or identity-protective way\" (ibid., 48). To test the polarization theory, we examine whether Democratic partisans are willing to share politically congruent fake news from *pro*-Democratic sources or if the partisanship asymmetry identified by Guess, Nagler and Tucker (2019) and others prevails. Second, an important question in the literature on affective polarization is whether in-party love (Lelkes and Westwood 2017) or out-party hatred (Abramowitz and Webster 2018) sparks polarization. We provide the first test of which of these dynamics explain fake news sharing by relying on two types of individual differences as predictors of news sharing: 1) standard party identification measures and 2) batteries of positive versus negative feelings towards Democrats and Republicans. Finally, we turn to the goal-oriented disruption theory. A central focus in media discussions and psychology has been on so-called \"online trolls\" who take pleasure in misleading and harassing others online (Buckels, Trapnell and Paulhus 2014). Trolling experts speculated that sharing sensational and often implausible fake news reflects a specific form of apolitical online trolling. However, political cynicism may also drive fake news sharing. For example, a recent study showed that Americans and Danes who are discontent with political elites and the political system report substantially greater willingness to circulate conspiracy theories online (Petersen, Osmundsen and Arceneaux 2018). Here, we provide the first behavioral test of this theory and zoom in on two individual differences as predictors of fake news sharing: a psychological measure of trolling behavior and a measure of political cynicism.",
          "To elucidate the psychological motivations underlying fake news sharing, we also need to understand what news content people share. Specifically, to adjudicate between the three theories, the analyses differentiate news stories along two dimensions: (1) Whether a news story originates from a fake news source (i.e., a source containing registered debunked stories) or a real news source (i.e., a generally credible source), and (2) whether a news story comes from a *pro*-Republican or a *pro*-Democratic source. The three theories make very different predictions about which dimension matters for people's sharing preferences. The ignorance theory predicts that the veracity dimension matters most and that people with high cognitive abilities will be less willing to share fake news stories and more likely to share real news stories, irrespective of the political slant of the stories. The latter two theories make the opposite prediction: they posit that people place less value on whether stories are true or false as long as the stories are politically useful. Specifically, the disruption theory implies that disruptive individuals should selectively share as many fake news as possible to sow confusion and uncertainty. For the polarization theory, however, the content-related predictions are more complex, and so it is these predictions we unfold now. The polarization theory holds that fake news sharing is not an accident caused by ignorance, it is partisan business as usual. The media landscape in the US is politically fragmented, with some news sources catering to Republicans while others offer content that fit Democrats' taste (Groeling 2013). From the perspective of the polarization theory, fake news is an extreme version of traditional partisan news that polarized partisans may find attractive due to its rarity and extremity. Accordingly, partisans motivated to selectively share hyper-partisan content will tend to view the online media landscape of news sources as a one-dimensional continuum reflecting the political usefulness of the stories they provide. For a polarized Republican social media user, the order of the usefulness of news sources goes from *pro*-Republican fake news sources, to *pro*-Republican real news sources over centrist sources to *pro*-Democratic real news sources, and finally, to *pro*-Democratic fake news. For Democrats, the order reverses. This insight, we contend, leads to three observable implications about the content of shared news. First, partisan social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. Accordingly, we expect that sharing fake news favoring one party over the other should be best predicted by sharing real news strongly favoring the same party. Conversely, sharing fake news favoring one party should correlate negatively with sharing fake news favoring the other party. Second, if news sources differ systematically in how strongly they cater to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Specifically, fake news should show the strongest partisan sorting in terms of who wants to share from these sources. Third, and finally, the negativity of news source content should change as we move across the partisan news continuum. If partisans selectively share stories with content that fits their political tastes, the observable implication is that stories shared from news sources located towards the Democratic extreme on the partisan news continuum should portray Republican elites negatively while stories from the Republican extreme should cast Democratic elites in a negative light.",
          "",
          "We commissioned the YouGov survey agency to recruit U.S. Twitter users to complete a 20-minute online survey fielded from mid-December 2018 to mid-January 2019. The survey contained questions measuring our set of potential predictors of fake news sharing (see below) and a range of demographic variables. The survey also asked participants to provide their Twitter ID and asked for permission to scrape their publicly available Twitter data and link this data to their confidential survey responses. See SI Section 1 for the invitation letter and link to the consent form. We scraped respondents' Twitter accounts by early March, 2019. Our data includes tweets and retweets posted by the panelists before that period. YouGov invited over 27,000 U.S. Twitter users from their standing web panel to participate in our study. 8,741 participants accepted the invitation and answered all questions before the final prompt to give consent to link their public Twitter data to the survey. A majority of these respondents (5,725 people) did not share their Twitter IDs at the end of the survey. Of the 2,976 respondents who did attempt to share their Twitter handles, another 639 participants presented IDs that we could not access via Twitter's Application Programming Interface. Consequently, our final sample size is N = 2,337. Because of high and non-random attrition, our sample is unlikely to perfectly reflect any welldefined population. Still, our study participants' demographic characteristics resemble the characteristics obtained from participants in other high-quality Twitter studies. Relying on a recent Pew study (Wojcik and Hughes 2019), SM Section 2a demonstrates that our sample is comparable to the Twitter-population on race, gender, income, education, and age. It also shows that participants in our final sample were demographically very similar to the 8,741 participants who accepted to participate in the survey but failed to provide their Twitter handle. In our final sample, 50% of participants are female, 70% are white/Caucasian, the median age is 40 years old (SD = 14), the median highest level of completed education is \"4-year college degree,\" and the median family income level is \"$60,000-$69,999.\" We scraped Twitter data from all our 2,337 participants. In total, we extracted 2,709,052 tweets and retweets posted by our panelists until mid-January 2019. The median number of (re)tweets for our participants was 488 (Min #of tweets = 1, Max #of tweets = 6433). Importantly, a large majority of the tweets -around 70% -were posted after the 2016 US presidential election; the period that has been the focus of previous work. This allows us to extend earlier findings and test how they hold up in periods characterized by less intense electoral competition.",
          "We follow best practices (Guess, Nyhan and Reifler 2020, Grinberg et al. 2019, Lazer et al. 2018) and operationalize \"sharing of fake news sources\" as when panel members tweet or retweet URLs to external web domains known to publish factually incorrect news. To this end, we cross-referenced all tweeted URLs against a list of 608 fake news sources, constructed by journalists and scholars (for details, see SM Section 10 and Guess et al. 2019, Allcott, Gentzkow andYu 2019). The list was compiled to document the prevalence of fake news during the 2016 U.S. presidential election but has been updated to account for fake news publishers that have emerged since. To the extent possible, the list creators have coded the most popular fake news sources for partisan slant: 12 of the most popular were *pro*-Democratic fake news sources while 30 were *pro*-Republican sources. While this number may seem low, note that almost 86% of all shared fake news sources originated from one of these 42 domains for which we have partisan ratings. (See SM Section 6 for the complete list of fake news domains and Figure SM 2b for the 20 fake news domains that were shared the most.) As a robustness test, supplementary analyses discussed below group the fake news sources according to a more fine-grained coding scheme proposed by Grinberg and colleagues (2019) that distinguishes among three classes of fake news sources differing in their likelihood of publishing misinformation. In SM Section 8, we also replicate the main results using a different, but partially overlapping list of fake news sources. Our measure of fake news sharing records actual online behavior and thus does not suffer from biases that can plague self-reported measures of online activity. Further, while the present study is certainly not the first to couple survey and social media data (e.g., Guess, Nagler and Tucker 2019), it has the most detailed set of psychological measures; see below. Still, it is important to keep in mind when interpreting our results that we cannot tell if the specific stories people share are true or false: Our fake news measure is at the publisher level, not the story level. To measure the \"sharing of real news sources,\" we cross-referenced participants' tweets against a list of real news publishers obtained from the AllSides organization. AllSides seeks to help citizens navigate the online media landscape by providing \"balanced news and civil discourse.\" Their web page (www.allsides.com/media-bias/) contains a list of 260 real news publishers with ideological ratings: \"Left\" (labeled \"Strong Democratic\" below, n = 45), \"Lean Left\" (\"Leaning Democratic\", n = 63), \"Center/Mixed\" (n = 88), \"Lean Right\" (\"Leaning Republican\", n = 29) or \"Right\" (\"Strong Republican\", n = 39). While AllSides combines different methods for estimating ideological bias (e.g., editorial reviews, third-party judgments), SM Section 7 shows that their ratings correlate highly -Pearson's r = .90with network-based measures of ideological alignment of media sites (Bakshy, Messing and Adamic 2015). See SM Section 7 and Figure SM 2b for the 20 most shared real news domains. Our measures of real and fake news sharing account for ideological and partisan slant at the source level and are thus useful for understanding the content people care about when sharing news. To gain further leverage on testing the relevance of content for sharing behavior, we also built two unique data sets of news headlines (see SM Section 11 and 12 for details). We focus on headlines because these are more accessible than full texts and because prior research demonstrates that simply seeing a headline can exert large effects on decisions to share news on social media (Effron and Raj 2019). The first data set was based on the headlines of stories shared by our participants, which gives us a clear representation of the content they deemed sufficiently important to share. To construct the data set, we first retrieved the full URLs from all 85,084 tweeted news links shared by our participants. Based on these URLs, we parsed the news domains' HTML code and successfully extracted the headlines of 75,560 news articles (89% of all article headlines). To construct our second data set, we used the Archive.org web page -an indexing system for the Internet's web pages for historical archiving -to scrape headlines that had appeared on the daily front pages of the most popular (among our participants; see Figure 5 below) news sites in the period 2016-2019 (i.e., covering roughly the same period for which we have tweets from our participants). Compared to our Twitter headlines, this data set contains a much larger number of news story headlines (we successfully scraped a little over 500,000 headlines). More importantly, the front page headlines offer a glimpse of the universe of news stories our participants could potentially see and read before deciding which stories to share. In this way, it sidesteps issues of \"selection\" biases in sharing decisions due to psychologically motivated reasoning; a point we return to during the analyses. We subjected these headlines to two types of automated analyses. To measure the sentiment of the headlines, we relied on the sentimentR R package, which includes a sophisticated algorithm for calculating text polarity sentiment. This algorithm improves the performance of more traditional dictionary-based methods by calculating sentiment on the sentence level and considering valence shifters (e.g. not, very, hardly). To identify the political affiliations of those mentioned in the headlines, we created two custom dictionaries of prominent Republican and Democratic elites. Our primary dictionary is based on YouGov's list of the 100 most famous Democrats and Republicans. 2 We replicate our results with an alternative dictionary listing all members of the US Congress, Senate and the Trump Administration in the supplemental materials. Finally, we matched these dictionaries to the headline data sets and recorded whether a given headline mentions anyone from the Republican or Democratic party. We use these more detailed content measures in the final part of the analysis, whereas the initial parts focus on the crude distinction between fake and real news sources.",
          "All participants responded to the same survey questions, designed to measure the potential predictors of fake news sharing derived from the three theories discussed earlier: the ignorance theory, the disruption theory, and the polarization theory. SM Section 1c lists the questions asked and specific wordings.  (Pennycook and Rand 2019b). Second, we included participant age to test if we could replicate earlier findings that older social media users share more fake news stories than younger users. Finally, we included measures of factual political knowledge and digital media literacy to test the claims about why older social media users share more \"fake news.\" We included a 5-item battery from the American National Election Studies (ANES) to measure factual political knowledge (e.g., \"How many members of the U.S. Supreme Court are there?\"). To measure digital media literacy in the domain of politics, we created a 10-item battery based on previous research (Feezell 2016, Gil   2 Accessible at https://today.yougov.com/ratings/politics/fame/Democrats/all and https:// today.yougov.com/ratings/politics/fame/Republicans/all de Zúñiga, Molyneux and Zheng 2014) asking participants how often they engage in specific online political behaviors (\"Use the Internet to participate in volunteer activities related to a political or social campaign\") (α = .93). As this measure also taps political engagement and online civic skills, we adjust for political interest in the analyses. In addition, SM Section 4 replicates the main analysis using a more narrow, validated measure of digital literacy from Hargittai and Hsieh (2011). The disruption theory. We included two measures of disruptive motivations. To measure apolitical trolling behavior, we solicited the \"cyber-bullying\" literature and implemented a 4-item trolling scale (Buckels, Trapnell and Paulhus 2014), asking participants whether they agreed or disagreed with statements like \"I like to troll people in forums or the comments section of websites\" (α = .86). We measured political cynicism via an 8-item survey instrument (Dekker and Meijerink 2012), asking participants whether they agreed or disagreed with statements like \"Politics in the United States is sick\" (α = .74). The polarization theory. We included two measures of political polarization: Partisanship and feelings towards Republicans and Democrats. We used a standard ANES question to measure partisanship on a seven-point scale, ranging from \"Strong Democrat\" to \"Strong Republican\" identifier. To measure partisan feelings, we asked participants how strongly they felt various positive (hope, enthusiasm, pride) and negative (anger, frustration, fear) emotions when thinking about Republicans and Democrats. We created two feelings scales to test how in-party love and out-group hate related to news sharing by aggregating responses and taking the averages for each participant (the negative emotion items were flipped). The key models presented below analyze motivations to share news from *pro*-Republican and *pro*-Democratic news sources separately. Accordingly, we coded the \"in-party\" emotion scale such that higher values always correspond to more positive emotions towards the political side supported by the news source (i.e., higher values indicate the participant has more positive feelings towards Republicans (Democrats) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). Analogously, we constructed the \"out-party\" emotion scale such that higher values indicated more negative feelings towards the political side opposed by the news source (i.e., higher values indicate the participant has more nega-tive feelings towards Democrats (Republicans) in the analyses of sharing *pro*-Republican (*pro*-Democratic) sources). SM Section 2b presents summary statistics for all main variables.",
          "",
          "Before testing the key hypotheses about the psychological motivations for fake news sharing, we provide descriptive analyses of the data. Overall, we find that news sharing is a small part of our sample's activity on Twitter. (See Figure SM2c for an overview.) Of the 2.7 million tweets in our data, only 85,344 (≈ 3%) contain links to national news websites, fake or real. Of the tweeted news links, 3,269 (≈ 4%) come from websites with a history of publishing fake news (see Grinberg et al. 2019, for slightly larger estimates during the 2016 US presidential election). Of these stories, a large majority -2,563 stories -originates from *pro*-Republican fake news publishers. In contrast, over twice as many real news stories come from *pro*-Democratic sources (60%) than from *pro*-Republican sources (23%). Together, these two observations imply that exposure to fake news depends heavily on the ideological leanings of one's Twitter network. In *pro*-Democratic networks, only a small fraction of the news diet is likely to come from fake news sources, whereas *pro*-Republican networks are likely to display a substantially higher fake-to-real news ratio. While these aggregate numbers suggest fake news sharing could be widespread within certain social networks, they also mask large heterogeneity in sharing behavior. The left-hand panel in Figure 1 below makes clear that sharing content from fake news sources flourishes mostly among small segments of the population: 1% of the panelists are responsible for sharing about 75% of all links to fake news publishers and 11% of them are responsible for sharing all fake news sources. Meanwhile, the top 1% of real news \"supersharers\" are only responsible for sharing about 30% of all real news sources. Figure 1's two other panels show similar results when splitting the data based on the partisan slant of the news sources. Importantly, not all fake news is equally troublesome. SM Section 8 uses Grinberg and colleagues' (2019) fine-grained categorization of fake news sources and find that about 30% of the fake news links come from sources \"publishing almost exclusively fabricated stories\" (i.e. \"black\" sites). Another 5% come from sites that \"spread falsehoods that clearly reflected a flawed editorial process\" (i.e. \"red\" sites). Finally, two-thirds of the links referred to fake news sources \"where annotators were less certain that the falsehoods stemmed from a systematically flawed process\" (i.e. \"orange\" sites). While this means that the number of individual stories containing blatant falsehoods is likely smaller than 3,325 in our data, we note that sharing news stories from websites with a record of disseminating falsehoods can still be problematic. Given the still limited penetration of fact-checking, sharers most likely do not know whether a certain news story is true or false when deciding to share it. Instead, they must make informed guesses based on the reputation of the news source. Thus, decisions to share news from these sources still reflect either ignorance or lack of concern about such matters.  Note. The x-axes show percentage of participants responsible for sharing a given percentage of all news links (y-axes). The left-hand panel gives results for sharing links to fake (black line) versus real (grey line) news publishers (N Fake News = 3,269; N Real News = 78,400). The middle and right-hand panels display results for sharing links to *pro*-Democratic (blue line) versus *pro*-Republican (red line) fake and real news sources, respectively. For real news sources, we collapse the \"Leaning\" and \"Strong\" categories. The dashed black lines denote a hypothetical uniform distribution of news sharing. (N Fake News, pro-Rep = 2,563; N Fake News, pro-Dem = 263; N Real News, pro-Rep = 16,081; N Real News, pro-Dem = 46,385.)",
          "We now turn to the analysis of the psychological motivations for sharing fake news sources. Figure 2 gives average marginal effects (AME) 3 from logistic regression models estimating the association between our main predictors and the probability of sharing at least one news story from a fake news (left panel) or a real news source (right panel). To increase power and simplify the analyses, we here combine the real news sources that Allsides rate as strongly partisan and leaning partisan (e.g., we combine news stories from \"Strong Democratic\" and \"Leaning Democratic\" real news sources). As our main interest is in whether people share fake news or not, we dichotomize the dependent variables, with 1 indicating that a participant As shown in the lower right-hand panel, the partisan factors predict sharing from real news sources in an almost identical fashion. The motivations underlying fake news and real news sharing are, psychologically speaking, highly overlapping. One difference compared to sharing of fake news sources is that our estimates are obtained with greater precision due to the larger number of participants sharing real news sources. Another difference relates to the partisan asymmetry: Concerning real news sharing, we find no asymmetry between Democrats and Republicans; both are equally partisan and strongly motivated by negative feelings towards political opponents. Altogether, these results fit well with predictions by theories on partisan motivations: Partisans share politically congenial news, primarily because of hostile feelings towards the out-party. Further, the lack of evidence for the ignorance theory suggests that these sharers pay more attention to the political usefulness of news rather the information quality. Below, we put the polarization theory to an even stronger test by considering the role of news content in sharing decisions. ",
          "For the sake of clarity, our main analysis employs crude binary distinctions between fake and real news as well as between those who share any news and those who do not share news at all. This section tests the robustness of our results with respect to these distinctions. Most importantly, we operationalize the sharing of fake news as the sharing of all links to news sources with a history of publishing falsehoods. However, some fake news sources produce more misinformation than others. It is possible that when it comes to blatant falsehoods, ignorant rather than polarized people are the main culprits. To test this possibility, we rely on the fake news classification of Grinberg et al. ( 2019) described above and re-run our analysis separately for each of three fake news categories. Our results reported in SM Section 8 show high consistency across the three categories. People who share links from sources that publish \"almost exclusively fabricated stories\" are also more polarized, but not more ignorant, and only slightly more disruptive than those who do not share any such links. We also replicate our results relying on expert ratings of the trustworthiness of 60 news sources (20 mainstream news sources, 20 hyperpartisan websites, and 20 fake news sources) based on Pennycook and Rand (2019a). Whereas this conceptualization offers an even more fine-grained look into the quality of news sources, it comes at a cost of dropping more than 90% of all news sources from our analysis. Following Pennycook and Rand (2019a) we calculate the average trustworthiness of news shared for each individual in our sample and use it as our dependent variable. SM Section 9 demonstrates that the findings reflect those presented in the main text: Political variables -partisanship and feelings towards Republicans and Democrats -correlate strongly with the trustworthiness of shared news sources whereas \"ignorance variables\" -CRT and political knowledge -do not. Finally, we replicate our analysis by modeling the actual count of fake and real news shared instead of binary variables of sharing versus not sharing. This involves a change from logistic regression to Quasi-Poisson models. The two models have important substantive differences. After all, sharing a single fake news link amidst dozens of real news links could be an honest mistake. Relying on the count variables, in contrast, put more weight on respondents with higher numbers of shares within a category. SM section 5 indicates this alternative modelling procedure does not affect our main conclusions. While the effect of political cynicism is significantly reduced, partisan motivation -especially negative affect towards the out-party -remains the primary predictor of fake news sharing. 4 The Role of News Content: Fake news as the Extreme Ends of a",
          "News source sharing patterns across the partisan news continuum. If partisan motives dictate both fake and real news sharing, social media users should share news sources with similar political leanings, especially near the partisan extremes of the news continuum. To test this, Figure 3 plots a heatmap of correlations based on news sharing across the seven news source types. The emerging patterns firmly favor a partisan story: People who share the most articles from ideologically biased real news sources are also most likely to share fake news sources with a similar political bias. In contrast, the off-diagonal correlations show that people shy away from sharing politically opposed sources. Importantly, the plot also reveals a partisan asymmetry. Whereas sharing *pro*-Republican real news correlates highly with sharing *pro*-Republican fake news, the correlation is substantially weaker between sharing *pro*-Democratic fake and real news. Instead, participants sharing *pro*-Democratic real news often share centrist news as well. Thus, while social media users generally favor partisan news sources, the effect is most pronounced in the Republican end of the continuum. 4 In SM Section 4, we present a series of additional robustness tests. First, we show that our results hold after removing 61 panelists who followed -or were being followed by -at least one other panelist. Second, we demonstrate that applying YouGov sample-matching weights do not alter our results. Third, we re-run models that control for age and partisanship; this does not change our results. Fourth, we leverage a potentially better measure of digital literacy (Hargittai and Hsieh 2011); using this measure does not change our substantive conclusions. Fifth, we show that our results are robust to an alternative coding scheme that treats InfoWars as a *pro*-Republican fake news source rather than a real news source. Further, SM Section 8 shows that our conclusions remain intact when using an alternative list of fake news sources compiled by Grinberg et al. (2019). The predictive power of partisanship across the partisan news continuum. If news sources differ systematically in how strongly their content caters to partisan motivations, then the association between partisanship and sharing should be stronger for news sources located at the extremes of the news source continuum. Figure 4 tests this assertion. The left-hand panel plots estimated logistic regression coefficients from models that examine the association between a seven-point partisanship scale (re-scaled to range from 0 = Democratic Identifier to 1 = Republican Identifier) and sharing content from the seven news source types, while the right-hand panel gives raw counts of shared links stacked by participants' partisanship (where, to ease interpretation, we trichotomize the partisanship scale). Overall, Figure 4 provides more evidence in favor of the partisan logic of news sharing: Both panels show that Republicans are more likely than Democrats to share extreme *pro*-Republican news    Left-hand panels: Headlines shared on Twitter by panelists (upper panel) and headlines scraped from front pages of news sources (lower panel). Red dots: Headlines that mention Republican politicians. Blue dots: Headlines that mention Democratic politicians. In the lower panel, we scraped headlines from the following news sources: Fake Republican: babylonbee.com, dailywire.com, ilovemyfreedom.org, theconservativetreehouse.com, iotwreport.com; Strong Republican: breitbart.com, dailycaller.com, freebeacon.com, nypost.com, townhall.com; Lean Republican: foxnews.com, hotair.com, telegraph.co.uk, washingtonexaminer.com, washingtontimes.com; Center: thehill.com, usatoday.com, npr.com, bbc.com, wsj.com; Lean Democratic: politico.com, nbcnews.com, nytimes.com, theguardian.com, washingtonpost.com; Strong Democratic: alternet.org, dailykos.com, huffingtonpost.com, rawstory.com, vox.com; Fake Democratic: bipartisanreport.com, indiatimes.com, newspunch.com, palmerreport.com, themindunleashed.com. Righthand panels: Headline counts from news sources, conditional on whether headline mentions Republican elites, Democratic elites, or both. Note that headlines mentioning both Republicans and Democrats counted twice in the sentiment analysis. See SM Section 11 and 12 for details on the analysis. are negative towards Democrats. We can go further. For partisans, a critical consideration when promoting news sources on social media is not just the level of negativity expressed towards the out-party. Instead, the most politically useful news source is one that is negative towards the out-party and positive towards the in-party. This implies that we should also examine, for each type of news source, the difference in negativity between mentions of Republican and Democratic elites in Figure 5. Recall that Democrats are more likely to share centrist news sources while Republican partisans prefer fake news sources. The net differences in headline sentiment when mention- We suspect this explains why Republican partisans find fake news more appealing than do Democratic partisans. To get a steady supply of news that caters to their political tastes, Republicans must turn to more extreme news sources, including those known to publish falsehoods. Democrats, in contrast, will have to look no further than to Centrist and *pro*-Democratic real news sources to meet their partisan goals. Importantly, this explanation cannot easily be dismissed by a \"self-selection\" story in which Republican panelists simply decide to share more extreme stories. Focusing on the front page headlines of stories journalists and editors have deemed sufficiently newsworthy to write and publish leads to the same conclusion. Still, whether this speculation is warranted, the combined observations point to an astonishing overlap in the asymmetries in (1) the sources shared by Democrat and Republican partisans, respectively, and (2) the tone of the stories shared from these sources when covering Democratic and Republican elites, respectively. Together, these three analyses corroborate that Republicans approach news source types as if they are placed on a one-dimensional partisan continuum. Democratic partisans, to be sure, do too, but with the very important exception that *pro*-Democratic fake news sources are less attractive to highly polarized Democrats when we compare (a) Democrats to Republicans and (b) *pro*-Democratic fake news to traditional *pro*-Democratic news sources. As we turn to below, this overall pattern suggests that the sharing of fake news is This suggests that fake news sharing is \"business as usual,\" dictated by the same logic partisans use to process other types of information, including real news. From a partisan motivated perspective, fake news is not categorically different from other sources of political information. As the analyses of the news content showed, partisans' decisions to share both fake and real news sources depend on how politically useful they are in derogating the out-party. Under some circumstances, this goal is best achieved with stories from fake news websites. Under other circumstances, this can best be achieved by stories from more credible websites. To the partisan mind, fake news is the extreme end of a news source continuum where the news sources differ in how well they cater to partisan goals. Accordingly, when deciding to share a story from a fake news website, most people do not prioritize whether the story is true or not. This helps explain why cognitive reflection is not related to fake news sharing even though it correlates with the ability to discern fake news from real news. These abilities are not employed in the context of real-life sharing decisions. At the same time, the sum of findings does suggest a dual involvement of accuracy-oriented and goal-oriented motivations. First, fake news sharing is a relatively rare phenomenon that flourishes only among small segments of the population. Like others, and in contrast to what some commentators have feared, we find that only 11% of our participants shared stories from fake news sources during our period of study while half of the participants shared stories from real news web domains. Second, as discussed below, Republicans were more likely than Democrats to share fake news (e.g., Guess, Nagler and Tucker 2019), even though fake news could help Democrats achieve their partisan goals. This suggests that social media users prioritize the usefulness of information when making sharing decisions but prefer useful information that is true rather than false; they care about the veracity of news as long as it does not compromise its usefulness. Two observations merit further discussion. First, it is relevant to discuss the finding that negative feelings towards political opponents rather than positive feelings towards the inparty are the strongest predictor of fake news sharing. There is strong evidence that partisans generally prefer helping the in-party rather than hurting the out-party (Lelkes and Westwood 2017). A hostile political climate, however, may shift the balance. Research suggests that the danger of \"symbolic\" threats against the moral values of one's group can make it \"necessary to strike back, to undermine the credibility of the opposition, and to 'hit them where it hurts\"' (Amira, Wright and Goya-Tocchetto 2019). Twitter -with its fast diffusion of moralizing and emotion-laden political content (Brady et al. 2017) -may have fostered such a hostile environment, making considerations of hurting the out-party loom larger when deciding which news stories to share. Second, we found a significant partisan asymmetry: Republicans were more likely than Democrats to share fake news sources. This could reflect that Republicans are less motivated by accuracy-motivations than Democrats. Yet, accuracy motivations have low explanatory power overall and additional analyses presented in the replication code show that that the associations between ignorance-related variables and fake news sharing do not consistently differ between Democrats and Republicans. Accordingly, other explanations may be at play. One possibility highlights differences in the supply of news (Grinberg et al. 2019): Democrats and Republicans may be equally motivated by accuracy goals but Republicans end up sharing more falsehoods because their Twitter networks offer a greater supply of fake news sources to share from. Another -complementary -supply-side explanation shifts the focus from accuracy-to goal-oriented motivations: Democrats and Republicans are equally motivated to share information that is useful for furthering their political agenda but the supply of useful real news stories is lower for Republicans, propelling them to turn to fake news sites for material. The sentiment analyses of news headlines are consistent with this explanation. Only *pro*-Republican fake news sources were more negative towards Democrats than towards Republicans. This pattern was consistent across news stories shared by the panelists, both partisans and Independents (see SM section 11 for analyses of Independents), and the news stories from the news site front pages. This at least suggests that this asymmetry reflects a difference in the supply of content. A Republican scanning the Internet for materials to share must turn to fake news sources to find news that, on average, satisfy their partisan need to derogate Democrats. Democrats operating in the same manner, in contrast, can stop their search at posts linking to *pro*-Democratic real news websites or even centrist websites. Without individual-level data on news exposure, it is very difficult to tease apart these explanations (see Eady et al. (2019)) and, most likely, all are at play. Still, if the latter explanations are at least partly correct, the partisan asymmetry in fake news sharing may have less to do with psychological differences between Democrats and Republicans and more to do with differences in the supply of useful information (in a partisan sense). This would dovetail with studies demonstrating that Democrats and Republicans are equally motivated by partisan goals (Brandt et al. 2014). Also, we want to note that it is unclear whether a supply-oriented explanation is evidence of actual media bias. Bias is notoriously difficult to establish. Some prior studies have found similar evidence of a liberal bias in legacy media (Groseclose and Milyo 2005), but the general negative coverage of leading Republicans could also reflect a \"bias in reality\" (Groeling 2013) during an unusual period in US politics. Overall, these conclusions have important implications for how to intervene successfully against the circulation of \"fake news.\" If people care primarily about a story's ability to hurt political enemies, we should not be surprised if fact-checking fails to reduce sharing. In fact, our findings suggest that offline realities shape online sharing of \"fake news.\" Fake news sharing relates to the increasingly polarized political debates in the United States, and it may be difficult to fix the problem of fake news without fixing the larger problem of political polarization. Unfortunately, this is much harder to accomplish than adding automatic factchecks on social media platforms. This does not imply that it is impossible to demotivate sharing of \"fake news.\" Evidence is emerging that fake news sharing decreases other people's trust in the sharer (Altay, Hacquin and Mercier 2019). Reputational concerns may explain the low prevalence of fake news. This implies that successful interventions could work by reminding people to \"not make a fool of themselves\" when they are about to share information from non-credible sources. Yet, even if such interventions can drive down sharing of genuine fake news (Pennycook et al. 2019), the underlying problem remains. Polarized partisans can often find plenty of negative content on ordinary news sites. While each individual news story may be true, the overall picture painted by exclusively sharing derogatory news stories may not be. In this perspective, the real problem is the tendency to selectively engage with derogatory news"
        ],
        "ground_truth_definitions": {
          "fake news": {
            "definition": "fabricated information that mimics news media content in form but not in organizational process or intent",
            "context": "With the advent of social media, the circulation of \"fake news\" has emerged as a major societal concern. Fake news can be defined as \"fabricated information that mimics news media content in form but not in organizational process or intent\" (Lazer et al. 2018). While some studies suggest most social media users refrain from sharing fake news (Guess, Nagler and Tucker 2019), others argue that false news spread faster and with greater reach than true news (Vosoughi, Roy and Aral 2018).",
            "type": "explicit"
          }
        }
      },
      {
        "paper_id": "s10816-016-9274-2",
        "sections": [
          "When archaeologists present the results of an archaeological survey, whether to an academic audience or to cultural resource managers, the consumers of these reports product is not like television monitors or automobiles. Consequently, some aspects of QA require modification to fit our unique situation, although we do share with other research endeavors some aspects of quality research, and we also share with manufacturing businesses (especially in the case of CRM-related archaeology) the needs to monitor costs and maintain or improve the satisfaction of clients. The purpose of QA is not to create unwieldy bureaucracy or still more unwanted paperwork; it is to put mechanisms in place so that we may consistently maintain and improve the quality of work. Undoubtedly, this creates real costs, in time, money, and training, but the failure to ensure quality also has costs, such as the need to resurvey territory that was not adequately surveyed the first time or, worse yet, the need to mitigate a site that construction crews unexpectedly uncover in an area supposedly clear of significant archaeological resources. For example, Schiffer (1987: 353) mentions a survey that failed to record a large Hohokam site in Arizona. This led to enormous and unexpected mitigation costs ahead of construction of an aqueduct that could easily have been re-routed had the survey reported the site. Yet, another cost of such failure is the psychological and social impact to descendants and the community in general when a development project inadvertently exposes an unexpected cemetery site. In what follows, we will not attempt to grapple with the entire gamut of QA as it applies to archaeological surveys, which would have to include such topics as creating manuals and checklists, managing the quality of publications and reports, monitoring the condition of artifact collections, evaluating the effectiveness of sampling designs, and even improving the marketing of results for archaeological assessment firms. Instead, we focus on ways that the QA paradigm might apply to the design of archaeological surveys, especially by fieldwalking, the evaluation of completed surveys and the performance of field personnel, improvements to survey quality, and the mitigation of nonconformance to standards in survey. For the most part, this concerns those parts of the Quality System that deal with inspection and testing to verify that the product-in this case archaeological data-meets or exceeds expectations (ISO 9000, section 4.10-12, Paradis and Small 1996:41-49). As even this is a very broad topic, we will further focus on the important problem of survey coverage or, to put it another way, the probability that we have successfully detected the sites and artifacts that are potentially available to be found.",
          "In archaeological surveys, some very important, although by no means only, benchmarks of quality are a survey's ability to detect archaeological materials, classify them correctly, and adequately represent their character, density, and distribution. This ability in turn depends on the skill and attentiveness of field personnel, the quality of the survey design, the character of the archaeological materials, and characteristics of various Benvironmental^factors that affect surveyors' attention and ability to see artifacts (Banning et al. 2006(Banning et al. , 2011;;Hawkins et al. 2003). A number of archaeologists have called attention to the importance of ensuring survey effectiveness (e.g., Hirth 1978;Ives 1982;Miller 1989;Shott 1992;Sundstrom 1993;Zubrow 1984), yet this call has had relatively little impact on practice.",
          "Many jurisdictions have standards to which consulting archaeologists must adhere when undertaking archaeological survey of lands subject to impacts by development. In the USA, the Secretary of the Interior's Standards and Guidelines (SISG) serves as a basis that is modified or added to by individual states depending on local conditions (National Parks Service 1983). In Canada, standards are determined individually by province (e.g., Ontario Ministry of Tourism and Culture 2011). Both SISG and the Canadian systems define minimum qualifications of lead archaeologists (licensed archaeologists, permit holders, or principal investigators) and the minimum fieldwork standards to be met during survey. The SIGS qualifications for a professional archaeologist are to ensure that individuals leading projects are able to undertake complicated projects and carry them to fruition. These and other similar standards typically require license holders to have credentials, such as a graduate degree in archaeology, and minimum field experience and sometimes expect them to belong to an archaeological organization with a code of ethics. A number of jurisdictions have minimum requirements for field directors, but few have specific standards for the qualifications of field workers (Ontario Ministry of Tourism and Culture 2011; Georgia Council of Professional Archaeologists 2014; Tennessee Division of Archaeology 2009). Where it is mentioned, it appears that QA is the responsibility of the lead archaeologist. Minnesota, for example, requires the principal investigator to ensure that standards are met and notes that Bqualified personnel^are one of the elements that contribute to quality (Anfinson 2005). In all cases, these standards are based on experience and education; none of them pertain to demonstrated skill of an individual to carry out survey effectively. Many jurisdictions set standards for survey in terms of transect interval for pedestrian survey, acceptable amount of cover (by snow or vegetation) of the surface to be examined, test-pit interval, depth of test pits for shovel testing, and screen mesh size. Table 1 shows that these vary widely. In some cases, states prefer that archaeologists determine methods appropriate to the specific project (e.g., Tennessee Division of Archaeology 2009), but the norm is for state guidelines to specify minimum standards. While there is some uniformity in the size of shovel tests, the maximum interval for these ranges from 5 to 30 m, and not all jurisdictions state that screening is necessary. While some state guidelines indicate that at least 75 % of a surface should be visible for pedestrian survey, others indicate that a switch should be made to test pitting when only 10 % of the surface can be seen. Transect intervals for pedestrian survey range from 5 to 30 m. A number of states indicate that survey methods should be adapted to the specific conditions. In some cases, for example, they indicate that transect intervals should be tightened in high-probability areas (Anfinson 2005;Council of South Carolina Professional Archaeologists 2009). Standards in some jurisdictions, most notably in Europe, show recognition that geomorphological and other factors can make pedestrian survey, or even shovel testing, inadequate for site discovery and require a range of other techniques, such as coring or augering (e.g., Crombé and Verhegge 2015;Willems and Brandt 2004: 45). ",
          "A large number of factors can affect the effectiveness of any survey.",
          "The personnel are only one element of the survey design. Other aspects include the allocation of search effort to a given area of space (usually expressed in the spacing of transects in fieldwalking or of test pits or auger locations in subsurface survey) and the selection of spaces to search, whether by formal sampling, predictive modeling, or more judgmental designs. In current archaeological survey, the selection of transect intervals is largely arbitrary. From the QA point of view, the best that can be said is that surveys meet or somewhat exceed the requirements of state or provincial laws and guidelines for the spacing of fieldworkers. Yet, as noted above, these guidelines vary substantially, often allow intervals as high as 30 m, and, most importantly, are never based on empirical data on the relationship of transect interval to discovery probabilities of artifacts or intersection probabilities for sites. There is no reason why this should be so. From the QA perspective, surveyors should be able to assure their clients or academic audiences that they have selected transect intervals that not only meet regulatory requirements, but will have resulted in some minimum (and reasonably high) probability of detecting the kinds of archaeological resources purportedly of interest. At the very least, survey reports should provide realistic estimates of survey coverage that take into account both imperfect detection by surveyors and the spacing of transects, augers, or test pits. For test pitting, a number of archaeologists have emphasized that the probability that test pits will both intersect a site and detect its presence (on the basis of detected artifacts) depends on the area and spacing of test pits, the area, shape, and orientation of the site, the density of artifacts within the site, and whether the distribution of these artifacts is random or clustered (e.g., Banning 2002: 88-105;Nance 1979;1983;Shott 1985;1989). However, much the same issues are involved in fieldwalking as in subsurface testing. The spacing of transects or test pits is related to the density of search effort, which some archaeologists describe as survey intensity (Plog et al. 1978: 389). More generally, we can summarize search density for fieldwalking as the amount of time or total length of transects devoted to searching each unit of space (Koopman 1980: 74;Stone 1975: 25). The relationship between search time and discovery probability is not linear, and increasing search effort exhibits diminishing returns (Banning et al. 2006). From the quality perspective, we should allocate enough effort to searching each space to meet some standard for the probability of detecting targets of interest, yet we should also not squander search effort on one space that could be more economically used in another one. That is, additional search effort in the first space might only increase discovery probabilities by 5 or 6 %, because the search has reached a plateau in the detection function, while the same amount of search effort in a new space might yield a discovery probability of 50 or 60 %. Since the total time available for survey is often limited (a Bfixed-budget survey^), the allocation of search time has important implications and is arguably even more important for relatively costly test pitting than for fieldwalking. Somewhat analogous to transect or test-pit spacing, the mesh size of screens and amount of time devoted to screening sediments from subsurface testing have a nonlinear relationship to the probability of detecting artifacts in the sediment (Crombé and Verhegge 2015). This is an issue that deserves attention in a separate paper, but at the very least QA requires us to report on the quality characteristics of screening in some detail. A survey design's plan for selecting areas to be searched, including, where relevant, the rationale for sampling, is a critical aspect of survey quality. For sample surveys, it is necessary to provide justification for the sample size and assurance that the correct formulae for statistics have been used (especially for cluster and stratified sampling, Orton 2000: 99, 211-213) and, for stratified samples, the rationale for stratification and a post-survey evaluation of the stratification's effectiveness (i.e., are there statistically significant differences between the strata?). Personnel There are good reasons to expect that crew members' training, experience, motivation, health, attentiveness, and ability to see and recognize small objects on a varying background of soil surfaces and vegetation will vary over time and among individuals (Plog et al. 1978: 413-15). However, implicit in the reports of typical archaeological surveys is the assumption that all field crews had closely similar or identical detection abilities, some even suggesting, implicitly or explicitly, that within some specified range they detected 100 % of the artifacts exposed on the surface (this is the definite detection model; Koopman 1980:59;Banning 2002:57-59). We suggest that this assumption is, to say the least, over-optimistic, and our field experiments have shown that surveyors' detection of artifacts, even under rather ideal conditions, is both variable and far from perfect (Banning et al. 2006(Banning et al. , 2011;;Hawkins et al. 2003). If we are to take the results of surveys seriously, especially in their claims for the absence of archaeological remains, we should have some assurance that they have taken surveyors' detection abilities into account. Such assurance requires both training of crew members and periodic assessment of their detection abilities under a variety of conditions. Artifact Obtrusiveness The characteristics of the artifacts we are trying to detect also affect our ability to detect them. For several decades, archaeologists have summarized this effect as obtrusiveness (Schiffer et al. 1978: 6). Research by us (Banning et al. 2006(Banning et al. , 2011) ) and others (e.g., Schon 2002;Wandsnider and Camilli 1992) has confirmed the intuitive expectation that, holding other conditions constant, the size, shape, color, and reflectivity of artifacts, especially as these contrast with their environment, create substantial variation in their detectability. For example, fairly bright and glossy sherds of glazed porcelain or glass are quite easy to detect on a variety of backgrounds, while mottled grey flakes of chert are quite difficult to see, especially on stony backgrounds. Since different kinds of archaeological resources vary in the kinds and diversity of remains that signal their presence, and these remains themselves vary in detectability, we should expect claims about the density and distribution of different kinds of archaeological sites to take artifact obtrusiveness into account. For example, we should be wary of claims for a particular ratio of Early Woodland sites to historic ones in the American Northeast if these claims did not take into account that the kinds of artifacts expected on an Early Woodland site are quite unobtrusive, while those on historic sites can include bright porcelain, reflective glass fragments, and even chunks of brick. In addition, aside from the obtrusiveness of individual artifacts, their distribution can also have an impact on their probability of detection (Krakker et al. 1983). In some cases, higher degrees of artifact clustering can result in greater artifact detection than in cases where the same kinds of artifacts are distributed randomly or relatively evenly. Since archaeologists are often interested in discovering precisely these highly clustered manifestations of artifacts, this can be advantageous. However, we also need to be aware of the potential bias that can result when archaeological resources that are more likely to exhibit high degrees of clustering are over-represented relative to less clustered remains. The tendency for surveyors to pay closer attention once they find even one artifact has the potential to exacerbate this bias. Visibility In addition, variations in surface texture and color, vegetation cover, lighting, rainfall, plowing, and other environmental conditions can have profound impacts on the detection even of artifacts that are exposed at the surface, not to mention those that may be buried to various depths by a number of geological and cultural processes (Schiffer et al. 1978: 6;Banning 2002: 46-48;Fanning and Holdaway 2004;Stark and Geraty 2008). Assurance of the quality of an archaeological survey and its reports requires analysis of how these factors may have impeded artifact detection and their potential impacts on the apparent distributions of archaeological materials. A number of archaeological studies have emphasized the effects of rainfall (Shott 1995;Shott et al. 2002) and plowing (e.g., Ammerman 1985;Clark and Schofield 1997). As noted above, some jurisdictions' standards and guidelines for archaeological survey do require reporting of fieldwork conditions, rainfall, vegetation cover, and other factors that affect visibility, yet subsequent research and predictive models that employ the data from surveys do not always take these factors into account.",
          "An extremely important aspect of archaeological survey, too often overlooked, is the evaluation of its effectiveness. That evaluation is a critical aspect of QA and should apply to all of the aspects discussed in the last section. Since space is limited, here we focus on sweep width and coverage as the best way to summarize the interacting factors of surveyor abilities, visibility, and artifact obtrusiveness.",
          "Some survey practictioners have attempted to evaluate shortcomings in surveys by reference to one or two of the factors that can affect detection, most usually focusing on visibility in pedestrian survey (e.g., Stark and Geraty 2008;Terrenato 2000), while others focus on inter-surveyor differences, especially for neighboring surveyors in parallel transects (e.g., Hawkins et al. 2003;cf. Gnaden and Holdaway 2000). But the quality of a survey depends on all these characteristics-survey personnel, artifacts, and environment-and they interact with one another. Consequently, it is most practical to summarize these effects simultaneously. Two basic ways to accomplish this are resurvey and a calibration approach. These are applicable to any survey method, but here we focus particularly on pedestrian survey or Bfieldwalking.T",
          "Any QA program requires periodic tests and audits to ensure compliance with standards, assess the effectiveness of designs and reliability of results, and provide data with which to improve those results. Typically, such tests are carried out by specially trained inspectors, who may observe people at work, make measurements on and statistical analyses of samples of their output, or both. One of the most obvious ways to test the effectiveness of archaeological surveys is an example of this approach and involves resurveying a sample of the spaces that have already been subject to survey. A number of survey projects have employed this strategy. An early example of this was the 1977-78 East Hampshire Survey, which detected marked differences in results when resurveying a sample of four fields (Shennan 1985: 44). Hirth (1978) used survey of the same spaces in three different seasons to evaluate the effects of rainfall and agricultural practices on the detection of artifacts in the Basin of Mexico. Wandsnider and colleagues used resurvey as one component in their evaluation of surveys in the Green River Basin, Wyoming, the Mesilla Bolson, and the Sandilla Mountains, both in New Mexico (Wandsnider and Camilli 1992;Wandsnider and Ebert 1986). In an example that one of us undertook in Cyprus in the 1980s and 1990s, there were also selective examples of resurvey to test survey effectiveness (Stewart 2006). To check on the accuracy of site and find-spot locations, and to assess changing site and artifact visibility in the field, the team resurveyed a number of units from two survey projects (Stewart 2006:85-86, 103-104, 106, 111, 146-147;Stewart and Morden 2016). Initial analysis of the overall numbers of artifacts within a survey unit or across the study area suggests no differences between the original and second surveys. However, when we consider the location of finds within the area, it turns out that findspots from the resurvey were never in the same location as those from the initial survey. Although the crews collected some artifacts on the first survey, they did not attempt to collect all artifacts so, theoretically, the findspots located on the first survey should still have been detectable on the resurvey. The mostly likely explanation for this marked difference is the survey's use of sinusoidal, rather than straight, transects, which made it impossible for the resurvey to duplicate exactly the pathways of the initial survey. Broadly speaking, Burger et al.'s (2004;2006) Bnested-intensity sampling^is also an example of the resurvey approach. Using a variation of the Whittaker multi-scale sampling plot, they resurvey a sample of spaces at increasingly smaller resolutions and intensities, including even crawling on hands and knees, and excavation and screening of the top 10 cm of small spaces. Comparison of the results at these different levels of intensity allows estimates of how much the less intensive survey has missed. In a particularly interesting case, Almagro-Gorbea et al. (1996;2002) attempted to ensure reliability of survey in the Comunidad de Madrid by using an independent Bcontrol survey^of a sample of the areas that had already been surveyed. Originally, this was a Bblind but directed^sample (Almagro et al. 1996:252), but the authors later realized that, for the quality control itself to be valid, it needed to be a random sample (Almagro et al. 2002: 47). In its final form, their method involved comparing the initial and Bcontrol^survey statistically, in terms of their classification of survey spaces as Bsites^and Bnonsites,^using an estimate of the actual number of sites and nonsites, and thus really evaluates the degree of agreement between the two surveys. This project comes closest among those we have seen to the formal principles of QA and involved use of a military standard for inspection sampling to decide how many units required resurvey. From the quality perspective, the survey-resurvey approach works best when the resurvey is always by the same, highly experienced, and highly skilled team, whose members were not involved in the original survey. In an attempt to check on the reliability of surface collections from sites, Shott et al. (2002) re-collected an Oneota site in Iowa that had already been surveyed the previous year and compared the results. They concluded that the materials collected during resurvey differed significantly from those from the original survey and attributed a good deal of the difference to rainfall. As they note in the paper, Barchaeologists rarely allow for the effects of rainfall amount when interpreting survey results^ (Shott et al. 2002: 180). While the survey-resurvey strategy is clearly much better than no testing at all, it does have several disadvantages. First, the resurvey is not always by a team of experts, and not even always by the same team. Consequently, we cannot be certain that the second survey is any more reliable or accurate than the first, and it may not be justified to take comfort even in a high degree of agreement between the two surveys. Second, even if the resurvey is by our best, most reliable surveyors, it is not plausible to assume that their results will be perfect. Indeed, our experiments have shown that even highly skilled and experienced surveyors only detect a fraction of the artifacts exposed on the surface under typical conditions. Third, since the resurvey strategy usually operates in the absence of knowledge of the actual distribution of artifacts, we cannot assess the real detection rates of any of the surveyors, including the most skilled ones, but can only assess their relative abilities. Fourth, the resurvey strategy is highly vulnerable to other threats to validity. It is an example of the test-retest research design and thus can suffer from the effects of history and maturation (Campbell and Stanley 1963: 5). That is, ground conditions, weather conditions, or some other relevant factor could have changed during the interval between the first and second surveys, making them not strictly comparable. This is especially likely in the event that the first survey removed significant numbers of artifacts from the surface, when surveys were in different seasons (Hirth 1978) or, as Shott et al. (2002) point out, when there were significant differences in rainfall. Only a few examples of archaeological re-survey have employed controls that help mitigate some of these differences. One method for control is for the initial surveyors to seed survey areas with a known number of artifacts that are similar in some respects to the target artifacts, yet clearly identifiable as nonarchaeological (Wandsnider and Camilli 1992: 173-76). The proportion of these that the resurvey team discovers serves as a means to estimate their survey effectiveness, rather than just assuming that the resurvey team is perfect.",
          "Our preferred methods for assessing and improving the quality of archaeological surveys depend on tests of surveyors' abilities to detect a variety of artifact types under a variety of controlled but realistic field conditions. To ensure that we know in advance what the population of artifacts is like, we carry out this assessment on a series of test fields, which we have seeded with a variety of artifacts in known locations (cf. Schon 2002;Wandsnider and Camilli 1992). To control for maturation and history, we assess a group of surveyors repeatedly over the course of a survey project to produce Baverage^results. To control for the effects of range, we measure the variation in artifact recovery at different distances from the transect lines. To control for search speed, we ask surveyors to walk at a speed that mirrors their typical speed on Breal^survey. Effective sweep width (W) is the single most useful measure of survey effectiveness because it summarizes all of the variables that affect survey detection in a single number, with the proviso that we either need to measure it separately for each artifact type or make some assumptions about the likely mixture of artifact types. Where possible, it is better to take the former approach, since we rarely have much a priori information about the likely proportions of artifacts to expect. In the example below, however, we compromise by grouping artifacts into two broad categories, lithics and pottery, and omit very small artifacts, such as microliths, that are very difficult to detect. One reason for this is that it improves our sample size, as we have found that our data sets for individual artifact classes are sometimes too sparse for really good estimates of sweep width. On the basis of data from controlled calibration transects, described below, we can calculate sweep widths from the exponential detection function where p(r) is the probability of detection at range r in meters, b is the y-intercept (expected detection probability right on the transect, where r = 0), e is the exponential constant (approximately 2.718), k is a constant that summarizes the effects of the various contributors to detectability per square meter, r is the range or perpendicular distance away from the transect line in meters, and kr 2 describes the steepness of falloff in detectability away from that line (Koopman 1980: 64;Banning et al. 2011: 3448). This function typically describes an S-shaped curve of declining probability away from the transect line (Fig. 1). Sweep width in meters is equal to the area under this curve (or double the area if we only consider positive values of range). The reason that this area is in meters (not square meters) is that the y-value is a probability, with no units at all. This is clear from the fact that the sweep width corresponds with a box (Fig. 1) whose height is 1.0 (the probability), whose width is W (in meters), and whose area, in the figure, is 2(a + b).",
          "), whereby we expect to find all the artifacts within W/2 of the transect line, but none at all beyond that range. In the more realistic exponential model described by the curve, the area under the curve, likewise, has an area of 2(a + b). Consequently, the box describes how many artifacts we can expect to detect as though we found all of them within W/2 of the transect line, when in reality we found 2b artifacts within W/2 and 2a artifacts outside W/2. What this means, and makes sweep width useful and intuitively attractive, is that we expect to find the same number of artifacts as we would if we had perfect detection within the sweep width (Fig. 2). Keep in mind that our sweep width and the exponential detection function on which it is based only apply to the artifacts that are potentially visible on the surface, when we are using fieldwalking and visual inspection as our search method. We would have a very different sweep width if we were employing some other method, such as magnetic survey, augering, or test pitting, and wanted to consider buried artifacts, but in principal the sweep width measure is applicable to all these methods. 0 0 . 5 -1.0 1.0 -0.5 1.5 Range (W) 1.0 -1.5 a a a a b b Fig. 1 Detection functions and the definition of sweep width (W). The rectangle defines the definite detection model, with perfect detection within W/2 of the transect line. With a width of W m and a height of 1.0 (with no units), the area of this box is W m = 2(a + b). The curve is an example of a more realistic exponential model, with 2b detections within W/2 and 2a detections outside that range, in the tails. Consequently, the area under the curve is also 2(a + b) = W m",
          "",
          "To demonstrate the operation and results of such calibrations, and their use in the planning and evaluation of actual surveys, here we use data from the Wadi Quseiba Survey of 2012-13 in northern Jordan and the 2014 Tremithos Valley Survey in southwestern Cyprus. The main objective of the Wadi Quseiba Survey was to discover late prehistoric (Epipalaeolithic, and Chalcolithic) sites, which are commonly underrepresented in Jordanian surveys because of erosion, colluviation, the rarity and poor preservation of Neolithic pottery (cf. Bintliff et al. 1999), and sometimes the shortage of lithics expertise on survey teams. In an attempt to maximize the recovery of such sites, we employed a Bayesian algorithm for allocating search effort. Shortage of space precludes detailed discussion of this approach here, but suffice to say that it involves allocating new search effort on the basis of a predictive model that is continually updated in light of information on results, including coverage, as survey progresses (Hitchings et al. 2013). The Tremithos Neolithic Survey Project is a pedestrian survey of sections of the Tremithos River Valley, running southeast from the Troodos foothills to the sea, in south-central Cyprus (Stewart n.d.). The goal of the project is to identify early Neolithic use of the valley to access the resource-rich areas in the Troodos foothills, particularly their abundant and high-quality chert sources, and whether this river system provided a transportation route from the sea to this central area. As in the Wadi Quseiba Survey, the Tremithos Neolithic Survey Project has employed Bayesian optimal allocation of survey effort, making it critical to estimate coverage and to update these estimates daily.",
          "In contrast to our previous simulations that employed a grid of strings as the test fields for Bseeded^artifacts (Banning et al. 2006(Banning et al. , 2011)), to calibrate these surveys we experimented with test fields without a formal grid so as to be more similar to actual survey conditions, but that appeared to be devoid of ancient artifacts. In Jordan, test fields were set up in pasture that had a mixture of bare rock and patchy vegetation, a plowed guava field, a plowed olive grove, and a Bmixed^field that had nearly equal portions of plowed field with some stubble and sparse trees, pasture with denser trees, and pasture with bare rock and sparse shrubs (Fig. 3a-d). In Cyprus, the test fields included a bare, plowed field and a field with wheat stubble (Fig. 3e-f). In each test field, we laid out 50-m tapes to mark the path of a transect 120 or 150 m long, in some cases needing to bend this transect once or twice to avoid obstacles or allow for the shape of the field, and planted artifacts at locations randomly assigned to various locations left and right of the tape at distances up to 10 m (in the case of the Bpasture^) or 20 m. Artifacts included the categories of large and small lithic flakes and large and small ceramic sherds in several colors, mostly Bred^and Byellow^and somewhat similar to those anticipated in survey. However, for the purposes of this paper, we group the artifacts into all lithics and all pottery, whose sizes range from about 4 to 12 cm in greatest length. Initially, the calibrations also included very small lithics (microliths), but surveyors never detected them and we exclude them from the examples here. Each member of the survey team walked along the tape multiple times over the course of several hours on each occasion that we conducted a calibration. Using a form on an iPad, on each attempt the surveyor marked his or her start and finish times and listed (but did not collect) each artifact seen along with its artifact category, distance along the 50 m tape, and, because there was no grid, estimated distance to left or right. We kept the density of seeded artifacts low enough for us to be reasonably certain of our identification of Bsuccessful^detections (as opposed to Bfalse targets^), even given the imprecision with which surveyors sometimes estimated the distances to artifacts they saw. Generally, these estimates were accurate and precise at close range, but we allowed for errors of up to 2 m in any direction at ranges of 15 to 20 m, and 1 m in any direction between 5 and 15 m. By checking the data for all the transects and all crew members against the known locations of seeded artifacts, we are able to tabulate the number of successful detections by range and use the data to fit to a curve for the detection function (1) using nonlinear regression in either the SPSS ™ or R platform to provide robust estimates of b and k using guesses of their values at the beginning of a constrained (0 ≤ b ≤ 1.0) model with estimation by sequential quadratic programming. We then use a numerical integration function in R to plot curves with these values and estimate the areas under the curves between ranges of -100 and 100 m to obtain sweep widths (Figs. 4 and 2). As the curves and Table 2 make clear, there is considerable variation with type of field. Although small sample size may be a factor, the Bbest^sweep widths are in the pasture and the plowed olive grove in Jordan, where the sweep widths for lithics are on the order of 6-7 m and those for pottery are more than 4 m. The guava orchard, despite also being plowed, has noticeably poorer visibility. This probably has less to do with the plowing (although plowing was coarser, with deeper furrows than in the olive grove) than with the spacing and leafiness of the trees, which were young and had low branches. The very low value of b for lithics on the stubble field in Cyprus and the depressed shape of the curve close to the y-axis compared with that for lithics in other locations can best be explained by our calibration methods. We chose to randomize locations of seeded artifacts, and, in this case, no lithics were seeded within 2 m of the transect and the resulting curve was based on artifacts observed at distances of 3 m and more. Consequently, we recommend, rather than using a completely random distribution, to use a uniform distribution with range and only randomize with distance along the transect. We should not generalize too much from these results, since of course they depend in part on the abilities of the particular crew members involved, and not just on visibility or artifact obtrusiveness. We should also consider these somewhat optimistic estimates, since the artifacts were fresh and clean, rather than half-buried or dirty, and a few crew members appear to have walked a bit more slowly than they would normally 0 2 4 6 8 1 0 r 1.0 0.8 0.6 0.4 0.2 p(r) a b 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) c d e f 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r p(r) 0 2 4 6 8 1 0 1.0 0.8 0.6 0.4 0.2 r 0 2 4 6 8 1 0 r p(r) 1.0 0.8 0.6 0.4 0.2 p(r) Fig. 4 Detection functions for all lithics (solid curve) and all pottery (dashed curve) along with half the corresponding sweep widths (vertical lines) at the calibration sites a pasture, b olive grove, c guava orchard, d mixed field, all in Jordan, e stubble field, and f plowed field in Cyprus. p(r) is Bprobability at range r^and r is range in meters do. However, they did provide us with some empirical data with which to assess our survey effectiveness, beginning with very preliminary estimates of sweep width (after only day of calibration) and leading up to the estimates provided here. Since the sweep widths for different artifact types differ, we were Bconservative^by using the lesser of the lithic or pottery one. In addition, because visibility conditions in our actual surveys could only be approximately matched with the test fields, we adjusted our estimates slightly upward or downward from the estimates derived from the most similar test field, again attempting to be conservative by risking under-estimate of sweep width rather than over-estimate. However, because we have photographs of every transect segment in our database, it remains possible to gauge these estimates with image analysis at some later date, a method we will leave to another paper.",
          "During the surveys in both Jordan and Cyprus, we used the sweep widths in conjunction with other information to help us allocate our survey effort each day through an iteratively updated predictive model. GPS coordinates for the beginnings and ends of transects walked allowed us to estimate the lengths of transects. Multiplying these lengths by our estimated sweep widths yields the total area Bswept^by our transects in each survey unit. Dividing area swept by the total area of the survey unit tells us the coverage of that unit. This estimate of coverage is the key for us to reassess the probability that any survey unit (or Bpolygon^in our usage) might still contain undetected resources of interest. Because the kinds of sites that interest us tend to have very low artifact densities that make clustering not very evident, we did not explicitly take clustering into account but at each iteration of the predictive model recalculated the probability that each polygon might contain an undetected site of interest given the total amount of coverage to date. Thus, when we surveyed a polygon without finding identifiable late prehistoric materials, the polygon's probability went down, but not necessarily by very much. Applying our allocation algorithm on the next iteration of the predictive model, it might easily lead us to survey that polygon again, and, indeed, we surveyed some polygons several times before finding anything of interest (Fig. 5; Hitchings et al. 2013). Note that in these instances we varied the orientation of transects to allow viewing of the ground from different angles and reduce redundancy in coverage (Banning 2002: 90-91;Koopman 1980: 218).",
          "In addition, the final coverage estimates provide our final assessment of how thorough our surveys were in both Wadi Quseiba and the Tremithos Valley, allowing other researchers to assess which parts of these survey regions, if any, might still repay further fieldwork. One way to express the overall quality of the survey's coverage is to plot the coverage values on an Bexhaustion map,^which displays our final estimate of how thoroughly each space in the survey region has been searched (Fig. 6). Exhaustion, for the purposes of this map, expresses the probability that there could still be undetected archaeological resources in a particular space, given the total amount of coverage that the survey has completed. Exhaustion maps are particularly useful when a survey has only covered a portion of the survey region. When, for example, the survey has only intensively examined a sample of the region's spaces, it is important to show exactly where survey took place and where it did not, so as not to give the misleading impression that unsurveyed spaces are devoid of archaeological materials. Reports should also provide justification for differential allocation of search effort, and sometimes tests to ensure that these differences did not bias the results in unintended ways. For particular spaces in the survey region, another approach for assessing survey thoroughness is to determine whether the detection function has begun to level off with increases in search effort. Using the examples of polygons 229 and 240 in the Tremithos Valley survey, we can see how the cumulative numbers of lithic finds increase over three increments of survey effort (Fig. 7). This graph would likely level off sooner had we been collecting or flagging all artifacts as they were found, but we only collected a sample, so some of the artifacts found in the second and third increments of survey could be ones that the first transects already identified. Even so, once this curve begins to level off, it is usually more effective to move onto a different space that has not received as much survey effort (Banning et al. 2006).",
          "Not only do the tests described in the previous section allow us to assess the quality of present and past surveys, they also provide information with which we can better design future ones.  ",
          "An excellent example of this is our ability to use the range functions to help us select appropriate transect intervals. The range functions allow us to estimate, within error margins, what transect interval is necessary to result in recovery of some proportion of the artifacts exposed in each transect. Effective visibility (E), for example, is the conventional term for half the interval that we would expect to result in detection of 50 % of the targets by parallel transects (Koopman 1980: 76). In some circumstances, finding half the artifacts might be adequate for achieving the goals of a research project or heritage assessment. For example, blue transfer ware might be found in high enough densities at typical nineteenth-century farmsteads in southern Ontario that only finding half of them is still adequate for the identification of these farms. In others, however, we might require better recovery than this and could calculate the appropriate interval for discovery of perhaps 80 % of the artifacts exposed on the surface or even use a transect spacing of W, which has an expected yield of 100 % of the artifacts. In still others, even 50 % might be a hopelessly unrealistic or costly goal, and we might have to settle for, say 20 %. Furthermore, the recovery of different artifact types varies, requiring us to prioritize their use in deciding transect intervals. In any of these cases, we can assure clients or academic audiences that we have met some predetermined level of artifact recovery, which constitutes the standard for the project or assessment firm.",
          "Members of field crews can vary substantially in their overall detection abilities as well as in which artifact types they are most effective at detecting (Bintliff et al. 1999: 153;Stewart 2006: 140-144). This has a number of implications for our use of these personnel in future surveys, and, although here we have only considered the combined or average abilities of crews, it is possible to use our methods to assess individual performance as well. In some cases, repeated testing might show that some crew members, despite further training, simply do not have the knack for artifact detection during fieldwalking. It might therefore make sense to reassign them to other tasks, such as test pitting, excavation, or lab work, where their talents are more suited. In most cases, however, the tests will identify weaknesses in surveyors' detection of certain artifact types that training or relatively simple changes in behavior can mitigate. Increasing their familiarity with artifact types that they tend to overlook, perhaps by having them spend some time in processing or helping to analyse them in the lab, is one way to improve their recognition of these types. Repeated practice survey on a test field seeded with this artifact type, or on an archaeological scatter that includes a good deal of this type of material, will also likely improve their detection of more difficult artifact types. In a few cases, something as simple as updating a prescription for corrective lenses or removing sunglasses may make the necessary difference. Even after training and repeated testing, it is still likely that some surveyors will be better at detecting, say, lithics, while others are better at detecting nails or pottery. This has important implications for the mix of surveyors on field crews. Doubtless, a crew whose members complement one another's strengths and weaknesses will have greater overall success at detecting a range of materials and site types than one that simply reinforces one kind of strength at the expense of others (Hawkins et al. 2003;Stewart 2006: 144-147). Consequently, survey managers should make crew assignments that tend to maximize the overall probability of detecting a range of artifact types that are likely to be important to the survey's goals. In the Wadi Quseiba and Tremithos Valley surveys, we only used sweep widths for the aggregate data of entire field crews, providing an average assessment of survey quality that is adequate for estimating overall survey coverage by those crews. This allowed us to have a reasonable sample size with which to fit the detection functions, although the raw data do indeed show considerable variation among surveyors. Where the composition of field crews varies substantially over the course of a survey, or there are other reasons to measure individuals' sweep widths, it is necessary to ensure an adequate sample size of calibration transects for each surveyor.",
          "The reputability and professionalism of archaeological surveys depend, in part, on their ability to convince us that spaces in which they report a lack of archaeological materials are actually devoid of such materials. They also depend on the ability to characterize the existing materials correctly in terms of their type, density, and distribution. Consequently, ensuring the quality of archaeological surveys requires both careful attention to design and periodic testing of surveyors and survey procedures to determine their effectiveness and to strive for improvement. Periodic assessments of crews' effectiveness at artifact detection, using real artifacts under fairly realistic conditions, provide the data with which to accomplish these goals. In some regions, these assessments would need to be repeated in different seasons or be calibrated to recent rainfall amounts, but the key is to conduct them under conditions that are as close as possible to those of the actual survey. In surveys in northern Jordan and central Cyprus, where it virtually never rains during seasons when surveys typically occur, our calibration transects were across spaces with realistic field conditions. We had seeded these with modern artifacts similar to the kinds of artifacts we expected to find, which allowed us to estimate average sweep widths for the field crews. Multiplying the transect lengths by these sweep widths allows us to calculate total area swept in each survey space or Bpolygon,^and dividing this by the polygon's area provides a reliable measure of coverage. These coverage estimates themselves provide a measure of survey thoroughness and thus one of the most important aspects of quality in survey. Using these coverage estimates in a Bayesian modeling environment further allows us to update the probabilities that each space in our survey frame might contain undiscovered archaeological materials, which we use both for allocating further survey effort and as another measure of the thoroughness of survey. The result, we argue, provides a much stronger empirical basis for inferences about the distributions and character of regional archaeological resources."
        ],
        "ground_truth_definitions": {
          "Quality Assurance": {
            "definition": "policies, procedures, manuals, standards, and systems, arranged with the goal of ensuring and continually improving the quality of products or services and consumers’ satisfaction with them",
            "context": "In the USA, standards for quality are found in ISO9000 and ISO9001, and ANSI/ASQC Q91-87 through Q94-87, while the Canadian equivalents are CSA Z299.0 and CSA Z299.1/.4, and those in the United Kingdom are BS 4891 and BS 5750. Briefly, QA involves policies, procedures, manuals, standards, and systems, arranged with the goal of ensuring and continually improving the quality of products or services and consumers’ satisfaction with them. Unfortunately for archaeologists, most of the literature on Quality Assurance and Management concerns manufacturing and service industries.",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "Rosen_gjcpi_2018_3",
        "sections": [
          "Humanity and societies today face challenges to sustainability, which fundamentally relates to the ability to sustain humanity, civilizations and ecosystems on Earth. These challenges are expected to become more significant in the future. Achieving sustainability is therefore one of the most important objectives of a society and its people. The issues and concerns embodied by sustainability are broad and numerous. They cover such diverse issues as resource supply (water, energy, mineral, food), climate change and pollution, management of wastes (toxic, hazardous, radioactive, convention- ISSN 2283-7949 GLOCALISM: JOURNAL OF CULTURE, POLITICS AND INNOVATION 2018, 3, DOI: 10.12893/gjcpi.2018.3.4 Published online by \"Globus et Locus\" at www.glocalismjournal.net  al), sanitation, land use and desertification, species extinction and loss of biodiversity, ecosystem degradation, water quality and drought, industrial development, production and consumption patterns, population growth, urbanization, globalization, cultural and social sustainability, natural and anthropogenic disasters, peace and stability and government policies. The breadth of topics related to sustainability suggests that a holistic and comprehensive approach to sustainability is necessary. Sustainability activities are increasingly becoming part of the agendas and operating plans of governments and businesses. The elements of sustainability need to be connected if activities are to attain sustainability goals. In the remainder of this chapter, the historical context for sustainability is briefly described, sustainability concepts and definitions are reviewed, and sustainability is discussed, focusing on its economic, environmental and social dimensions as well as issues and concerns. The related concept of sustainable development is examined. Then, assessment measures for sustainability are considered, and some sustainability applications are presented.",
          "The challenge of achieving sustainability for societies is not limited to the present day, even though the global nature of the problem has come to the fore in recent decades. Sustainability was an objective, at least implicitly, for early civilizations. Since humans transitioned from mobile hunter-gatherers to agriculture and settlements approximately 12,000 years ago during the Neolithic age, in what has come to be known as the agricultural revolution, the sustainability of the lifestyle was crucial in determining if society would thrive or collapse. The decline of the Western Roman Empire constitutes another noteworthy historical illustration of a society that failed to sustain itself and ultimately collapsed. Waning returns from natural resource production as well as social factors contributed to the decline (Tainter 1998). The multidimensional nature of sustainability became more evident in more recent history, especially with concerns over environmental degradation. ISSN 2283-7949 GLOCALISM: JOURNAL OF CULTURE, POLITICS AND INNOVATION 2018, 3, DOI: 10.12893/gjcpi.2018.3.4 Published online by \"Globus et Locus\" at www.glocalismjournal.net  The case of the Polynesians on Easter Island, located in the Pacific Ocean, provides an important historical illustration of how unsustainable practices and lifestyles can contribute or lead to societal collapse. The inhabitants of Easter Island exhausted the resources of their remote habitat to such an extent that they could no longer feed themselves or even build canoes to escape. The ecological damage to Easter Island led to a total societal collapse that decimated its population compared to its peak in the 1600s (Graedel, Allenby 2010).",
          "Numerous definitions of sustainability exist, but none apply for all circumstances. Making sustainability operational rather than vague and theoretical is a challenge, which is made all the more daunting due to the numerous definitions of sustainability. In theory, sustainability can be defined as \"enduring in perpetuity\". However, such a definition is neither practical nor useful. Little if anything can be sustained forever, while much can be sustained over the very short term. A timescale of 50 to 100 years, representing two to four generations, is often viewed as more practical. A simple definition sustainability was stated by Ehrenfeld, who defined sustainability as \"the possibility that human and other forms of life will flourish on the planet forever\". Although this definition incorporates environmental and societal aspects, the timescale is impractical. Taking a more technical perspective, sustainability can be defined in terms of carrying capacity, which indicates the maximum number of people that can be supported in a given area, accounting for availability of resources and the ability of the environment to accept waste emissions. The carrying capacity for a region is in particular dependent on the demand and supply of natural resources. But sustainability encompasses more than technical factors and cannot be defined from an environmental perspective exclusively. Rather, sustainability is often conceptually defined as having three dimensions: environmental, economic and social. An important feature of this approach is the extension of sustainability beyond carrying capacity to include economic and social fac- tors. A multidimensional view of sustainability is consistent with the understanding of how several factors affect whether societies thrive or decline. But these three dimensions are often in tension (e.g., environmental and social sustainability may be achieved to the detriment of economic sustainability). Achieving a sustainable balance is challenging. Sustainability, and its three main dimensions, can be conceptually illustrated in various ways: a) Sustainability can be viewed as founded or supported by three legs or pillars, representing environmental, economic and social sustainability (fig. 1). This is akin to a three-leg table that can only remain standing if environmental, economic and social sustainability dimensions are satisfied. This balance can prove challenging, since the sustainability dimensions are often in conflict; b) Sustainability can also can be thought of as the common point of overlap of three intersecting circles, where the three circles represent the environmental, economic and social dimensions of sustainability (fig. 2); c) A variation on this representation displays sustainability using concentric circles (fig. 3). Economic sustainability, being an activity of people and occurring within communities, is seen to be a subset of social sustainability, which involves many social and cultural dimensions as well as economic factors. Then social sustainability is seen to be a subset of environmental sustainability, since activities of people and societies are carried out within the environment and thus threaten environmental sustainability; d) The three dimensions of sustainability can be also shown in a hierarchal structure, where the economy is a subsystem of society, and society is embedded in the environment (fig. 4). These illustrations are similar, but have subtle differences. All of the illustrations demonstrate the interconnected nature of the environment, the economy and society, and their relations to and effects on sustainability for any a system or process. The above points also demonstrate the truly multidisciplinary nature of sustainability, and its linkages to diverse fields such as science, engineering, environment, ecology, economics, business, sociology and philosophy. Addressing sustainability requires consideration of topics such as resource use (energy and material), economic development, social development, health, environmental stewardship, engineering, design and architecture. It also requires an understanding of how people in the different disciplines interact and relate in addressing these factors.     Sustainability concepts can be focused around a discipline when needed or useful. For example, consider sustainability in engineering, which constitutes the application of science and mathematics principles for practical purposes such as the design, manufacturing and operation of products and processes, while accounting for economic, environmental, sociological and other constraints. Engineering significantly affects economic development, standards of living, societal well-being, the environment and cultural development. Numerous factors need to be addressed in moving towards engineering sustainability (fig. 5), including appropriate selection of resources bearing in mind sustainability criteria, use of sustainable engineering processes, enhancement of efficiency of engineering processes and resource use, and holistic adoption of environmental stewardship in engineering activities. In moving towards engineering sustainability, other key sustainability measures also need to be addressed, such as economics, equity, land use, lifestyle, sociopolitical factors and population. The multidimensional nature of sustainability suggests that achieving it is a systems challenge. That is because the overall in- teraction between subsystems needs to be integrated to achieve a sustainable system or process. For clarity, the three main dimensions of sustainability are each examined below.",
          "The economy and society are subsystems of the environment, which is the source and sink of all material and energy interactions on Earth. The sustainability of humanity implies ensuring an ability of the Earth to support human and activities related to it. Human economies and populations have grown such that anthropogenic activities now have global and long-term impacts, with various consequences. These can degrade the ability of the planet to support life. Many environmental issues affect sustainability. The loss of biodiversity throughout the world, due to economic development and other factors, and threatens the sustainability. Deforestation and development often lead to the destruction of natural habitats and cause the migration of animals to suitable environs. The extraction or use of limited resources and the release into the environment of emissions and wastes to the air, water and land also put sustainability at risk (Aghbashlo, Rosen 2018). Air pollutants released by internal combustion engines in vehicles, fossil fuel-fired electrical power plants and various industrial activities can adversely affect air quality and the health of humans and other living species. Industrial liquid emissions, including wastewater, and runoff from agricultural activity can lead to environmental problems, e.g., bioaccumulation of toxic compounds in the cells of aquatic animals, and eutrophication of water bodies. Two of the most significant challenges regarding environmental sustainability follow: Climate change. Stabilizing the concentrations of greenhouse gases (GHGs) in the atmosphere, in order to prevent the harmful effects of global warming and climate change, constitutes according to most studies one of today's most significant challenges. In the atmosphere, GHGs absorb infrared radiation emitted at the surface of the Earth. This leads to a greenhouse effect and the associated planetary warming. The primary GHG is carbon dioxide (CO 2 ), but there are others such as methane (CH 4 ) and nitrous oxide (N 2 O). The main anthropogenic sources of GHG emissions include fossil fuel combustion, agricultural nitrogen utilization, and enteric fermentation in ruminant animals. Global warming and the risks of climate destabilization are exacerbated by positive feedback effects, such as increased solar radiation absorption from the loss of reflecting surfaces like ice. The Intergovernmental Panel on Climate Change (IPCC) has since 1990 published comprehensive assessment reports reviewing the latest climate science and predictions about future trends. The fifth assessment report, finalized in 2014, reports warming trends due to anthropogenic activities as \"very likely\". As climate models become more sophisticated, the predicted amount of warming has increased, as have the related climate and other impacts. Much effort globally and at the United Nations is focused on achieving international agreements or treaties to stabilize greenhouse gas concentrations in the atmosphere at a level that avoids dangerous anthropogenic climate change (e.g. Berthiaume, Rosen 2017). Stratospheric ozone depletion. The layer of ozone (O 3 ) in the stratosphere reduces the transmission of solar radiation to the surface of the Earth. This stratospheric ozone absorbs ultraviolet radiation, particularly UV-B, which can harm animals and plants. Declines in ozone levels in the stratosphere, especially over polar regions, were observed since the late 1970s. The main ozone depleting substances are chlorofluorocarbons (CFCs), used for decades as refrigerants, which participate in ozone-destroying chemical reactions in the stratosphere. A phase out of CFCs was agreed to in 1989 via the Montreal Protocol, but they are still used in low-income countries. The long residence times of CFCs in the atmosphere implies that the reduced ozone layer concentrations will persist for many decades.",
          "An economy that provides good living standards, the services that people require and jobs is necessary for sustainability in society. A sustainable society requires long-term economic development rather than just economic growth. This latter, often meas- ured as growth in gross domestic product, is what occurs today, where capitalist economies depend on economic growth to generate wealth and jobs. Since the economy operates within a planet having finite resources and capacities (Aghbashlo, Rosen 2018), a continually growing economy is not necessarily sustainable over the long term. Eventually, therefore, the global economy needs to operate more in a steady-state manner, with little or zero growth. The best options for economic sustainability may vary from country to country. For instance, Daly (1990) suggests that wealthy countries need to develop instead of grow their economies in order to conserve resources and waste-assimilation capacities, and that poor countries benefit more from economic growth. Sometimes, economic sustainability arguments are divided into strong and weak sustainability categories: Strong sustainability. This view of sustainability is based on natural capital (the planet's stock of natural resources, including air, water, geology, soils and all living organisms) providing ecosystem services that are not substitutable with human capital (the stock of knowledge, habits, social and personality attributes, creativity, and labor). Environmentalists and natural scientists often prefer this viewpoint of sustainability, seeing natural and human capital as complementary, but not interchangeable. Weak sustainability. This view of sustainability is based on the concept of a constant total capital stock, which is the sum of human and natural capital, and allows for human capital to substitute for natural capital. Economists often favor this view of sustainability, which permits natural resources to decline provided there is an increase in human capital, including know-how and knowledge. The difference between these two views of economic sustainability mainly focuses on the substitutability of human for natural capital. What constitutes societal sustainability may change temporally. For instance, people in the future may inhabit smaller houses, live in higher density neighborhoods, possess less material goods and travel less. Even if this leads to a lower gross domestic product per capita in the future, it may lead to a higher quality of life. Two major factors in societal sustainability follow: Equity. The concept of societal sustainability includes equity within and between generations. Intragenerational equity speaks to equity between people of the same generation, and requires more balanced distribution of wealth. This may necessitate shifts in resources between rich and poor nations and within them. Intergenerational equity focuses on equity between present and future generations, so as to ensure that future generations have are able to a attain a reasonably good quality of life compared to preceding generations. Intergenerational equity spanning two to four generations into the future, roughly equivalent to 50 to 100 years, is often envisioned as a necessary factor for societal sustainability. Health. Human health and well-being is an important factor social sustainability. Important measures of human health include life expectancy and infant mortality (Smil 2007). Many factors contribute to human health, including access to healthy and clean food and drinking water, safe waste disposal, and an environment without harmful substances that can lead to chronic or acute diseases (e.g. toxins and carcinogens). SUSTAINABLE DEVELOPMENT To be developed: people (e.g., life expectancy, quality of life, education, equity), society (e.g., security, institutions) and the economy (e.g. wealth creation, jobs). Social and economic systems of people can be enhanced through development. For instance, the quality of life of people in developing countries can be improved through more equitable distribution of wealth, better education, and less reliance on non-renewable resources, leading to a more equitable society; b) To be developed: people (e.g., life expectancy, quality of life, education, equity), society (e.g., security, institutions) and the economy (e.g. wealth creation, jobs). Social and economic systems of people can be enhanced through development. For instance, the quality of life of people in developing countries can be improved through more equitable distribution of wealth, better education, and less reliance on non-renewable resources, leading to a more equitable society; c) To be sustained: life support systems (e.g. resources, land, soil, water, air), nature (e.g. ecosystems, habitats, biodiversity) and communities (e.g. cultures). Earth and its ecosystems provide ecosystem services (e.g. climate stability, water and air purification) and natural resources. These permit human communities to endure into the future and thrive. Thus, to sustain the services provided by nature that people have adapted to and depend upon, the Earth and its life-support systems need to be sustained. Ensuring that sustainable development is incorporated pervasively and comprehensively into industry, government, planners and people's lives is of great importance. Policies and strategies for sustainable development are needed that are sound, coordinated, meaningful realistic and achievable.  Tab 1. Descriptive Outline of Sustainable Development Goals adopted in 2015 by the United Nations Goal number Description 1 End poverty everywhere, in all its forms 2 End hunger, achieve food security and improved nutrition, promote sustainable agriculture 3 Ensure healthy lives, promote well-being for all at all ages 4 Ensure inclusive and equitable quality education, promote lifelong learning for all 5 Achieve gender equality, empower all women and girls 6 Ensure availability and sustainable management of water and sanitation for all 7 Ensure access to affordable, reliable, sustainable and modern energy for all 8 Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all 9 Build resilient infrastructure, promote inclusive and sustainable industrialization, foster innovation 10 Reduce inequality within and among countries 11 Make cities and human settlements inclusive, safe, resilient and sustainable 12 Ensure sustainable consumption and production patterns 13 14 Take urgent action to combat climate change and its impacts Conserve and sustainably use the oceans, seas and marine resources for sustainable development 15 Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, halt and reverse land degradation, halt biodiversity loss 16 Promote peaceful and inclusive societies for sustainable development, provide access to justice for all, build effective, accountable and inclusive institutions at all levels 17 Strengthen the means of implementation and revitalize the global partnership for sustainable development Note that the term sustainable development is similar to but different from sustainability, even though they are often used interchangeably. Sustainability is a state that can be maintained into the future. But, since development means \"bringing gradually to an improved state\", sustainable development implies a course of action that improves the quality of life of people and societies, and that can endure into the future.",
          "The assessment of sustainability is important for making sustainability operational and measuring and monitoring progress towards sustainability. But assessing sustainability is challenging, since no universally accepted method exists for the task. There are many reasons for this, including the difficulty in measuring the main dimensions of sustainability (Hacatoglu et al. 2016). For example, although emissions of GHGs and ozone depleting substances are measurable, quantifying their economic and social impacts is challenging. Also, despite standard of living often being measured as gross domestic product per capita, quality of life can be a more significant measure of human well-being and satisfaction. Sustainability assessment can also be contentious (Morse, Fraser 2005). Nonetheless, various methods for assessing or measuring sustainability have been developed. Some examples: a) Some assessment methods utilize sustainability indicators, which are typically simple quantitative proxies that measure economic, social, and environmental factors. Some indicators are integrated, combining various dimensions of the environment, the economy and society, while others are not integrated, measuring only a single aspect of sustainability; b) Some sustainability indexes have been developed based on an aggregate or composite of selected sustainability indicators. A single-value measure of sustainability based on an aggregate index is beneficial for understanding and communication, due to its simplicity. But the determination of such indicators necessitates normalization, weighting and aggregation of data. These steps usually lead to a loss of useful information and can be challenging in their own right. A single-value sustainability measure can mask details associated with the multidimensional nature of sustainability and thus be misleading. Nonetheless, aggregate sustainability indexes are often preferred by policy makers in industry and government since they can be more easily interpreted and communicated to stakeholders or the public than multiple sustainability values; c) Daly (1990) developed operational principles for sustainable development. Although useful, these are limited to quasi-sustainable use of non-renewable resources and the use of renewable resources. Few if any sustainability assessment methods consider the environment, the economy and society together. Rather, many assessment methodologies focus on only one dimension of sustainability (e.g. economic development or environmental sustainability). Several examples are considered to illustrate this. First, biophysical assessment approaches are suited to assessing environmental sustainability, by quantifying resource use and environmental impact. But they are normally inadequate for addressing societal and economic facets of sustainability. Second, monetary values can be applied to social and environmental capital to assess sustainability. However, financial valuations for non-market goods and services are not well developed, improper valuations, and difficult due to our limited understanding of ecosystems and the services they provide. Third, the Environmental Sustainability Index ranks countries based on an aggregate of several environmental indicators. But these are usually unable to correlate environmental sustainability and economic growth. A weakness of many existing methods of assessing sustainability is their lack of a systems approach, which regards the system being considered as a whole and accounts for the interactions among its subsystems. This is important because achieving a more sustainable society is a systems problem, where the environment, the economy and society are all interdependent. Coupled humanenvironmental systems have interactions among different systems that lead to trade-offs, e.g. reducing costs may cause a process to have higher emissions or lower efficiency. A non-systems approach focusing on single factors can usually be seen to be inadequate for assessing sustainability holistically. As noted earlier, for example, biophysical approaches focus mainly on environmental sustainability and neglect economic and social dimensions, while methods based on weak sustainability focus on economic factors and neglect the biophysical facets of sustainability. Clearly, the sustainability of a system needs to be assessed with a systems approach. Life cycle analysis is usually part of such an approach, as it identifies material and energy inputs and outputs of a system or process and uses this information to assess environmental, economic and social effects. Although there are environmental concerns regarding the project, it nonetheless is viewed as providing significant environmental benefits. The Red-Mediterranean-Dead Seas Canal project has been shown to be the type of large project that may contribute significantly to sustainability. The benefits, which in many ways drive its contribution to numerous facets of sustainability, are shown in fig. 8. Surveys also indicate examples of common applications of sustainability, and these typically show that there is a strong focus on the implementation of sustainability concepts, actions, by both individuals and corporations. For example, the present author recently examined the actions of engineering practitioners regarding sustainability (Rosen 2013). Some specific findings were that the main sustainable technology priorities are using less energy and natural resources, reducing emissions and material wastes, and utilizing renewable, recyclable and recycled materials, and that challenges to sustainability exist, like economics viability. A breakdown of the degree of involvement of engineers with   sustainability or sustainable technologies showed that more than two-thirds of engineers are involved to some degree in sustainability, usually by working on sustainable products and processes. Also, the factors most likely to influence an organization's use of green design practices and procedures are regulatory requirements, client demand and rising energy costs (fig. 9).",
          "Although sometimes vague and often complex, sustainability is an important aim for societies and humanity. Sustainability is a multidimensional concept, encompassing economic, social, environmental and other factors. Sustainability assessment tools are needed to evaluate processes and systems, and how changes affect their sustainability. This usually necessitates a comprehensive set of indicators of the sustainability of the environment, the economy, technology, society, and institutional systems. Many human civilizations in the past proved to be unsustainable to some degree. It is hoped that the coverage provided in this chapter, including sustainability definitions and historical contexts, its economic, environmental and social dimensions, sustainable development, sustainability assessment measures and relevant applications, will assist efforts to address sustainability challenges today and, as an even more important priority, in the future. "
        ],
        "ground_truth_definitions": {
          "Sustainable development": {
            "definition": "development that has the ability to be sustained into the future for some lengthy period of time, and can be viewed as the sustained and continuous improvement of a system or process.",
            "context": "Sustainable development can be thought of as development that has the ability to be sustained into the future for some lengthy period of time, and can be viewed as the sustained and continuous improvement of a system or process (Kates et al. 2005; Rosen 2017a). Development implies a qualitative improvement and dif- fers from simple growth, which is a quantitative increase scale.",
            "type": "explicit"
          },
          "Sustainability": {
            "definition": "“the possibility that human and other forms of life will flourish on the planet forever”.",
            "context": "A simple definition sustainability was stated by Ehrenfeld, who defined sustainability as “the possibility that human and oth- er forms of life will flourish on the planet forever”. Although this definition incorporates environmental and societal aspects, the timescale is impractical.",
            "type": "implicit"
          }
        }
      },
      {
        "paper_id": "https://aclanthology.org/2024.lrec-main.952",
        "sections": [
          "Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022). Existing research on media bias detection primarily involves training classifiers on small in-domain datasets (Krieger et al., 2022), which exhibit limited generalizability across diverse domains (Wessel et al., 2023). This paper builds upon the work of Wessel et al. (2023), emphasizing that the multifaceted nature of media bias detection requires a shift from isolated approaches to multi-task methodologies, considering a broad spectrum of bias types and datasets. The recent advancements in Multi-Task Learning (MTL) (Aribandi et al., 2021a;Chen et al., 2021;Kirstein et al., 2022) open up promising opportunities to overcome these challenges by enabling knowledge transfer across domains and tasks. Despite the potential, a comprehensive MTL approach for media bias detection is yet to be realized. The only other media bias MTL method (Spinde et al., 2022) underperforms due to its narrow task focus and does not surpass baseline outcomes. In this study, we make five main contributions: ). This enables it to outperform models based on single-task learning (STL) paradigms. 1. We present MAGPIE -the first large-scale multi-task pre-training approach for media bias detection. By pre-training on diverse bias types such as persuasive and subjective, a classifier based on MAGPIE correctly classifies sentences that state-of-the-art single-task models misidentify (we show an example in Figure 1). 2. We introduce LBM (Large Bias Mixture), a pretraining composition of 59 bias-related tasks encompassing wide range of biases such as linguistic bias, gender bias and group bias. 3. We provide an analysis of a task selection and demonstrate the effectiveness of scaling the number of tasks. 4. We demonstrate that MAGPIE outperforms the previous state-of-the-art model by 3.3% on the Media Bias Annotation by Experts (BABE) dataset (Spinde et al., 2021c) and achieves competetive results on the Media Bias Identification Benchmark (MBIB) collection (Wessel et al., 2023). 5. We make all resources, including datasets, training framework, documentation, and models, publicly available on GitHub: github.com/magpie-multi-task These contributions highlight the potential of MTL in improving media bias detection. Our findings show, e.g., that tasks like sentiment and emotionality enhance overall learning, all tasks boost fake news detection, and scaling tasks leads to optimal results. Another key insight of our research is the value of MTL in contexts where the primary dataset is smallfoot_0 . By learning generalized bias knowledge from a range of tasks, we can improve the accuracy and efficiency of existing models, even in the face of limited data. Overall, our research offers a multitask learning approach to media bias detection with first large-scale resources in the domain.",
          "",
          "Media bias is a complex issue (Lee et al., 2021a;Recasens et al., 2013;Raza et al., 2022) composed of varying definitions of bias subtypes such as linguistic bias, context bias, or group bias (Wessel et al., 2023). In their literature review, Spinde et al. (2023) provide an extensive overview of research on media bias and related subtypes of bias. Media bias detection approaches have evolved from hand-crafted features (Recasens et al., 2013;Hube and Fetahu, 2018;Spinde et al., 2021d) to neural models (Spinde et al., 2022;Chen et al., 2021;Spinde et al., 2021c;Huguet Cabot et al., 2021;Sinha and Dasgupta, 2021;Raza et al., 2022). However, existing models, so far, focus only on single tasks and saturate quickly on smaller datasets (Wessel et al., 2023). As most neural approaches require large quantities of data, those relying on single and small datasets cannot provide a realistic scenario for their solutions (e.g., Fan et al. (2019)). We will first provide an overview of existing datasets and then show how to exploit their diversity within the media bias domain. Media bias tasks and datasets mainly cover individual, self-contained tasks such as binary classifications (Recasens et al., 2013;Spinde et al., 2021b), which, so far, are not explored in relation to each other (Spinde et al., 2023). Wessel et al. (2023) systematically form the media bias detection benchmark MBIB by reviewing over 100 media bias datasets and consolidating 22 of them into eight distinct tasks like linguistic, racial, and political bias. Their study highlights that methods only focused on one of these tasks exhibit limitations in their detection capabilities. MAGPIE encompasses all the tasks identified in the MBIB but also significantly expands its scope by incorporating an additional 51 media bias-related tasks to mitigate a variety of limitations in MBIB (see Section 3.1).",
          "MTL shows significant improvements in various NLP tasks, including sentiment analysis (He et al., 2019), text summarization (Kirstein et al., 2022), and natural language understanding (Raffel et al., 2020). In MTL, a model leverages knowledge gained from one task to improve the performance of others. Aribandi et al. (2021b) demonstrate that increasing the number of tasks generally leads to improved performance for downstream NLP applications. Aghajanyan et al. (2021) show that prefinetuning, a large-scale multi-task learning phase, consistently improves the performance and efficiency of pre-trained models across diverse tasks, with results improving linearly with the number of tasks beyond a certain threshold. As described above, media bias can be seen as a composite problem composed of various interrelated bias types (Wessel et al., 2023). In the realm of Natural Language Understanding (NLU), MTL has proven to be highly effective when incorporating related tasks Aribandi et al. (2021b). For instance, benchmarks such as GLUE and Super-GLUE successfully decompose the NLU problem into a suite of proxy tasks, including paraphrase detection (Wahle et al., 2023(Wahle et al., , 2022)), and semantic evaluation (Ruas et al., 2019(Ruas et al., , 2020)), thereby substantially improving performance across a range of NLU tasks (Wang et al., 2018(Wang et al., , 2019)). Motivated by this success in NLU, we propose to jointly learn from different bias types within the media bias domain. With this approach, we aim to treat media bias not as a singular entity but as many interconnected issues. The selection of tasks is pivotal to the efficacy of MTL. There have been several attempts to automate task selection, including learning the data selection with Bayesian optimization (Bingel and Søgaard, 2017) or estimating task relations (Ruder and Plank, 2017). The most model-agnostic approach is GradTS (Ma et al., 2021), which is highly scalable due to low resource requirements, and is therefore implemented within MAGPIE. GradTS accumulates gradients of attention heads and selects tasks based on their correlation with the primary task's attention. The selected tasks are trained jointly and share representations across tasks.",
          "We implement MAGPIE using pre-finetuning as introduced in (Aghajanyan et al., 2021) (See also Section 2). As such, MAGPIE is an encoder-only MTL transformer model pre-finetuned on 59 media bias-related auxiliary tasks provided by Large Bias Mixture (LBM), a large-scale task collection of bias-related datasets. We incorporate a novel approach of a Head-Specific Early Stopping and Resurrection to effectively handle tasks of varying sizes (Section 3.3.1). As outlined in Figure 2, our first step involves constructing the LBM. Following this, we define the model and multi-task learning (MTL) framework employed to train MAGPIE, which includes optimization strategies, task sampling, and auxiliary task selection. Lastly, we evaluate MAGPIE on two primary resources: the Media Bias Annotation by Experts (BABE) dataset 2 (Spinde et al., 2021c), and 2 BABE provides high-quality labels that capture a broad range of linguistic biases, thus allowing us to evaluate our model's generalizability within a single dataset the Media Bias Identification Benchmark (MBIB) collection.",
          "Currently, MBIB is the only collection of media bias tasks. However, it does not include tasks that constitute a form of media bias more indirectly, such as sentiment analysis or emotion classification. MAG-PIE aims to integrate tasks both directly linked to media bias and those peripherally related, such as sentiment analysis, to provide broader coverage of linguistic features in the media bias context. Therefore, we introduce Large Bias Mixture (LBM), a more general collection of relevant media bias and media bias-related tasks, more suitable for our MTL approach. We show our task selection process in Figure 3. First, we manually assess a list of 115 media bias-related datasets in English language, categorized into task families by Wessel et al. (2023). A task family is a conceptual grouping of tasks that share similar objectives, such as those related to gender bias, encompassing pronoun coreference resolution, gender classification, and sexism detection. We use this notion of task families to analyze general knowledge transfer between media bias tasks in Section 4.3, such as Aribandi et al. (2021b) proposed for general NLP tasks. We filter the collection of the 115 datasets based on the following criteriafoot_2 : • Accessibility: Datasets have to be publicly accessible. • Text granularity: We only use datasets labeled on a sentence level or its fragments (tokens)(not on, for instance, article level) • Quality of annotations: We exclude datasets with no documentation, low annotation agreement or employment of machine annotation. • Duplicates: We filter out datasets that contain full or partial duplicates of each other Of the 115 datasets collected, we discard 11 datasets that are not publicly available. We discard 52 with article-level annotations and 5 with annotations on other levelsfoot_3 . We remove 5 datasets due to or unreliable source of annotations and discard 4 duplicates. Applying these criteria leaves context. originally listed gives us 46 datasets. These are categorized into task families ensuring no overlap and more than two datasets per family. Finally, ten datasets with multi-level annotations, e.g., token and sentence level, are split into tasks, yielding final number of 59 tasks. In order to standardize examples from various domains, we apply a unified text-cleaning procedure to each dataset. The process involves: (1) discarding sentences with fewer than 20 characters, (2) removing URL links, special characters, and whitespace, and (3) eliminating duplicates potentially created with steps (1) and (2). The final Large Bias Mixture (LBM) includes 59 tasks, categorized into 9 distinct task families, encompassing 1,210,084 labeled sentences. We make the LBM publicly accessible, to facilitate research in media bias detection and other computational-social-science tasks. References and short descriptions of all datasets and corresponding tasks and task families can be found in Table 4.",
          "In terms of the base language model for our procedure, we adopt a pre-trained RoBERTa (Liu et al., 2019) encoder due to its proven state-of-the-art performances across various media bias applications (Spinde et al., 2021c;Krieger et al., 2022).",
          "Pre-finetuning. To effectively harness the generalization abilities of MTL for media bias detec-tion, we adopt a pre-finetuning procedure (Aghajanyan et al., 2021). Pre-finetuning is a complementary approach to pre-training, where a model, already pre-trained on a typically unsupervised task, is subjected to another intermediate step of pre-training. While incorporating MTL directly into the pre-training stage has demonstrated performance gains (Aribandi et al., 2021b), we opt for pre-finetuning as it offers significantly reduced computational demands while still capitalizing on the benefits of MTL (Aghajanyan et al., 2021). Sharing representations. We use hard parameter sharing to share the underlying encoder among all tasks while using separate task-specific heads. For each task, we attach dense layers, or \"heads\", to the shared encoder. These heads are optimized individually per task while the shared encoder learns general bias representations. However, multi-task optimization presents challenges due to differing gradient directions and magnitudes (Yu et al., 2020). For instance, two tasks, A and B, may have opposing gradients with the same magnitude, nullifying their sum. On the other hand, if Task A's gradient greatly surpasses that of Task B, gradient A becomes dominant. We counter the gradient misalignment by using a variation of the PCGrad de-confliction algorithm and loss scaling (Yu et al., 2020). Conflicting gradients and loss scaling. In multi-task training involving n tasks, encoder parameters receive n potentially conflicting gradients. Efficient handling of this conflict, such as PCGrad (Yu et al., 2020), requires storing a set of gradients for each task involved in the update, leading to infeasible memory requirements among our 59 LBM tasks. Therefore, we propose a variation of PCGrad we call PCGrad-online which preserves the fundamental idea of the original algorithm but is more memory efficient, requiring only one set of gradients instead of n sets per update. Adopting Muppet's method, we solve the issue of varying task gradient magnitudes by re-scaling the task loss with the inverse log size of its output space, ensuring balanced gradients and preventing task dominance in training steps (Aghajanyan et al., 2021).",
          "To prevent large tasks from dominating the optimization, we ensure uniform data distribution by sampling one fixed-size sub-batch from each task per training step, a regular approach in MTL (Aribandi et al., 2021b;Spinde et al., 2022). We also employ early stopping as a regularization for each task individually to prevent over-fitting of tasks that converge faster. However, these methods often fall short when confronted with tasks of varied complexity and differing convergence speeds, which both is the case for tasks in LBM. When task A stops early while task B takes longer to converge, the latent representation of the shared encoder shifts toward task B. We aim to mitigate this issue by employing a training strategy that tackles the latent representation shift using two complementary approaches: 1. Head-Specific-Early-Stopping (HSES) 2. Resurrection HSES. When the task stops, we stop updating its specific head parameters while still backpropagating its language model gradients. This method stems from the observation that not all tasks benefit from the shared layers' continuous learning, especially after they have reached an optimal state. Resurrection. When the task stops, we allow it to resume the training after its validation loss starts to increase again. This enables the task to adapt its head parameters to the latest latent representation. HSES maintains quality of faster-converging tasks, while Resurrection allows further adaptation when needed. Their combination aims for balanced, adaptive learning for tasks with varied complexities and convergence speeds. We perform a preliminary evaluation of the effectiveness of this method in the in Section 4.4. However, we stress the need for further extensive analysis in Section 5.",
          "Multi-Task Learning often involves selecting wellused datasets, leading to potential selection biases. Furthermore, manually handpicking datasets becomes challenging due to varying and potentially ambiguous bias annotation schemes. To automate the process of task selection, we utilize the GradTS algorithm (Ma et al., 2021). We choose GradTS due to its demonstrated efficiency and its simplicity of implementation, which enhances its usability. Gradient-based Task selection. In line with GradTS, we construct auxiliary tasks as follows. We individually train all tasks, accumulate absolute gradients, then extract and layer-wise normalize these in the attention heads, forming a 12x12 importance matrixfoot_4 for each task. Tasks are sorted by correlation between each task's matrix and BABE task's matrix. We pre-finetune m -1 models on the first k tasks from the sorted list, where k varies from 1 to m -1 and m is the size of LBM. The BABE task is then evaluated on these pre-finetuned models, with the optimal k determined by evaluation loss. For further details on the GradTS algorithm, please see Ma et al. (2021).",
          "We split the BABE dataset into train, dev and test split with 70%,15%,15% portions respectively. For both hyperparameter tuning and auxiliary task selection, we evaluate on dev split and only use test split for the final evaluation (Section 4.2). As fine-tuning transformers on small datasets often leads to inconsistent results, such as high performance variance (Dodge et al., 2020), we use a fixed random seed 321 for all runs in auxilliary task selection. For the final evaluation on the test set (Table 2) we evaluate the models using 30 random seeds and report the average performance, to minimize the influence of random weight initializations. We use values 0..29. For optimizing the models, we use a per-task batch size of 32, an AdamW optimizer, and a polynomial learning scheduler. We run all experiments on 1 NVIDIA TESLA A100 GPU with 40 GB of memory.",
          "In this section, we present the results of our MTL approach. First, we report the set of auxiliary tasks selected by GradTS for pre-finetuning. Next, we assess how the model pre-finetuned on the GradTS set performs during subsequent finetuning on the BABE task, compared to a random choice of tasks and a full set of LBM tasks. We also compare the MTL approach to a single-task baseline and multiple MTL baselines and evaluate the performance of our best model, MAGPIE, on the MBIB benchmark. Then, we analyze the LBM taxonomy through a study on knowledge transfer between families. Lastly, we evaluate the effects of the proposed methods, HSES and Resurrection, through a preliminary study.",
          "We select suitable auxiliary tasks by calculating the correlation between attention-head importance matrices. We use Kendall's correlation coefficient, as suggested by Puth et al. (2015). We find a local minimum for the BABE evaluation loss when pre-finetuning on the first k = 10 most correlated tasks. The final set of the ten most correlated tasks referred to as gradts set, is displayed in Table 1. The tasks in the gradts set demonstrate a strong semantic connection to media bias, encompassing areas such as lexical bias, rumor detection, and fake news detection. Task Type τ (correlation) Persuasive techniques 0.73 Lexical/Informational bias 0.72 Rumour detection 0.69 Sentiment analysis 0.68 Global warming stance detection 0.68 Subjective bias 0.67 Veracity classification 0.64 Gender bias 0.64 Fake news detection 0.63 Table 1: Attention importance correlation w.r.t. the media bias task.",
          "First, we finetune a model pre-finetuned on three different multi-task sets on the BABE task and compare it against the single-task RoBERTa baseline. The multi-task sets are the following: • MTL:Random -Random Subset of 10 tasks • MTL:GradTS -Subset of 10 tasks selected by GradTS algorithm • MTL:All -Set of all tasks We also evaluate the model pre-finetuned on the set of all tasks on MBIB. We follow the guidelines set by Wessel et al. (2023) for the evaluation. Given that MAGPIE's pre-training data includes portions of the MBIB data, we ensure that the test set for each task in MBIB is not exposed to the model during its training or validation phases. Multi-Task Learning Performance. Table 2 summarizes our performance results on the BABE dataset. We observe that all of our MTL prefinetuning schemas lead to performance improvements. In particular, pre-finetuning on all tasks from LBM yields a SOTA performance on the BABE dataset, achieving an 84.1% F1 score and a relative improvement of 3.3% compared to the previous baseline by Spinde et al. (2021c). While both MTL baselines -Muppet (Aghajanyan et al., 2021) and UnifiedM2 (Lee et al., 2021b) outperform singletask baseline, they underperform all of our MTL models. On MBIB benchmark, MAGPIE ranks first on 5 out of 8 tasks. However, the improvements are only marginal. The results can be found in Appendix in Table 5.  Step efficiency. In addition to the performance improvements achieved, we also assess our model training efficiency. In Figure 5, we show the F1 score on the development set for the BABE task, averaged over all 30 runs. Our findings show that Multi-Task Learning only requires 15% of the training steps used in single-task finetuning on BABE.",
          "F1 Acc loss Baseline (RoBERTa base) 80.83 (±0.69) 81.19 (±0.69) 43.6 (±3.54) DA-RoBERTa 77.83 (±1.4) 78.56 (±1.3) 47.84 (±2.97) MUPPET 80.56 (±1.3) 81.18 (±1.16) 44.19 (±4.65) UnifiedM2 81.91 (±0.91) 82.41 (±0.88) 44.86 (±3.99) MTL:Random 81.88 (±1.02) 82.28 (±0.97) 40.35 (±1.73) MTL:GradTS 82.32 (±0.79) 82.64 (±0.8) 40.96 (±2.36) MTL:All 84.1 (±1.33) 84.44 (±1.25) 39.46 (±2.41) Table 2: Performance of two MTL baseline models (Muppet, UnifiedM2) two single-task baslines (RoBERTa and DA-RoBERTa) and our three MTL models, on fine-tuning on BABE dataset and evaluating on the held-out test set. The results are averaged over 30 random seeds. This result demonstrates the high training-step efficiency of MAGPIE in media bias classification, making MTL implementations in the media bias domain more viable in the future.",
          "In Section 3.1, following Aribandi et al. (2021b), we introduce data task families. Aribandi et al. (2021b) uses task families for selection and knowledge transfer. To assess task families' significance in LBM taxonomy, we train each pair of families together, investigating knowledge transfer. To account for potential negative transfer within families, we first calculate the average transfer within each family and use it as a baseline for measuring transfer between families. We train tasks from the same family together and report the average change in task performance, as depicted in Figure 6. Negative knowledge transfer is prevalent across most of our task families. However, we observe two exceptions: the hate-speech and stance detection families, where multi-task training leads to an average improvement in performance. Next, we measure knowledge transfer between families by training each pair together. We report the average impact of each family on others and the average benefit each family gets through training with others, summarized in Section 4.3. Our results show that, on average, the Emotionality and Sentiment analysis families provide positive transfer learning to other families. Conversely, we observe that the Fake News family benefits from knowledge transfer from every other family, with an average improvement of 1.7%. On the other hand, the Emotionality family is significantly impaired by negative transfer from other task families. The full table of transfers can be found in Figure 7. Considering that only two families show positive transfer learning, with marginal effects of 0.11% and 0.34%, we conclude that the task families used in the construction of LBM are generally unsuitable for effectively utilizing knowledge transfer. We discuss this again in Section 5. Task Family Transfer from Transfer to media bias -2.07% -0.94% subjective bias -1.26% 0.89% hate speech -0.87% 0.17% gender bias -1.01% -1.07% sentiment analysis 0.11% 0.72% fake news -0.13% 1.79% group bias -1.04% 0.09% emotionality 0.34% -6.56% stance detection -0.79% -1.83% Table 3: Evaluation of averaged transfer between task families.",
          "To evaluate the Resurrection and HSES methods in combination with other training strategies, we run a grid search on the following training strategies: HSES, Resurrection, Loss Scaling and Gradient Aggregation. We calculate the average evalua- tion loss for both Resurrection and HSES methods across 20 tasks randomly selected from the LBM collection. The boxplot in Figure 8 shows that both methods reduce the loss by 5% and decrease the variance across different training setups by 85%. However, we hypothesize that a random constellation of tasks 6 can have a non-trivial effect on the evaluation of our technique; thus, we opt for robust examination of the methods in future work.",
          "This paper contributes to media bias detection by the development of MAGPIE, our large-scale multitask learning (MTL) approach that enhances traditional models with a multi-task structure. Additionally, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. This broad collection serves as a resource for the pretraining of MAGPIE. To the best of our knowledge, it is the first available MTL resource tailored specifically towards media bias. Our study investigates the dynamics of transfer learning across tasks and task families within media bias. Despite the occurrence of negative trans- 6 Particularly variance in task sizes and quality. fer among several tasks, scaling the pre-training setup to all collected tasks in Multi-Task Learning results in a 3.3% improvement over the previous state-of-the-art, making it the biggest advancement in neural media bias classification so far. Furthermore, we report that finetuning MAGPIE on the BABE dataset only requires 15% of steps compared to RoBERTa single-task approaches. These findings underscore the effectiveness and potency of Multi-Task Learning in highly specific classification domains such as media bias. While results suggest benefits in scaling tasks, we see more promise in novel tasks rooted in media bias, suggesting deeper exploration over simply expanding the task spectrum. Understanding families and tasks in datasets necessitates systematic analysis of label definitions, rater agreement, and inter-relatedness of dataset creation strategies. As media bias is emerging globally, incorporating multilingual models is a natural extension.",
          "We acknowledge the necessity for a more comprehensive analysis of the performance of the HSES and Resurrection methods to ensure robust evaluation. Given the significant computational resources required for a single multi-task training of all tasks, we allocated these resources towards robustly evaluating the model performance rather than conducting an in-depth analysis of the optimization techniques. Consequently, the presented methods may be less reliable and have limited applicability. While there are various models available, previous research suggests that RoBERTa exhibits strong performance on media bias tasks (Wessel et al., 2023;Spinde et al., 2021c). However, due to resource constraints, we were unable to explore models with different architectures and further refine our selection. As the landscape of publicly available highquality datasets for media bias is not as extensive as desired, we acknowledge our inability to capture all manifestations of media bias comprehensively. As mentioned in Section 5, a systematic and comprehensive analysis of the dataset landscape will be part of our future work. Furthermore, although analyzing media bias on a sentence level enables a detailed examination of occurring biases, certain forms of bias, such as bias resulting from the omission of information, extend beyond the linguistic aspects of individual statements. Detecting such biases may require considering an external context. We leave it to future work to investigate other levels of granularity, such as paragraph or article levels. In addition to these technical limitations, conducting a detailed analysis of agreement and label distribution in all utilized datasets will be necessary to make stronger claims about which datasets provide more reliable coverage of the overall concept of media bias. This is particularly important since media bias is a complex phenomenon that is not always easily identified during the annotationgathering process.",
          "Detecting (and as a result also highlighting) media bias instances can mitigate the negative effects of media bias, e.g., on collective decisionmaking (Baumer et al., 2015). However, media bias strongly depends on the context and individuals involved, making it a sensitive issue. Some bias forms depend on factors other than the content, e.g., a different text perception due to a reader's background. Therefore, a public classification of possible polarization and one-sidedness in the news must be performed transparently and reliably. The datasets used for training the classifier must be transparent and, ideally, constantly monitored by independent experts. To address possible implications of MAGPIE or related media bias applications cautiously, it is crucial to constantly control the classifications and datasets with recent findings on the perception of media bias, which is a main part of our ongoing and future work. To do so, we use resources such as, e.g., standardized questionnaires on media bias perception (Spinde et al., 2021a). When bias detection is balanced and transparent, it can positively affect collective opinion formation and decision-making. We see no immediate negative ethical or societal impacts of our work. However, in addition to system transparency, we want to highlight that we believe it to be required for future datasets to report in greater detail about a manipulation protection strategy when developing, training, and presenting any media bias classifier. To ensure the validity of media bias detection systems, it is essential to prevent participants, especially in public studies, from manipulating algorithms by, for example, flagging neutral content as biased. Therefore, annotations should be compared among multiple users to ensure a higher level of trustworthiness. Most of the datasets available report only limitedly about such strategies. In open or crowdsourcing scenarios, it is important to collect user characteristics and deliberately include specific content that aims to provide obvious answers but may be answered differently when users follow certain patterns. This approach helps in detecting and mitigating potential biases introduced by participants, thereby maintaining the integrity and reliability of the media bias detection process. To ensure and propose stronger standards in the future, we aim to analyze all LBM datasets with regard to potential inherent bias in future work."
        ],
        "ground_truth_definitions": {
          "media bias": {
            "definition": "a skewed portrayal of information favoring certain group interests, which manifests in multiple facets, including political, gender, racial, and linguistic biases.",
            "context": "1. Introduction Media bias is a skewed portrayal of information favoring certain group interests (Recasens et al., 2013), which manifests in multiple facets, including political, gender, racial, and linguistic biases. Such subtypes of bias, which can intersect and coexist in complex combinations, make the classification of media bias a challenging task (Raza et al., 2022).",
            "type": "explicit"
          }
        }
      }
    ],
    "signature": {
      "instructions": "Determine if the given text contains a definition of a term.\nRules:\n- A definition typically includes a clear explanation of a term's meaning.\n- Look for cue phrases like 'is defined as', 'refers to', or 'means'.\n- If unsure, prefer 'no' to avoid false positives.",
      "fields": [
        {
          "prefix": "Section:",
          "description": "Section of a paper to analyze."
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Is Definition:",
          "description": "Does this section contain an obvious definition?"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.1"
    }
  }
}
